<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." />
    <node id=" Mike Shire in his thesis um , {vocalsound} did a {disfmarker} a series of experiments , um , training LDA filters in d on different conditions . And you were interested in having me repeat this for {disfmarker} for this mean subtraction approach ? Is {disfmarker} is that right ? Or for these long analysis windows , I guess , is the right way to put it .&#10;Speaker: Professor D&#10;Content: I guess , the {disfmarker} the {disfmarker} the issue I was {disfmarker} the general issue I was bringing up was that if you 're {disfmarker} have a moving {disfmarker} {vocalsound} moving window , uh , a wa a {disfmarker} a set of weights times things that , uh , move along , shift along in time , that you have in fact a linear time invariant filter . And you just happened to have picked a particular one by setting all the weights to be equal . And so the issue is what are some other filters that you could use , uh , in that sense of &quot; filter &quot; ?&#10;Speaker: Grad C&#10;Content:" />
    <node id=" the issue is what are some other filters that you could use , uh , in that sense of &quot; filter &quot; ?&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: And , um , as I was saying , I think the simplest thing to do is not to train anything , but just to do some sort of , uh , uh , hamming or Hanning , uh , kind of window , kind of thing ,&#10;Speaker: Grad C&#10;Content: Right . Mm - hmm .&#10;Speaker: Professor D&#10;Content: just sort of to de - emphasize the jarring . So I think that would sort of be the first thing to do . But then , yeah , the LDA i uh , is interesting because it would sort of say well , suppose you actually trained this up to do the best you could by some criterion , what would the filter look like then ?&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: Professor D&#10;Content: Uh , and , um , that 's sort of what we 're doing in this Aur - Aurora stuff . And , uh , it 's still not clear to me in the long run whether the best" />
    <node id=" I can complete that s like this . Well .&#10;Speaker: Professor D&#10;Content: Uh . Right .&#10;Speaker: PhD H&#10;Content: One thing that I {comment} note are not here in this result {vocalsound} but are speak {disfmarker} are spoken before with Sunil I {disfmarker} I improve my result using clean LDA filter .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: Mm - hmm .&#10;Speaker: PhD H&#10;Content: If I use , {vocalsound} eh , the LDA filter that are training with the noisy speech , {vocalsound} that hurts the res my results .&#10;Speaker: Professor D&#10;Content: So what are these numbers here ? Are these with the clean or with the noisy ?&#10;Speaker: PhD H&#10;Content: This is with the clean .&#10;Speaker: Professor D&#10;Content: OK .&#10;Speaker: PhD H&#10;Content: With the noise I have worse result , that if I doesn't use it .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD H&#10;Content:" />
    <node id=" you think it 's worth looking into .&#10;Speaker: Professor D&#10;Content: You could imagine that .&#10;Speaker: Grad C&#10;Content: I mean , it {disfmarker} it is getting a little away from reverberation .&#10;Speaker: Professor D&#10;Content: Um , yeah . It 's just that you 're making a choice {disfmarker} uh , I was thinking more from the system aspect , if you 're making a choice for SmartKom , that {disfmarker} that {disfmarker} that it might be that it 's {disfmarker} it c the optimal number could be different , depending on {disfmarker}&#10;Speaker: Grad C&#10;Content: Yeah . Right .&#10;Speaker: Professor D&#10;Content: Could be . I don't know .&#10;Speaker: Grad C&#10;Content: And {disfmarker} and th the third thing , um , uh , is , um , Barry explained LDA filtering to me yesterday . And so , um , Mike Shire in his thesis um , {vocalsound} did a {disfmarker} a series of experiments , um , training LDA filters" />
    <node id="Content: Yeah .&#10;Speaker: Professor G&#10;Content: And it was surprising {disfmarker} At the beginning it was not surprising to me that you get really the best results on doing it this way , I mean , in comparison to any type of training on clean data and any type of processing . But it was {disfmarker} So , u u it {disfmarker} it seems to be the best what {disfmarker} wh wh what {disfmarker} what we can do in this moment is multi - condition training . And every when we now start introducing some {disfmarker} some noise reduction technique we {disfmarker} we introduce also somehow artificial distortions .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And these artificial distortions {disfmarker} uh , I have the feeling that they are the reason why {disfmarker} why we have the problems in this multi - condition training . That means the H M Ms we trained , they are {disfmarker} they are based on Gaussians ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content:" />
    <node id="isfmarker} they are based on Gaussians ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: and on modeling Gaussians . And if you {disfmarker} Can I move a little bit with this ? Yeah . And if we introduce now this {disfmarker} this u spectral subtraction , or Wiener filtering stuff {disfmarker} So , usually what you have is maybe , um {disfmarker} I 'm {disfmarker} I 'm showing now an envelope um maybe you 'll {disfmarker} f for this time . So usually you have {disfmarker} maybe in clean condition you have something which looks like this . And if it is noisy it is somewhere here . And then you try to subtract it or Wiener filter or whatever . And what you get is you have always these problems , that you have this {disfmarker} these {disfmarker} these {disfmarker} these zeros in there .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And you have to do something if you get these negative values ." />
    <node id="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise." />
    <node id="disfmarker}&#10;Speaker: Professor D&#10;Content: Right at the point where you 've done the subtraction .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: Um , essentially you 're adding a constant into everything .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: But the way Stephane did it , it is exactly the way I have implemented in the phone , so .&#10;Speaker: Professor D&#10;Content: Oh , yeah , better do it different , then . Yeah .&#10;Speaker: PhD E&#10;Content: Um .&#10;Speaker: Professor D&#10;Content: Just you {disfmarker} you just ta you just set it for a particular signal - to - noise ratio that you want ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Yeah I {disfmarker} I made s similar investigations like Stephane did here , just uh , adding this constant and {disfmarker} and looking how dependent is it on the value of the constant&#10;Speaker: Professor D&#10;Content: Yeah . Yeah .&#10;Speaker:" />
    <node id=" Professor G&#10;Content: And , uh , I {disfmarker} I found the same problem . Just taking um , what we were used to u {vocalsound} use , I mean , uh , some type of spectral subtraction , {comment} y {vocalsound} you get even worse results than {vocalsound} the basis&#10;Speaker: PhD B&#10;Content: Yeah . Yeah ,&#10;Speaker: Professor G&#10;Content: and uh {disfmarker}&#10;Speaker: PhD B&#10;Content: yeah .&#10;Speaker: Professor G&#10;Content: I {disfmarker} I tried to find an explanation for it ,&#10;Speaker: Professor D&#10;Content: Mmm .&#10;Speaker: Professor G&#10;Content: so {disfmarker}&#10;Speaker: PhD B&#10;Content: So . Yes . Stephane also has the same experience of using the spectral subtraction right ?&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Yeah . So here {disfmarker} here I mean , I found that it 's {disfmarker" />
    <node id=" D&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Th - That 's true . Yeah {disfmarker} the c the models are not complex enough to absorb that additional variability that you 're introducing .&#10;Speaker: Professor G&#10;Content: s&#10;Speaker: PhD F&#10;Content: Thanks Adam .&#10;Speaker: Professor G&#10;Content: Yeah . Yes .&#10;Speaker: PhD B&#10;Content: Well , that 's {disfmarker} Yeah . So {disfmarker}&#10;Speaker: PhD E&#10;Content: I also have the feeling that um , the reason ye why it doesn't work is {disfmarker} yeah , that the models are much {disfmarker} are t um , not complex enough . Because I {disfmarker} actually I als always had a good experience with spectral subtraction , just a straight spectral subtraction algorithm when I was using neural networks , big neural networks , which maybe are more able to model strange distributions and {disfmarker}&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: But {disfmarker}" />
    <node id="1. The process of measuring the average energy involves obtaining the average speech energy from a dataset of one language, such as Italian. This value is then used as the mean speech energy for all other languages. (PhD E's contribution)&#10;2. There was confusion regarding whether the amount to be subtracted from the signal energy depends on the utterance or not. PhD E later clarified that it does not. (Professor D and PhD E's contributions)&#10;3. Some systems add random noise to the log energy with a certain mean and variance, which is language-specific. This addition of random noise is done only after cleaning up the signal. (PhD B's contribution)&#10;4. The speakers also mention that Finnish has a lower average energy compared to other databases. Therefore, the threshold for Finnish should be lower accordingly. (Professor G's contribution)&#10;5. In real-world applications, measuring the average energy over an extended period, like half an hour or an hour, is not feasible. Instead, a fixed number derived from other methods should be used. (PhD D and PhD E's contributions)&#10;6. The process of measuring the average energy is not language-dependent; instead, it involves using the mean speech energy obtained from one language for all languages. However, there might be language-specific adjustments to noise addition or thresholds based on the average energy levels in each language." />
    <node id="isfmarker} ?&#10;Speaker: PhD E&#10;Content: It 's not . It 's just something that 's fixed .&#10;Speaker: Professor G&#10;Content: No . It 's overall .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Um {disfmarker}&#10;Speaker: Professor D&#10;Content: But what he is doing language dependent is measuring what that number i reference is that he comes down twenty - five down from .&#10;Speaker: PhD E&#10;Content: Yeah , so I g No . It {disfmarker} No .&#10;Speaker: Professor D&#10;Content: No ?&#10;Speaker: PhD E&#10;Content: Because I did it {disfmarker} I started working on Italian . I obtained this average energy&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: and then I used this one .&#10;Speaker: PhD B&#10;Content: For all the languages . OK .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: So it 's" />
    <node id="} right now this {disfmarker} this is c a constant that does not depend on {disfmarker} {comment} on anything that you can learn from the utterance . It 's just a constant noise addition . Um . And I {disfmarker} I think w w&#10;Speaker: Professor D&#10;Content: I {disfmarker} I 'm sorry . Then {disfmarker} then I 'm confused .&#10;Speaker: PhD E&#10;Content: I think {disfmarker}&#10;Speaker: Professor D&#10;Content: I thought {disfmarker} you 're saying it doesn't depend on the utterance but I thought you were adding an amount that was twenty - five DB down from the signal energy .&#10;Speaker: PhD E&#10;Content: Yeah , so the way I did that , {comment} i I just measured the average speech energy of the {disfmarker} all the Italian data .&#10;Speaker: Professor D&#10;Content: Oh !&#10;Speaker: PhD E&#10;Content: And then {disfmarker} I {disfmarker} I have {disfmarker} I used this as mean speech energy ." />
    <node id=": Professor D&#10;Content: Oh , they do !&#10;Speaker: PhD B&#10;Content: Yep .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: C - z C - zero and log energy also , yeah .&#10;Speaker: PhD E&#10;Content: Yeah . Um , But I don't know how much effect it {disfmarker} this have , but they do that .&#10;Speaker: PhD B&#10;Content: Now ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Oh .&#10;Speaker: Professor G&#10;Content: Uh - huh .&#10;Speaker: Professor D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: So it {disfmarker} it {disfmarker} it {disfmarker} it {disfmarker} it is l somehow similar to what {disfmarker}&#10;Speaker: PhD E&#10;Content: I think because they have th log energy , yeah , and then just generate random number . They have some kind of mean and variance , and they add this number to {disfmarker} to the log energy simply . Um {d" />
    <node id=" number . They have some kind of mean and variance , and they add this number to {disfmarker} to the log energy simply . Um {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah {disfmarker} the {disfmarker} the log energy , the {disfmarker} after the clean {disfmarker} cleaning up .&#10;Speaker: Professor D&#10;Content: To the l&#10;Speaker: PhD B&#10;Content: So they add a random {disfmarker} random noise to it .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: To the {disfmarker} just the energy , or to the mel {disfmarker} uh , to the mel filter ?&#10;Speaker: PhD B&#10;Content: No . On - only to the log energy .&#10;Speaker: PhD E&#10;Content: Only {disfmarker} Yeah .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: Professor G&#10;Content: Uh - huh .&#10;Speaker: Professor D&#10;Content: So it {disfmarker} Cuz I mean , I" />
    <node id=" . It 's {disfmarker} it 's a {disfmarker}&#10;Speaker: PhD B&#10;Content: So that 's {disfmarker}&#10;Speaker: Professor D&#10;Content: Which means decrease in word error rate ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: OK , so &quot; percentage increase &quot; means decrease ?&#10;Speaker: PhD B&#10;Content: Yeah , yeah .&#10;Speaker: Professor D&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: Yeah . The {disfmarker} the w there was a very long discussion about this on {disfmarker} on the {disfmarker} on the , uh , Amsterdam meeting .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: How to {disfmarker} how to calculate it then .&#10;Speaker: PhD B&#10;Content: Yeah . There 's {disfmarker} there 's a {disfmarker}&#10;Speaker: Professor G&#10;Content: I {disfmarker} I {disfmarker} I guess you are using finally this {" />
    <node id=" thirty to twenty - five . And {disfmarker} I have the feeling that maybe it 's because just Finnish has a mean energy that 's lower than {disfmarker} than the other databases . And due to this the thresholds should be {disfmarker}&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: the {disfmarker} the a the noise addition should be lower&#10;Speaker: Professor D&#10;Content: But in {disfmarker} I mean , in the real thing you 're not gonna be able to measure what people are doing over half an hour or an hour , or anything , right ?&#10;Speaker: PhD E&#10;Content: and {disfmarker}&#10;Speaker: Professor D&#10;Content: So you have to come up with this number from something else .&#10;Speaker: PhD E&#10;Content: Yeah . So {disfmarker}&#10;Speaker: Professor G&#10;Content: Uh , but you are not doing it now language dependent ? Or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: It 's not . It 's just something that 's fixed .&#10;Speaker" />
    <node id="1. To determine the constant noise addition, PhD E measured the average speech energy from a dataset of Italian language. This value was then used as the mean speech energy for all other languages. The constant noise addition was calculated as a fixed amount (e.g., 25 dB or 30 dB) below the average speech energy.&#10;2. The specific method used to calculate this constant may not be explicitly stated in the transcript, but it is mentioned that PhD E measured the average speech energy of all Italian data and then subtracted a certain amount from the signal energy to create a signal-to-noise ratio." />
    <node id=" , it 's like {disfmarker} Uh , you are like r r reducing the floor of the noisy regions , right ?&#10;Speaker: Professor G&#10;Content: s&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . The floor is lower . Um ,&#10;Speaker: PhD B&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: mm - hmm .&#10;Speaker: Professor D&#10;Content: I 'm sorry . So when you say minus twenty - five or minus thirty DB , with respect to what ?&#10;Speaker: PhD E&#10;Content: To the average um , speech energy which is estimated on the world database .&#10;Speaker: Professor D&#10;Content: OK , so basically you 're creating a signal - to - noise ratio of twenty - five or thirty DB ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: uh r&#10;Speaker: PhD E&#10;Content: But it 's not {disfmarker}&#10;Speaker: Professor G&#10;Content: I {disfmarker} I {disfmarker} I think what you do is this .&#10;Speaker: PhD E&#10;Content: it" />
    <node id="marker} and looking how dependent is it on the value of the constant&#10;Speaker: Professor D&#10;Content: Yeah . Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: and then , must choose them somehow {vocalsound} to give on average the best results for a certain range of the signal - to - noise ratios .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor G&#10;Content: So {disfmarker}&#10;Speaker: PhD E&#10;Content: Oh , it 's clear . I should have gi given other results . Also it 's clear when you don't add noise , it 's much worse . Like , around five percent worse I guess .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: And if you add too much noise it get worse also . And it seems that {vocalsound} right now this {disfmarker} this is c a constant that does not depend on {disfmarker} {comment} on anything that" />
    <node id=" this to smooth the probabilities , right ? Um {disfmarker} I didn't use the {disfmarker} the scheme that 's currently in the proposal because {vocalsound} I don't want to {disfmarker} In the proposal {disfmarker} Well , in {disfmarker} in the system we want to add like speech frame before every word and a little bit of {disfmarker} of , uh , s a couple of frames after also . Uh , but to estimate the performance of the VAD , we don't want to do that , because it would artificially increase the um {disfmarker} the false alarm rate of speech detection . Right ? Um , so , there is u normally a figure for the Finnish and one for Italian . And maybe someone has two for the Italian because I 'm missing one figure here .&#10;Speaker: PhD B&#10;Content: No .&#10;Speaker: PhD E&#10;Content: Well {disfmarker} Well , whatever . Uh {disfmarker} Yeah , so one surprising thing that we can notice first is that apparently the speech miss rate is uh , higher than the false alarm rate . So . It means {d" />
    <node id=" PhD B&#10;Content: So I have like a forty - five {vocalsound} percent for &quot; Car noise &quot; and then there 's a minus five percent for the &quot; Babble &quot; ,&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: and there 's this thirty - three for the &quot; Station &quot; . And so {vocalsound} it 's {disfmarker} it 's not {disfmarker} it 's not actually very consistent across . So . The only correlation between the SpeechDat - Car and this performance is the c stationarity of the noise that is there in these conditions and the SpeechDat - Car .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: And , uh {disfmarker} so {disfmarker} so the overall result is like in the last page , which is like forty - seven , which is still very imbalanced because there are like fifty - six percent on the SpeechDat - Car and thirty - five percent on the TI - digits . And {disfmarker} uh , ps the fifty - six percent is like comparable to what the French" />
    <node id="1. Company X changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. This change allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The compression and packing of frames are also modified so that they can pack the frames without waiting for an additional frame." />
    <node id=" need that forty milliseconds also .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: No . They actually changed the compression scheme altogether .&#10;Speaker: Professor D&#10;Content: Right ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So they have their own compression and decoding scheme and they {disfmarker} I don't know what they have .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: But they have coded zero delay for that . Because they ch I know they changed it , their compression . They have their own CRC , their {disfmarker} their own {vocalsound} error correction mechanism .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: So they don't have to wait more than one more frame to know whether the current frame is in error .&#10;Speaker: Professor D&#10;Content: Oh , OK .&#10;Speaker: PhD B&#10;Content: So they changed the whole thing so that there 's no delay for that compression and {disfmarker} part also .&#10;" />
    <node id="&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: f so {disfmarker}&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: they didn't include that .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So {disfmarker}&#10;Speaker: PhD E&#10;Content: Where does the comprish compression in decoding delay comes from ?&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: &#10;Speaker: PhD B&#10;Content: That 's the way the {disfmarker} the {disfmarker} the frames are packed , like you have to wait for one more frame to pack . Because it 's {disfmarker} the CRC is computed for two frames always .&#10;Speaker: Professor D&#10;Content: Well , that {disfmarker} the they would need that forty milliseconds also .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: No ." />
    <node id=" PhD B&#10;Content: So they changed the whole thing so that there 's no delay for that compression and {disfmarker} part also .&#10;Speaker: Professor D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Even you have reported actually zero delay for the {pause} compression . I thought maybe you also have some different {disfmarker}&#10;Speaker: Professor G&#10;Content: Mmm . Mmm . No , I think I {disfmarker} I used this scheme as it was before .&#10;Speaker: PhD B&#10;Content: OK . Ah . Mm - hmm .&#10;Speaker: PhD F&#10;Content: OK , we 've got twenty minutes so we should {vocalsound} probably try to move along . Uh , did you wanna go next , Stephane ?&#10;Speaker: PhD E&#10;Content: I can go next . Yeah . Mmm .&#10;Speaker: Professor D&#10;Content: Oh . Wait a minute . It 's {disfmarker}&#10;Speaker: PhD E&#10;Content: It 's {disfmarker} Yeah , we have to" />
    <node id="&#10;Speaker: PhD F&#10;Content: What {disfmarker} where was the , um {disfmarker} the smallest latency of all the systems last time ?&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: The French Telecom .&#10;Speaker: Professor D&#10;Content: Well , France Telecom was {disfmarker} was {disfmarker} was very short latency&#10;Speaker: Professor G&#10;Content: It 's {disfmarker}&#10;Speaker: Professor D&#10;Content: and they had a very good result .&#10;Speaker: PhD F&#10;Content: What {disfmarker} what was it ?&#10;Speaker: Professor D&#10;Content: It was thirty - five .&#10;Speaker: Professor G&#10;Content: It was in the order of thirty milliseconds&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: or {disfmarker}&#10;Speaker: PhD F&#10;Content: Thirteen ?&#10;Speaker: Professor D&#10;Content: th th&#10;Speaker: Professor G&#10;Content: Thirty .&#10;Speaker: PhD F&#10;Content: Thirty .&#10;" />
    <node id="ss - uh .&#10;Speaker: Professor D&#10;Content: So it could reduce the dependence on the amplitude and so on . Yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah . Although {disfmarker}&#10;Speaker: Professor D&#10;Content: Maybe .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: So is , uh {disfmarker} Is that about it ?&#10;Speaker: PhD B&#10;Content: Uh , so the {disfmarker}&#10;Speaker: PhD F&#10;Content: Or {disfmarker} ?&#10;Speaker: PhD B&#10;Content: OK . So the other thing is the {disfmarker} I 'm just looking at a little bit on the delay issue where the delay of the system is like a hundred and eighty millisecond . So {vocalsound} I just {disfmarker} just tried another sk system {disfmarker} I mean , another filter which I 've like shown at the end . Which is very similar to the existing uh , filter . Only {disfmarker} Uh , only thing" />
    <node id="isfmarker} if I use that frame to calculate the mean , then I 'm just gonna get n nothing .&#10;Speaker: Professor D&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: Um ,&#10;Speaker: Professor D&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: so I should probably have some kind of default {vocalsound} mean for the first f couple of frames ?&#10;Speaker: Professor D&#10;Content: Yeah . Yeah .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: Yeah . Or subtract nothing . I mean , it 's {disfmarker}&#10;Speaker: Grad C&#10;Content: Or subtract nothing . And {disfmarker} and that 's {disfmarker} that 's {disfmarker} I guess that 's something that 's p people have figured out how to deal with in cepstral mean subtraction as well ?&#10;Speaker: Professor D&#10;Content: Yeah , yeah . Yeah , people do something . They {disfmarker} they , uh , they have some , um , uh , in {disfmarker" />
    <node id="1. Moving Average Filter: This is a simple finite impulse response (FIR) filter where each output value is the average of the previous 'n' input values. It can be used to reduce high frequency noise and fluctuations in signals.&#10;2. Median Filter: In this non-linear digital filter, the output value is the median of the input values within the moving window. This filter is useful for removing impulse noise, as it preserves edges better than the mean filter.&#10;3. Exponential (First Order IIR) Filter: This filter calculates the output based on a combination of the current and previous input values, as well as the previous output value. The weights in the filter decrease exponentially over time, making it effective for smoothing signals with slowly changing trends.&#10;4. Butterworth Filter: A type of infinite impulse response (IIR) filter, the Butterworth filter is designed to have a maximally flat frequency response in the passband and minimum attenuation in the stopband. This filter can be used in applications where it's essential to preserve signal shape while removing noise.&#10;5. Chebyshev Filter: Similar to the Butterworth filter, the Chebyshev filter offers a better stopband rejection at the expense of a more uneven passband response. These filters are useful when there is a need for high attenuation in the stopband while maintaining reasonable performance in the passband.&#10;6. Bessel Filter: This filter has a maximally flat group delay (the time difference between input and output signal) in the passband, which helps to preserve signals' wave shape. However, its roll-off rate is slower than Butterworth or Chebyshev filters.&#10;7. LDA Filters: As mentioned in the transcript, Linear Discriminant Analysis (LDA) filters can be trained using supervised learning techniques to optimize a certain criterion, such as minimizing within-class variance or maximizing between-class variance. This filter can help better separate different classes and improve overall performance in signal processing tasks.&#10;&#10;When using LDA for determining the optimal filter based on a criterion, it is essential to consider what specific aspect of the filter design you want to optimize. For instance, you could train an LDA filter to minimize the mean squared error between the original and filtered signals or maximize the signal-to-noise ratio.&#10;&#10;To implement this in practice, you would collect a dataset with input-output pairs representing your desired output signal (clean speech) and noisy input (speech affected by reverberation, background noise, etc.). Then, train an LDA filter using that data, evaluating its performance based on the chosen criterion. Once trained, this LDA filter could be used to process new, noisy signals in order to provide a filtered output signal that meets your desired criteria." />
    <node id=" result , that if I doesn't use it .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD H&#10;Content: But m that may be because {vocalsound} with this technique {vocalsound} we are using really {disfmarker} really clean speech . The speech {disfmarker} the {comment} representation that go to the HTK is really clean speech because it 's from the dictionary , the code book and maybe from that . I don't know .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD H&#10;Content: Because I think that you {disfmarker} did some experiments using the two {disfmarker} the two LDA filter , clean and noi and noise ,&#10;Speaker: PhD E&#10;Content: It 's {disfmarker}&#10;Speaker: PhD H&#10;Content: and it doesn't matter too much .&#10;Speaker: PhD E&#10;Content: Um , yeah , I did that but it doesn't matter on SpeechDat - Car , but , it matters , uh , a lot on TI - digits .&#10;Speaker: PhD B&#10;Content: Using the" />
    <node id="Based on the transcript provided, it appears that there is a difference in performance when using an LDA filter trained with noisy speech versus one trained with clean speech. Specifically, in this context, PhD H mentions that &quot;with the noise I have worse result, that if I doesn't use it&quot; and &quot;it matters, uh, a lot on TI - digits.&quot; Later in the conversation, it is clarified that the results being discussed are associated with the clean LDA filter, rather than the noisy one.&#10;&#10;This suggests that, in this particular application, using an LDA filter trained with clean speech leads to better results compared to using a filter trained with noisy speech. This may be due to the fact that, as PhD H notes, &quot;with this technique we are using really clean speech&quot; and &quot;the representation that goes to the HTK is really clean speech because it's from the dictionary, the code book and maybe from that.&quot; Therefore, a filter trained with such high-quality data may be more effective at improving signal processing performance.&#10;&#10;It's also worth noting that there might be specific cases or datasets where an LDA filter trained with noisy speech could lead to better results, as PhD E mentions &quot;it doesn't matter too much on SpeechDat - Car, but it matters, uh, a lot on TI - digits.&quot; However, in the context of this conversation, the consensus seems to be that using an LDA filter trained with clean speech is more beneficial.&#10;&#10;To summarize, using an LDA filter trained with clean speech in this specific application yields better results compared to using a filter trained with noisy speech. This may be due to the high-quality data used for training the filter, which helps improve signal processing performance in this case. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech." />
    <node id=" matter on SpeechDat - Car , but , it matters , uh , a lot on TI - digits .&#10;Speaker: PhD B&#10;Content: Using the clean filter .&#10;Speaker: PhD H&#10;Content: It 's better to use clean .&#10;Speaker: PhD E&#10;Content: Yeah , d uh , it 's much better when you {disfmarker} we used the clean derived LDA filter .&#10;Speaker: PhD H&#10;Content: Mm - hmm . Maybe you can do d also this .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD H&#10;Content: To use clean speech .&#10;Speaker: PhD B&#10;Content: Yeah , I 'll try .&#10;Speaker: PhD E&#10;Content: Uh , but , yeah , Sunil in {disfmarker} in your result it 's {disfmarker}&#10;Speaker: PhD B&#10;Content: I {disfmarker} I 'll try the cle No , I {disfmarker} I {disfmarker} my result is with the noisy {disfmarker} noisy LDA .&#10;Speaker: PhD E&#10;Content: It 's with the" />
    <node id="er} my result is with the noisy {disfmarker} noisy LDA .&#10;Speaker: PhD E&#10;Content: It 's with the noisy one . Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: Oh !&#10;Speaker: PhD B&#10;Content: It 's with the noisy . Yeah . It 's {disfmarker} it 's not the clean LDA .&#10;Speaker: PhD E&#10;Content: So {disfmarker}&#10;Speaker: Professor D&#10;Content: Um {disfmarker}&#10;Speaker: PhD B&#10;Content: It 's {disfmarker} In {disfmarker} in the front sheet , I have like {disfmarker} like the summary . Yeah .&#10;Speaker: Professor D&#10;Content: And {disfmarker} and your result {comment} is with the {disfmarker}&#10;Speaker: PhD E&#10;Content: It 's with the clean LDA .&#10;Speaker: PhD B&#10;Content: Oh . This is {disfmarker} Your results are all with the clean LDA result ?&#10;Speaker: PhD" />
    <node id="1. The text discusses a meeting between several individuals, referred to as PhD B, Professor G, PhD E, PhD D, PhD E, and PhD H, who are involved in a project that involves analyzing audio data.&#10;2. The speakers mention that the audio data contains a small set of words (few transcriptions) and has music in the background. This is indicated when PhD B says &quot;It has very few...words also&quot; and &quot;It's a very, very small set, actually&quot; and Professor G says &quot;there is a lot of utterances with music in the background.&quot;&#10;3. Furthermore, the speakers express their dislike for the music, as indicated when PhD B says &quot;It has some music also...I mean, very horrible music like I know.&quot;&#10;4. Therefore, it can be said that the text contains audio data with a small set of words and music in the background, and the speakers express their dislike for the music." />
    <node id=": PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: It has a very few at {disfmarker} uh , actually , c uh , tran I mean , words also .&#10;Speaker: Professor G&#10;Content: I mean , that is {disfmarker} Yeah ,&#10;Speaker: PhD B&#10;Content: It 's a very , very small set , actually .&#10;Speaker: Professor G&#10;Content: that too . Yeah . Uh - huh .&#10;Speaker: PhD B&#10;Content: So there is {disfmarker}&#10;Speaker: Professor G&#10;Content: There is a l a {disfmarker} There is a lot of {disfmarker} Uh , there are a lot of utterances with music in {disfmarker} with music in the background .&#10;Speaker: PhD B&#10;Content: Yeah . Yeah , yeah , yeah . Yeah .&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: Yeah . It has some music also . I mean , very horrible music like like I know .&#10;Speaker: Professor D" />
    <node id=" to talk about that . But , well , the {disfmarker} the &quot; Car &quot; noises are below like five hundred hertz . And we were looking at the &quot; Music &quot; utterances and in this case the noise is more about two thousand hertz .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Well , the music energy 's very low apparently . Uh , uh , from zero to two {disfmarker} two thousand hertz . So maybe just looking at this frequency range for {disfmarker} from five hundred to two thousand would improve somewhat the VAD&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: PhD E&#10;Content: and {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mmm {disfmarker}&#10;Speaker: PhD B&#10;Content: So there are like some {disfmarker} some s some parameters you wanted to use or something ?&#10;Speaker: PhD E&#10;Content: Yeah , but {disfmarker} Yes .&#10;Speaker: PhD B&#10;Content: Or {disfmarker} Yeah ." />
    <node id=" 's Italian TI - digits .&#10;Speaker: Professor D&#10;Content: Yeah . Oh , it 's trained on Italian ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: Yeah , OK .&#10;Speaker: PhD E&#10;Content: Mm - hmm . And {disfmarker}&#10;Speaker: PhD B&#10;Content: That 's right .&#10;Speaker: Professor D&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: And also there are like funny noises on Finnish more than on Italian . I mean , like music&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Yeah . Yeah , the {disfmarker} Yeah , it 's true .&#10;Speaker: PhD E&#10;Content: and {vocalsound} um {disfmarker} So , yeah , we were looking at this . But for most of the noises , noises are {disfmarker} um , I don't know if we want to talk about that . But , well , the {disfmarker} the &quot; Car &quot; noises are below like five hundred hertz . And we were" />
    <node id="&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: so .&#10;Speaker: Professor D&#10;Content: Yeah . Yeah . OK .&#10;Speaker: PhD F&#10;Content: Carmen ? Do you , uh {disfmarker}&#10;Speaker: PhD H&#10;Content: Well , I only say that the {disfmarker} this is , a summary of the {disfmarker} of all the VTS experiments and say that the result in the last {comment} um , for Italian {disfmarker} the last experiment for Italian , {vocalsound} are bad . I make a mistake when I write . Up at D I copy {vocalsound} one of the bad result .&#10;Speaker: PhD B&#10;Content: So you {disfmarker}&#10;Speaker: PhD H&#10;Content: And {disfmarker} There . {vocalsound} You know , this . Um , well . If we put everything , we improve a lot u the spectral use of the VTS but the final result {vocalsound} are not still mmm , good {vocalsound} like the Wiener filter for example . I don't" />
    <node id="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." />
    <node id="&#10;Speaker: PhD B&#10;Content:  It 's {disfmarker}&#10;Speaker: Professor G&#10;Content: And that we , so , introduce again some natural behavior in this trajectory .&#10;Speaker: Professor D&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Very different from speech . Still , I mean , it shouldn't confuse the {disfmarker}&#10;Speaker: Professor G&#10;Content: Yeah , I mean , similar to what {disfmarker} what you see really u in {disfmarker} in the real um noisy situation .&#10;Speaker: PhD B&#10;Content: OK . Mm - hmm .&#10;Speaker: Professor G&#10;Content: Or i in the clean situation . But {disfmarker} but somehow a {disfmarker} a natural distribution .&#10;Speaker: Professor D&#10;Content: But isn't that s again sort of the idea of the additive thing , if it {disfmarker} as {disfmarker} as we had in the J stuff ? I mean , basically if {disfmarker} {vocalsound} if you have random" />
    <node id="m proposing to do , and s and stuff ?&#10;Speaker: Professor D&#10;Content: Yes , briefly .&#10;Speaker: PhD F&#10;Content: Yeah briefly .&#10;Speaker: Grad A&#10;Content: OK . Um , so briefly , {vocalsound} I 'm proposing to do a n a new p approach to speech recognition using um , a combination of , uh , multi - band ideas and ideas , um , {vocalsound} {vocalsound} {comment} about the uh , acoustic phonec phonetic approach to speech recognition . Um , so I will be using {vocalsound} these graphical models that {disfmarker} um , that implement the multi - band approach {vocalsound} to recognize a set of intermediate categories that might involve , uh , things like phonetic features {vocalsound} or other {disfmarker} other f feature things that are more closely related to the acoustic signal itself . Um , and the hope in all of this is that by going multi - band and by going into these , {vocalsound} um intermediate classifications , {vocalsound} that we can get a system that 's more robust to {disfmarker} to unseen noises" />
    <node id="isfmarker} as we had in the J stuff ? I mean , basically if {disfmarker} {vocalsound} if you have random data , um , in {disfmarker} in the time domain , then when you look at the s spectrum it 's gonna be pretty flat . And {disfmarker} and ,&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: uh , so just add something everywhere rather than just in those places . It 's just a constant , right ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Yeah . I think {disfmarker} e yeah . It 's {disfmarker} it 's just especially in these segments , I mean , you introduce , um , very artificial behavior .&#10;Speaker: Professor D&#10;Content: Yeah . Yeah .&#10;Speaker: Professor G&#10;Content: And {disfmarker}&#10;Speaker: Professor D&#10;Content: Well , see if you add something everywhere , it has almost no effect up {disfmarker} up {disfmarker} up on {" />
    <node id="1. Company B changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. The new compression scheme allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The packing of frames is also modified so that they can pack the frames without waiting for an additional frame. This change eliminates the delay associated with error correction." />
    <node id="1. Improved performance: Using an LDA filter trained with clean speech results in better performance for TI-digits compared to using a filter trained with noisy speech. This is because the high-quality data used for training the filter helps improve signal processing performance in this case.&#10;2. Consistency: In the context of the conversation, PhD B mentions that they get the best performance in the case of &quot;Car&quot; (the third column in condition A) when using the clean LDA filter.&#10;3. Specific application: The speakers are focusing on a specific application where clean speech is more beneficial. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.&#10;4. Confirmation of LDA filter type: The speakers clarify that they have been discussing the performance of a clean LDA filter as opposed to a noisy one, and this confirmation highlights the significance of using the correct LDA filter for specific datasets like TI-digits." />
    <node id="&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: Yeah . The reference drops like a very fast {disfmarker}&#10;Speaker: Professor D&#10;Content: Oh , oh , oh , oh , oh , oh .&#10;Speaker: PhD E&#10;Content: Like for clean {disfmarker} clean training condition .&#10;Speaker: Professor D&#10;Content: I see .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: I see .&#10;Speaker: PhD E&#10;Content: Nnn .&#10;Speaker: Professor D&#10;Content: This is {disfmarker} this is TI digits {comment} we 're looking at ?&#10;Speaker: PhD B&#10;Content: Yeah . Yeah . Oh {disfmarker}&#10;Speaker: Professor D&#10;Content: This whole page is TI - digits&#10;Speaker: PhD B&#10;Content: Oh . Yeah .&#10;Speaker: Professor D&#10;Content: or this is {disfmarker} ?&#10;Speaker: PhD B&#10;Content: It 's not written anywhere . Yeah , it 's TI - digits . The first r spreadsheet is TI - digits .&#10;Speaker: Professor" />
    <node id="Content: It 's not written anywhere . Yeah , it 's TI - digits . The first r spreadsheet is TI - digits .&#10;Speaker: Professor D&#10;Content: Mmm . How does clean training do for the , uh , &quot; Car &quot;&#10;Speaker: Professor G&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: The &quot; Car &quot; ?&#10;Speaker: Professor D&#10;Content: stuff ?&#10;Speaker: PhD B&#10;Content: Oh . Still {disfmarker} it still , uh {disfmarker} that {disfmarker} that 's still consistent . I mean , I get the best performance in the case of &quot; Car &quot; , which is the third column in the A condition .&#10;Speaker: Professor D&#10;Content: No . I mean , this is added noise . I mean , this is TI - digits . I 'm sorry . I meant {disfmarker} in {disfmarker} in the {disfmarker} in the , uh , multi - language , uh , uh , Finnish and {disfmarker}&#10;Speaker: PhD B&#10;Content: Uh {disfmarker}&#10;Speaker: Professor G&#10;Content" />
    <node id=": PhD B&#10;Content: Oh . This is {disfmarker} Your results are all with the clean LDA result ?&#10;Speaker: PhD H&#10;Content: Yeah , with the clean LDA .&#10;Speaker: PhD B&#10;Content: OK . @ @ .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: &#10;Speaker: PhD E&#10;Content: And in your case it 's all {disfmarker} all noisy ,&#10;Speaker: PhD H&#10;Content: Is that the reason ?&#10;Speaker: PhD B&#10;Content: All noisy , yeah .&#10;Speaker: PhD E&#10;Content: yeah . But {disfmarker}&#10;Speaker: PhD H&#10;Content: And {disfmarker}&#10;Speaker: PhD B&#10;Content: Uh {disfmarker} &#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Uh {disfmarker}&#10;Speaker: PhD E&#10;Content: But I observe my case it 's in , uh , uh , at least on SpeechDat - Car it doesn't matter but TI - digits it 's like two or three" />
    <node id="1. Fine-tuning ten or twelve small aspects, each contributing around 0.3%, can significantly improve a system's accuracy from 95% to 9" />
    <node id=" the analysis window is two seconds .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: So what you just said , about what do you start with , raises a question of {vocalsound} what do I start with then ?&#10;Speaker: Professor D&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: I guess it {disfmarker} because {disfmarker}&#10;Speaker: Professor D&#10;Content: Well , w OK , so in that situation , though , th maybe what 's a little different there , is I think you 're talking about {disfmarker} there 's only one {disfmarker} it {disfmarker} it {disfmarker} it also depends {disfmarker} we 're getting a little off track here .&#10;Speaker: Grad C&#10;Content: Oh , right .&#10;Speaker: Professor D&#10;Content: r But {disfmarker} but {disfmarker} but {disfmarker} Uh , there 's been some discussion about whether the work we 're doing in that project is gonna be for the kiosk or" />
    <node id=" doing , uh , research , you may , eh {disfmarker} you might find that the way that you build up a change from a ninety - five percent accurate system to a ninety - eight percent accurate system is through ten or twelve little things that you do that each are point three percent . So {disfmarker} so the {disfmarker} they {disfmarker} they {disfmarker} it 's {disfmarker} I don't mean to say that they 're {disfmarker} they 're irrelevant . Uh , they are relevant . But , um , {vocalsound} i for a demo , you won't see it .&#10;Speaker: Grad C&#10;Content: Mm - hmm . Right . OK .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: And , um , Let 's {disfmarker} l let 's see . Um , OK . And then there 's um , another thing I wanna start looking at , um , {vocalsound} wi is , um , the choice of the analysis window length . So I 've just been using two seconds just because that '" />
    <node id=" been that much with this long {disfmarker} long - time , uh , spectra work .&#10;Speaker: Grad C&#10;Content: Oh , o Oh , OK .&#10;Speaker: Professor D&#10;Content: Uh ,&#10;Speaker: Grad C&#10;Content: So that 's {disfmarker} that 's {disfmarker} that 's standard . Um {disfmarker}&#10;Speaker: Professor D&#10;Content: Yeah . Pretty common .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: Yeah . Um , but , u uh , yes . No , it is interesting . And the other thing is , I mean , there 's two sides to these really small , uh , gradations in performance . Um , I mean , on the one hand in a practical system if something is , uh , four point four percent error , four point one percent error , people won't really tell {disfmarker} be able to tell the difference . On the other hand , when you 're doing , uh , research , you may , eh {disfmarker} you might find that the way that you build up a change from a ninety -" />
    <node id="1. The speaker, referred to as PhD F, has been researching speech recognition with a focus on creating a more robust system for word or phone recognition. This involves developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. One of the specific research questions they have been addressing is what types of intermediate categories are needed for classification. Another question relates to the structure of multi-band graphical models, specifically what types of structures should be considered in order to effectively combine evidence from sub-bands.&#10;3. Additionally, PhD F has been investigating how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. To accomplish this, they have proposed a new approach to speech recognition that combines multi-band ideas with an acoustic phonetic approach. This approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features more closely related to the acoustic signal itself." />
    <node id="} um intermediate classifications , {vocalsound} that we can get a system that 's more robust to {disfmarker} to unseen noises , and situations like that . Um , and so , some of the research issues involved in this are , {vocalsound} um , {vocalsound} {comment} one , what kind of intermediate categories do we need to classify ? Um , another one is {vocalsound} um , what {disfmarker} what other types of structures in these multi - band graphical models should we consider in order to um , combine evidence from {vocalsound} the sub - bands ? And , uh , the third one is how do we {disfmarker} how do we merge all the , uh , information from the individual uh , multi - band classifiers to come up with word {disfmarker} word recognition or {disfmarker} or phone recognition things . Um , so basically that 's {disfmarker} that 's what I 've been doing . And ,&#10;Speaker: PhD F&#10;Content: So you 've got two weeks , huh ?&#10;Speaker: Grad A&#10;Content: I got two weeks to brush up on" />
    <node id="aker: PhD B&#10;Content: Yeah . So here {disfmarker} here I mean , I found that it 's {disfmarker} if I changed the noise estimate I could get an improvement .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: So that 's {disfmarker} so it 's something which I can actually pursue , is the noise estimate .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: And {disfmarker}&#10;Speaker: Professor G&#10;Content: Yeah , I think what you do is in {disfmarker} when {disfmarker} when you have the {disfmarker} the {disfmarker} this multi - condition training mode , um then you have {disfmarker} then you can train models for the speech , for the words , as well as for the pauses where you really have all information about the noise available .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And it was surprising {disfmarker} At the beginning it was not surprising to me" />
    <node id=" using the neural network , I guess . That 's right ?&#10;Speaker: PhD B&#10;Content: Yeah . We actually trained , uh , the {disfmarker} on the Italian training part .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: We {disfmarker} we had another {vocalsound} system with u&#10;Speaker: PhD E&#10;Content: So it was a f f a phonetic classification system for the Italian Aurora data .&#10;Speaker: PhD B&#10;Content: Yeah . It must be somewhere . Yeah .&#10;Speaker: PhD E&#10;Content: For the Aurora data that it was trained on , it was different . Like , for TI - digits you used a {disfmarker} a previous system that you had , I guess .&#10;Speaker: PhD B&#10;Content: What {disfmarker}  No it {disfmarker} Yeah , yeah . That 's true .&#10;Speaker: PhD E&#10;Content: So the alignments from the different database that are used for training came from different system .&#10;Speaker: PhD B&#10;Content: Syste Yeah .&#10;Speaker: PhD E&#10;Content:" />
    <node id="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable." />
    <node id=" but it might make a difference . I don't know .&#10;Speaker: Grad C&#10;Content: Uh , yeah , I don't {disfmarker} I don't think the TI - digits data that I have , um , {vocalsound} i is {disfmarker} would be appropriate for that .&#10;Speaker: Professor D&#10;Content: Yeah , probably not . Yeah .&#10;Speaker: Grad C&#10;Content: But what do you {disfmarker} What about if I w I fed it through some kind of , um , speech processing algorithm that changed the speech rate ?&#10;Speaker: Professor D&#10;Content: Yeah , but then you 'll have the degradation of {disfmarker} of , uh , whatever you do uh , added onto that . But maybe . Yeah , maybe if you get something that sounds {disfmarker} that {disfmarker} that 's {disfmarker} does a pretty job at that .&#10;Speaker: Grad C&#10;Content: Yeah . Well , uh , just if you think it 's worth looking into .&#10;Speaker: Professor D&#10;Content: You could imagine that .&#10;Speaker: Grad C&#10;Content:" />
    <node id=" uh , five - percent improvement , and fifty - eight point one . So again , it 's around fifty - six , fifty - seven . Uh {disfmarker}&#10;Speaker: Professor D&#10;Content: Cuz I notice the TI - digits number is exactly the same for these last two ?&#10;Speaker: PhD E&#10;Content: Yeah , because I didn't {disfmarker} For the France Telecom uh , spectral subtraction included in the {disfmarker} our system , the TI - digits number are the right one , but not for the other system because I didn't test it yet {disfmarker} this system , including {disfmarker} with spectral subtraction on the TI - digits data . I just tested it on SpeechDat - Car .&#10;Speaker: Professor D&#10;Content: Ah ! So {disfmarker} so that means the only thing {disfmarker}&#10;Speaker: Professor G&#10;Content: Mm - hmm . So {disfmarker} so {disfmarker} so these numbers are simply {disfmarker}&#10;Speaker: PhD E&#10;Content: This , we have to {disfmarker} Yeah" />
    <node id=" thirty - five percent on the TI - digits . And {disfmarker} uh , ps the fifty - six percent is like comparable to what the French Telecom gets , but the thirty - five percent is way off .&#10;Speaker: Professor D&#10;Content: I 'm sort of confused but {disfmarker} this {disfmarker} I 'm looking on the second page ,&#10;Speaker: PhD B&#10;Content: Oh , yep .&#10;Speaker: Professor D&#10;Content: and it says &quot; fifty percent &quot; {disfmarker} looking in the lower right - hand corner , &quot; fifty percent relative performance &quot; .&#10;Speaker: Professor G&#10;Content: For the clean training .&#10;Speaker: Professor D&#10;Content: Is that {disfmarker}&#10;Speaker: Professor G&#10;Content: u And if you {disfmarker} if you look {disfmarker}&#10;Speaker: Professor D&#10;Content: is that fifty percent improvement ?&#10;Speaker: PhD B&#10;Content: Yeah . For {disfmarker} that 's for the clean training and the noisy testing for the TI - digits .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;" />
    <node id="1. The professors and PhDs are discussing adjusting the value of the constant added to the signal energy to achieve a desired signal-to-noise ratio (SNR). This constant is determined based on the average speech energy measured from a dataset of one language, such as Italian, and then used as the mean speech energy for all other languages.&#10;2. They are also considering how dependent the performance is on the value of this constant and exploring different values to find the optimal SNR range for the system.&#10;3. Additionally, there is mention of potentially adjusting noise addition or thresholds based on the average energy levels in each language. For example, Finnish has a lower average energy compared to other databases, so the threshold for Finnish should be lower accordingly.&#10;4. Lastly, they touch upon the impracticality of measuring the average energy over an extended period in real-world applications and suggest using a fixed number derived from other methods instead." />
    <node id="1. The speakers believe that the models are not complex enough to absorb additional variability in the data because they have observed that introducing a &quot;natural distribution&quot; to the audio data, which would make it resemble real-world noisy situations more closely, increases the degradation of the speech processing algorithm. They suggest that this could be due to the models' inability to effectively model and account for the increased variability. Instead, the added noise results in additional errors or degradation in the system's performance.&#10;2. This explanation can be found when PhD B says, &quot;the models are not complex enough to absorb that additional variability that you're introducing&quot; and later reiterated by PhD E, who shares their experience with neural networks being more capable of modeling strange distributions.&#10;3. The speakers also suggest that this natural distribution approach more accurately represents real-world noisy situations, indicating that the current models might not be sophisticated enough to account for these complexities without compromising performance." />
    <node id=" And {disfmarker} and this {disfmarker} this curves are the average over the whole database , so .&#10;Speaker: PhD E&#10;Content: Yeah . Right .&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD E&#10;Content: Um {disfmarker} Yeah , and the different points of the curves are for five uh , thresholds on the probability {comment} uh from point three to point seven .&#10;Speaker: PhD B&#10;Content: So that threshold {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm . Yeah .&#10;Speaker: PhD B&#10;Content: OK . S OK {disfmarker} so d the detection threshold is very {disfmarker}&#10;Speaker: PhD E&#10;Content: So the v&#10;Speaker: PhD B&#10;Content: Yeah , yeah .&#10;Speaker: PhD E&#10;Content: The VAD ? Yeah . There first , a threshold on the probability {comment} @ @ {comment} That puts all the values to zero or one .&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: PhD E&#10;Content: And then the median filtering" />
    <node id="Based on the transcript, the Spanish government requests that all participants in their program provide a report. The content of this report includes details about the approach and work done by each participant. There is no explicit mention of a decision regarding including digits in the report, but earlier in the conversation, they discuss processing TI-digits data for their system. It can be inferred that the report should cover the process, methods, and results related to working with the TI-digits data as part of their participation in the program." />
    <node id="} the Spanish government , uh , requires that anyway . They want some kind of report from everybody who 's in the program .&#10;Speaker: PhD H&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: So . And of course I 'd {disfmarker} we 'd {disfmarker} we 'd like to see it too . So ,&#10;Speaker: PhD H&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: yeah .&#10;Speaker: PhD F&#10;Content: So , um , what 's {disfmarker} Do you think we , uh , should do the digits or skip it ? Or what are {disfmarker} what do you think ?&#10;Speaker: Professor D&#10;Content: Uh , we have them now ?&#10;Speaker: PhD F&#10;Content: Yeah , got them .&#10;Speaker: Professor D&#10;Content: Uh , why don why don't we do it ?&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: Just {comment} {disfmarker} just take a minute .&#10;Speaker: PhD H&#10;Content: I can send yet ." />
    <node id=" final number as {disfmarker} as Sunil did it&#10;Speaker: PhD H&#10;Content: And prepare at the s&#10;Speaker: Professor G&#10;Content: and {vocalsound} um and maybe also to {disfmarker} to write somehow a document where you describe your approach , and what you have done .&#10;Speaker: PhD H&#10;Content: Yeah , I was thinking to do that next week .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: Yeah , I 'll {disfmarker} I 'll borrow the head back and {disfmarker} and agree . Yeah ,&#10;Speaker: PhD H&#10;Content: Yeah , I wi I {disfmarker} I will do that next week .&#10;Speaker: Professor D&#10;Content: that 's {disfmarker} that 's {disfmarker} Right . In fact , actually I g I guess the , uh {disfmarker} the Spanish government , uh , requires that anyway . They want some kind of report from everybody who 's in the program .&#10;Speaker: PhD H" />
    <node id="1. The project's focus regarding designing acoustics for either a kiosk or a mobile platform is determined by the specific requirements and use cases of the target application. If the application is designed to be stationary and user-specific, like in a kiosk, then it would make more sense to optimize the acoustics for that setting as the physical situation remains relatively stable. In contrast, if the application is intended for a mobile platform with variable acoustic conditions, the system should be designed to adapt to changing acoustics accordingly.&#10;2. The decision may also depend on the complexity and adaptability of the speech processing algorithms being used. More sophisticated models might be able to handle varying acoustic environments better than simpler ones. In this case, designing for a mobile platform could still provide an acceptable user experience if the models are complex enough to absorb additional variability.&#10;3. Lastly, practical considerations such as development time, resources, and budget may influence the decision. Developing a system that works well across multiple platforms might require more effort and resources than focusing on a single platform. However, designing with scalability and adaptability in mind could make it easier to expand the application to other platforms in the future." />
    <node id="disfmarker} Uh , there 's been some discussion about whether the work we 're doing in that project is gonna be for the kiosk or for the mobile or for both . And I think for this kind of discussion it matters . If it 's in the kiosk , then the physical situation is the same . It 's gonna {disfmarker} you know , the exact interaction of the microphone 's gonna differ depending on the person and so forth . But at least the basic acoustics are gonna be the same . So f if it 's really in one kiosk , then I think that you could just chain together and {disfmarker} and you know , as much {disfmarker} as much speech as possible to {disfmarker} because what you 're really trying to get at is the {disfmarker} is the reverberation characteristic .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: But in {disfmarker} in the case of the mobile , uh , {comment} presumably the acoustic 's changing all over the place .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor D" />
    <node id="Based on the discussion transcript, the outcome of the discussion regarding delay in compression is that both Company B and Company X have changed their compression schemes altogether. They now have their own compression and decoding schemes, as well as their own Cyclic Redundancy Check (CRC) and error correction mechanisms.&#10;&#10;Company B is able to determine within one more frame if the current frame is in error, reducing the delay to less than one frame. They have also modified the packing of frames to eliminate the delay associated with error correction. Similarly, Company X can also detect errors in one more frame, reducing the delay to less than one frame. They have altered both their compression and packing of frames to accomplish this.&#10;&#10;Therefore, both companies have made significant changes to their previous compression schemes to minimize or completely eliminate delays in compression and error correction." />
    <node id="1. The group's current position on constraining the latency is that they are not unduly restricting it at this stage because they are still experimenting with improving performance in noisy conditions. There is a minority within the group who argue for further constraining of the latency, but the majority believes that longer latencies provide benefits, so they are continuing to explore the trade-offs between longer and shorter latencies.&#10;2. The individual who argues for further constraint of latency is not explicitly named in the transcript, but it is mentioned that there is a minority within the group who hold this view." />
    <node id="}&#10;Speaker: Professor D&#10;Content: This is {disfmarker} So {disfmarker} So , basically our {disfmarker} our position is {vocalsound} that , um , we shouldn't be unduly constraining the latency at this point because we 're all still experimenting with trying to make the performance better in the presence of noise . Uh , there is a minority in that group who is a arguing {disfmarker} who are arguing for {vocalsound} um , uh , having a further constraining of the latency . So we 're s just continuing to keep aware of what the trade - offs are and , you know , what {disfmarker} what do we gain from having longer or shorter latencies ?&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: Professor D&#10;Content: But since we always seem to at least get something out of longer latencies not being so constrained , we 're tending to go with that if we 're not told we can't do it .&#10;Speaker: PhD F&#10;Content: What {disfmarker} where was the , um {disfmarker} the smallest latency of all" />
    <node id=" .&#10;Speaker: PhD E&#10;Content: Well , um , for Italian and Spanish it 's {disfmarker} th this value works good but not necessarily for Finnish . Mmm . But unfortunately there is , like , this forty millisecond latency and , um {disfmarker} Yeah , so I would try to somewhat reduce this @ @ . I already know that if I completely remove this latency , so . {vocalsound} um , {comment} it {disfmarker} um there is a three percent hit on Italian .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: d Does latency {disfmarker}&#10;Speaker: Professor G&#10;Content: i&#10;Speaker: PhD B&#10;Content: Sorry . Go ahead .&#10;Speaker: Professor G&#10;Content: Yeah . Your {disfmarker} your smoothing was @ @ {comment} uh , over this s so to say , the {disfmarker} the factor of the Wiener . And then it 's , uh {disfmarker} What was it ? This {disfmarker}&#10;Speaker: PhD E&#10;Content: M" />
    <node id="1. The process of smoothing the gain in this context involves using a non-linear smoothing technique for low gain values, while not applying any smoothing for high gain values. This approach is taken to balance the need for noise reduction with the potential distortion or loss of signal information that can result from aggressive smoothing.&#10;2. The subtraction factor or algorithm's gain is being smoothed in this case, as mentioned by Professor G. This suggests that the group is trying to improve the accuracy and stability of the noise reduction technique by reducing the variability of the gain value over time.&#10;3. The decision on whether to smooth or not, and the choice of smoothing method, depends on various factors such as the desired latency, the characteristics of the signal and noise, the performance trade-offs between different techniques, and the specific requirements of the application.&#10;4. The group's discussion suggests that they are still experimenting with different approaches to smoothing the gain, as there is no consensus on whether further constraining the latency would be beneficial or not. This indicates that the process involves ongoing evaluation and adjustment based on empirical results and theoretical considerations." />
    <node id="isfmarker} {vocalsound} and later on you smooth also this subtraction factor .&#10;Speaker: PhD E&#10;Content: Uh , no , it 's {disfmarker} it 's just the gain that 's smoothed actually&#10;Speaker: PhD B&#10;Content: Uh , actually I d I do all the smoothing .&#10;Speaker: PhD E&#10;Content: but it 's smoothed {disfmarker}&#10;Speaker: Professor G&#10;Content: Ah . Oh , it w it was you .&#10;Speaker: PhD B&#10;Content: Yeah , yeah .&#10;Speaker: PhD E&#10;Content: Uh {disfmarker} Yeah .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Yeah . No , in this case it 's just the gain .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: And {disfmarker}&#10;Speaker: Professor G&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: But the way it 's done is that um , for low gain , there is this non nonlinear smoothing actually . For low" />
    <node id=" it 's , uh {disfmarker} What was it ? This {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: this smoothing , it was over the subtraction factor , so to say .&#10;Speaker: PhD E&#10;Content: It 's a smoothing over the {disfmarker} the gain of the subtraction algorithm .&#10;Speaker: Professor G&#10;Content: Was this done {disfmarker} Mm - hmm . And {disfmarker} and you are looking into the future , into the past .&#10;Speaker: PhD E&#10;Content: Right .&#10;Speaker: Professor G&#10;Content: And smoothing .&#10;Speaker: PhD E&#10;Content: So , to smooth this {pause} thing .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Yeah . Um {disfmarker}&#10;Speaker: Professor G&#10;Content: And did {disfmarker} did you try simply to smooth um to smooth the {disfmarker} the {disfmarker} t to {d" />
    <node id=" PhD E&#10;Content: But the way it 's done is that um , for low gain , there is this non nonlinear smoothing actually . For low gains um , I use the smoothed sm uh , smoothed version but {disfmarker} for high gain @ @ {comment} it 's {disfmarker} I don't smooth .&#10;Speaker: Professor G&#10;Content: Uh . Mm - hmm . I just , uh {disfmarker} it {disfmarker} Experience shows you , if {disfmarker} if you do the {disfmarker}  The best is to do the smoo smoothing as early as possible .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Professor G&#10;Content: So w when you start up . I mean , you start up with the {disfmarker} with the {disfmarker} somehow with the noisy envelope .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: And , best is to smooth this somehow .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Uh , yeah , I" />
    <node id="1. Hamming or Hanning Window: These window functions can be used to reduce the weight of values at the edges when applying a moving window to a signal. They decrease the impact of abrupt changes between windows and help smooth the signal, which can make the training process more stable. In the context of the &quot;Aurora stuff,&quot; these window functions could be applied when processing audio data as part of a speech recognition system.&#10;2. LDA Filter (Linear Discriminant Analysis filter): The LDA filter is a supervised learning algorithm that can optimize some criterion, such as minimizing within-class variance or maximizing between-class variance. This results in a filter that better separates different classes and improves the performance of the system. In the context of the &quot;Aurora stuff,&quot; an LDA filter could be trained on labeled data to improve speech recognition accuracy.&#10;3. Spectral Subtraction or Wiener Filtering: These techniques reduce noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum, improving the signal-to-noise ratio and making it easier to extract useful information from noisy data. In the context of the &quot;Aurora stuff,&quot; these methods could be applied when processing audio signals to improve speech recognition performance.&#10;&#10;These filters, along with a uniform window, are options for moving window setups in training LDA filters or other signal processing tasks. The choice of filter will depend on the specific application and the characteristics of the data being processed. In the transcript, it is mentioned that spectral subtraction was not effective for certain Aurora tasks, suggesting that a different filter or approach might be needed." />
    <node id="}&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: But {disfmarker} Yeah . Then I tried the same {disfmarker} exactly the same spectral subtraction algorithm on these Aurora tasks and it simply doesn't work . It 's even {disfmarker} it , uh , hurts even .&#10;Speaker: Professor G&#10;Content: Hmm .&#10;Speaker: PhD E&#10;Content: So .&#10;Speaker: Professor D&#10;Content: We probably should at some point here try the tandem {disfmarker} the {disfmarker} the {disfmarker} the system - two kind of stuff with this , with the spectral subtraction for that reason .&#10;Speaker: Professor G&#10;Content: Hmm .&#10;Speaker: Professor D&#10;Content: Cuz {vocalsound} again , it should do a transformation to a domain where it maybe {disfmarker} looks more Gaussian .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Hmm . Yeah , y I" />
    <node id="Yes, based on the transcript, it appears that the neural network was trained on previously estimated alignments rather than being directly based on the provided label files for the Italian data. Speaker PhD E states that &quot;we trained the neural network on embedded training... That's right?&quot; and later clarifies that &quot;For the Italian data, I think we trained the neural network on re-estimation of the alignment using the neural network.&quot; There is no indication in the transcript that the neural network was directly trained on the provided label files." />
    <node id="&#10;Content: Yeah , but {disfmarker} Yes .&#10;Speaker: PhD B&#10;Content: Or {disfmarker} Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Uh , the next , um {disfmarker} Oh , it 's there .&#10;Speaker: Professor G&#10;Content: So is the {disfmarker} is the {disfmarker} is the training {disfmarker} is the training based on these labels files which you take as reference here ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Wh - when you train the neural net y y you {disfmarker}&#10;Speaker: PhD E&#10;Content: No . It 's not . It 's {disfmarker} it was trained on some alignment obtained um , uh {disfmarker} For the Italian data , I think we trained the neural network on {disfmarker} with embedded training . So re - estimation of the alignment using the neural network , I guess . That 's right ?&#10;Speaker: PhD B&#10;Content: Yeah . We actually trained , uh , the {" />
    <node id="aker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: so the VAD was trained on maybe different set of labels for channel zero and channel one&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: was the alignments were w were different for {disfmarker} s certainly different because they were independently trained .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: We didn't copy the channel zero alignments to channel one .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: But for the new alignments what you generated , you just copied the channel zero to channel one , right ? Yeah .&#10;Speaker: PhD E&#10;Content: Right . Yeah . Um . And eh , h" />
    <node id=" that are used for training came from different system .&#10;Speaker: PhD B&#10;Content: Syste Yeah .&#10;Speaker: PhD E&#10;Content: Then we put them tog together . Well , you put them together and trained the VAD on them .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Hmm .&#10;Speaker: PhD E&#10;Content: Uh , But did you use channel {disfmarker} did you align channel one also ? Or {disfmarker}&#10;Speaker: PhD B&#10;Content: I just took their entire Italian training part .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: So it was both channel zero plus channel one .&#10;Speaker: PhD E&#10;Content: So di Yeah . So the alignments might be wrong then on channel one , right ?&#10;Speaker: PhD B&#10;Content: On one . Possible .&#10;Speaker: PhD E&#10;Content: So we might ,&#10;Speaker: PhD B&#10;Content: We can do a realignment .&#10;Speaker" />
    <node id="&#10;Speaker: PhD E&#10;Content: So we might ,&#10;Speaker: PhD B&#10;Content: We can do a realignment .&#10;Speaker: PhD E&#10;Content: yeah ,&#10;Speaker: PhD B&#10;Content: That 's true .&#10;Speaker: PhD E&#10;Content: at least want to retrain on these alignments , which should be better because they come from close - talking microphone .&#10;Speaker: Professor G&#10;Content: Yeah , the {disfmarker} that was my idea . I mean , if {disfmarker} if it ha if it is not the same labeling which is taking the spaces .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Yeah , possible .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: I mean , it {disfmarker} so the system {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: so the VAD was trained on maybe different set of labels for channel" />
    <node id="1. The advantage of using a fixed filter, such as the RASTA filter, is that it remains unchanged and does not degrade over time, making it a reliable choice for signal processing. This is because it does not need to be retrained or adapted for different conditions, unlike trained filters.&#10;2. In the context of the SmartKom system, using a fixed amount of data for mean subtraction can be beneficial when less than twelve seconds of data are available. By training the models with a fixed amount of data (e.g., six or twelve seconds), the system can ensure stability and consistency in the processing steps that follow.&#10;3. However, if there is not enough data available to estimate a reliable mean value for mean subtraction, concatenating utterances together can help increase the amount of available data. This approach may be more suitable than training a filter specifically for certain conditions, as it allows the system to use all the available data and avoid potential overfitting or poor generalization issues that can arise with trained filters.&#10;4. Overall, using a fixed filter or a fixed amount of data for mean subtraction provides a stable and consistent processing approach, while concatenating utterances together can help increase the amount of available data when there is less than twelve seconds of data in the SmartKom system." />
    <node id=" SmartKom system to do the mean subtraction . You said in {vocalsound} systems where you use cepstral mean subtraction , they concatenate utterances and , {vocalsound} do you know how they address this issue of , um , testing versus training ? Can {disfmarker}&#10;Speaker: Professor D&#10;Content: Go ahead .&#10;Speaker: Professor G&#10;Content: I think what they do is they do it always on - line , I mean , that you just take what you have from the past , that you calculate the mean of this and subtract the mean .&#10;Speaker: Grad C&#10;Content: OK . Um {disfmarker}&#10;Speaker: Professor G&#10;Content: And then you can {disfmarker} yeah , you {disfmarker} you can increase your window whi while you get {disfmarker} while you are getting more samples .&#10;Speaker: Grad C&#10;Content: OK , um , and , um , so {disfmarker} so in tha in that case , wh what do they do when they 're t um , performing the cepstral mean subtraction on the training data ? So {" />
    <node id=" of what we 're doing in this Aur - Aurora stuff . And , uh , it 's still not clear to me in the long run whether the best thing to do would be to do that or to have some stylized version of the filter that looks like these things you 've trained up , because you always have the problem that it 's trained up for one condition and it isn't quite right for another . So . uh {disfmarker} that 's {disfmarker} that 's why {disfmarker} that 's why RASTA filter has actually ended up lasting a long time , people still using it quite a bit , because y you don't change it . So doesn't get any worse . Uh ,&#10;Speaker: Grad C&#10;Content: Huh .&#10;Speaker: Professor D&#10;Content: Anyway .&#10;Speaker: Grad C&#10;Content: o OK . So , um , a actually I was just thinking about what I was asking about earlier , wi which is about having {vocalsound} less than say twelve seconds in the SmartKom system to do the mean subtraction . You said in {vocalsound} systems where you use cepstral mean subtraction , they" />
    <node id=" , in my tests before with HTK I found it worked {disfmarker} it worked the best with about twelve seconds of data used to estimate the mean , but , we 'll often have less {comment} in the SmartKom system . Um . So I think we 'll use as much data as we have {pause} at a particular time , and we 'll {disfmarker} {vocalsound} we 'll concatenate utterances together , um , to get as much data as we possibly can from the user . But , {vocalsound} um , {vocalsound} there 's a question of how to set up the models . So um , we could train the models . If we think twelve seconds is ideal we could train the models using twelve seconds to calculate the mean , to mean subtract the training data . Or we could , um , use some other amount . So {disfmarker} like I did an experiment where I , um , was using six seconds in test , um , but , for {disfmarker} I tried twelve seconds in train . And I tried , um , um , the same in train {disfmarker} I 'm a I tried six" />
    <node id="1. Cepstral Mean Subtraction (CMS) is a technique used in short-term window analysis to remove general characteristics from data. It involves calculating the mean of the cepstrum (a transform of the log power spectrum) for each window and subtracting it from the cepstrum of that window. This helps get rid of very general characteristics, such as those caused by the recording device or room acoustics.&#10;2. Flipping frames from the front to create negative values is a technique used in some signal processing applications, including CMS. By taking a certain number of frames from the beginning of the signal and flipping them around, the original frames are replaced with their mirror images. This can help balance the signal by reducing the impact of any abrupt changes at the beginning of the window. In the context of CMS, flipping frames creates negative values in the cepstrum that are then subtracted from the original cepstrum to achieve mean subtraction." />
    <node id=" . Yeah , people do something . They {disfmarker} they , uh , they have some , um , uh , in {disfmarker} in cepstral mean subtraction , for short - term window {disfmarker} analysis windows , as is usually done , you 're trying to get rid of some very general characteristic . And so , uh , if you have any other information about what a general kind of characteristic would be , then you {disfmarker} you can do it there .&#10;Speaker: PhD F&#10;Content: You can also {disfmarker} you can also reflect the data . So you take , uh {disfmarker} you know , I 'm not sure how many frames you need .&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: But you take that many from the front and flip it around to {disfmarker} a as the negative value .&#10;Speaker: Professor D&#10;Content: Yeah , that 's {disfmarker} Yeah .&#10;Speaker: PhD F&#10;Content: So you can always {disfmarker}&#10;Speaker: Professor D&#10;Content:" />
    <node id="Scaling the noise estimate by a factor less than one resulted in improved performance for the TI-digits because, as mentioned in the transcript, there were a lot of zeros in the spectrogram when using the initial approach. By scaling the noise estimate, the speakers aimed to reduce this issue and improve the system's performance.&#10;&#10;The complete results of using this new technique involve applying a noise estimate scaled by a factor of 0.5. This is an ad-hoc method, meaning it is not optimized for any specific goal but serves as an intermediate step in their exploration. The only trend observed from the results was that the current noise estimation or composition scheme worked well for car noise types, while performing less effectively for non-stationary noises like &quot;Babble,&quot; &quot;Subway,&quot; &quot;Street,&quot; or &quot;Restaurant&quot; sounds.&#10;&#10;In summary, scaling the noise estimate by a factor of 0.5 improved performance in this case, but it is an ad-hoc method and not optimized for any specific goal. The complete results show that the technique works well for car noise types but may struggle with non-stationary noises." />
    <node id=" , I didn't h I mean , I {disfmarker} I found that the results {disfmarker} I mean , I wasn't getting that r results on the TI - digit . So I was like looking into &quot; why , what is wrong with the TI - digits ? &quot; . Why {disfmarker} why I was not getting it . And I found that , the noise estimation is a reason for the TI - digits to perform worse than the baseline . So , uh , I actually , picked th I mean , the first thing I did was I just scaled the noise estimate by a factor which is less than one to see if that {disfmarker} because I found there are a lot of zeros in the spectrogram for the TI - digits when I used this approach . So the first thing I did was I just scaled the noise estimate . And I found {disfmarker} So the {disfmarker} the results that I 've shown here are the complete results using the new {disfmarker} Well , the n the new technique is nothing but the noise estimate scaled by a factor of point five . So it 's just an ad - hoc {disfmarker} I mean" />
    <node id=" technique is nothing but the noise estimate scaled by a factor of point five . So it 's just an ad - hoc {disfmarker} I mean , some intermediate result , because it 's not optimized for anything . So the results {disfmarker} The trend {disfmarker} the only trend I could see from those results was like the {disfmarker} the p the current noise estimation or the , uh , noise composition scheme is working good for like the car noise type of thing . Because I 've {disfmarker} the only {disfmarker} only {disfmarker} p very good result in the TI - digits is the noise {disfmarker} car noise condition for their test - A , which is like the best I could see that uh , for any non - stationary noise like &quot; Babble &quot; or &quot; Subway &quot; or any {disfmarker} &quot; Street &quot; , some &quot; Restaurant &quot; noise , it 's like {disfmarker} it 's not performing w very well . So , the {disfmarker} {vocalsound} So that {disfmarker} that 's the first thing I c uh , I could" />
    <node id="1. There is a mention of a &quot;point three percent hit&quot; in the context of a mismatch between training and testing conditions. This means that there is a decrease in performance or accuracy of around 0.3% when there's a discrepancy between how the model was trained (training link) and what it encounters during testing (testing link).&#10;2. The specific instance of this &quot;point three percent hit&quot; is related to changing the prototype for initializing the HMM (Hidden Markov Model) in order to avoid getting stuck in a local minimum during training. This change resulted in a 50% improvement, from 75.79 to 88, indicating that the mismatch between training and testing conditions had a noticeable impact on performance.&#10;3. The context suggests that this &quot;point three percent hit&quot; is not just for any mismatch but could be specific to certain types of discrepancies between training and testing conditions or specific model configurations. It's important to note that the exact nature and circumstances of this &quot;point three percent hit&quot; aren't explicitly detailed in the transcript, so further investigation would be required to provide a more precise answer." />
    <node id=" take a hit .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: i In some cases it might be u better to have a mismatch . Like I think I saw something like {disfmarker} like if you only have two seconds in test , or , um , maybe it was something like four seconds , you actually do a little better if you , um , {vocalsound} train on six seconds than if you train on four seconds .&#10;Speaker: Professor D&#10;Content: Yeah . Right .&#10;Speaker: Grad C&#10;Content: Um , but the case , uh {disfmarker} with the point three percent hit was {vocalsound} using six seconds in test , um , comparing train on twelve seconds {comment} versus train on six seconds .&#10;Speaker: Professor D&#10;Content: And which was worse ?&#10;Speaker: Grad C&#10;Content: The train on twelve seconds .&#10;Speaker: Professor D&#10;Content: OK . But point three percent , uh , w from what to what ? That 's point three percent {disfmarker}&#10;Speaker: Grad C&#10;Content: On {disfmarker} The {disfmark" />
    <node id="isfmarker} we changed the proto for initializing the HMM {disfmarker} I mean , this {disfmarker} this is basically because it gets stuck in some local minimum in the training . That seventy - five point seven nine in the Finnish mismatch which is that {disfmarker} the eleven point nine six what we see .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: So we have to jiggle it somehow ?&#10;Speaker: PhD B&#10;Content: Yeah {disfmarker} so we start with that different proto and it becomes eighty - eight , which is like some fifty percent improvement .&#10;Speaker: Professor D&#10;Content: S Wait a minute . Start with a different what ?&#10;Speaker: PhD B&#10;Content: Different prototype , which is like a different initialization for the , uh , s transition probabilities . It 's just that right now , the initialization is to stay more in the current state , which is point four point six , right ? Yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Spe" />
    <node id=" out what 's the optimal way to set this up . So , um , {vocalsound} I 'll try to make the plots and then put some postscript up on my {disfmarker} on my web page . And I 'll mention it in my status report if people wanna take a look .&#10;Speaker: Professor D&#10;Content: You could clarify something for me . You 're saying point three percent , you take a point three percent hit , {vocalsound} when the training and testing links are {disfmarker} don't match or something ?&#10;Speaker: PhD E&#10;Content: Hello .&#10;Speaker: Professor D&#10;Content: Is that what it is ?&#10;Speaker: Grad C&#10;Content: w Well , it c&#10;Speaker: Professor D&#10;Content: Or {disfmarker} ?&#10;Speaker: Grad C&#10;Content: I {disfmarker} I don't think it {disfmarker} it 's {vocalsound} just for any mismatch {vocalsound} you take a hit .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: i In some cases it might be u" />
    <node id=" 's point three percent {disfmarker}&#10;Speaker: Grad C&#10;Content: On {disfmarker} The {disfmarker} the {disfmarker} the accuracies {vocalsound} w went from {disfmarker} it was something vaguely like ninety - five point six accuracy , um , improved to ninety - five point nine wh when I {disfmarker}&#10;Speaker: Professor D&#10;Content: So four point four to four point one .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: So {disfmarker} yeah . So about a {disfmarker} about an eight percent , uh , seven or eight percent relative ?&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: Uh , Yeah . Well , I think in a p You know , if {disfmarker} if you were going for an evaluation system you 'd care . But if you were doing a live system that people were actually using nobody would notice . It 's {disfmarker} uh , I think the thing is to get something that 's practical , that {disf" />
    <node id=" Yeah .&#10;Speaker: Professor D&#10;Content: so I 'm confused .&#10;Speaker: PhD B&#10;Content: Uh , actually the noise compensation whatever , uh , we are put in it works very well for the high mismatch condition . I mean , it 's consistent in the SpeechDat - Car and in the clean training also it gives it {disfmarker} But this fifty percent is {disfmarker} is that the {disfmarker} the high mismatch performance {disfmarker} equivalent to the high mismatch performance in the speech .&#10;Speaker: PhD F&#10;Content: So n s So since the high mismatch performance is much worse to begin with , it 's easier to get a better relative improvement .&#10;Speaker: PhD B&#10;Content: Yeah . Yeah . I do . Yeah , yeah . So by putting this noise {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah . Yeah , if we look at the figures on the right , we see that the reference system is very bad .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: Yeah . The reference drops like a very fast {disfmarker}&#10;Speaker" />
    <node id="1. The speaker, Grad A, is proposing a new approach to speech recognition that combines multi-band ideas with an acoustic-phonetic approach. This approach aims to create a more robust system for word or phone recognition by developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. The new approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features closely related to the acoustic signal itself, within the context of a multi-band model. The hope is that by going multi-band and using these intermediate classifications, the system will be more robust in unseen noisy situations.&#10;3. Grad A plans to investigate several research questions, including what types of intermediate categories are needed for classification, the structure of multi-band graphical models (specifically, what types of structures should be considered to effectively combine evidence from sub-bands), and how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. This new approach is motivated by the need to address real-world noisy situations more accurately, potentially through the introduction of a &quot;natural distribution&quot; of noise rather than simply adding constant or flat noise across all regions. This could lead to a decrease in word error rate and better performance in signal processing." />
    <node id="1. The best performance for the &quot;Car&quot; category was achieved when using an LDA filter trained with clean speech (TI-digits). This resulted in better signal processing performance compared to a filter trained with noisy speech.&#10;2. Although this specific conversation focuses on the improved performance of the clean LDA filter for TI-digits, it is essential to consider that different contexts or datasets might require different LDA filter designs, including those based on noisy speech.&#10;3. In this case, the &quot;Car&quot; category had the best performance in the third column of condition A, but specific numerical values or metrics were not provided in the conversation." />
    <node id=" PhD E&#10;Content: Yeah . Uh , ppp . I don't know , you have questions about that , or suggestions ?&#10;Speaker: PhD B&#10;Content: Mmm . S so {disfmarker}&#10;Speaker: PhD E&#10;Content: It seems {disfmarker} the performance seems worse in Finnish , which {disfmarker}&#10;Speaker: PhD B&#10;Content: Well , it 's not trained on Finnish .&#10;Speaker: PhD E&#10;Content: uh {disfmarker}&#10;Speaker: PhD H&#10;Content: It 's worse .&#10;Speaker: PhD E&#10;Content: It 's not trained on Finnish , yeah .&#10;Speaker: Professor D&#10;Content: What 's it trained on ?&#10;Speaker: PhD B&#10;Content: I mean , the MLP 's not trained on Finnish .&#10;Speaker: Professor D&#10;Content: Right , what 's it trained on ?&#10;Speaker: PhD B&#10;Content: Oh {disfmarker} oh . Sorry . Uh , it 's Italian TI - digits .&#10;Speaker: Professor D&#10;Content: Yeah . Oh , it 's trained on Italian ?&#10;Speaker: PhD" />
    <node id="1. The speakers discussed introducing a &quot;natural distribution&quot; of noise to the audio data analyzed in the SmartKom project's ASRU paper. This approach aims to make the audio data resemble real-world noisy situations more closely by reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;&#10;2. Two research questions arise from this discussion:&#10;  a. Does this natural distribution approach still work effectively even if the speakers are involved?&#10;  b. This question is cut off in the transcript but presumably relates to testing the effectiveness of the natural distribution approach under different conditions or with various types of noise." />
    <node id=" nobody would notice . It 's {disfmarker} uh , I think the thing is to get something that 's practical , that {disfmarker} that you could really use .&#10;Speaker: Grad C&#10;Content: Huh . That 's {disfmarker} that 's interesting . Alright , the e uh , I see your point . I guess I was thinking of it as , um , {vocalsound} an interesting research problem . The {disfmarker} how to g I was thinking that for the ASRU paper we could have a section saying , {vocalsound} &quot; For SmartKom , we {disfmarker} we d in {disfmarker} we tried this approach in , uh , {vocalsound} interactive system &quot; , which I don't think has been done before .&#10;Speaker: Professor D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Grad C&#10;Content: And {disfmarker} and then there was two research questions from that .&#10;Speaker: Professor D&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: And one is the k does it still work if" />
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target=" Mike Shire in his thesis um , {vocalsound} did a {disfmarker} a series of experiments , um , training LDA filters in d on different conditions . And you were interested in having me repeat this for {disfmarker} for this mean subtraction approach ? Is {disfmarker} is that right ? Or for these long analysis windows , I guess , is the right way to put it .&#10;Speaker: Professor D&#10;Content: I guess , the {disfmarker} the {disfmarker} the issue I was {disfmarker} the general issue I was bringing up was that if you 're {disfmarker} have a moving {disfmarker} {vocalsound} moving window , uh , a wa a {disfmarker} a set of weights times things that , uh , move along , shift along in time , that you have in fact a linear time invariant filter . And you just happened to have picked a particular one by setting all the weights to be equal . And so the issue is what are some other filters that you could use , uh , in that sense of &quot; filter &quot; ?&#10;Speaker: Grad C&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target=" the issue is what are some other filters that you could use , uh , in that sense of &quot; filter &quot; ?&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: And , um , as I was saying , I think the simplest thing to do is not to train anything , but just to do some sort of , uh , uh , hamming or Hanning , uh , kind of window , kind of thing ,&#10;Speaker: Grad C&#10;Content: Right . Mm - hmm .&#10;Speaker: Professor D&#10;Content: just sort of to de - emphasize the jarring . So I think that would sort of be the first thing to do . But then , yeah , the LDA i uh , is interesting because it would sort of say well , suppose you actually trained this up to do the best you could by some criterion , what would the filter look like then ?&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: Professor D&#10;Content: Uh , and , um , that 's sort of what we 're doing in this Aur - Aurora stuff . And , uh , it 's still not clear to me in the long run whether the best">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target=" I can complete that s like this . Well .&#10;Speaker: Professor D&#10;Content: Uh . Right .&#10;Speaker: PhD H&#10;Content: One thing that I {comment} note are not here in this result {vocalsound} but are speak {disfmarker} are spoken before with Sunil I {disfmarker} I improve my result using clean LDA filter .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: Mm - hmm .&#10;Speaker: PhD H&#10;Content: If I use , {vocalsound} eh , the LDA filter that are training with the noisy speech , {vocalsound} that hurts the res my results .&#10;Speaker: Professor D&#10;Content: So what are these numbers here ? Are these with the clean or with the noisy ?&#10;Speaker: PhD H&#10;Content: This is with the clean .&#10;Speaker: Professor D&#10;Content: OK .&#10;Speaker: PhD H&#10;Content: With the noise I have worse result , that if I doesn't use it .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD H&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target=" you think it 's worth looking into .&#10;Speaker: Professor D&#10;Content: You could imagine that .&#10;Speaker: Grad C&#10;Content: I mean , it {disfmarker} it is getting a little away from reverberation .&#10;Speaker: Professor D&#10;Content: Um , yeah . It 's just that you 're making a choice {disfmarker} uh , I was thinking more from the system aspect , if you 're making a choice for SmartKom , that {disfmarker} that {disfmarker} that it might be that it 's {disfmarker} it c the optimal number could be different , depending on {disfmarker}&#10;Speaker: Grad C&#10;Content: Yeah . Right .&#10;Speaker: Professor D&#10;Content: Could be . I don't know .&#10;Speaker: Grad C&#10;Content: And {disfmarker} and th the third thing , um , uh , is , um , Barry explained LDA filtering to me yesterday . And so , um , Mike Shire in his thesis um , {vocalsound} did a {disfmarker} a series of experiments , um , training LDA filters">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target="Content: Yeah .&#10;Speaker: Professor G&#10;Content: And it was surprising {disfmarker} At the beginning it was not surprising to me that you get really the best results on doing it this way , I mean , in comparison to any type of training on clean data and any type of processing . But it was {disfmarker} So , u u it {disfmarker} it seems to be the best what {disfmarker} wh wh what {disfmarker} what we can do in this moment is multi - condition training . And every when we now start introducing some {disfmarker} some noise reduction technique we {disfmarker} we introduce also somehow artificial distortions .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And these artificial distortions {disfmarker} uh , I have the feeling that they are the reason why {disfmarker} why we have the problems in this multi - condition training . That means the H M Ms we trained , they are {disfmarker} they are based on Gaussians ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target="isfmarker} they are based on Gaussians ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: and on modeling Gaussians . And if you {disfmarker} Can I move a little bit with this ? Yeah . And if we introduce now this {disfmarker} this u spectral subtraction , or Wiener filtering stuff {disfmarker} So , usually what you have is maybe , um {disfmarker} I 'm {disfmarker} I 'm showing now an envelope um maybe you 'll {disfmarker} f for this time . So usually you have {disfmarker} maybe in clean condition you have something which looks like this . And if it is noisy it is somewhere here . And then you try to subtract it or Wiener filter or whatever . And what you get is you have always these problems , that you have this {disfmarker} these {disfmarker} these {disfmarker} these zeros in there .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And you have to do something if you get these negative values .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target="1. Moving Average Filter: This is a simple finite impulse response (FIR) filter where each output value is the average of the previous 'n' input values. It can be used to reduce high frequency noise and fluctuations in signals.&#10;2. Median Filter: In this non-linear digital filter, the output value is the median of the input values within the moving window. This filter is useful for removing impulse noise, as it preserves edges better than the mean filter.&#10;3. Exponential (First Order IIR) Filter: This filter calculates the output based on a combination of the current and previous input values, as well as the previous output value. The weights in the filter decrease exponentially over time, making it effective for smoothing signals with slowly changing trends.&#10;4. Butterworth Filter: A type of infinite impulse response (IIR) filter, the Butterworth filter is designed to have a maximally flat frequency response in the passband and minimum attenuation in the stopband. This filter can be used in applications where it's essential to preserve signal shape while removing noise.&#10;5. Chebyshev Filter: Similar to the Butterworth filter, the Chebyshev filter offers a better stopband rejection at the expense of a more uneven passband response. These filters are useful when there is a need for high attenuation in the stopband while maintaining reasonable performance in the passband.&#10;6. Bessel Filter: This filter has a maximally flat group delay (the time difference between input and output signal) in the passband, which helps to preserve signals' wave shape. However, its roll-off rate is slower than Butterworth or Chebyshev filters.&#10;7. LDA Filters: As mentioned in the transcript, Linear Discriminant Analysis (LDA) filters can be trained using supervised learning techniques to optimize a certain criterion, such as minimizing within-class variance or maximizing between-class variance. This filter can help better separate different classes and improve overall performance in signal processing tasks.&#10;&#10;When using LDA for determining the optimal filter based on a criterion, it is essential to consider what specific aspect of the filter design you want to optimize. For instance, you could train an LDA filter to minimize the mean squared error between the original and filtered signals or maximize the signal-to-noise ratio.&#10;&#10;To implement this in practice, you would collect a dataset with input-output pairs representing your desired output signal (clean speech) and noisy input (speech affected by reverberation, background noise, etc.). Then, train an LDA filter using that data, evaluating its performance based on the chosen criterion. Once trained, this LDA filter could be used to process new, noisy signals in order to provide a filtered output signal that meets your desired criteria.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target="Based on the transcript provided, it appears that there is a difference in performance when using an LDA filter trained with noisy speech versus one trained with clean speech. Specifically, in this context, PhD H mentions that &quot;with the noise I have worse result, that if I doesn't use it&quot; and &quot;it matters, uh, a lot on TI - digits.&quot; Later in the conversation, it is clarified that the results being discussed are associated with the clean LDA filter, rather than the noisy one.&#10;&#10;This suggests that, in this particular application, using an LDA filter trained with clean speech leads to better results compared to using a filter trained with noisy speech. This may be due to the fact that, as PhD H notes, &quot;with this technique we are using really clean speech&quot; and &quot;the representation that goes to the HTK is really clean speech because it's from the dictionary, the code book and maybe from that.&quot; Therefore, a filter trained with such high-quality data may be more effective at improving signal processing performance.&#10;&#10;It's also worth noting that there might be specific cases or datasets where an LDA filter trained with noisy speech could lead to better results, as PhD E mentions &quot;it doesn't matter too much on SpeechDat - Car, but it matters, uh, a lot on TI - digits.&quot; However, in the context of this conversation, the consensus seems to be that using an LDA filter trained with clean speech is more beneficial.&#10;&#10;To summarize, using an LDA filter trained with clean speech in this specific application yields better results compared to using a filter trained with noisy speech. This may be due to the high-quality data used for training the filter, which helps improve signal processing performance in this case. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target="1. Fine-tuning ten or twelve small aspects, each contributing around 0.3%, can significantly improve a system's accuracy from 95% to 9">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target="1. The process of smoothing the gain in this context involves using a non-linear smoothing technique for low gain values, while not applying any smoothing for high gain values. This approach is taken to balance the need for noise reduction with the potential distortion or loss of signal information that can result from aggressive smoothing.&#10;2. The subtraction factor or algorithm's gain is being smoothed in this case, as mentioned by Professor G. This suggests that the group is trying to improve the accuracy and stability of the noise reduction technique by reducing the variability of the gain value over time.&#10;3. The decision on whether to smooth or not, and the choice of smoothing method, depends on various factors such as the desired latency, the characteristics of the signal and noise, the performance trade-offs between different techniques, and the specific requirements of the application.&#10;4. The group's discussion suggests that they are still experimenting with different approaches to smoothing the gain, as there is no consensus on whether further constraining the latency would be beneficial or not. This indicates that the process involves ongoing evaluation and adjustment based on empirical results and theoretical considerations.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target="1. Hamming or Hanning Window: These window functions can be used to reduce the weight of values at the edges when applying a moving window to a signal. They decrease the impact of abrupt changes between windows and help smooth the signal, which can make the training process more stable. In the context of the &quot;Aurora stuff,&quot; these window functions could be applied when processing audio data as part of a speech recognition system.&#10;2. LDA Filter (Linear Discriminant Analysis filter): The LDA filter is a supervised learning algorithm that can optimize some criterion, such as minimizing within-class variance or maximizing between-class variance. This results in a filter that better separates different classes and improves the performance of the system. In the context of the &quot;Aurora stuff,&quot; an LDA filter could be trained on labeled data to improve speech recognition accuracy.&#10;3. Spectral Subtraction or Wiener Filtering: These techniques reduce noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum, improving the signal-to-noise ratio and making it easier to extract useful information from noisy data. In the context of the &quot;Aurora stuff,&quot; these methods could be applied when processing audio signals to improve speech recognition performance.&#10;&#10;These filters, along with a uniform window, are options for moving window setups in training LDA filters or other signal processing tasks. The choice of filter will depend on the specific application and the characteristics of the data being processed. In the transcript, it is mentioned that spectral subtraction was not effective for certain Aurora tasks, suggesting that a different filter or approach might be needed.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target="1. The advantage of using a fixed filter, such as the RASTA filter, is that it remains unchanged and does not degrade over time, making it a reliable choice for signal processing. This is because it does not need to be retrained or adapted for different conditions, unlike trained filters.&#10;2. In the context of the SmartKom system, using a fixed amount of data for mean subtraction can be beneficial when less than twelve seconds of data are available. By training the models with a fixed amount of data (e.g., six or twelve seconds), the system can ensure stability and consistency in the processing steps that follow.&#10;3. However, if there is not enough data available to estimate a reliable mean value for mean subtraction, concatenating utterances together can help increase the amount of available data. This approach may be more suitable than training a filter specifically for certain conditions, as it allows the system to use all the available data and avoid potential overfitting or poor generalization issues that can arise with trained filters.&#10;4. Overall, using a fixed filter or a fixed amount of data for mean subtraction provides a stable and consistent processing approach, while concatenating utterances together can help increase the amount of available data when there is less than twelve seconds of data in the SmartKom system.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These are types of window functions that can be applied to a moving window to reduce the weight of values at the edges and decrease the impact of abrupt changes between windows. They can help to smooth the signal and make the training process more stable.&#10;2. LDA Filter: The Linear Discriminant Analysis filter is a type of supervised learning algorithm that can be trained to optimize some criterion, such as minimizing the within-class variance or maximizing the between-class variance. This can result in a filter that better separates different classes and improves the performance of the system.&#10;3. Spectral Subtraction or Wiener Filtering: These are techniques for reducing noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum. They can help to improve the signal-to-noise ratio and make it easier to extract useful information from noisy data.&#10;&#10;These filters, along with the one where all the weights are equal (often referred to as a rectangular or uniform window), are some of the options that can be used in a moving window setup for training LDA filters. The choice of filter will depend on the specific application and the characteristics of the data being processed." target="1. Cepstral Mean Subtraction (CMS) is a technique used in short-term window analysis to remove general characteristics from data. It involves calculating the mean of the cepstrum (a transform of the log power spectrum) for each window and subtracting it from the cepstrum of that window. This helps get rid of very general characteristics, such as those caused by the recording device or room acoustics.&#10;2. Flipping frames from the front to create negative values is a technique used in some signal processing applications, including CMS. By taking a certain number of frames from the beginning of the signal and flipping them around, the original frames are replaced with their mirror images. This can help balance the signal by reducing the impact of any abrupt changes at the beginning of the window. In the context of CMS, flipping frames creates negative values in the cepstrum that are then subtracted from the original cepstrum to achieve mean subtraction.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Mike Shire in his thesis um , {vocalsound} did a {disfmarker} a series of experiments , um , training LDA filters in d on different conditions . And you were interested in having me repeat this for {disfmarker} for this mean subtraction approach ? Is {disfmarker} is that right ? Or for these long analysis windows , I guess , is the right way to put it .&#10;Speaker: Professor D&#10;Content: I guess , the {disfmarker} the {disfmarker} the issue I was {disfmarker} the general issue I was bringing up was that if you 're {disfmarker} have a moving {disfmarker} {vocalsound} moving window , uh , a wa a {disfmarker} a set of weights times things that , uh , move along , shift along in time , that you have in fact a linear time invariant filter . And you just happened to have picked a particular one by setting all the weights to be equal . And so the issue is what are some other filters that you could use , uh , in that sense of &quot; filter &quot; ?&#10;Speaker: Grad C&#10;Content:" target="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Mike Shire in his thesis um , {vocalsound} did a {disfmarker} a series of experiments , um , training LDA filters in d on different conditions . And you were interested in having me repeat this for {disfmarker} for this mean subtraction approach ? Is {disfmarker} is that right ? Or for these long analysis windows , I guess , is the right way to put it .&#10;Speaker: Professor D&#10;Content: I guess , the {disfmarker} the {disfmarker} the issue I was {disfmarker} the general issue I was bringing up was that if you 're {disfmarker} have a moving {disfmarker} {vocalsound} moving window , uh , a wa a {disfmarker} a set of weights times things that , uh , move along , shift along in time , that you have in fact a linear time invariant filter . And you just happened to have picked a particular one by setting all the weights to be equal . And so the issue is what are some other filters that you could use , uh , in that sense of &quot; filter &quot; ?&#10;Speaker: Grad C&#10;Content:" target="1. Moving Average Filter: This is a simple finite impulse response (FIR) filter where each output value is the average of the previous 'n' input values. It can be used to reduce high frequency noise and fluctuations in signals.&#10;2. Median Filter: In this non-linear digital filter, the output value is the median of the input values within the moving window. This filter is useful for removing impulse noise, as it preserves edges better than the mean filter.&#10;3. Exponential (First Order IIR) Filter: This filter calculates the output based on a combination of the current and previous input values, as well as the previous output value. The weights in the filter decrease exponentially over time, making it effective for smoothing signals with slowly changing trends.&#10;4. Butterworth Filter: A type of infinite impulse response (IIR) filter, the Butterworth filter is designed to have a maximally flat frequency response in the passband and minimum attenuation in the stopband. This filter can be used in applications where it's essential to preserve signal shape while removing noise.&#10;5. Chebyshev Filter: Similar to the Butterworth filter, the Chebyshev filter offers a better stopband rejection at the expense of a more uneven passband response. These filters are useful when there is a need for high attenuation in the stopband while maintaining reasonable performance in the passband.&#10;6. Bessel Filter: This filter has a maximally flat group delay (the time difference between input and output signal) in the passband, which helps to preserve signals' wave shape. However, its roll-off rate is slower than Butterworth or Chebyshev filters.&#10;7. LDA Filters: As mentioned in the transcript, Linear Discriminant Analysis (LDA) filters can be trained using supervised learning techniques to optimize a certain criterion, such as minimizing within-class variance or maximizing between-class variance. This filter can help better separate different classes and improve overall performance in signal processing tasks.&#10;&#10;When using LDA for determining the optimal filter based on a criterion, it is essential to consider what specific aspect of the filter design you want to optimize. For instance, you could train an LDA filter to minimize the mean squared error between the original and filtered signals or maximize the signal-to-noise ratio.&#10;&#10;To implement this in practice, you would collect a dataset with input-output pairs representing your desired output signal (clean speech) and noisy input (speech affected by reverberation, background noise, etc.). Then, train an LDA filter using that data, evaluating its performance based on the chosen criterion. Once trained, this LDA filter could be used to process new, noisy signals in order to provide a filtered output signal that meets your desired criteria.">
      <data key="d0">1</data>
    </edge>
    <edge source=" the issue is what are some other filters that you could use , uh , in that sense of &quot; filter &quot; ?&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: And , um , as I was saying , I think the simplest thing to do is not to train anything , but just to do some sort of , uh , uh , hamming or Hanning , uh , kind of window , kind of thing ,&#10;Speaker: Grad C&#10;Content: Right . Mm - hmm .&#10;Speaker: Professor D&#10;Content: just sort of to de - emphasize the jarring . So I think that would sort of be the first thing to do . But then , yeah , the LDA i uh , is interesting because it would sort of say well , suppose you actually trained this up to do the best you could by some criterion , what would the filter look like then ?&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: Professor D&#10;Content: Uh , and , um , that 's sort of what we 're doing in this Aur - Aurora stuff . And , uh , it 's still not clear to me in the long run whether the best" target="1. Moving Average Filter: This is a simple finite impulse response (FIR) filter where each output value is the average of the previous 'n' input values. It can be used to reduce high frequency noise and fluctuations in signals.&#10;2. Median Filter: In this non-linear digital filter, the output value is the median of the input values within the moving window. This filter is useful for removing impulse noise, as it preserves edges better than the mean filter.&#10;3. Exponential (First Order IIR) Filter: This filter calculates the output based on a combination of the current and previous input values, as well as the previous output value. The weights in the filter decrease exponentially over time, making it effective for smoothing signals with slowly changing trends.&#10;4. Butterworth Filter: A type of infinite impulse response (IIR) filter, the Butterworth filter is designed to have a maximally flat frequency response in the passband and minimum attenuation in the stopband. This filter can be used in applications where it's essential to preserve signal shape while removing noise.&#10;5. Chebyshev Filter: Similar to the Butterworth filter, the Chebyshev filter offers a better stopband rejection at the expense of a more uneven passband response. These filters are useful when there is a need for high attenuation in the stopband while maintaining reasonable performance in the passband.&#10;6. Bessel Filter: This filter has a maximally flat group delay (the time difference between input and output signal) in the passband, which helps to preserve signals' wave shape. However, its roll-off rate is slower than Butterworth or Chebyshev filters.&#10;7. LDA Filters: As mentioned in the transcript, Linear Discriminant Analysis (LDA) filters can be trained using supervised learning techniques to optimize a certain criterion, such as minimizing within-class variance or maximizing between-class variance. This filter can help better separate different classes and improve overall performance in signal processing tasks.&#10;&#10;When using LDA for determining the optimal filter based on a criterion, it is essential to consider what specific aspect of the filter design you want to optimize. For instance, you could train an LDA filter to minimize the mean squared error between the original and filtered signals or maximize the signal-to-noise ratio.&#10;&#10;To implement this in practice, you would collect a dataset with input-output pairs representing your desired output signal (clean speech) and noisy input (speech affected by reverberation, background noise, etc.). Then, train an LDA filter using that data, evaluating its performance based on the chosen criterion. Once trained, this LDA filter could be used to process new, noisy signals in order to provide a filtered output signal that meets your desired criteria.">
      <data key="d0">1</data>
    </edge>
    <edge source=" the issue is what are some other filters that you could use , uh , in that sense of &quot; filter &quot; ?&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: And , um , as I was saying , I think the simplest thing to do is not to train anything , but just to do some sort of , uh , uh , hamming or Hanning , uh , kind of window , kind of thing ,&#10;Speaker: Grad C&#10;Content: Right . Mm - hmm .&#10;Speaker: Professor D&#10;Content: just sort of to de - emphasize the jarring . So I think that would sort of be the first thing to do . But then , yeah , the LDA i uh , is interesting because it would sort of say well , suppose you actually trained this up to do the best you could by some criterion , what would the filter look like then ?&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: Professor D&#10;Content: Uh , and , um , that 's sort of what we 're doing in this Aur - Aurora stuff . And , uh , it 's still not clear to me in the long run whether the best" target="1. Hamming or Hanning Window: These window functions can be used to reduce the weight of values at the edges when applying a moving window to a signal. They decrease the impact of abrupt changes between windows and help smooth the signal, which can make the training process more stable. In the context of the &quot;Aurora stuff,&quot; these window functions could be applied when processing audio data as part of a speech recognition system.&#10;2. LDA Filter (Linear Discriminant Analysis filter): The LDA filter is a supervised learning algorithm that can optimize some criterion, such as minimizing within-class variance or maximizing between-class variance. This results in a filter that better separates different classes and improves the performance of the system. In the context of the &quot;Aurora stuff,&quot; an LDA filter could be trained on labeled data to improve speech recognition accuracy.&#10;3. Spectral Subtraction or Wiener Filtering: These techniques reduce noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum, improving the signal-to-noise ratio and making it easier to extract useful information from noisy data. In the context of the &quot;Aurora stuff,&quot; these methods could be applied when processing audio signals to improve speech recognition performance.&#10;&#10;These filters, along with a uniform window, are options for moving window setups in training LDA filters or other signal processing tasks. The choice of filter will depend on the specific application and the characteristics of the data being processed. In the transcript, it is mentioned that spectral subtraction was not effective for certain Aurora tasks, suggesting that a different filter or approach might be needed.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I can complete that s like this . Well .&#10;Speaker: Professor D&#10;Content: Uh . Right .&#10;Speaker: PhD H&#10;Content: One thing that I {comment} note are not here in this result {vocalsound} but are speak {disfmarker} are spoken before with Sunil I {disfmarker} I improve my result using clean LDA filter .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: Mm - hmm .&#10;Speaker: PhD H&#10;Content: If I use , {vocalsound} eh , the LDA filter that are training with the noisy speech , {vocalsound} that hurts the res my results .&#10;Speaker: Professor D&#10;Content: So what are these numbers here ? Are these with the clean or with the noisy ?&#10;Speaker: PhD H&#10;Content: This is with the clean .&#10;Speaker: Professor D&#10;Content: OK .&#10;Speaker: PhD H&#10;Content: With the noise I have worse result , that if I doesn't use it .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD H&#10;Content:" target="Based on the transcript provided, it appears that there is a difference in performance when using an LDA filter trained with noisy speech versus one trained with clean speech. Specifically, in this context, PhD H mentions that &quot;with the noise I have worse result, that if I doesn't use it&quot; and &quot;it matters, uh, a lot on TI - digits.&quot; Later in the conversation, it is clarified that the results being discussed are associated with the clean LDA filter, rather than the noisy one.&#10;&#10;This suggests that, in this particular application, using an LDA filter trained with clean speech leads to better results compared to using a filter trained with noisy speech. This may be due to the fact that, as PhD H notes, &quot;with this technique we are using really clean speech&quot; and &quot;the representation that goes to the HTK is really clean speech because it's from the dictionary, the code book and maybe from that.&quot; Therefore, a filter trained with such high-quality data may be more effective at improving signal processing performance.&#10;&#10;It's also worth noting that there might be specific cases or datasets where an LDA filter trained with noisy speech could lead to better results, as PhD E mentions &quot;it doesn't matter too much on SpeechDat - Car, but it matters, uh, a lot on TI - digits.&quot; However, in the context of this conversation, the consensus seems to be that using an LDA filter trained with clean speech is more beneficial.&#10;&#10;To summarize, using an LDA filter trained with clean speech in this specific application yields better results compared to using a filter trained with noisy speech. This may be due to the high-quality data used for training the filter, which helps improve signal processing performance in this case. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.">
      <data key="d0">1</data>
    </edge>
    <edge source=" you think it 's worth looking into .&#10;Speaker: Professor D&#10;Content: You could imagine that .&#10;Speaker: Grad C&#10;Content: I mean , it {disfmarker} it is getting a little away from reverberation .&#10;Speaker: Professor D&#10;Content: Um , yeah . It 's just that you 're making a choice {disfmarker} uh , I was thinking more from the system aspect , if you 're making a choice for SmartKom , that {disfmarker} that {disfmarker} that it might be that it 's {disfmarker} it c the optimal number could be different , depending on {disfmarker}&#10;Speaker: Grad C&#10;Content: Yeah . Right .&#10;Speaker: Professor D&#10;Content: Could be . I don't know .&#10;Speaker: Grad C&#10;Content: And {disfmarker} and th the third thing , um , uh , is , um , Barry explained LDA filtering to me yesterday . And so , um , Mike Shire in his thesis um , {vocalsound} did a {disfmarker} a series of experiments , um , training LDA filters" target="1. Moving Average Filter: This is a simple finite impulse response (FIR) filter where each output value is the average of the previous 'n' input values. It can be used to reduce high frequency noise and fluctuations in signals.&#10;2. Median Filter: In this non-linear digital filter, the output value is the median of the input values within the moving window. This filter is useful for removing impulse noise, as it preserves edges better than the mean filter.&#10;3. Exponential (First Order IIR) Filter: This filter calculates the output based on a combination of the current and previous input values, as well as the previous output value. The weights in the filter decrease exponentially over time, making it effective for smoothing signals with slowly changing trends.&#10;4. Butterworth Filter: A type of infinite impulse response (IIR) filter, the Butterworth filter is designed to have a maximally flat frequency response in the passband and minimum attenuation in the stopband. This filter can be used in applications where it's essential to preserve signal shape while removing noise.&#10;5. Chebyshev Filter: Similar to the Butterworth filter, the Chebyshev filter offers a better stopband rejection at the expense of a more uneven passband response. These filters are useful when there is a need for high attenuation in the stopband while maintaining reasonable performance in the passband.&#10;6. Bessel Filter: This filter has a maximally flat group delay (the time difference between input and output signal) in the passband, which helps to preserve signals' wave shape. However, its roll-off rate is slower than Butterworth or Chebyshev filters.&#10;7. LDA Filters: As mentioned in the transcript, Linear Discriminant Analysis (LDA) filters can be trained using supervised learning techniques to optimize a certain criterion, such as minimizing within-class variance or maximizing between-class variance. This filter can help better separate different classes and improve overall performance in signal processing tasks.&#10;&#10;When using LDA for determining the optimal filter based on a criterion, it is essential to consider what specific aspect of the filter design you want to optimize. For instance, you could train an LDA filter to minimize the mean squared error between the original and filtered signals or maximize the signal-to-noise ratio.&#10;&#10;To implement this in practice, you would collect a dataset with input-output pairs representing your desired output signal (clean speech) and noisy input (speech affected by reverberation, background noise, etc.). Then, train an LDA filter using that data, evaluating its performance based on the chosen criterion. Once trained, this LDA filter could be used to process new, noisy signals in order to provide a filtered output signal that meets your desired criteria.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker} they are based on Gaussians ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: and on modeling Gaussians . And if you {disfmarker} Can I move a little bit with this ? Yeah . And if we introduce now this {disfmarker} this u spectral subtraction , or Wiener filtering stuff {disfmarker} So , usually what you have is maybe , um {disfmarker} I 'm {disfmarker} I 'm showing now an envelope um maybe you 'll {disfmarker} f for this time . So usually you have {disfmarker} maybe in clean condition you have something which looks like this . And if it is noisy it is somewhere here . And then you try to subtract it or Wiener filter or whatever . And what you get is you have always these problems , that you have this {disfmarker} these {disfmarker} these {disfmarker} these zeros in there .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And you have to do something if you get these negative values ." target="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise." target="disfmarker}&#10;Speaker: Professor D&#10;Content: Right at the point where you 've done the subtraction .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: Um , essentially you 're adding a constant into everything .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: But the way Stephane did it , it is exactly the way I have implemented in the phone , so .&#10;Speaker: Professor D&#10;Content: Oh , yeah , better do it different , then . Yeah .&#10;Speaker: PhD E&#10;Content: Um .&#10;Speaker: Professor D&#10;Content: Just you {disfmarker} you just ta you just set it for a particular signal - to - noise ratio that you want ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Yeah I {disfmarker} I made s similar investigations like Stephane did here , just uh , adding this constant and {disfmarker} and looking how dependent is it on the value of the constant&#10;Speaker: Professor D&#10;Content: Yeah . Yeah .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise." target=" Professor G&#10;Content: And , uh , I {disfmarker} I found the same problem . Just taking um , what we were used to u {vocalsound} use , I mean , uh , some type of spectral subtraction , {comment} y {vocalsound} you get even worse results than {vocalsound} the basis&#10;Speaker: PhD B&#10;Content: Yeah . Yeah ,&#10;Speaker: Professor G&#10;Content: and uh {disfmarker}&#10;Speaker: PhD B&#10;Content: yeah .&#10;Speaker: Professor G&#10;Content: I {disfmarker} I tried to find an explanation for it ,&#10;Speaker: Professor D&#10;Content: Mmm .&#10;Speaker: Professor G&#10;Content: so {disfmarker}&#10;Speaker: PhD B&#10;Content: So . Yes . Stephane also has the same experience of using the spectral subtraction right ?&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Yeah . So here {disfmarker} here I mean , I found that it 's {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise." target=" D&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Th - That 's true . Yeah {disfmarker} the c the models are not complex enough to absorb that additional variability that you 're introducing .&#10;Speaker: Professor G&#10;Content: s&#10;Speaker: PhD F&#10;Content: Thanks Adam .&#10;Speaker: Professor G&#10;Content: Yeah . Yes .&#10;Speaker: PhD B&#10;Content: Well , that 's {disfmarker} Yeah . So {disfmarker}&#10;Speaker: PhD E&#10;Content: I also have the feeling that um , the reason ye why it doesn't work is {disfmarker} yeah , that the models are much {disfmarker} are t um , not complex enough . Because I {disfmarker} actually I als always had a good experience with spectral subtraction , just a straight spectral subtraction algorithm when I was using neural networks , big neural networks , which maybe are more able to model strange distributions and {disfmarker}&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: But {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise." target="1. Moving Average Filter: This is a simple finite impulse response (FIR) filter where each output value is the average of the previous 'n' input values. It can be used to reduce high frequency noise and fluctuations in signals.&#10;2. Median Filter: In this non-linear digital filter, the output value is the median of the input values within the moving window. This filter is useful for removing impulse noise, as it preserves edges better than the mean filter.&#10;3. Exponential (First Order IIR) Filter: This filter calculates the output based on a combination of the current and previous input values, as well as the previous output value. The weights in the filter decrease exponentially over time, making it effective for smoothing signals with slowly changing trends.&#10;4. Butterworth Filter: A type of infinite impulse response (IIR) filter, the Butterworth filter is designed to have a maximally flat frequency response in the passband and minimum attenuation in the stopband. This filter can be used in applications where it's essential to preserve signal shape while removing noise.&#10;5. Chebyshev Filter: Similar to the Butterworth filter, the Chebyshev filter offers a better stopband rejection at the expense of a more uneven passband response. These filters are useful when there is a need for high attenuation in the stopband while maintaining reasonable performance in the passband.&#10;6. Bessel Filter: This filter has a maximally flat group delay (the time difference between input and output signal) in the passband, which helps to preserve signals' wave shape. However, its roll-off rate is slower than Butterworth or Chebyshev filters.&#10;7. LDA Filters: As mentioned in the transcript, Linear Discriminant Analysis (LDA) filters can be trained using supervised learning techniques to optimize a certain criterion, such as minimizing within-class variance or maximizing between-class variance. This filter can help better separate different classes and improve overall performance in signal processing tasks.&#10;&#10;When using LDA for determining the optimal filter based on a criterion, it is essential to consider what specific aspect of the filter design you want to optimize. For instance, you could train an LDA filter to minimize the mean squared error between the original and filtered signals or maximize the signal-to-noise ratio.&#10;&#10;To implement this in practice, you would collect a dataset with input-output pairs representing your desired output signal (clean speech) and noisy input (speech affected by reverberation, background noise, etc.). Then, train an LDA filter using that data, evaluating its performance based on the chosen criterion. Once trained, this LDA filter could be used to process new, noisy signals in order to provide a filtered output signal that meets your desired criteria.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise." target="1. Fine-tuning ten or twelve small aspects, each contributing around 0.3%, can significantly improve a system's accuracy from 95% to 9">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise." target="1. The process of smoothing the gain in this context involves using a non-linear smoothing technique for low gain values, while not applying any smoothing for high gain values. This approach is taken to balance the need for noise reduction with the potential distortion or loss of signal information that can result from aggressive smoothing.&#10;2. The subtraction factor or algorithm's gain is being smoothed in this case, as mentioned by Professor G. This suggests that the group is trying to improve the accuracy and stability of the noise reduction technique by reducing the variability of the gain value over time.&#10;3. The decision on whether to smooth or not, and the choice of smoothing method, depends on various factors such as the desired latency, the characteristics of the signal and noise, the performance trade-offs between different techniques, and the specific requirements of the application.&#10;4. The group's discussion suggests that they are still experimenting with different approaches to smoothing the gain, as there is no consensus on whether further constraining the latency would be beneficial or not. This indicates that the process involves ongoing evaluation and adjustment based on empirical results and theoretical considerations.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise." target="1. The advantage of using a fixed filter, such as the RASTA filter, is that it remains unchanged and does not degrade over time, making it a reliable choice for signal processing. This is because it does not need to be retrained or adapted for different conditions, unlike trained filters.&#10;2. In the context of the SmartKom system, using a fixed amount of data for mean subtraction can be beneficial when less than twelve seconds of data are available. By training the models with a fixed amount of data (e.g., six or twelve seconds), the system can ensure stability and consistency in the processing steps that follow.&#10;3. However, if there is not enough data available to estimate a reliable mean value for mean subtraction, concatenating utterances together can help increase the amount of available data. This approach may be more suitable than training a filter specifically for certain conditions, as it allows the system to use all the available data and avoid potential overfitting or poor generalization issues that can arise with trained filters.&#10;4. Overall, using a fixed filter or a fixed amount of data for mean subtraction provides a stable and consistent processing approach, while concatenating utterances together can help increase the amount of available data when there is less than twelve seconds of data in the SmartKom system.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise." target="1. Cepstral Mean Subtraction (CMS) is a technique used in short-term window analysis to remove general characteristics from data. It involves calculating the mean of the cepstrum (a transform of the log power spectrum) for each window and subtracting it from the cepstrum of that window. This helps get rid of very general characteristics, such as those caused by the recording device or room acoustics.&#10;2. Flipping frames from the front to create negative values is a technique used in some signal processing applications, including CMS. By taking a certain number of frames from the beginning of the signal and flipping them around, the original frames are replaced with their mirror images. This can help balance the signal by reducing the impact of any abrupt changes at the beginning of the window. In the context of CMS, flipping frames creates negative values in the cepstrum that are then subtracted from the original cepstrum to achieve mean subtraction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The use of spectral subtraction or Wiener filtering for noise reduction can result in negative values in the processed signal. This occurs because these techniques attempt to estimate and subtract the noise spectrum from the signal spectrum, but the models used may not be complex enough to fully capture the additional variability introduced by this process.&#10;2. Negative values in the signal can cause issues because they are not meaningful and can affect the performance of subsequent processing steps. To handle this, some approaches set negative values to zero or replace them with a small positive value. However, these methods can introduce distortions in the signal and may not be suitable for all applications.&#10;3. Another issue with spectral subtraction is that it can result in worse performance than the baseline approach, as mentioned in the transcript. This may be due to the limitations of the models used or other factors related to the specific application.&#10;4. In summary, spectral subtraction and Wiener filtering can be useful for noise reduction in signal processing, but they can also introduce negative values and may not always result in improved performance compared to the baseline approach. Careful consideration should be given to the choice of noise reduction technique and how to handle any negative values that may arise." target="Scaling the noise estimate by a factor less than one resulted in improved performance for the TI-digits because, as mentioned in the transcript, there were a lot of zeros in the spectrogram when using the initial approach. By scaling the noise estimate, the speakers aimed to reduce this issue and improve the system's performance.&#10;&#10;The complete results of using this new technique involve applying a noise estimate scaled by a factor of 0.5. This is an ad-hoc method, meaning it is not optimized for any specific goal but serves as an intermediate step in their exploration. The only trend observed from the results was that the current noise estimation or composition scheme worked well for car noise types, while performing less effectively for non-stationary noises like &quot;Babble,&quot; &quot;Subway,&quot; &quot;Street,&quot; or &quot;Restaurant&quot; sounds.&#10;&#10;In summary, scaling the noise estimate by a factor of 0.5 improved performance in this case, but it is an ad-hoc method and not optimized for any specific goal. The complete results show that the technique works well for car noise types but may struggle with non-stationary noises.">
      <data key="d0">1</data>
    </edge>
    <edge source="disfmarker}&#10;Speaker: Professor D&#10;Content: Right at the point where you 've done the subtraction .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: Um , essentially you 're adding a constant into everything .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: But the way Stephane did it , it is exactly the way I have implemented in the phone , so .&#10;Speaker: Professor D&#10;Content: Oh , yeah , better do it different , then . Yeah .&#10;Speaker: PhD E&#10;Content: Um .&#10;Speaker: Professor D&#10;Content: Just you {disfmarker} you just ta you just set it for a particular signal - to - noise ratio that you want ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Yeah I {disfmarker} I made s similar investigations like Stephane did here , just uh , adding this constant and {disfmarker} and looking how dependent is it on the value of the constant&#10;Speaker: Professor D&#10;Content: Yeah . Yeah .&#10;Speaker:" target="1. The professors and PhDs are discussing adjusting the value of the constant added to the signal energy to achieve a desired signal-to-noise ratio (SNR). This constant is determined based on the average speech energy measured from a dataset of one language, such as Italian, and then used as the mean speech energy for all other languages.&#10;2. They are also considering how dependent the performance is on the value of this constant and exploring different values to find the optimal SNR range for the system.&#10;3. Additionally, there is mention of potentially adjusting noise addition or thresholds based on the average energy levels in each language. For example, Finnish has a lower average energy compared to other databases, so the threshold for Finnish should be lower accordingly.&#10;4. Lastly, they touch upon the impracticality of measuring the average energy over an extended period in real-world applications and suggest using a fixed number derived from other methods instead.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Professor G&#10;Content: And , uh , I {disfmarker} I found the same problem . Just taking um , what we were used to u {vocalsound} use , I mean , uh , some type of spectral subtraction , {comment} y {vocalsound} you get even worse results than {vocalsound} the basis&#10;Speaker: PhD B&#10;Content: Yeah . Yeah ,&#10;Speaker: Professor G&#10;Content: and uh {disfmarker}&#10;Speaker: PhD B&#10;Content: yeah .&#10;Speaker: Professor G&#10;Content: I {disfmarker} I tried to find an explanation for it ,&#10;Speaker: Professor D&#10;Content: Mmm .&#10;Speaker: Professor G&#10;Content: so {disfmarker}&#10;Speaker: PhD B&#10;Content: So . Yes . Stephane also has the same experience of using the spectral subtraction right ?&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Yeah . So here {disfmarker} here I mean , I found that it 's {disfmarker" target="1. The text discusses a meeting between several individuals, referred to as PhD B, Professor G, PhD E, PhD D, PhD E, and PhD H, who are involved in a project that involves analyzing audio data.&#10;2. The speakers mention that the audio data contains a small set of words (few transcriptions) and has music in the background. This is indicated when PhD B says &quot;It has very few...words also&quot; and &quot;It's a very, very small set, actually&quot; and Professor G says &quot;there is a lot of utterances with music in the background.&quot;&#10;3. Furthermore, the speakers express their dislike for the music, as indicated when PhD B says &quot;It has some music also...I mean, very horrible music like I know.&quot;&#10;4. Therefore, it can be said that the text contains audio data with a small set of words and music in the background, and the speakers express their dislike for the music.">
      <data key="d0">1</data>
    </edge>
    <edge source=" D&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Th - That 's true . Yeah {disfmarker} the c the models are not complex enough to absorb that additional variability that you 're introducing .&#10;Speaker: Professor G&#10;Content: s&#10;Speaker: PhD F&#10;Content: Thanks Adam .&#10;Speaker: Professor G&#10;Content: Yeah . Yes .&#10;Speaker: PhD B&#10;Content: Well , that 's {disfmarker} Yeah . So {disfmarker}&#10;Speaker: PhD E&#10;Content: I also have the feeling that um , the reason ye why it doesn't work is {disfmarker} yeah , that the models are much {disfmarker} are t um , not complex enough . Because I {disfmarker} actually I als always had a good experience with spectral subtraction , just a straight spectral subtraction algorithm when I was using neural networks , big neural networks , which maybe are more able to model strange distributions and {disfmarker}&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: But {disfmarker}" target="1. The speakers believe that the models are not complex enough to absorb additional variability in the data because they have observed that introducing a &quot;natural distribution&quot; to the audio data, which would make it resemble real-world noisy situations more closely, increases the degradation of the speech processing algorithm. They suggest that this could be due to the models' inability to effectively model and account for the increased variability. Instead, the added noise results in additional errors or degradation in the system's performance.&#10;2. This explanation can be found when PhD B says, &quot;the models are not complex enough to absorb that additional variability that you're introducing&quot; and later reiterated by PhD E, who shares their experience with neural networks being more capable of modeling strange distributions.&#10;3. The speakers also suggest that this natural distribution approach more accurately represents real-world noisy situations, indicating that the current models might not be sophisticated enough to account for these complexities without compromising performance.">
      <data key="d0">1</data>
    </edge>
    <edge source=" D&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Th - That 's true . Yeah {disfmarker} the c the models are not complex enough to absorb that additional variability that you 're introducing .&#10;Speaker: Professor G&#10;Content: s&#10;Speaker: PhD F&#10;Content: Thanks Adam .&#10;Speaker: Professor G&#10;Content: Yeah . Yes .&#10;Speaker: PhD B&#10;Content: Well , that 's {disfmarker} Yeah . So {disfmarker}&#10;Speaker: PhD E&#10;Content: I also have the feeling that um , the reason ye why it doesn't work is {disfmarker} yeah , that the models are much {disfmarker} are t um , not complex enough . Because I {disfmarker} actually I als always had a good experience with spectral subtraction , just a straight spectral subtraction algorithm when I was using neural networks , big neural networks , which maybe are more able to model strange distributions and {disfmarker}&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: But {disfmarker}" target="Yes, based on the transcript, it appears that the neural network was trained on previously estimated alignments rather than being directly based on the provided label files for the Italian data. Speaker PhD E states that &quot;we trained the neural network on embedded training... That's right?&quot; and later clarifies that &quot;For the Italian data, I think we trained the neural network on re-estimation of the alignment using the neural network.&quot; There is no indication in the transcript that the neural network was directly trained on the provided label files.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of measuring the average energy involves obtaining the average speech energy from a dataset of one language, such as Italian. This value is then used as the mean speech energy for all other languages. (PhD E's contribution)&#10;2. There was confusion regarding whether the amount to be subtracted from the signal energy depends on the utterance or not. PhD E later clarified that it does not. (Professor D and PhD E's contributions)&#10;3. Some systems add random noise to the log energy with a certain mean and variance, which is language-specific. This addition of random noise is done only after cleaning up the signal. (PhD B's contribution)&#10;4. The speakers also mention that Finnish has a lower average energy compared to other databases. Therefore, the threshold for Finnish should be lower accordingly. (Professor G's contribution)&#10;5. In real-world applications, measuring the average energy over an extended period, like half an hour or an hour, is not feasible. Instead, a fixed number derived from other methods should be used. (PhD D and PhD E's contributions)&#10;6. The process of measuring the average energy is not language-dependent; instead, it involves using the mean speech energy obtained from one language for all languages. However, there might be language-specific adjustments to noise addition or thresholds based on the average energy levels in each language." target="isfmarker} ?&#10;Speaker: PhD E&#10;Content: It 's not . It 's just something that 's fixed .&#10;Speaker: Professor G&#10;Content: No . It 's overall .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Um {disfmarker}&#10;Speaker: Professor D&#10;Content: But what he is doing language dependent is measuring what that number i reference is that he comes down twenty - five down from .&#10;Speaker: PhD E&#10;Content: Yeah , so I g No . It {disfmarker} No .&#10;Speaker: Professor D&#10;Content: No ?&#10;Speaker: PhD E&#10;Content: Because I did it {disfmarker} I started working on Italian . I obtained this average energy&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: and then I used this one .&#10;Speaker: PhD B&#10;Content: For all the languages . OK .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: So it 's">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of measuring the average energy involves obtaining the average speech energy from a dataset of one language, such as Italian. This value is then used as the mean speech energy for all other languages. (PhD E's contribution)&#10;2. There was confusion regarding whether the amount to be subtracted from the signal energy depends on the utterance or not. PhD E later clarified that it does not. (Professor D and PhD E's contributions)&#10;3. Some systems add random noise to the log energy with a certain mean and variance, which is language-specific. This addition of random noise is done only after cleaning up the signal. (PhD B's contribution)&#10;4. The speakers also mention that Finnish has a lower average energy compared to other databases. Therefore, the threshold for Finnish should be lower accordingly. (Professor G's contribution)&#10;5. In real-world applications, measuring the average energy over an extended period, like half an hour or an hour, is not feasible. Instead, a fixed number derived from other methods should be used. (PhD D and PhD E's contributions)&#10;6. The process of measuring the average energy is not language-dependent; instead, it involves using the mean speech energy obtained from one language for all languages. However, there might be language-specific adjustments to noise addition or thresholds based on the average energy levels in each language." target="} right now this {disfmarker} this is c a constant that does not depend on {disfmarker} {comment} on anything that you can learn from the utterance . It 's just a constant noise addition . Um . And I {disfmarker} I think w w&#10;Speaker: Professor D&#10;Content: I {disfmarker} I 'm sorry . Then {disfmarker} then I 'm confused .&#10;Speaker: PhD E&#10;Content: I think {disfmarker}&#10;Speaker: Professor D&#10;Content: I thought {disfmarker} you 're saying it doesn't depend on the utterance but I thought you were adding an amount that was twenty - five DB down from the signal energy .&#10;Speaker: PhD E&#10;Content: Yeah , so the way I did that , {comment} i I just measured the average speech energy of the {disfmarker} all the Italian data .&#10;Speaker: Professor D&#10;Content: Oh !&#10;Speaker: PhD E&#10;Content: And then {disfmarker} I {disfmarker} I have {disfmarker} I used this as mean speech energy .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of measuring the average energy involves obtaining the average speech energy from a dataset of one language, such as Italian. This value is then used as the mean speech energy for all other languages. (PhD E's contribution)&#10;2. There was confusion regarding whether the amount to be subtracted from the signal energy depends on the utterance or not. PhD E later clarified that it does not. (Professor D and PhD E's contributions)&#10;3. Some systems add random noise to the log energy with a certain mean and variance, which is language-specific. This addition of random noise is done only after cleaning up the signal. (PhD B's contribution)&#10;4. The speakers also mention that Finnish has a lower average energy compared to other databases. Therefore, the threshold for Finnish should be lower accordingly. (Professor G's contribution)&#10;5. In real-world applications, measuring the average energy over an extended period, like half an hour or an hour, is not feasible. Instead, a fixed number derived from other methods should be used. (PhD D and PhD E's contributions)&#10;6. The process of measuring the average energy is not language-dependent; instead, it involves using the mean speech energy obtained from one language for all languages. However, there might be language-specific adjustments to noise addition or thresholds based on the average energy levels in each language." target=": Professor D&#10;Content: Oh , they do !&#10;Speaker: PhD B&#10;Content: Yep .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: C - z C - zero and log energy also , yeah .&#10;Speaker: PhD E&#10;Content: Yeah . Um , But I don't know how much effect it {disfmarker} this have , but they do that .&#10;Speaker: PhD B&#10;Content: Now ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Oh .&#10;Speaker: Professor G&#10;Content: Uh - huh .&#10;Speaker: Professor D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: So it {disfmarker} it {disfmarker} it {disfmarker} it {disfmarker} it is l somehow similar to what {disfmarker}&#10;Speaker: PhD E&#10;Content: I think because they have th log energy , yeah , and then just generate random number . They have some kind of mean and variance , and they add this number to {disfmarker} to the log energy simply . Um {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of measuring the average energy involves obtaining the average speech energy from a dataset of one language, such as Italian. This value is then used as the mean speech energy for all other languages. (PhD E's contribution)&#10;2. There was confusion regarding whether the amount to be subtracted from the signal energy depends on the utterance or not. PhD E later clarified that it does not. (Professor D and PhD E's contributions)&#10;3. Some systems add random noise to the log energy with a certain mean and variance, which is language-specific. This addition of random noise is done only after cleaning up the signal. (PhD B's contribution)&#10;4. The speakers also mention that Finnish has a lower average energy compared to other databases. Therefore, the threshold for Finnish should be lower accordingly. (Professor G's contribution)&#10;5. In real-world applications, measuring the average energy over an extended period, like half an hour or an hour, is not feasible. Instead, a fixed number derived from other methods should be used. (PhD D and PhD E's contributions)&#10;6. The process of measuring the average energy is not language-dependent; instead, it involves using the mean speech energy obtained from one language for all languages. However, there might be language-specific adjustments to noise addition or thresholds based on the average energy levels in each language." target=" number . They have some kind of mean and variance , and they add this number to {disfmarker} to the log energy simply . Um {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah {disfmarker} the {disfmarker} the log energy , the {disfmarker} after the clean {disfmarker} cleaning up .&#10;Speaker: Professor D&#10;Content: To the l&#10;Speaker: PhD B&#10;Content: So they add a random {disfmarker} random noise to it .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: To the {disfmarker} just the energy , or to the mel {disfmarker} uh , to the mel filter ?&#10;Speaker: PhD B&#10;Content: No . On - only to the log energy .&#10;Speaker: PhD E&#10;Content: Only {disfmarker} Yeah .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: Professor G&#10;Content: Uh - huh .&#10;Speaker: Professor D&#10;Content: So it {disfmarker} Cuz I mean , I">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of measuring the average energy involves obtaining the average speech energy from a dataset of one language, such as Italian. This value is then used as the mean speech energy for all other languages. (PhD E's contribution)&#10;2. There was confusion regarding whether the amount to be subtracted from the signal energy depends on the utterance or not. PhD E later clarified that it does not. (Professor D and PhD E's contributions)&#10;3. Some systems add random noise to the log energy with a certain mean and variance, which is language-specific. This addition of random noise is done only after cleaning up the signal. (PhD B's contribution)&#10;4. The speakers also mention that Finnish has a lower average energy compared to other databases. Therefore, the threshold for Finnish should be lower accordingly. (Professor G's contribution)&#10;5. In real-world applications, measuring the average energy over an extended period, like half an hour or an hour, is not feasible. Instead, a fixed number derived from other methods should be used. (PhD D and PhD E's contributions)&#10;6. The process of measuring the average energy is not language-dependent; instead, it involves using the mean speech energy obtained from one language for all languages. However, there might be language-specific adjustments to noise addition or thresholds based on the average energy levels in each language." target=" . It 's {disfmarker} it 's a {disfmarker}&#10;Speaker: PhD B&#10;Content: So that 's {disfmarker}&#10;Speaker: Professor D&#10;Content: Which means decrease in word error rate ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: OK , so &quot; percentage increase &quot; means decrease ?&#10;Speaker: PhD B&#10;Content: Yeah , yeah .&#10;Speaker: Professor D&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: Yeah . The {disfmarker} the w there was a very long discussion about this on {disfmarker} on the {disfmarker} on the , uh , Amsterdam meeting .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: How to {disfmarker} how to calculate it then .&#10;Speaker: PhD B&#10;Content: Yeah . There 's {disfmarker} there 's a {disfmarker}&#10;Speaker: Professor G&#10;Content: I {disfmarker} I {disfmarker} I guess you are using finally this {">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of measuring the average energy involves obtaining the average speech energy from a dataset of one language, such as Italian. This value is then used as the mean speech energy for all other languages. (PhD E's contribution)&#10;2. There was confusion regarding whether the amount to be subtracted from the signal energy depends on the utterance or not. PhD E later clarified that it does not. (Professor D and PhD E's contributions)&#10;3. Some systems add random noise to the log energy with a certain mean and variance, which is language-specific. This addition of random noise is done only after cleaning up the signal. (PhD B's contribution)&#10;4. The speakers also mention that Finnish has a lower average energy compared to other databases. Therefore, the threshold for Finnish should be lower accordingly. (Professor G's contribution)&#10;5. In real-world applications, measuring the average energy over an extended period, like half an hour or an hour, is not feasible. Instead, a fixed number derived from other methods should be used. (PhD D and PhD E's contributions)&#10;6. The process of measuring the average energy is not language-dependent; instead, it involves using the mean speech energy obtained from one language for all languages. However, there might be language-specific adjustments to noise addition or thresholds based on the average energy levels in each language." target=" thirty to twenty - five . And {disfmarker} I have the feeling that maybe it 's because just Finnish has a mean energy that 's lower than {disfmarker} than the other databases . And due to this the thresholds should be {disfmarker}&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: the {disfmarker} the a the noise addition should be lower&#10;Speaker: Professor D&#10;Content: But in {disfmarker} I mean , in the real thing you 're not gonna be able to measure what people are doing over half an hour or an hour , or anything , right ?&#10;Speaker: PhD E&#10;Content: and {disfmarker}&#10;Speaker: Professor D&#10;Content: So you have to come up with this number from something else .&#10;Speaker: PhD E&#10;Content: Yeah . So {disfmarker}&#10;Speaker: Professor G&#10;Content: Uh , but you are not doing it now language dependent ? Or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: It 's not . It 's just something that 's fixed .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of measuring the average energy involves obtaining the average speech energy from a dataset of one language, such as Italian. This value is then used as the mean speech energy for all other languages. (PhD E's contribution)&#10;2. There was confusion regarding whether the amount to be subtracted from the signal energy depends on the utterance or not. PhD E later clarified that it does not. (Professor D and PhD E's contributions)&#10;3. Some systems add random noise to the log energy with a certain mean and variance, which is language-specific. This addition of random noise is done only after cleaning up the signal. (PhD B's contribution)&#10;4. The speakers also mention that Finnish has a lower average energy compared to other databases. Therefore, the threshold for Finnish should be lower accordingly. (Professor G's contribution)&#10;5. In real-world applications, measuring the average energy over an extended period, like half an hour or an hour, is not feasible. Instead, a fixed number derived from other methods should be used. (PhD D and PhD E's contributions)&#10;6. The process of measuring the average energy is not language-dependent; instead, it involves using the mean speech energy obtained from one language for all languages. However, there might be language-specific adjustments to noise addition or thresholds based on the average energy levels in each language." target="1. To determine the constant noise addition, PhD E measured the average speech energy from a dataset of Italian language. This value was then used as the mean speech energy for all other languages. The constant noise addition was calculated as a fixed amount (e.g., 25 dB or 30 dB) below the average speech energy.&#10;2. The specific method used to calculate this constant may not be explicitly stated in the transcript, but it is mentioned that PhD E measured the average speech energy of all Italian data and then subtracted a certain amount from the signal energy to create a signal-to-noise ratio.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of measuring the average energy involves obtaining the average speech energy from a dataset of one language, such as Italian. This value is then used as the mean speech energy for all other languages. (PhD E's contribution)&#10;2. There was confusion regarding whether the amount to be subtracted from the signal energy depends on the utterance or not. PhD E later clarified that it does not. (Professor D and PhD E's contributions)&#10;3. Some systems add random noise to the log energy with a certain mean and variance, which is language-specific. This addition of random noise is done only after cleaning up the signal. (PhD B's contribution)&#10;4. The speakers also mention that Finnish has a lower average energy compared to other databases. Therefore, the threshold for Finnish should be lower accordingly. (Professor G's contribution)&#10;5. In real-world applications, measuring the average energy over an extended period, like half an hour or an hour, is not feasible. Instead, a fixed number derived from other methods should be used. (PhD D and PhD E's contributions)&#10;6. The process of measuring the average energy is not language-dependent; instead, it involves using the mean speech energy obtained from one language for all languages. However, there might be language-specific adjustments to noise addition or thresholds based on the average energy levels in each language." target="1. The professors and PhDs are discussing adjusting the value of the constant added to the signal energy to achieve a desired signal-to-noise ratio (SNR). This constant is determined based on the average speech energy measured from a dataset of one language, such as Italian, and then used as the mean speech energy for all other languages.&#10;2. They are also considering how dependent the performance is on the value of this constant and exploring different values to find the optimal SNR range for the system.&#10;3. Additionally, there is mention of potentially adjusting noise addition or thresholds based on the average energy levels in each language. For example, Finnish has a lower average energy compared to other databases, so the threshold for Finnish should be lower accordingly.&#10;4. Lastly, they touch upon the impracticality of measuring the average energy over an extended period in real-world applications and suggest using a fixed number derived from other methods instead.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of measuring the average energy involves obtaining the average speech energy from a dataset of one language, such as Italian. This value is then used as the mean speech energy for all other languages. (PhD E's contribution)&#10;2. There was confusion regarding whether the amount to be subtracted from the signal energy depends on the utterance or not. PhD E later clarified that it does not. (Professor D and PhD E's contributions)&#10;3. Some systems add random noise to the log energy with a certain mean and variance, which is language-specific. This addition of random noise is done only after cleaning up the signal. (PhD B's contribution)&#10;4. The speakers also mention that Finnish has a lower average energy compared to other databases. Therefore, the threshold for Finnish should be lower accordingly. (Professor G's contribution)&#10;5. In real-world applications, measuring the average energy over an extended period, like half an hour or an hour, is not feasible. Instead, a fixed number derived from other methods should be used. (PhD D and PhD E's contributions)&#10;6. The process of measuring the average energy is not language-dependent; instead, it involves using the mean speech energy obtained from one language for all languages. However, there might be language-specific adjustments to noise addition or thresholds based on the average energy levels in each language." target="1. The best performance for the &quot;Car&quot; category was achieved when using an LDA filter trained with clean speech (TI-digits). This resulted in better signal processing performance compared to a filter trained with noisy speech.&#10;2. Although this specific conversation focuses on the improved performance of the clean LDA filter for TI-digits, it is essential to consider that different contexts or datasets might require different LDA filter designs, including those based on noisy speech.&#10;3. In this case, the &quot;Car&quot; category had the best performance in the third column of condition A, but specific numerical values or metrics were not provided in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="} right now this {disfmarker} this is c a constant that does not depend on {disfmarker} {comment} on anything that you can learn from the utterance . It 's just a constant noise addition . Um . And I {disfmarker} I think w w&#10;Speaker: Professor D&#10;Content: I {disfmarker} I 'm sorry . Then {disfmarker} then I 'm confused .&#10;Speaker: PhD E&#10;Content: I think {disfmarker}&#10;Speaker: Professor D&#10;Content: I thought {disfmarker} you 're saying it doesn't depend on the utterance but I thought you were adding an amount that was twenty - five DB down from the signal energy .&#10;Speaker: PhD E&#10;Content: Yeah , so the way I did that , {comment} i I just measured the average speech energy of the {disfmarker} all the Italian data .&#10;Speaker: Professor D&#10;Content: Oh !&#10;Speaker: PhD E&#10;Content: And then {disfmarker} I {disfmarker} I have {disfmarker} I used this as mean speech energy ." target="1. To determine the constant noise addition, PhD E measured the average speech energy from a dataset of Italian language. This value was then used as the mean speech energy for all other languages. The constant noise addition was calculated as a fixed amount (e.g., 25 dB or 30 dB) below the average speech energy.&#10;2. The specific method used to calculate this constant may not be explicitly stated in the transcript, but it is mentioned that PhD E measured the average speech energy of all Italian data and then subtracted a certain amount from the signal energy to create a signal-to-noise ratio.">
      <data key="d0">1</data>
    </edge>
    <edge source=" number . They have some kind of mean and variance , and they add this number to {disfmarker} to the log energy simply . Um {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah {disfmarker} the {disfmarker} the log energy , the {disfmarker} after the clean {disfmarker} cleaning up .&#10;Speaker: Professor D&#10;Content: To the l&#10;Speaker: PhD B&#10;Content: So they add a random {disfmarker} random noise to it .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: To the {disfmarker} just the energy , or to the mel {disfmarker} uh , to the mel filter ?&#10;Speaker: PhD B&#10;Content: No . On - only to the log energy .&#10;Speaker: PhD E&#10;Content: Only {disfmarker} Yeah .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: Professor G&#10;Content: Uh - huh .&#10;Speaker: Professor D&#10;Content: So it {disfmarker} Cuz I mean , I" target="1. The speakers believe that the models are not complex enough to absorb additional variability in the data because they have observed that introducing a &quot;natural distribution&quot; to the audio data, which would make it resemble real-world noisy situations more closely, increases the degradation of the speech processing algorithm. They suggest that this could be due to the models' inability to effectively model and account for the increased variability. Instead, the added noise results in additional errors or degradation in the system's performance.&#10;2. This explanation can be found when PhD B says, &quot;the models are not complex enough to absorb that additional variability that you're introducing&quot; and later reiterated by PhD E, who shares their experience with neural networks being more capable of modeling strange distributions.&#10;3. The speakers also suggest that this natural distribution approach more accurately represents real-world noisy situations, indicating that the current models might not be sophisticated enough to account for these complexities without compromising performance.">
      <data key="d0">1</data>
    </edge>
    <edge source=" . It 's {disfmarker} it 's a {disfmarker}&#10;Speaker: PhD B&#10;Content: So that 's {disfmarker}&#10;Speaker: Professor D&#10;Content: Which means decrease in word error rate ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: OK , so &quot; percentage increase &quot; means decrease ?&#10;Speaker: PhD B&#10;Content: Yeah , yeah .&#10;Speaker: Professor D&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: Yeah . The {disfmarker} the w there was a very long discussion about this on {disfmarker} on the {disfmarker} on the , uh , Amsterdam meeting .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: How to {disfmarker} how to calculate it then .&#10;Speaker: PhD B&#10;Content: Yeah . There 's {disfmarker} there 's a {disfmarker}&#10;Speaker: Professor G&#10;Content: I {disfmarker} I {disfmarker} I guess you are using finally this {" target="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To determine the constant noise addition, PhD E measured the average speech energy from a dataset of Italian language. This value was then used as the mean speech energy for all other languages. The constant noise addition was calculated as a fixed amount (e.g., 25 dB or 30 dB) below the average speech energy.&#10;2. The specific method used to calculate this constant may not be explicitly stated in the transcript, but it is mentioned that PhD E measured the average speech energy of all Italian data and then subtracted a certain amount from the signal energy to create a signal-to-noise ratio." target=" , it 's like {disfmarker} Uh , you are like r r reducing the floor of the noisy regions , right ?&#10;Speaker: Professor G&#10;Content: s&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . The floor is lower . Um ,&#10;Speaker: PhD B&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: mm - hmm .&#10;Speaker: Professor D&#10;Content: I 'm sorry . So when you say minus twenty - five or minus thirty DB , with respect to what ?&#10;Speaker: PhD E&#10;Content: To the average um , speech energy which is estimated on the world database .&#10;Speaker: Professor D&#10;Content: OK , so basically you 're creating a signal - to - noise ratio of twenty - five or thirty DB ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: uh r&#10;Speaker: PhD E&#10;Content: But it 's not {disfmarker}&#10;Speaker: Professor G&#10;Content: I {disfmarker} I {disfmarker} I think what you do is this .&#10;Speaker: PhD E&#10;Content: it">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To determine the constant noise addition, PhD E measured the average speech energy from a dataset of Italian language. This value was then used as the mean speech energy for all other languages. The constant noise addition was calculated as a fixed amount (e.g., 25 dB or 30 dB) below the average speech energy.&#10;2. The specific method used to calculate this constant may not be explicitly stated in the transcript, but it is mentioned that PhD E measured the average speech energy of all Italian data and then subtracted a certain amount from the signal energy to create a signal-to-noise ratio." target="marker} and looking how dependent is it on the value of the constant&#10;Speaker: Professor D&#10;Content: Yeah . Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: and then , must choose them somehow {vocalsound} to give on average the best results for a certain range of the signal - to - noise ratios .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor G&#10;Content: So {disfmarker}&#10;Speaker: PhD E&#10;Content: Oh , it 's clear . I should have gi given other results . Also it 's clear when you don't add noise , it 's much worse . Like , around five percent worse I guess .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: And if you add too much noise it get worse also . And it seems that {vocalsound} right now this {disfmarker} this is c a constant that does not depend on {disfmarker} {comment} on anything that">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To determine the constant noise addition, PhD E measured the average speech energy from a dataset of Italian language. This value was then used as the mean speech energy for all other languages. The constant noise addition was calculated as a fixed amount (e.g., 25 dB or 30 dB) below the average speech energy.&#10;2. The specific method used to calculate this constant may not be explicitly stated in the transcript, but it is mentioned that PhD E measured the average speech energy of all Italian data and then subtracted a certain amount from the signal energy to create a signal-to-noise ratio." target=" this to smooth the probabilities , right ? Um {disfmarker} I didn't use the {disfmarker} the scheme that 's currently in the proposal because {vocalsound} I don't want to {disfmarker} In the proposal {disfmarker} Well , in {disfmarker} in the system we want to add like speech frame before every word and a little bit of {disfmarker} of , uh , s a couple of frames after also . Uh , but to estimate the performance of the VAD , we don't want to do that , because it would artificially increase the um {disfmarker} the false alarm rate of speech detection . Right ? Um , so , there is u normally a figure for the Finnish and one for Italian . And maybe someone has two for the Italian because I 'm missing one figure here .&#10;Speaker: PhD B&#10;Content: No .&#10;Speaker: PhD E&#10;Content: Well {disfmarker} Well , whatever . Uh {disfmarker} Yeah , so one surprising thing that we can notice first is that apparently the speech miss rate is uh , higher than the false alarm rate . So . It means {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To determine the constant noise addition, PhD E measured the average speech energy from a dataset of Italian language. This value was then used as the mean speech energy for all other languages. The constant noise addition was calculated as a fixed amount (e.g., 25 dB or 30 dB) below the average speech energy.&#10;2. The specific method used to calculate this constant may not be explicitly stated in the transcript, but it is mentioned that PhD E measured the average speech energy of all Italian data and then subtracted a certain amount from the signal energy to create a signal-to-noise ratio." target=" PhD B&#10;Content: So I have like a forty - five {vocalsound} percent for &quot; Car noise &quot; and then there 's a minus five percent for the &quot; Babble &quot; ,&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: and there 's this thirty - three for the &quot; Station &quot; . And so {vocalsound} it 's {disfmarker} it 's not {disfmarker} it 's not actually very consistent across . So . The only correlation between the SpeechDat - Car and this performance is the c stationarity of the noise that is there in these conditions and the SpeechDat - Car .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: And , uh {disfmarker} so {disfmarker} so the overall result is like in the last page , which is like forty - seven , which is still very imbalanced because there are like fifty - six percent on the SpeechDat - Car and thirty - five percent on the TI - digits . And {disfmarker} uh , ps the fifty - six percent is like comparable to what the French">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To determine the constant noise addition, PhD E measured the average speech energy from a dataset of Italian language. This value was then used as the mean speech energy for all other languages. The constant noise addition was calculated as a fixed amount (e.g., 25 dB or 30 dB) below the average speech energy.&#10;2. The specific method used to calculate this constant may not be explicitly stated in the transcript, but it is mentioned that PhD E measured the average speech energy of all Italian data and then subtracted a certain amount from the signal energy to create a signal-to-noise ratio." target="1. The speaker, referred to as PhD F, has been researching speech recognition with a focus on creating a more robust system for word or phone recognition. This involves developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. One of the specific research questions they have been addressing is what types of intermediate categories are needed for classification. Another question relates to the structure of multi-band graphical models, specifically what types of structures should be considered in order to effectively combine evidence from sub-bands.&#10;3. Additionally, PhD F has been investigating how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. To accomplish this, they have proposed a new approach to speech recognition that combines multi-band ideas with an acoustic phonetic approach. This approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features more closely related to the acoustic signal itself.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To determine the constant noise addition, PhD E measured the average speech energy from a dataset of Italian language. This value was then used as the mean speech energy for all other languages. The constant noise addition was calculated as a fixed amount (e.g., 25 dB or 30 dB) below the average speech energy.&#10;2. The specific method used to calculate this constant may not be explicitly stated in the transcript, but it is mentioned that PhD E measured the average speech energy of all Italian data and then subtracted a certain amount from the signal energy to create a signal-to-noise ratio." target="1. The professors and PhDs are discussing adjusting the value of the constant added to the signal energy to achieve a desired signal-to-noise ratio (SNR). This constant is determined based on the average speech energy measured from a dataset of one language, such as Italian, and then used as the mean speech energy for all other languages.&#10;2. They are also considering how dependent the performance is on the value of this constant and exploring different values to find the optimal SNR range for the system.&#10;3. Additionally, there is mention of potentially adjusting noise addition or thresholds based on the average energy levels in each language. For example, Finnish has a lower average energy compared to other databases, so the threshold for Finnish should be lower accordingly.&#10;4. Lastly, they touch upon the impracticality of measuring the average energy over an extended period in real-world applications and suggest using a fixed number derived from other methods instead.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To determine the constant noise addition, PhD E measured the average speech energy from a dataset of Italian language. This value was then used as the mean speech energy for all other languages. The constant noise addition was calculated as a fixed amount (e.g., 25 dB or 30 dB) below the average speech energy.&#10;2. The specific method used to calculate this constant may not be explicitly stated in the transcript, but it is mentioned that PhD E measured the average speech energy of all Italian data and then subtracted a certain amount from the signal energy to create a signal-to-noise ratio." target="1. The project's focus regarding designing acoustics for either a kiosk or a mobile platform is determined by the specific requirements and use cases of the target application. If the application is designed to be stationary and user-specific, like in a kiosk, then it would make more sense to optimize the acoustics for that setting as the physical situation remains relatively stable. In contrast, if the application is intended for a mobile platform with variable acoustic conditions, the system should be designed to adapt to changing acoustics accordingly.&#10;2. The decision may also depend on the complexity and adaptability of the speech processing algorithms being used. More sophisticated models might be able to handle varying acoustic environments better than simpler ones. In this case, designing for a mobile platform could still provide an acceptable user experience if the models are complex enough to absorb additional variability.&#10;3. Lastly, practical considerations such as development time, resources, and budget may influence the decision. Developing a system that works well across multiple platforms might require more effort and resources than focusing on a single platform. However, designing with scalability and adaptability in mind could make it easier to expand the application to other platforms in the future.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To determine the constant noise addition, PhD E measured the average speech energy from a dataset of Italian language. This value was then used as the mean speech energy for all other languages. The constant noise addition was calculated as a fixed amount (e.g., 25 dB or 30 dB) below the average speech energy.&#10;2. The specific method used to calculate this constant may not be explicitly stated in the transcript, but it is mentioned that PhD E measured the average speech energy of all Italian data and then subtracted a certain amount from the signal energy to create a signal-to-noise ratio." target="1. The speaker, Grad A, is proposing a new approach to speech recognition that combines multi-band ideas with an acoustic-phonetic approach. This approach aims to create a more robust system for word or phone recognition by developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. The new approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features closely related to the acoustic signal itself, within the context of a multi-band model. The hope is that by going multi-band and using these intermediate classifications, the system will be more robust in unseen noisy situations.&#10;3. Grad A plans to investigate several research questions, including what types of intermediate categories are needed for classification, the structure of multi-band graphical models (specifically, what types of structures should be considered to effectively combine evidence from sub-bands), and how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. This new approach is motivated by the need to address real-world noisy situations more accurately, potentially through the introduction of a &quot;natural distribution&quot; of noise rather than simply adding constant or flat noise across all regions. This could lead to a decrease in word error rate and better performance in signal processing.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , it 's like {disfmarker} Uh , you are like r r reducing the floor of the noisy regions , right ?&#10;Speaker: Professor G&#10;Content: s&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . The floor is lower . Um ,&#10;Speaker: PhD B&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: mm - hmm .&#10;Speaker: Professor D&#10;Content: I 'm sorry . So when you say minus twenty - five or minus thirty DB , with respect to what ?&#10;Speaker: PhD E&#10;Content: To the average um , speech energy which is estimated on the world database .&#10;Speaker: Professor D&#10;Content: OK , so basically you 're creating a signal - to - noise ratio of twenty - five or thirty DB ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: uh r&#10;Speaker: PhD E&#10;Content: But it 's not {disfmarker}&#10;Speaker: Professor G&#10;Content: I {disfmarker} I {disfmarker} I think what you do is this .&#10;Speaker: PhD E&#10;Content: it" target="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , it 's like {disfmarker} Uh , you are like r r reducing the floor of the noisy regions , right ?&#10;Speaker: Professor G&#10;Content: s&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . The floor is lower . Um ,&#10;Speaker: PhD B&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: mm - hmm .&#10;Speaker: Professor D&#10;Content: I 'm sorry . So when you say minus twenty - five or minus thirty DB , with respect to what ?&#10;Speaker: PhD E&#10;Content: To the average um , speech energy which is estimated on the world database .&#10;Speaker: Professor D&#10;Content: OK , so basically you 're creating a signal - to - noise ratio of twenty - five or thirty DB ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: uh r&#10;Speaker: PhD E&#10;Content: But it 's not {disfmarker}&#10;Speaker: Professor G&#10;Content: I {disfmarker} I {disfmarker} I think what you do is this .&#10;Speaker: PhD E&#10;Content: it" target="1. The professors and PhDs are discussing adjusting the value of the constant added to the signal energy to achieve a desired signal-to-noise ratio (SNR). This constant is determined based on the average speech energy measured from a dataset of one language, such as Italian, and then used as the mean speech energy for all other languages.&#10;2. They are also considering how dependent the performance is on the value of this constant and exploring different values to find the optimal SNR range for the system.&#10;3. Additionally, there is mention of potentially adjusting noise addition or thresholds based on the average energy levels in each language. For example, Finnish has a lower average energy compared to other databases, so the threshold for Finnish should be lower accordingly.&#10;4. Lastly, they touch upon the impracticality of measuring the average energy over an extended period in real-world applications and suggest using a fixed number derived from other methods instead.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker} and looking how dependent is it on the value of the constant&#10;Speaker: Professor D&#10;Content: Yeah . Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: and then , must choose them somehow {vocalsound} to give on average the best results for a certain range of the signal - to - noise ratios .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor G&#10;Content: So {disfmarker}&#10;Speaker: PhD E&#10;Content: Oh , it 's clear . I should have gi given other results . Also it 's clear when you don't add noise , it 's much worse . Like , around five percent worse I guess .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: And if you add too much noise it get worse also . And it seems that {vocalsound} right now this {disfmarker} this is c a constant that does not depend on {disfmarker} {comment} on anything that" target="1. The professors and PhDs are discussing adjusting the value of the constant added to the signal energy to achieve a desired signal-to-noise ratio (SNR). This constant is determined based on the average speech energy measured from a dataset of one language, such as Italian, and then used as the mean speech energy for all other languages.&#10;2. They are also considering how dependent the performance is on the value of this constant and exploring different values to find the optimal SNR range for the system.&#10;3. Additionally, there is mention of potentially adjusting noise addition or thresholds based on the average energy levels in each language. For example, Finnish has a lower average energy compared to other databases, so the threshold for Finnish should be lower accordingly.&#10;4. Lastly, they touch upon the impracticality of measuring the average energy over an extended period in real-world applications and suggest using a fixed number derived from other methods instead.">
      <data key="d0">1</data>
    </edge>
    <edge source=" PhD B&#10;Content: So I have like a forty - five {vocalsound} percent for &quot; Car noise &quot; and then there 's a minus five percent for the &quot; Babble &quot; ,&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: and there 's this thirty - three for the &quot; Station &quot; . And so {vocalsound} it 's {disfmarker} it 's not {disfmarker} it 's not actually very consistent across . So . The only correlation between the SpeechDat - Car and this performance is the c stationarity of the noise that is there in these conditions and the SpeechDat - Car .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: And , uh {disfmarker} so {disfmarker} so the overall result is like in the last page , which is like forty - seven , which is still very imbalanced because there are like fifty - six percent on the SpeechDat - Car and thirty - five percent on the TI - digits . And {disfmarker} uh , ps the fifty - six percent is like comparable to what the French" target="1. The text discusses a meeting between several individuals, referred to as PhD B, Professor G, PhD E, PhD D, PhD E, and PhD H, who are involved in a project that involves analyzing audio data.&#10;2. The speakers mention that the audio data contains a small set of words (few transcriptions) and has music in the background. This is indicated when PhD B says &quot;It has very few...words also&quot; and &quot;It's a very, very small set, actually&quot; and Professor G says &quot;there is a lot of utterances with music in the background.&quot;&#10;3. Furthermore, the speakers express their dislike for the music, as indicated when PhD B says &quot;It has some music also...I mean, very horrible music like I know.&quot;&#10;4. Therefore, it can be said that the text contains audio data with a small set of words and music in the background, and the speakers express their dislike for the music.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Company X changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. This change allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The compression and packing of frames are also modified so that they can pack the frames without waiting for an additional frame." target=" need that forty milliseconds also .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: No . They actually changed the compression scheme altogether .&#10;Speaker: Professor D&#10;Content: Right ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So they have their own compression and decoding scheme and they {disfmarker} I don't know what they have .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: But they have coded zero delay for that . Because they ch I know they changed it , their compression . They have their own CRC , their {disfmarker} their own {vocalsound} error correction mechanism .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: So they don't have to wait more than one more frame to know whether the current frame is in error .&#10;Speaker: Professor D&#10;Content: Oh , OK .&#10;Speaker: PhD B&#10;Content: So they changed the whole thing so that there 's no delay for that compression and {disfmarker} part also .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Company X changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. This change allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The compression and packing of frames are also modified so that they can pack the frames without waiting for an additional frame." target="&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: f so {disfmarker}&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: they didn't include that .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So {disfmarker}&#10;Speaker: PhD E&#10;Content: Where does the comprish compression in decoding delay comes from ?&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: &#10;Speaker: PhD B&#10;Content: That 's the way the {disfmarker} the {disfmarker} the frames are packed , like you have to wait for one more frame to pack . Because it 's {disfmarker} the CRC is computed for two frames always .&#10;Speaker: Professor D&#10;Content: Well , that {disfmarker} the they would need that forty milliseconds also .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: No .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Company X changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. This change allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The compression and packing of frames are also modified so that they can pack the frames without waiting for an additional frame." target=" PhD B&#10;Content: So they changed the whole thing so that there 's no delay for that compression and {disfmarker} part also .&#10;Speaker: Professor D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Even you have reported actually zero delay for the {pause} compression . I thought maybe you also have some different {disfmarker}&#10;Speaker: Professor G&#10;Content: Mmm . Mmm . No , I think I {disfmarker} I used this scheme as it was before .&#10;Speaker: PhD B&#10;Content: OK . Ah . Mm - hmm .&#10;Speaker: PhD F&#10;Content: OK , we 've got twenty minutes so we should {vocalsound} probably try to move along . Uh , did you wanna go next , Stephane ?&#10;Speaker: PhD E&#10;Content: I can go next . Yeah . Mmm .&#10;Speaker: Professor D&#10;Content: Oh . Wait a minute . It 's {disfmarker}&#10;Speaker: PhD E&#10;Content: It 's {disfmarker} Yeah , we have to">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Company X changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. This change allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The compression and packing of frames are also modified so that they can pack the frames without waiting for an additional frame." target="&#10;Speaker: PhD F&#10;Content: What {disfmarker} where was the , um {disfmarker} the smallest latency of all the systems last time ?&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: The French Telecom .&#10;Speaker: Professor D&#10;Content: Well , France Telecom was {disfmarker} was {disfmarker} was very short latency&#10;Speaker: Professor G&#10;Content: It 's {disfmarker}&#10;Speaker: Professor D&#10;Content: and they had a very good result .&#10;Speaker: PhD F&#10;Content: What {disfmarker} what was it ?&#10;Speaker: Professor D&#10;Content: It was thirty - five .&#10;Speaker: Professor G&#10;Content: It was in the order of thirty milliseconds&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: or {disfmarker}&#10;Speaker: PhD F&#10;Content: Thirteen ?&#10;Speaker: Professor D&#10;Content: th th&#10;Speaker: Professor G&#10;Content: Thirty .&#10;Speaker: PhD F&#10;Content: Thirty .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Company X changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. This change allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The compression and packing of frames are also modified so that they can pack the frames without waiting for an additional frame." target="ss - uh .&#10;Speaker: Professor D&#10;Content: So it could reduce the dependence on the amplitude and so on . Yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah . Although {disfmarker}&#10;Speaker: Professor D&#10;Content: Maybe .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: So is , uh {disfmarker} Is that about it ?&#10;Speaker: PhD B&#10;Content: Uh , so the {disfmarker}&#10;Speaker: PhD F&#10;Content: Or {disfmarker} ?&#10;Speaker: PhD B&#10;Content: OK . So the other thing is the {disfmarker} I 'm just looking at a little bit on the delay issue where the delay of the system is like a hundred and eighty millisecond . So {vocalsound} I just {disfmarker} just tried another sk system {disfmarker} I mean , another filter which I 've like shown at the end . Which is very similar to the existing uh , filter . Only {disfmarker} Uh , only thing">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Company X changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. This change allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The compression and packing of frames are also modified so that they can pack the frames without waiting for an additional frame." target="isfmarker} if I use that frame to calculate the mean , then I 'm just gonna get n nothing .&#10;Speaker: Professor D&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: Um ,&#10;Speaker: Professor D&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: so I should probably have some kind of default {vocalsound} mean for the first f couple of frames ?&#10;Speaker: Professor D&#10;Content: Yeah . Yeah .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: Yeah . Or subtract nothing . I mean , it 's {disfmarker}&#10;Speaker: Grad C&#10;Content: Or subtract nothing . And {disfmarker} and that 's {disfmarker} that 's {disfmarker} I guess that 's something that 's p people have figured out how to deal with in cepstral mean subtraction as well ?&#10;Speaker: Professor D&#10;Content: Yeah , yeah . Yeah , people do something . They {disfmarker} they , uh , they have some , um , uh , in {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Company X changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. This change allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The compression and packing of frames are also modified so that they can pack the frames without waiting for an additional frame." target="1. Company B changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. The new compression scheme allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The packing of frames is also modified so that they can pack the frames without waiting for an additional frame. This change eliminates the delay associated with error correction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Company X changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. This change allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The compression and packing of frames are also modified so that they can pack the frames without waiting for an additional frame." target="Based on the discussion transcript, the outcome of the discussion regarding delay in compression is that both Company B and Company X have changed their compression schemes altogether. They now have their own compression and decoding schemes, as well as their own Cyclic Redundancy Check (CRC) and error correction mechanisms.&#10;&#10;Company B is able to determine within one more frame if the current frame is in error, reducing the delay to less than one frame. They have also modified the packing of frames to eliminate the delay associated with error correction. Similarly, Company X can also detect errors in one more frame, reducing the delay to less than one frame. They have altered both their compression and packing of frames to accomplish this.&#10;&#10;Therefore, both companies have made significant changes to their previous compression schemes to minimize or completely eliminate delays in compression and error correction.">
      <data key="d0">1</data>
    </edge>
    <edge source=" need that forty milliseconds also .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: No . They actually changed the compression scheme altogether .&#10;Speaker: Professor D&#10;Content: Right ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So they have their own compression and decoding scheme and they {disfmarker} I don't know what they have .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: But they have coded zero delay for that . Because they ch I know they changed it , their compression . They have their own CRC , their {disfmarker} their own {vocalsound} error correction mechanism .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: So they don't have to wait more than one more frame to know whether the current frame is in error .&#10;Speaker: Professor D&#10;Content: Oh , OK .&#10;Speaker: PhD B&#10;Content: So they changed the whole thing so that there 's no delay for that compression and {disfmarker} part also .&#10;" target="1. Company B changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. The new compression scheme allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The packing of frames is also modified so that they can pack the frames without waiting for an additional frame. This change eliminates the delay associated with error correction.">
      <data key="d0">1</data>
    </edge>
    <edge source=" need that forty milliseconds also .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: No . They actually changed the compression scheme altogether .&#10;Speaker: Professor D&#10;Content: Right ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So they have their own compression and decoding scheme and they {disfmarker} I don't know what they have .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: But they have coded zero delay for that . Because they ch I know they changed it , their compression . They have their own CRC , their {disfmarker} their own {vocalsound} error correction mechanism .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: So they don't have to wait more than one more frame to know whether the current frame is in error .&#10;Speaker: Professor D&#10;Content: Oh , OK .&#10;Speaker: PhD B&#10;Content: So they changed the whole thing so that there 's no delay for that compression and {disfmarker} part also .&#10;" target="Based on the discussion transcript, the outcome of the discussion regarding delay in compression is that both Company B and Company X have changed their compression schemes altogether. They now have their own compression and decoding schemes, as well as their own Cyclic Redundancy Check (CRC) and error correction mechanisms.&#10;&#10;Company B is able to determine within one more frame if the current frame is in error, reducing the delay to less than one frame. They have also modified the packing of frames to eliminate the delay associated with error correction. Similarly, Company X can also detect errors in one more frame, reducing the delay to less than one frame. They have altered both their compression and packing of frames to accomplish this.&#10;&#10;Therefore, both companies have made significant changes to their previous compression schemes to minimize or completely eliminate delays in compression and error correction.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: f so {disfmarker}&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: they didn't include that .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So {disfmarker}&#10;Speaker: PhD E&#10;Content: Where does the comprish compression in decoding delay comes from ?&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: &#10;Speaker: PhD B&#10;Content: That 's the way the {disfmarker} the {disfmarker} the frames are packed , like you have to wait for one more frame to pack . Because it 's {disfmarker} the CRC is computed for two frames always .&#10;Speaker: Professor D&#10;Content: Well , that {disfmarker} the they would need that forty milliseconds also .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: No ." target="1. Company B changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. The new compression scheme allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The packing of frames is also modified so that they can pack the frames without waiting for an additional frame. This change eliminates the delay associated with error correction.">
      <data key="d0">1</data>
    </edge>
    <edge source=" PhD B&#10;Content: So they changed the whole thing so that there 's no delay for that compression and {disfmarker} part also .&#10;Speaker: Professor D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Even you have reported actually zero delay for the {pause} compression . I thought maybe you also have some different {disfmarker}&#10;Speaker: Professor G&#10;Content: Mmm . Mmm . No , I think I {disfmarker} I used this scheme as it was before .&#10;Speaker: PhD B&#10;Content: OK . Ah . Mm - hmm .&#10;Speaker: PhD F&#10;Content: OK , we 've got twenty minutes so we should {vocalsound} probably try to move along . Uh , did you wanna go next , Stephane ?&#10;Speaker: PhD E&#10;Content: I can go next . Yeah . Mmm .&#10;Speaker: Professor D&#10;Content: Oh . Wait a minute . It 's {disfmarker}&#10;Speaker: PhD E&#10;Content: It 's {disfmarker} Yeah , we have to" target="1. Company B changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. The new compression scheme allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The packing of frames is also modified so that they can pack the frames without waiting for an additional frame. This change eliminates the delay associated with error correction.">
      <data key="d0">1</data>
    </edge>
    <edge source=" PhD B&#10;Content: So they changed the whole thing so that there 's no delay for that compression and {disfmarker} part also .&#10;Speaker: Professor D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Even you have reported actually zero delay for the {pause} compression . I thought maybe you also have some different {disfmarker}&#10;Speaker: Professor G&#10;Content: Mmm . Mmm . No , I think I {disfmarker} I used this scheme as it was before .&#10;Speaker: PhD B&#10;Content: OK . Ah . Mm - hmm .&#10;Speaker: PhD F&#10;Content: OK , we 've got twenty minutes so we should {vocalsound} probably try to move along . Uh , did you wanna go next , Stephane ?&#10;Speaker: PhD E&#10;Content: I can go next . Yeah . Mmm .&#10;Speaker: Professor D&#10;Content: Oh . Wait a minute . It 's {disfmarker}&#10;Speaker: PhD E&#10;Content: It 's {disfmarker} Yeah , we have to" target="Based on the discussion transcript, the outcome of the discussion regarding delay in compression is that both Company B and Company X have changed their compression schemes altogether. They now have their own compression and decoding schemes, as well as their own Cyclic Redundancy Check (CRC) and error correction mechanisms.&#10;&#10;Company B is able to determine within one more frame if the current frame is in error, reducing the delay to less than one frame. They have also modified the packing of frames to eliminate the delay associated with error correction. Similarly, Company X can also detect errors in one more frame, reducing the delay to less than one frame. They have altered both their compression and packing of frames to accomplish this.&#10;&#10;Therefore, both companies have made significant changes to their previous compression schemes to minimize or completely eliminate delays in compression and error correction.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: PhD F&#10;Content: What {disfmarker} where was the , um {disfmarker} the smallest latency of all the systems last time ?&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: The French Telecom .&#10;Speaker: Professor D&#10;Content: Well , France Telecom was {disfmarker} was {disfmarker} was very short latency&#10;Speaker: Professor G&#10;Content: It 's {disfmarker}&#10;Speaker: Professor D&#10;Content: and they had a very good result .&#10;Speaker: PhD F&#10;Content: What {disfmarker} what was it ?&#10;Speaker: Professor D&#10;Content: It was thirty - five .&#10;Speaker: Professor G&#10;Content: It was in the order of thirty milliseconds&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: or {disfmarker}&#10;Speaker: PhD F&#10;Content: Thirteen ?&#10;Speaker: Professor D&#10;Content: th th&#10;Speaker: Professor G&#10;Content: Thirty .&#10;Speaker: PhD F&#10;Content: Thirty .&#10;" target="1. Company B changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. The new compression scheme allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The packing of frames is also modified so that they can pack the frames without waiting for an additional frame. This change eliminates the delay associated with error correction.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: PhD F&#10;Content: What {disfmarker} where was the , um {disfmarker} the smallest latency of all the systems last time ?&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: The French Telecom .&#10;Speaker: Professor D&#10;Content: Well , France Telecom was {disfmarker} was {disfmarker} was very short latency&#10;Speaker: Professor G&#10;Content: It 's {disfmarker}&#10;Speaker: Professor D&#10;Content: and they had a very good result .&#10;Speaker: PhD F&#10;Content: What {disfmarker} what was it ?&#10;Speaker: Professor D&#10;Content: It was thirty - five .&#10;Speaker: Professor G&#10;Content: It was in the order of thirty milliseconds&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: or {disfmarker}&#10;Speaker: PhD F&#10;Content: Thirteen ?&#10;Speaker: Professor D&#10;Content: th th&#10;Speaker: Professor G&#10;Content: Thirty .&#10;Speaker: PhD F&#10;Content: Thirty .&#10;" target="1. The group's current position on constraining the latency is that they are not unduly restricting it at this stage because they are still experimenting with improving performance in noisy conditions. There is a minority within the group who argue for further constraining of the latency, but the majority believes that longer latencies provide benefits, so they are continuing to explore the trade-offs between longer and shorter latencies.&#10;2. The individual who argues for further constraint of latency is not explicitly named in the transcript, but it is mentioned that there is a minority within the group who hold this view.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker} if I use that frame to calculate the mean , then I 'm just gonna get n nothing .&#10;Speaker: Professor D&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: Um ,&#10;Speaker: Professor D&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: so I should probably have some kind of default {vocalsound} mean for the first f couple of frames ?&#10;Speaker: Professor D&#10;Content: Yeah . Yeah .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: Yeah . Or subtract nothing . I mean , it 's {disfmarker}&#10;Speaker: Grad C&#10;Content: Or subtract nothing . And {disfmarker} and that 's {disfmarker} that 's {disfmarker} I guess that 's something that 's p people have figured out how to deal with in cepstral mean subtraction as well ?&#10;Speaker: Professor D&#10;Content: Yeah , yeah . Yeah , people do something . They {disfmarker} they , uh , they have some , um , uh , in {disfmarker" target="1. Cepstral Mean Subtraction (CMS) is a technique used in short-term window analysis to remove general characteristics from data. It involves calculating the mean of the cepstrum (a transform of the log power spectrum) for each window and subtracting it from the cepstrum of that window. This helps get rid of very general characteristics, such as those caused by the recording device or room acoustics.&#10;2. Flipping frames from the front to create negative values is a technique used in some signal processing applications, including CMS. By taking a certain number of frames from the beginning of the signal and flipping them around, the original frames are replaced with their mirror images. This can help balance the signal by reducing the impact of any abrupt changes at the beginning of the window. In the context of CMS, flipping frames creates negative values in the cepstrum that are then subtracted from the original cepstrum to achieve mean subtraction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Moving Average Filter: This is a simple finite impulse response (FIR) filter where each output value is the average of the previous 'n' input values. It can be used to reduce high frequency noise and fluctuations in signals.&#10;2. Median Filter: In this non-linear digital filter, the output value is the median of the input values within the moving window. This filter is useful for removing impulse noise, as it preserves edges better than the mean filter.&#10;3. Exponential (First Order IIR) Filter: This filter calculates the output based on a combination of the current and previous input values, as well as the previous output value. The weights in the filter decrease exponentially over time, making it effective for smoothing signals with slowly changing trends.&#10;4. Butterworth Filter: A type of infinite impulse response (IIR) filter, the Butterworth filter is designed to have a maximally flat frequency response in the passband and minimum attenuation in the stopband. This filter can be used in applications where it's essential to preserve signal shape while removing noise.&#10;5. Chebyshev Filter: Similar to the Butterworth filter, the Chebyshev filter offers a better stopband rejection at the expense of a more uneven passband response. These filters are useful when there is a need for high attenuation in the stopband while maintaining reasonable performance in the passband.&#10;6. Bessel Filter: This filter has a maximally flat group delay (the time difference between input and output signal) in the passband, which helps to preserve signals' wave shape. However, its roll-off rate is slower than Butterworth or Chebyshev filters.&#10;7. LDA Filters: As mentioned in the transcript, Linear Discriminant Analysis (LDA) filters can be trained using supervised learning techniques to optimize a certain criterion, such as minimizing within-class variance or maximizing between-class variance. This filter can help better separate different classes and improve overall performance in signal processing tasks.&#10;&#10;When using LDA for determining the optimal filter based on a criterion, it is essential to consider what specific aspect of the filter design you want to optimize. For instance, you could train an LDA filter to minimize the mean squared error between the original and filtered signals or maximize the signal-to-noise ratio.&#10;&#10;To implement this in practice, you would collect a dataset with input-output pairs representing your desired output signal (clean speech) and noisy input (speech affected by reverberation, background noise, etc.). Then, train an LDA filter using that data, evaluating its performance based on the chosen criterion. Once trained, this LDA filter could be used to process new, noisy signals in order to provide a filtered output signal that meets your desired criteria." target=" result , that if I doesn't use it .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD H&#10;Content: But m that may be because {vocalsound} with this technique {vocalsound} we are using really {disfmarker} really clean speech . The speech {disfmarker} the {comment} representation that go to the HTK is really clean speech because it 's from the dictionary , the code book and maybe from that . I don't know .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD H&#10;Content: Because I think that you {disfmarker} did some experiments using the two {disfmarker} the two LDA filter , clean and noi and noise ,&#10;Speaker: PhD E&#10;Content: It 's {disfmarker}&#10;Speaker: PhD H&#10;Content: and it doesn't matter too much .&#10;Speaker: PhD E&#10;Content: Um , yeah , I did that but it doesn't matter on SpeechDat - Car , but , it matters , uh , a lot on TI - digits .&#10;Speaker: PhD B&#10;Content: Using the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Moving Average Filter: This is a simple finite impulse response (FIR) filter where each output value is the average of the previous 'n' input values. It can be used to reduce high frequency noise and fluctuations in signals.&#10;2. Median Filter: In this non-linear digital filter, the output value is the median of the input values within the moving window. This filter is useful for removing impulse noise, as it preserves edges better than the mean filter.&#10;3. Exponential (First Order IIR) Filter: This filter calculates the output based on a combination of the current and previous input values, as well as the previous output value. The weights in the filter decrease exponentially over time, making it effective for smoothing signals with slowly changing trends.&#10;4. Butterworth Filter: A type of infinite impulse response (IIR) filter, the Butterworth filter is designed to have a maximally flat frequency response in the passband and minimum attenuation in the stopband. This filter can be used in applications where it's essential to preserve signal shape while removing noise.&#10;5. Chebyshev Filter: Similar to the Butterworth filter, the Chebyshev filter offers a better stopband rejection at the expense of a more uneven passband response. These filters are useful when there is a need for high attenuation in the stopband while maintaining reasonable performance in the passband.&#10;6. Bessel Filter: This filter has a maximally flat group delay (the time difference between input and output signal) in the passband, which helps to preserve signals' wave shape. However, its roll-off rate is slower than Butterworth or Chebyshev filters.&#10;7. LDA Filters: As mentioned in the transcript, Linear Discriminant Analysis (LDA) filters can be trained using supervised learning techniques to optimize a certain criterion, such as minimizing within-class variance or maximizing between-class variance. This filter can help better separate different classes and improve overall performance in signal processing tasks.&#10;&#10;When using LDA for determining the optimal filter based on a criterion, it is essential to consider what specific aspect of the filter design you want to optimize. For instance, you could train an LDA filter to minimize the mean squared error between the original and filtered signals or maximize the signal-to-noise ratio.&#10;&#10;To implement this in practice, you would collect a dataset with input-output pairs representing your desired output signal (clean speech) and noisy input (speech affected by reverberation, background noise, etc.). Then, train an LDA filter using that data, evaluating its performance based on the chosen criterion. Once trained, this LDA filter could be used to process new, noisy signals in order to provide a filtered output signal that meets your desired criteria." target="Based on the transcript provided, it appears that there is a difference in performance when using an LDA filter trained with noisy speech versus one trained with clean speech. Specifically, in this context, PhD H mentions that &quot;with the noise I have worse result, that if I doesn't use it&quot; and &quot;it matters, uh, a lot on TI - digits.&quot; Later in the conversation, it is clarified that the results being discussed are associated with the clean LDA filter, rather than the noisy one.&#10;&#10;This suggests that, in this particular application, using an LDA filter trained with clean speech leads to better results compared to using a filter trained with noisy speech. This may be due to the fact that, as PhD H notes, &quot;with this technique we are using really clean speech&quot; and &quot;the representation that goes to the HTK is really clean speech because it's from the dictionary, the code book and maybe from that.&quot; Therefore, a filter trained with such high-quality data may be more effective at improving signal processing performance.&#10;&#10;It's also worth noting that there might be specific cases or datasets where an LDA filter trained with noisy speech could lead to better results, as PhD E mentions &quot;it doesn't matter too much on SpeechDat - Car, but it matters, uh, a lot on TI - digits.&quot; However, in the context of this conversation, the consensus seems to be that using an LDA filter trained with clean speech is more beneficial.&#10;&#10;To summarize, using an LDA filter trained with clean speech in this specific application yields better results compared to using a filter trained with noisy speech. This may be due to the high-quality data used for training the filter, which helps improve signal processing performance in this case. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.">
      <data key="d0">1</data>
    </edge>
    <edge source=" result , that if I doesn't use it .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD H&#10;Content: But m that may be because {vocalsound} with this technique {vocalsound} we are using really {disfmarker} really clean speech . The speech {disfmarker} the {comment} representation that go to the HTK is really clean speech because it 's from the dictionary , the code book and maybe from that . I don't know .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD H&#10;Content: Because I think that you {disfmarker} did some experiments using the two {disfmarker} the two LDA filter , clean and noi and noise ,&#10;Speaker: PhD E&#10;Content: It 's {disfmarker}&#10;Speaker: PhD H&#10;Content: and it doesn't matter too much .&#10;Speaker: PhD E&#10;Content: Um , yeah , I did that but it doesn't matter on SpeechDat - Car , but , it matters , uh , a lot on TI - digits .&#10;Speaker: PhD B&#10;Content: Using the" target="Based on the transcript provided, it appears that there is a difference in performance when using an LDA filter trained with noisy speech versus one trained with clean speech. Specifically, in this context, PhD H mentions that &quot;with the noise I have worse result, that if I doesn't use it&quot; and &quot;it matters, uh, a lot on TI - digits.&quot; Later in the conversation, it is clarified that the results being discussed are associated with the clean LDA filter, rather than the noisy one.&#10;&#10;This suggests that, in this particular application, using an LDA filter trained with clean speech leads to better results compared to using a filter trained with noisy speech. This may be due to the fact that, as PhD H notes, &quot;with this technique we are using really clean speech&quot; and &quot;the representation that goes to the HTK is really clean speech because it's from the dictionary, the code book and maybe from that.&quot; Therefore, a filter trained with such high-quality data may be more effective at improving signal processing performance.&#10;&#10;It's also worth noting that there might be specific cases or datasets where an LDA filter trained with noisy speech could lead to better results, as PhD E mentions &quot;it doesn't matter too much on SpeechDat - Car, but it matters, uh, a lot on TI - digits.&quot; However, in the context of this conversation, the consensus seems to be that using an LDA filter trained with clean speech is more beneficial.&#10;&#10;To summarize, using an LDA filter trained with clean speech in this specific application yields better results compared to using a filter trained with noisy speech. This may be due to the high-quality data used for training the filter, which helps improve signal processing performance in this case. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.">
      <data key="d0">1</data>
    </edge>
    <edge source=" result , that if I doesn't use it .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD H&#10;Content: But m that may be because {vocalsound} with this technique {vocalsound} we are using really {disfmarker} really clean speech . The speech {disfmarker} the {comment} representation that go to the HTK is really clean speech because it 's from the dictionary , the code book and maybe from that . I don't know .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD H&#10;Content: Because I think that you {disfmarker} did some experiments using the two {disfmarker} the two LDA filter , clean and noi and noise ,&#10;Speaker: PhD E&#10;Content: It 's {disfmarker}&#10;Speaker: PhD H&#10;Content: and it doesn't matter too much .&#10;Speaker: PhD E&#10;Content: Um , yeah , I did that but it doesn't matter on SpeechDat - Car , but , it matters , uh , a lot on TI - digits .&#10;Speaker: PhD B&#10;Content: Using the" target="1. Improved performance: Using an LDA filter trained with clean speech results in better performance for TI-digits compared to using a filter trained with noisy speech. This is because the high-quality data used for training the filter helps improve signal processing performance in this case.&#10;2. Consistency: In the context of the conversation, PhD B mentions that they get the best performance in the case of &quot;Car&quot; (the third column in condition A) when using the clean LDA filter.&#10;3. Specific application: The speakers are focusing on a specific application where clean speech is more beneficial. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.&#10;4. Confirmation of LDA filter type: The speakers clarify that they have been discussing the performance of a clean LDA filter as opposed to a noisy one, and this confirmation highlights the significance of using the correct LDA filter for specific datasets like TI-digits.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, it appears that there is a difference in performance when using an LDA filter trained with noisy speech versus one trained with clean speech. Specifically, in this context, PhD H mentions that &quot;with the noise I have worse result, that if I doesn't use it&quot; and &quot;it matters, uh, a lot on TI - digits.&quot; Later in the conversation, it is clarified that the results being discussed are associated with the clean LDA filter, rather than the noisy one.&#10;&#10;This suggests that, in this particular application, using an LDA filter trained with clean speech leads to better results compared to using a filter trained with noisy speech. This may be due to the fact that, as PhD H notes, &quot;with this technique we are using really clean speech&quot; and &quot;the representation that goes to the HTK is really clean speech because it's from the dictionary, the code book and maybe from that.&quot; Therefore, a filter trained with such high-quality data may be more effective at improving signal processing performance.&#10;&#10;It's also worth noting that there might be specific cases or datasets where an LDA filter trained with noisy speech could lead to better results, as PhD E mentions &quot;it doesn't matter too much on SpeechDat - Car, but it matters, uh, a lot on TI - digits.&quot; However, in the context of this conversation, the consensus seems to be that using an LDA filter trained with clean speech is more beneficial.&#10;&#10;To summarize, using an LDA filter trained with clean speech in this specific application yields better results compared to using a filter trained with noisy speech. This may be due to the high-quality data used for training the filter, which helps improve signal processing performance in this case. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech." target=" matter on SpeechDat - Car , but , it matters , uh , a lot on TI - digits .&#10;Speaker: PhD B&#10;Content: Using the clean filter .&#10;Speaker: PhD H&#10;Content: It 's better to use clean .&#10;Speaker: PhD E&#10;Content: Yeah , d uh , it 's much better when you {disfmarker} we used the clean derived LDA filter .&#10;Speaker: PhD H&#10;Content: Mm - hmm . Maybe you can do d also this .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD H&#10;Content: To use clean speech .&#10;Speaker: PhD B&#10;Content: Yeah , I 'll try .&#10;Speaker: PhD E&#10;Content: Uh , but , yeah , Sunil in {disfmarker} in your result it 's {disfmarker}&#10;Speaker: PhD B&#10;Content: I {disfmarker} I 'll try the cle No , I {disfmarker} I {disfmarker} my result is with the noisy {disfmarker} noisy LDA .&#10;Speaker: PhD E&#10;Content: It 's with the">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, it appears that there is a difference in performance when using an LDA filter trained with noisy speech versus one trained with clean speech. Specifically, in this context, PhD H mentions that &quot;with the noise I have worse result, that if I doesn't use it&quot; and &quot;it matters, uh, a lot on TI - digits.&quot; Later in the conversation, it is clarified that the results being discussed are associated with the clean LDA filter, rather than the noisy one.&#10;&#10;This suggests that, in this particular application, using an LDA filter trained with clean speech leads to better results compared to using a filter trained with noisy speech. This may be due to the fact that, as PhD H notes, &quot;with this technique we are using really clean speech&quot; and &quot;the representation that goes to the HTK is really clean speech because it's from the dictionary, the code book and maybe from that.&quot; Therefore, a filter trained with such high-quality data may be more effective at improving signal processing performance.&#10;&#10;It's also worth noting that there might be specific cases or datasets where an LDA filter trained with noisy speech could lead to better results, as PhD E mentions &quot;it doesn't matter too much on SpeechDat - Car, but it matters, uh, a lot on TI - digits.&quot; However, in the context of this conversation, the consensus seems to be that using an LDA filter trained with clean speech is more beneficial.&#10;&#10;To summarize, using an LDA filter trained with clean speech in this specific application yields better results compared to using a filter trained with noisy speech. This may be due to the high-quality data used for training the filter, which helps improve signal processing performance in this case. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech." target="er} my result is with the noisy {disfmarker} noisy LDA .&#10;Speaker: PhD E&#10;Content: It 's with the noisy one . Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: Oh !&#10;Speaker: PhD B&#10;Content: It 's with the noisy . Yeah . It 's {disfmarker} it 's not the clean LDA .&#10;Speaker: PhD E&#10;Content: So {disfmarker}&#10;Speaker: Professor D&#10;Content: Um {disfmarker}&#10;Speaker: PhD B&#10;Content: It 's {disfmarker} In {disfmarker} in the front sheet , I have like {disfmarker} like the summary . Yeah .&#10;Speaker: Professor D&#10;Content: And {disfmarker} and your result {comment} is with the {disfmarker}&#10;Speaker: PhD E&#10;Content: It 's with the clean LDA .&#10;Speaker: PhD B&#10;Content: Oh . This is {disfmarker} Your results are all with the clean LDA result ?&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, it appears that there is a difference in performance when using an LDA filter trained with noisy speech versus one trained with clean speech. Specifically, in this context, PhD H mentions that &quot;with the noise I have worse result, that if I doesn't use it&quot; and &quot;it matters, uh, a lot on TI - digits.&quot; Later in the conversation, it is clarified that the results being discussed are associated with the clean LDA filter, rather than the noisy one.&#10;&#10;This suggests that, in this particular application, using an LDA filter trained with clean speech leads to better results compared to using a filter trained with noisy speech. This may be due to the fact that, as PhD H notes, &quot;with this technique we are using really clean speech&quot; and &quot;the representation that goes to the HTK is really clean speech because it's from the dictionary, the code book and maybe from that.&quot; Therefore, a filter trained with such high-quality data may be more effective at improving signal processing performance.&#10;&#10;It's also worth noting that there might be specific cases or datasets where an LDA filter trained with noisy speech could lead to better results, as PhD E mentions &quot;it doesn't matter too much on SpeechDat - Car, but it matters, uh, a lot on TI - digits.&quot; However, in the context of this conversation, the consensus seems to be that using an LDA filter trained with clean speech is more beneficial.&#10;&#10;To summarize, using an LDA filter trained with clean speech in this specific application yields better results compared to using a filter trained with noisy speech. This may be due to the high-quality data used for training the filter, which helps improve signal processing performance in this case. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech." target="1. Improved performance: Using an LDA filter trained with clean speech results in better performance for TI-digits compared to using a filter trained with noisy speech. This is because the high-quality data used for training the filter helps improve signal processing performance in this case.&#10;2. Consistency: In the context of the conversation, PhD B mentions that they get the best performance in the case of &quot;Car&quot; (the third column in condition A) when using the clean LDA filter.&#10;3. Specific application: The speakers are focusing on a specific application where clean speech is more beneficial. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.&#10;4. Confirmation of LDA filter type: The speakers clarify that they have been discussing the performance of a clean LDA filter as opposed to a noisy one, and this confirmation highlights the significance of using the correct LDA filter for specific datasets like TI-digits.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, it appears that there is a difference in performance when using an LDA filter trained with noisy speech versus one trained with clean speech. Specifically, in this context, PhD H mentions that &quot;with the noise I have worse result, that if I doesn't use it&quot; and &quot;it matters, uh, a lot on TI - digits.&quot; Later in the conversation, it is clarified that the results being discussed are associated with the clean LDA filter, rather than the noisy one.&#10;&#10;This suggests that, in this particular application, using an LDA filter trained with clean speech leads to better results compared to using a filter trained with noisy speech. This may be due to the fact that, as PhD H notes, &quot;with this technique we are using really clean speech&quot; and &quot;the representation that goes to the HTK is really clean speech because it's from the dictionary, the code book and maybe from that.&quot; Therefore, a filter trained with such high-quality data may be more effective at improving signal processing performance.&#10;&#10;It's also worth noting that there might be specific cases or datasets where an LDA filter trained with noisy speech could lead to better results, as PhD E mentions &quot;it doesn't matter too much on SpeechDat - Car, but it matters, uh, a lot on TI - digits.&quot; However, in the context of this conversation, the consensus seems to be that using an LDA filter trained with clean speech is more beneficial.&#10;&#10;To summarize, using an LDA filter trained with clean speech in this specific application yields better results compared to using a filter trained with noisy speech. This may be due to the high-quality data used for training the filter, which helps improve signal processing performance in this case. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech." target="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable.">
      <data key="d0">1</data>
    </edge>
    <edge source=" matter on SpeechDat - Car , but , it matters , uh , a lot on TI - digits .&#10;Speaker: PhD B&#10;Content: Using the clean filter .&#10;Speaker: PhD H&#10;Content: It 's better to use clean .&#10;Speaker: PhD E&#10;Content: Yeah , d uh , it 's much better when you {disfmarker} we used the clean derived LDA filter .&#10;Speaker: PhD H&#10;Content: Mm - hmm . Maybe you can do d also this .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD H&#10;Content: To use clean speech .&#10;Speaker: PhD B&#10;Content: Yeah , I 'll try .&#10;Speaker: PhD E&#10;Content: Uh , but , yeah , Sunil in {disfmarker} in your result it 's {disfmarker}&#10;Speaker: PhD B&#10;Content: I {disfmarker} I 'll try the cle No , I {disfmarker} I {disfmarker} my result is with the noisy {disfmarker} noisy LDA .&#10;Speaker: PhD E&#10;Content: It 's with the" target="1. Improved performance: Using an LDA filter trained with clean speech results in better performance for TI-digits compared to using a filter trained with noisy speech. This is because the high-quality data used for training the filter helps improve signal processing performance in this case.&#10;2. Consistency: In the context of the conversation, PhD B mentions that they get the best performance in the case of &quot;Car&quot; (the third column in condition A) when using the clean LDA filter.&#10;3. Specific application: The speakers are focusing on a specific application where clean speech is more beneficial. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.&#10;4. Confirmation of LDA filter type: The speakers clarify that they have been discussing the performance of a clean LDA filter as opposed to a noisy one, and this confirmation highlights the significance of using the correct LDA filter for specific datasets like TI-digits.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The text discusses a meeting between several individuals, referred to as PhD B, Professor G, PhD E, PhD D, PhD E, and PhD H, who are involved in a project that involves analyzing audio data.&#10;2. The speakers mention that the audio data contains a small set of words (few transcriptions) and has music in the background. This is indicated when PhD B says &quot;It has very few...words also&quot; and &quot;It's a very, very small set, actually&quot; and Professor G says &quot;there is a lot of utterances with music in the background.&quot;&#10;3. Furthermore, the speakers express their dislike for the music, as indicated when PhD B says &quot;It has some music also...I mean, very horrible music like I know.&quot;&#10;4. Therefore, it can be said that the text contains audio data with a small set of words and music in the background, and the speakers express their dislike for the music." target=": PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: It has a very few at {disfmarker} uh , actually , c uh , tran I mean , words also .&#10;Speaker: Professor G&#10;Content: I mean , that is {disfmarker} Yeah ,&#10;Speaker: PhD B&#10;Content: It 's a very , very small set , actually .&#10;Speaker: Professor G&#10;Content: that too . Yeah . Uh - huh .&#10;Speaker: PhD B&#10;Content: So there is {disfmarker}&#10;Speaker: Professor G&#10;Content: There is a l a {disfmarker} There is a lot of {disfmarker} Uh , there are a lot of utterances with music in {disfmarker} with music in the background .&#10;Speaker: PhD B&#10;Content: Yeah . Yeah , yeah , yeah . Yeah .&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: Yeah . It has some music also . I mean , very horrible music like like I know .&#10;Speaker: Professor D">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The text discusses a meeting between several individuals, referred to as PhD B, Professor G, PhD E, PhD D, PhD E, and PhD H, who are involved in a project that involves analyzing audio data.&#10;2. The speakers mention that the audio data contains a small set of words (few transcriptions) and has music in the background. This is indicated when PhD B says &quot;It has very few...words also&quot; and &quot;It's a very, very small set, actually&quot; and Professor G says &quot;there is a lot of utterances with music in the background.&quot;&#10;3. Furthermore, the speakers express their dislike for the music, as indicated when PhD B says &quot;It has some music also...I mean, very horrible music like I know.&quot;&#10;4. Therefore, it can be said that the text contains audio data with a small set of words and music in the background, and the speakers express their dislike for the music." target=" to talk about that . But , well , the {disfmarker} the &quot; Car &quot; noises are below like five hundred hertz . And we were looking at the &quot; Music &quot; utterances and in this case the noise is more about two thousand hertz .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Well , the music energy 's very low apparently . Uh , uh , from zero to two {disfmarker} two thousand hertz . So maybe just looking at this frequency range for {disfmarker} from five hundred to two thousand would improve somewhat the VAD&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: PhD E&#10;Content: and {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mmm {disfmarker}&#10;Speaker: PhD B&#10;Content: So there are like some {disfmarker} some s some parameters you wanted to use or something ?&#10;Speaker: PhD E&#10;Content: Yeah , but {disfmarker} Yes .&#10;Speaker: PhD B&#10;Content: Or {disfmarker} Yeah .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The text discusses a meeting between several individuals, referred to as PhD B, Professor G, PhD E, PhD D, PhD E, and PhD H, who are involved in a project that involves analyzing audio data.&#10;2. The speakers mention that the audio data contains a small set of words (few transcriptions) and has music in the background. This is indicated when PhD B says &quot;It has very few...words also&quot; and &quot;It's a very, very small set, actually&quot; and Professor G says &quot;there is a lot of utterances with music in the background.&quot;&#10;3. Furthermore, the speakers express their dislike for the music, as indicated when PhD B says &quot;It has some music also...I mean, very horrible music like I know.&quot;&#10;4. Therefore, it can be said that the text contains audio data with a small set of words and music in the background, and the speakers express their dislike for the music." target=" 's Italian TI - digits .&#10;Speaker: Professor D&#10;Content: Yeah . Oh , it 's trained on Italian ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: Yeah , OK .&#10;Speaker: PhD E&#10;Content: Mm - hmm . And {disfmarker}&#10;Speaker: PhD B&#10;Content: That 's right .&#10;Speaker: Professor D&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: And also there are like funny noises on Finnish more than on Italian . I mean , like music&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Yeah . Yeah , the {disfmarker} Yeah , it 's true .&#10;Speaker: PhD E&#10;Content: and {vocalsound} um {disfmarker} So , yeah , we were looking at this . But for most of the noises , noises are {disfmarker} um , I don't know if we want to talk about that . But , well , the {disfmarker} the &quot; Car &quot; noises are below like five hundred hertz . And we were">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The text discusses a meeting between several individuals, referred to as PhD B, Professor G, PhD E, PhD D, PhD E, and PhD H, who are involved in a project that involves analyzing audio data.&#10;2. The speakers mention that the audio data contains a small set of words (few transcriptions) and has music in the background. This is indicated when PhD B says &quot;It has very few...words also&quot; and &quot;It's a very, very small set, actually&quot; and Professor G says &quot;there is a lot of utterances with music in the background.&quot;&#10;3. Furthermore, the speakers express their dislike for the music, as indicated when PhD B says &quot;It has some music also...I mean, very horrible music like I know.&quot;&#10;4. Therefore, it can be said that the text contains audio data with a small set of words and music in the background, and the speakers express their dislike for the music." target="&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: so .&#10;Speaker: Professor D&#10;Content: Yeah . Yeah . OK .&#10;Speaker: PhD F&#10;Content: Carmen ? Do you , uh {disfmarker}&#10;Speaker: PhD H&#10;Content: Well , I only say that the {disfmarker} this is , a summary of the {disfmarker} of all the VTS experiments and say that the result in the last {comment} um , for Italian {disfmarker} the last experiment for Italian , {vocalsound} are bad . I make a mistake when I write . Up at D I copy {vocalsound} one of the bad result .&#10;Speaker: PhD B&#10;Content: So you {disfmarker}&#10;Speaker: PhD H&#10;Content: And {disfmarker} There . {vocalsound} You know , this . Um , well . If we put everything , we improve a lot u the spectral use of the VTS but the final result {vocalsound} are not still mmm , good {vocalsound} like the Wiener filter for example . I don't">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The text discusses a meeting between several individuals, referred to as PhD B, Professor G, PhD E, PhD D, PhD E, and PhD H, who are involved in a project that involves analyzing audio data.&#10;2. The speakers mention that the audio data contains a small set of words (few transcriptions) and has music in the background. This is indicated when PhD B says &quot;It has very few...words also&quot; and &quot;It's a very, very small set, actually&quot; and Professor G says &quot;there is a lot of utterances with music in the background.&quot;&#10;3. Furthermore, the speakers express their dislike for the music, as indicated when PhD B says &quot;It has some music also...I mean, very horrible music like I know.&quot;&#10;4. Therefore, it can be said that the text contains audio data with a small set of words and music in the background, and the speakers express their dislike for the music." target="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The text discusses a meeting between several individuals, referred to as PhD B, Professor G, PhD E, PhD D, PhD E, and PhD H, who are involved in a project that involves analyzing audio data.&#10;2. The speakers mention that the audio data contains a small set of words (few transcriptions) and has music in the background. This is indicated when PhD B says &quot;It has very few...words also&quot; and &quot;It's a very, very small set, actually&quot; and Professor G says &quot;there is a lot of utterances with music in the background.&quot;&#10;3. Furthermore, the speakers express their dislike for the music, as indicated when PhD B says &quot;It has some music also...I mean, very horrible music like I know.&quot;&#10;4. Therefore, it can be said that the text contains audio data with a small set of words and music in the background, and the speakers express their dislike for the music." target="1. The best performance for the &quot;Car&quot; category was achieved when using an LDA filter trained with clean speech (TI-digits). This resulted in better signal processing performance compared to a filter trained with noisy speech.&#10;2. Although this specific conversation focuses on the improved performance of the clean LDA filter for TI-digits, it is essential to consider that different contexts or datasets might require different LDA filter designs, including those based on noisy speech.&#10;3. In this case, the &quot;Car&quot; category had the best performance in the third column of condition A, but specific numerical values or metrics were not provided in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="&#10;Speaker: PhD B&#10;Content:  It 's {disfmarker}&#10;Speaker: Professor G&#10;Content: And that we , so , introduce again some natural behavior in this trajectory .&#10;Speaker: Professor D&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Very different from speech . Still , I mean , it shouldn't confuse the {disfmarker}&#10;Speaker: Professor G&#10;Content: Yeah , I mean , similar to what {disfmarker} what you see really u in {disfmarker} in the real um noisy situation .&#10;Speaker: PhD B&#10;Content: OK . Mm - hmm .&#10;Speaker: Professor G&#10;Content: Or i in the clean situation . But {disfmarker} but somehow a {disfmarker} a natural distribution .&#10;Speaker: Professor D&#10;Content: But isn't that s again sort of the idea of the additive thing , if it {disfmarker} as {disfmarker} as we had in the J stuff ? I mean , basically if {disfmarker} {vocalsound} if you have random">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="m proposing to do , and s and stuff ?&#10;Speaker: Professor D&#10;Content: Yes , briefly .&#10;Speaker: PhD F&#10;Content: Yeah briefly .&#10;Speaker: Grad A&#10;Content: OK . Um , so briefly , {vocalsound} I 'm proposing to do a n a new p approach to speech recognition using um , a combination of , uh , multi - band ideas and ideas , um , {vocalsound} {vocalsound} {comment} about the uh , acoustic phonec phonetic approach to speech recognition . Um , so I will be using {vocalsound} these graphical models that {disfmarker} um , that implement the multi - band approach {vocalsound} to recognize a set of intermediate categories that might involve , uh , things like phonetic features {vocalsound} or other {disfmarker} other f feature things that are more closely related to the acoustic signal itself . Um , and the hope in all of this is that by going multi - band and by going into these , {vocalsound} um intermediate classifications , {vocalsound} that we can get a system that 's more robust to {disfmarker} to unseen noises">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="isfmarker} as we had in the J stuff ? I mean , basically if {disfmarker} {vocalsound} if you have random data , um , in {disfmarker} in the time domain , then when you look at the s spectrum it 's gonna be pretty flat . And {disfmarker} and ,&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: uh , so just add something everywhere rather than just in those places . It 's just a constant , right ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Yeah . I think {disfmarker} e yeah . It 's {disfmarker} it 's just especially in these segments , I mean , you introduce , um , very artificial behavior .&#10;Speaker: Professor D&#10;Content: Yeah . Yeah .&#10;Speaker: Professor G&#10;Content: And {disfmarker}&#10;Speaker: Professor D&#10;Content: Well , see if you add something everywhere , it has almost no effect up {disfmarker} up {disfmarker} up on {">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="1. Company B changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. The new compression scheme allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The packing of frames is also modified so that they can pack the frames without waiting for an additional frame. This change eliminates the delay associated with error correction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="1. Fine-tuning ten or twelve small aspects, each contributing around 0.3%, can significantly improve a system's accuracy from 95% to 9">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="1. The speaker, referred to as PhD F, has been researching speech recognition with a focus on creating a more robust system for word or phone recognition. This involves developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. One of the specific research questions they have been addressing is what types of intermediate categories are needed for classification. Another question relates to the structure of multi-band graphical models, specifically what types of structures should be considered in order to effectively combine evidence from sub-bands.&#10;3. Additionally, PhD F has been investigating how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. To accomplish this, they have proposed a new approach to speech recognition that combines multi-band ideas with an acoustic phonetic approach. This approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features more closely related to the acoustic signal itself.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="1. The professors and PhDs are discussing adjusting the value of the constant added to the signal energy to achieve a desired signal-to-noise ratio (SNR). This constant is determined based on the average speech energy measured from a dataset of one language, such as Italian, and then used as the mean speech energy for all other languages.&#10;2. They are also considering how dependent the performance is on the value of this constant and exploring different values to find the optimal SNR range for the system.&#10;3. Additionally, there is mention of potentially adjusting noise addition or thresholds based on the average energy levels in each language. For example, Finnish has a lower average energy compared to other databases, so the threshold for Finnish should be lower accordingly.&#10;4. Lastly, they touch upon the impracticality of measuring the average energy over an extended period in real-world applications and suggest using a fixed number derived from other methods instead.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="1. The speakers believe that the models are not complex enough to absorb additional variability in the data because they have observed that introducing a &quot;natural distribution&quot; to the audio data, which would make it resemble real-world noisy situations more closely, increases the degradation of the speech processing algorithm. They suggest that this could be due to the models' inability to effectively model and account for the increased variability. Instead, the added noise results in additional errors or degradation in the system's performance.&#10;2. This explanation can be found when PhD B says, &quot;the models are not complex enough to absorb that additional variability that you're introducing&quot; and later reiterated by PhD E, who shares their experience with neural networks being more capable of modeling strange distributions.&#10;3. The speakers also suggest that this natural distribution approach more accurately represents real-world noisy situations, indicating that the current models might not be sophisticated enough to account for these complexities without compromising performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="1. The group's current position on constraining the latency is that they are not unduly restricting it at this stage because they are still experimenting with improving performance in noisy conditions. There is a minority within the group who argue for further constraining of the latency, but the majority believes that longer latencies provide benefits, so they are continuing to explore the trade-offs between longer and shorter latencies.&#10;2. The individual who argues for further constraint of latency is not explicitly named in the transcript, but it is mentioned that there is a minority within the group who hold this view.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="1. Hamming or Hanning Window: These window functions can be used to reduce the weight of values at the edges when applying a moving window to a signal. They decrease the impact of abrupt changes between windows and help smooth the signal, which can make the training process more stable. In the context of the &quot;Aurora stuff,&quot; these window functions could be applied when processing audio data as part of a speech recognition system.&#10;2. LDA Filter (Linear Discriminant Analysis filter): The LDA filter is a supervised learning algorithm that can optimize some criterion, such as minimizing within-class variance or maximizing between-class variance. This results in a filter that better separates different classes and improves the performance of the system. In the context of the &quot;Aurora stuff,&quot; an LDA filter could be trained on labeled data to improve speech recognition accuracy.&#10;3. Spectral Subtraction or Wiener Filtering: These techniques reduce noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum, improving the signal-to-noise ratio and making it easier to extract useful information from noisy data. In the context of the &quot;Aurora stuff,&quot; these methods could be applied when processing audio signals to improve speech recognition performance.&#10;&#10;These filters, along with a uniform window, are options for moving window setups in training LDA filters or other signal processing tasks. The choice of filter will depend on the specific application and the characteristics of the data being processed. In the transcript, it is mentioned that spectral subtraction was not effective for certain Aurora tasks, suggesting that a different filter or approach might be needed.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="1. The speaker, Grad A, is proposing a new approach to speech recognition that combines multi-band ideas with an acoustic-phonetic approach. This approach aims to create a more robust system for word or phone recognition by developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. The new approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features closely related to the acoustic signal itself, within the context of a multi-band model. The hope is that by going multi-band and using these intermediate classifications, the system will be more robust in unseen noisy situations.&#10;3. Grad A plans to investigate several research questions, including what types of intermediate categories are needed for classification, the structure of multi-band graphical models (specifically, what types of structures should be considered to effectively combine evidence from sub-bands), and how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. This new approach is motivated by the need to address real-world noisy situations more accurately, potentially through the introduction of a &quot;natural distribution&quot; of noise rather than simply adding constant or flat noise across all regions. This could lead to a decrease in word error rate and better performance in signal processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the idea of introducing a &quot;natural distribution&quot; to the audio data they are analyzing, which would make it resemble real-world noisy situations more closely.&#10;2. This concept is contrasted with what they refer to as the &quot;additive thing&quot; in the J stuff, which may be a reference to a previous discussion or approach related to adding noise to the data.&#10;3. The natural distribution being discussed would involve reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;4. This approach is distinguished from simply adding noise everywhere, which would result in a flat spectrum and have little effect on the overall noise level.&#10;5. The speakers suggest that this natural distribution more accurately represents real-world noisy situations and could lead to a decrease in word error rate." target="1. The speakers discussed introducing a &quot;natural distribution&quot; of noise to the audio data analyzed in the SmartKom project's ASRU paper. This approach aims to make the audio data resemble real-world noisy situations more closely by reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;&#10;2. Two research questions arise from this discussion:&#10;  a. Does this natural distribution approach still work effectively even if the speakers are involved?&#10;  b. This question is cut off in the transcript but presumably relates to testing the effectiveness of the natural distribution approach under different conditions or with various types of noise.">
      <data key="d0">1</data>
    </edge>
    <edge source="m proposing to do , and s and stuff ?&#10;Speaker: Professor D&#10;Content: Yes , briefly .&#10;Speaker: PhD F&#10;Content: Yeah briefly .&#10;Speaker: Grad A&#10;Content: OK . Um , so briefly , {vocalsound} I 'm proposing to do a n a new p approach to speech recognition using um , a combination of , uh , multi - band ideas and ideas , um , {vocalsound} {vocalsound} {comment} about the uh , acoustic phonec phonetic approach to speech recognition . Um , so I will be using {vocalsound} these graphical models that {disfmarker} um , that implement the multi - band approach {vocalsound} to recognize a set of intermediate categories that might involve , uh , things like phonetic features {vocalsound} or other {disfmarker} other f feature things that are more closely related to the acoustic signal itself . Um , and the hope in all of this is that by going multi - band and by going into these , {vocalsound} um intermediate classifications , {vocalsound} that we can get a system that 's more robust to {disfmarker} to unseen noises" target="1. The speaker, referred to as PhD F, has been researching speech recognition with a focus on creating a more robust system for word or phone recognition. This involves developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. One of the specific research questions they have been addressing is what types of intermediate categories are needed for classification. Another question relates to the structure of multi-band graphical models, specifically what types of structures should be considered in order to effectively combine evidence from sub-bands.&#10;3. Additionally, PhD F has been investigating how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. To accomplish this, they have proposed a new approach to speech recognition that combines multi-band ideas with an acoustic phonetic approach. This approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features more closely related to the acoustic signal itself.">
      <data key="d0">1</data>
    </edge>
    <edge source="m proposing to do , and s and stuff ?&#10;Speaker: Professor D&#10;Content: Yes , briefly .&#10;Speaker: PhD F&#10;Content: Yeah briefly .&#10;Speaker: Grad A&#10;Content: OK . Um , so briefly , {vocalsound} I 'm proposing to do a n a new p approach to speech recognition using um , a combination of , uh , multi - band ideas and ideas , um , {vocalsound} {vocalsound} {comment} about the uh , acoustic phonec phonetic approach to speech recognition . Um , so I will be using {vocalsound} these graphical models that {disfmarker} um , that implement the multi - band approach {vocalsound} to recognize a set of intermediate categories that might involve , uh , things like phonetic features {vocalsound} or other {disfmarker} other f feature things that are more closely related to the acoustic signal itself . Um , and the hope in all of this is that by going multi - band and by going into these , {vocalsound} um intermediate classifications , {vocalsound} that we can get a system that 's more robust to {disfmarker} to unseen noises" target="1. Hamming or Hanning Window: These window functions can be used to reduce the weight of values at the edges when applying a moving window to a signal. They decrease the impact of abrupt changes between windows and help smooth the signal, which can make the training process more stable. In the context of the &quot;Aurora stuff,&quot; these window functions could be applied when processing audio data as part of a speech recognition system.&#10;2. LDA Filter (Linear Discriminant Analysis filter): The LDA filter is a supervised learning algorithm that can optimize some criterion, such as minimizing within-class variance or maximizing between-class variance. This results in a filter that better separates different classes and improves the performance of the system. In the context of the &quot;Aurora stuff,&quot; an LDA filter could be trained on labeled data to improve speech recognition accuracy.&#10;3. Spectral Subtraction or Wiener Filtering: These techniques reduce noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum, improving the signal-to-noise ratio and making it easier to extract useful information from noisy data. In the context of the &quot;Aurora stuff,&quot; these methods could be applied when processing audio signals to improve speech recognition performance.&#10;&#10;These filters, along with a uniform window, are options for moving window setups in training LDA filters or other signal processing tasks. The choice of filter will depend on the specific application and the characteristics of the data being processed. In the transcript, it is mentioned that spectral subtraction was not effective for certain Aurora tasks, suggesting that a different filter or approach might be needed.">
      <data key="d0">1</data>
    </edge>
    <edge source="m proposing to do , and s and stuff ?&#10;Speaker: Professor D&#10;Content: Yes , briefly .&#10;Speaker: PhD F&#10;Content: Yeah briefly .&#10;Speaker: Grad A&#10;Content: OK . Um , so briefly , {vocalsound} I 'm proposing to do a n a new p approach to speech recognition using um , a combination of , uh , multi - band ideas and ideas , um , {vocalsound} {vocalsound} {comment} about the uh , acoustic phonec phonetic approach to speech recognition . Um , so I will be using {vocalsound} these graphical models that {disfmarker} um , that implement the multi - band approach {vocalsound} to recognize a set of intermediate categories that might involve , uh , things like phonetic features {vocalsound} or other {disfmarker} other f feature things that are more closely related to the acoustic signal itself . Um , and the hope in all of this is that by going multi - band and by going into these , {vocalsound} um intermediate classifications , {vocalsound} that we can get a system that 's more robust to {disfmarker} to unseen noises" target="1. The speaker, Grad A, is proposing a new approach to speech recognition that combines multi-band ideas with an acoustic-phonetic approach. This approach aims to create a more robust system for word or phone recognition by developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. The new approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features closely related to the acoustic signal itself, within the context of a multi-band model. The hope is that by going multi-band and using these intermediate classifications, the system will be more robust in unseen noisy situations.&#10;3. Grad A plans to investigate several research questions, including what types of intermediate categories are needed for classification, the structure of multi-band graphical models (specifically, what types of structures should be considered to effectively combine evidence from sub-bands), and how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. This new approach is motivated by the need to address real-world noisy situations more accurately, potentially through the introduction of a &quot;natural distribution&quot; of noise rather than simply adding constant or flat noise across all regions. This could lead to a decrease in word error rate and better performance in signal processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Company B changed their compression scheme altogether.&#10;2. They implemented their own compression and decoding scheme.&#10;3. They have their own Cyclic Redundancy Check (CRC) and error correction mechanism.&#10;4. The new compression scheme allows them to know within one more frame whether the current frame is in error, reducing the delay to less than one frame.&#10;5. The packing of frames is also modified so that they can pack the frames without waiting for an additional frame. This change eliminates the delay associated with error correction." target="Based on the discussion transcript, the outcome of the discussion regarding delay in compression is that both Company B and Company X have changed their compression schemes altogether. They now have their own compression and decoding schemes, as well as their own Cyclic Redundancy Check (CRC) and error correction mechanisms.&#10;&#10;Company B is able to determine within one more frame if the current frame is in error, reducing the delay to less than one frame. They have also modified the packing of frames to eliminate the delay associated with error correction. Similarly, Company X can also detect errors in one more frame, reducing the delay to less than one frame. They have altered both their compression and packing of frames to accomplish this.&#10;&#10;Therefore, both companies have made significant changes to their previous compression schemes to minimize or completely eliminate delays in compression and error correction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Improved performance: Using an LDA filter trained with clean speech results in better performance for TI-digits compared to using a filter trained with noisy speech. This is because the high-quality data used for training the filter helps improve signal processing performance in this case.&#10;2. Consistency: In the context of the conversation, PhD B mentions that they get the best performance in the case of &quot;Car&quot; (the third column in condition A) when using the clean LDA filter.&#10;3. Specific application: The speakers are focusing on a specific application where clean speech is more beneficial. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.&#10;4. Confirmation of LDA filter type: The speakers clarify that they have been discussing the performance of a clean LDA filter as opposed to a noisy one, and this confirmation highlights the significance of using the correct LDA filter for specific datasets like TI-digits." target="&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: Yeah . The reference drops like a very fast {disfmarker}&#10;Speaker: Professor D&#10;Content: Oh , oh , oh , oh , oh , oh .&#10;Speaker: PhD E&#10;Content: Like for clean {disfmarker} clean training condition .&#10;Speaker: Professor D&#10;Content: I see .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: I see .&#10;Speaker: PhD E&#10;Content: Nnn .&#10;Speaker: Professor D&#10;Content: This is {disfmarker} this is TI digits {comment} we 're looking at ?&#10;Speaker: PhD B&#10;Content: Yeah . Yeah . Oh {disfmarker}&#10;Speaker: Professor D&#10;Content: This whole page is TI - digits&#10;Speaker: PhD B&#10;Content: Oh . Yeah .&#10;Speaker: Professor D&#10;Content: or this is {disfmarker} ?&#10;Speaker: PhD B&#10;Content: It 's not written anywhere . Yeah , it 's TI - digits . The first r spreadsheet is TI - digits .&#10;Speaker: Professor">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Improved performance: Using an LDA filter trained with clean speech results in better performance for TI-digits compared to using a filter trained with noisy speech. This is because the high-quality data used for training the filter helps improve signal processing performance in this case.&#10;2. Consistency: In the context of the conversation, PhD B mentions that they get the best performance in the case of &quot;Car&quot; (the third column in condition A) when using the clean LDA filter.&#10;3. Specific application: The speakers are focusing on a specific application where clean speech is more beneficial. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.&#10;4. Confirmation of LDA filter type: The speakers clarify that they have been discussing the performance of a clean LDA filter as opposed to a noisy one, and this confirmation highlights the significance of using the correct LDA filter for specific datasets like TI-digits." target="Content: It 's not written anywhere . Yeah , it 's TI - digits . The first r spreadsheet is TI - digits .&#10;Speaker: Professor D&#10;Content: Mmm . How does clean training do for the , uh , &quot; Car &quot;&#10;Speaker: Professor G&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: The &quot; Car &quot; ?&#10;Speaker: Professor D&#10;Content: stuff ?&#10;Speaker: PhD B&#10;Content: Oh . Still {disfmarker} it still , uh {disfmarker} that {disfmarker} that 's still consistent . I mean , I get the best performance in the case of &quot; Car &quot; , which is the third column in the A condition .&#10;Speaker: Professor D&#10;Content: No . I mean , this is added noise . I mean , this is TI - digits . I 'm sorry . I meant {disfmarker} in {disfmarker} in the {disfmarker} in the , uh , multi - language , uh , uh , Finnish and {disfmarker}&#10;Speaker: PhD B&#10;Content: Uh {disfmarker}&#10;Speaker: Professor G&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Improved performance: Using an LDA filter trained with clean speech results in better performance for TI-digits compared to using a filter trained with noisy speech. This is because the high-quality data used for training the filter helps improve signal processing performance in this case.&#10;2. Consistency: In the context of the conversation, PhD B mentions that they get the best performance in the case of &quot;Car&quot; (the third column in condition A) when using the clean LDA filter.&#10;3. Specific application: The speakers are focusing on a specific application where clean speech is more beneficial. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.&#10;4. Confirmation of LDA filter type: The speakers clarify that they have been discussing the performance of a clean LDA filter as opposed to a noisy one, and this confirmation highlights the significance of using the correct LDA filter for specific datasets like TI-digits." target=": PhD B&#10;Content: Oh . This is {disfmarker} Your results are all with the clean LDA result ?&#10;Speaker: PhD H&#10;Content: Yeah , with the clean LDA .&#10;Speaker: PhD B&#10;Content: OK . @ @ .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: &#10;Speaker: PhD E&#10;Content: And in your case it 's all {disfmarker} all noisy ,&#10;Speaker: PhD H&#10;Content: Is that the reason ?&#10;Speaker: PhD B&#10;Content: All noisy , yeah .&#10;Speaker: PhD E&#10;Content: yeah . But {disfmarker}&#10;Speaker: PhD H&#10;Content: And {disfmarker}&#10;Speaker: PhD B&#10;Content: Uh {disfmarker} &#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Uh {disfmarker}&#10;Speaker: PhD E&#10;Content: But I observe my case it 's in , uh , uh , at least on SpeechDat - Car it doesn't matter but TI - digits it 's like two or three">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Improved performance: Using an LDA filter trained with clean speech results in better performance for TI-digits compared to using a filter trained with noisy speech. This is because the high-quality data used for training the filter helps improve signal processing performance in this case.&#10;2. Consistency: In the context of the conversation, PhD B mentions that they get the best performance in the case of &quot;Car&quot; (the third column in condition A) when using the clean LDA filter.&#10;3. Specific application: The speakers are focusing on a specific application where clean speech is more beneficial. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.&#10;4. Confirmation of LDA filter type: The speakers clarify that they have been discussing the performance of a clean LDA filter as opposed to a noisy one, and this confirmation highlights the significance of using the correct LDA filter for specific datasets like TI-digits." target="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Improved performance: Using an LDA filter trained with clean speech results in better performance for TI-digits compared to using a filter trained with noisy speech. This is because the high-quality data used for training the filter helps improve signal processing performance in this case.&#10;2. Consistency: In the context of the conversation, PhD B mentions that they get the best performance in the case of &quot;Car&quot; (the third column in condition A) when using the clean LDA filter.&#10;3. Specific application: The speakers are focusing on a specific application where clean speech is more beneficial. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.&#10;4. Confirmation of LDA filter type: The speakers clarify that they have been discussing the performance of a clean LDA filter as opposed to a noisy one, and this confirmation highlights the significance of using the correct LDA filter for specific datasets like TI-digits." target="Based on the transcript, the Spanish government requests that all participants in their program provide a report. The content of this report includes details about the approach and work done by each participant. There is no explicit mention of a decision regarding including digits in the report, but earlier in the conversation, they discuss processing TI-digits data for their system. It can be inferred that the report should cover the process, methods, and results related to working with the TI-digits data as part of their participation in the program.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Improved performance: Using an LDA filter trained with clean speech results in better performance for TI-digits compared to using a filter trained with noisy speech. This is because the high-quality data used for training the filter helps improve signal processing performance in this case.&#10;2. Consistency: In the context of the conversation, PhD B mentions that they get the best performance in the case of &quot;Car&quot; (the third column in condition A) when using the clean LDA filter.&#10;3. Specific application: The speakers are focusing on a specific application where clean speech is more beneficial. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.&#10;4. Confirmation of LDA filter type: The speakers clarify that they have been discussing the performance of a clean LDA filter as opposed to a noisy one, and this confirmation highlights the significance of using the correct LDA filter for specific datasets like TI-digits." target="Scaling the noise estimate by a factor less than one resulted in improved performance for the TI-digits because, as mentioned in the transcript, there were a lot of zeros in the spectrogram when using the initial approach. By scaling the noise estimate, the speakers aimed to reduce this issue and improve the system's performance.&#10;&#10;The complete results of using this new technique involve applying a noise estimate scaled by a factor of 0.5. This is an ad-hoc method, meaning it is not optimized for any specific goal but serves as an intermediate step in their exploration. The only trend observed from the results was that the current noise estimation or composition scheme worked well for car noise types, while performing less effectively for non-stationary noises like &quot;Babble,&quot; &quot;Subway,&quot; &quot;Street,&quot; or &quot;Restaurant&quot; sounds.&#10;&#10;In summary, scaling the noise estimate by a factor of 0.5 improved performance in this case, but it is an ad-hoc method and not optimized for any specific goal. The complete results show that the technique works well for car noise types but may struggle with non-stationary noises.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Improved performance: Using an LDA filter trained with clean speech results in better performance for TI-digits compared to using a filter trained with noisy speech. This is because the high-quality data used for training the filter helps improve signal processing performance in this case.&#10;2. Consistency: In the context of the conversation, PhD B mentions that they get the best performance in the case of &quot;Car&quot; (the third column in condition A) when using the clean LDA filter.&#10;3. Specific application: The speakers are focusing on a specific application where clean speech is more beneficial. However, it's important to consider that different datasets or contexts might call for different LDA filter designs, including those based on noisy speech.&#10;4. Confirmation of LDA filter type: The speakers clarify that they have been discussing the performance of a clean LDA filter as opposed to a noisy one, and this confirmation highlights the significance of using the correct LDA filter for specific datasets like TI-digits." target="1. The best performance for the &quot;Car&quot; category was achieved when using an LDA filter trained with clean speech (TI-digits). This resulted in better signal processing performance compared to a filter trained with noisy speech.&#10;2. Although this specific conversation focuses on the improved performance of the clean LDA filter for TI-digits, it is essential to consider that different contexts or datasets might require different LDA filter designs, including those based on noisy speech.&#10;3. In this case, the &quot;Car&quot; category had the best performance in the third column of condition A, but specific numerical values or metrics were not provided in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: Yeah . The reference drops like a very fast {disfmarker}&#10;Speaker: Professor D&#10;Content: Oh , oh , oh , oh , oh , oh .&#10;Speaker: PhD E&#10;Content: Like for clean {disfmarker} clean training condition .&#10;Speaker: Professor D&#10;Content: I see .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: I see .&#10;Speaker: PhD E&#10;Content: Nnn .&#10;Speaker: Professor D&#10;Content: This is {disfmarker} this is TI digits {comment} we 're looking at ?&#10;Speaker: PhD B&#10;Content: Yeah . Yeah . Oh {disfmarker}&#10;Speaker: Professor D&#10;Content: This whole page is TI - digits&#10;Speaker: PhD B&#10;Content: Oh . Yeah .&#10;Speaker: Professor D&#10;Content: or this is {disfmarker} ?&#10;Speaker: PhD B&#10;Content: It 's not written anywhere . Yeah , it 's TI - digits . The first r spreadsheet is TI - digits .&#10;Speaker: Professor" target="Based on the transcript, the Spanish government requests that all participants in their program provide a report. The content of this report includes details about the approach and work done by each participant. There is no explicit mention of a decision regarding including digits in the report, but earlier in the conversation, they discuss processing TI-digits data for their system. It can be inferred that the report should cover the process, methods, and results related to working with the TI-digits data as part of their participation in the program.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: It 's not written anywhere . Yeah , it 's TI - digits . The first r spreadsheet is TI - digits .&#10;Speaker: Professor D&#10;Content: Mmm . How does clean training do for the , uh , &quot; Car &quot;&#10;Speaker: Professor G&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: The &quot; Car &quot; ?&#10;Speaker: Professor D&#10;Content: stuff ?&#10;Speaker: PhD B&#10;Content: Oh . Still {disfmarker} it still , uh {disfmarker} that {disfmarker} that 's still consistent . I mean , I get the best performance in the case of &quot; Car &quot; , which is the third column in the A condition .&#10;Speaker: Professor D&#10;Content: No . I mean , this is added noise . I mean , this is TI - digits . I 'm sorry . I meant {disfmarker} in {disfmarker} in the {disfmarker} in the , uh , multi - language , uh , uh , Finnish and {disfmarker}&#10;Speaker: PhD B&#10;Content: Uh {disfmarker}&#10;Speaker: Professor G&#10;Content" target="1. The best performance for the &quot;Car&quot; category was achieved when using an LDA filter trained with clean speech (TI-digits). This resulted in better signal processing performance compared to a filter trained with noisy speech.&#10;2. Although this specific conversation focuses on the improved performance of the clean LDA filter for TI-digits, it is essential to consider that different contexts or datasets might require different LDA filter designs, including those based on noisy speech.&#10;3. In this case, the &quot;Car&quot; category had the best performance in the third column of condition A, but specific numerical values or metrics were not provided in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Fine-tuning ten or twelve small aspects, each contributing around 0.3%, can significantly improve a system's accuracy from 95% to 9" target=" the analysis window is two seconds .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: So what you just said , about what do you start with , raises a question of {vocalsound} what do I start with then ?&#10;Speaker: Professor D&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: I guess it {disfmarker} because {disfmarker}&#10;Speaker: Professor D&#10;Content: Well , w OK , so in that situation , though , th maybe what 's a little different there , is I think you 're talking about {disfmarker} there 's only one {disfmarker} it {disfmarker} it {disfmarker} it also depends {disfmarker} we 're getting a little off track here .&#10;Speaker: Grad C&#10;Content: Oh , right .&#10;Speaker: Professor D&#10;Content: r But {disfmarker} but {disfmarker} but {disfmarker} Uh , there 's been some discussion about whether the work we 're doing in that project is gonna be for the kiosk or">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Fine-tuning ten or twelve small aspects, each contributing around 0.3%, can significantly improve a system's accuracy from 95% to 9" target=" doing , uh , research , you may , eh {disfmarker} you might find that the way that you build up a change from a ninety - five percent accurate system to a ninety - eight percent accurate system is through ten or twelve little things that you do that each are point three percent . So {disfmarker} so the {disfmarker} they {disfmarker} they {disfmarker} it 's {disfmarker} I don't mean to say that they 're {disfmarker} they 're irrelevant . Uh , they are relevant . But , um , {vocalsound} i for a demo , you won't see it .&#10;Speaker: Grad C&#10;Content: Mm - hmm . Right . OK .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: And , um , Let 's {disfmarker} l let 's see . Um , OK . And then there 's um , another thing I wanna start looking at , um , {vocalsound} wi is , um , the choice of the analysis window length . So I 've just been using two seconds just because that '">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Fine-tuning ten or twelve small aspects, each contributing around 0.3%, can significantly improve a system's accuracy from 95% to 9" target=" been that much with this long {disfmarker} long - time , uh , spectra work .&#10;Speaker: Grad C&#10;Content: Oh , o Oh , OK .&#10;Speaker: Professor D&#10;Content: Uh ,&#10;Speaker: Grad C&#10;Content: So that 's {disfmarker} that 's {disfmarker} that 's standard . Um {disfmarker}&#10;Speaker: Professor D&#10;Content: Yeah . Pretty common .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: Yeah . Um , but , u uh , yes . No , it is interesting . And the other thing is , I mean , there 's two sides to these really small , uh , gradations in performance . Um , I mean , on the one hand in a practical system if something is , uh , four point four percent error , four point one percent error , people won't really tell {disfmarker} be able to tell the difference . On the other hand , when you 're doing , uh , research , you may , eh {disfmarker} you might find that the way that you build up a change from a ninety -">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Fine-tuning ten or twelve small aspects, each contributing around 0.3%, can significantly improve a system's accuracy from 95% to 9" target="1. The advantage of using a fixed filter, such as the RASTA filter, is that it remains unchanged and does not degrade over time, making it a reliable choice for signal processing. This is because it does not need to be retrained or adapted for different conditions, unlike trained filters.&#10;2. In the context of the SmartKom system, using a fixed amount of data for mean subtraction can be beneficial when less than twelve seconds of data are available. By training the models with a fixed amount of data (e.g., six or twelve seconds), the system can ensure stability and consistency in the processing steps that follow.&#10;3. However, if there is not enough data available to estimate a reliable mean value for mean subtraction, concatenating utterances together can help increase the amount of available data. This approach may be more suitable than training a filter specifically for certain conditions, as it allows the system to use all the available data and avoid potential overfitting or poor generalization issues that can arise with trained filters.&#10;4. Overall, using a fixed filter or a fixed amount of data for mean subtraction provides a stable and consistent processing approach, while concatenating utterances together can help increase the amount of available data when there is less than twelve seconds of data in the SmartKom system.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker, referred to as PhD F, has been researching speech recognition with a focus on creating a more robust system for word or phone recognition. This involves developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. One of the specific research questions they have been addressing is what types of intermediate categories are needed for classification. Another question relates to the structure of multi-band graphical models, specifically what types of structures should be considered in order to effectively combine evidence from sub-bands.&#10;3. Additionally, PhD F has been investigating how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. To accomplish this, they have proposed a new approach to speech recognition that combines multi-band ideas with an acoustic phonetic approach. This approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features more closely related to the acoustic signal itself." target="} um intermediate classifications , {vocalsound} that we can get a system that 's more robust to {disfmarker} to unseen noises , and situations like that . Um , and so , some of the research issues involved in this are , {vocalsound} um , {vocalsound} {comment} one , what kind of intermediate categories do we need to classify ? Um , another one is {vocalsound} um , what {disfmarker} what other types of structures in these multi - band graphical models should we consider in order to um , combine evidence from {vocalsound} the sub - bands ? And , uh , the third one is how do we {disfmarker} how do we merge all the , uh , information from the individual uh , multi - band classifiers to come up with word {disfmarker} word recognition or {disfmarker} or phone recognition things . Um , so basically that 's {disfmarker} that 's what I 've been doing . And ,&#10;Speaker: PhD F&#10;Content: So you 've got two weeks , huh ?&#10;Speaker: Grad A&#10;Content: I got two weeks to brush up on">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker, referred to as PhD F, has been researching speech recognition with a focus on creating a more robust system for word or phone recognition. This involves developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. One of the specific research questions they have been addressing is what types of intermediate categories are needed for classification. Another question relates to the structure of multi-band graphical models, specifically what types of structures should be considered in order to effectively combine evidence from sub-bands.&#10;3. Additionally, PhD F has been investigating how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. To accomplish this, they have proposed a new approach to speech recognition that combines multi-band ideas with an acoustic phonetic approach. This approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features more closely related to the acoustic signal itself." target="aker: PhD B&#10;Content: Yeah . So here {disfmarker} here I mean , I found that it 's {disfmarker} if I changed the noise estimate I could get an improvement .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: So that 's {disfmarker} so it 's something which I can actually pursue , is the noise estimate .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: And {disfmarker}&#10;Speaker: Professor G&#10;Content: Yeah , I think what you do is in {disfmarker} when {disfmarker} when you have the {disfmarker} the {disfmarker} this multi - condition training mode , um then you have {disfmarker} then you can train models for the speech , for the words , as well as for the pauses where you really have all information about the noise available .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And it was surprising {disfmarker} At the beginning it was not surprising to me">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker, referred to as PhD F, has been researching speech recognition with a focus on creating a more robust system for word or phone recognition. This involves developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. One of the specific research questions they have been addressing is what types of intermediate categories are needed for classification. Another question relates to the structure of multi-band graphical models, specifically what types of structures should be considered in order to effectively combine evidence from sub-bands.&#10;3. Additionally, PhD F has been investigating how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. To accomplish this, they have proposed a new approach to speech recognition that combines multi-band ideas with an acoustic phonetic approach. This approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features more closely related to the acoustic signal itself." target=" using the neural network , I guess . That 's right ?&#10;Speaker: PhD B&#10;Content: Yeah . We actually trained , uh , the {disfmarker} on the Italian training part .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: We {disfmarker} we had another {vocalsound} system with u&#10;Speaker: PhD E&#10;Content: So it was a f f a phonetic classification system for the Italian Aurora data .&#10;Speaker: PhD B&#10;Content: Yeah . It must be somewhere . Yeah .&#10;Speaker: PhD E&#10;Content: For the Aurora data that it was trained on , it was different . Like , for TI - digits you used a {disfmarker} a previous system that you had , I guess .&#10;Speaker: PhD B&#10;Content: What {disfmarker}  No it {disfmarker} Yeah , yeah . That 's true .&#10;Speaker: PhD E&#10;Content: So the alignments from the different database that are used for training came from different system .&#10;Speaker: PhD B&#10;Content: Syste Yeah .&#10;Speaker: PhD E&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker, referred to as PhD F, has been researching speech recognition with a focus on creating a more robust system for word or phone recognition. This involves developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. One of the specific research questions they have been addressing is what types of intermediate categories are needed for classification. Another question relates to the structure of multi-band graphical models, specifically what types of structures should be considered in order to effectively combine evidence from sub-bands.&#10;3. Additionally, PhD F has been investigating how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. To accomplish this, they have proposed a new approach to speech recognition that combines multi-band ideas with an acoustic phonetic approach. This approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features more closely related to the acoustic signal itself." target="1. The speakers believe that the models are not complex enough to absorb additional variability in the data because they have observed that introducing a &quot;natural distribution&quot; to the audio data, which would make it resemble real-world noisy situations more closely, increases the degradation of the speech processing algorithm. They suggest that this could be due to the models' inability to effectively model and account for the increased variability. Instead, the added noise results in additional errors or degradation in the system's performance.&#10;2. This explanation can be found when PhD B says, &quot;the models are not complex enough to absorb that additional variability that you're introducing&quot; and later reiterated by PhD E, who shares their experience with neural networks being more capable of modeling strange distributions.&#10;3. The speakers also suggest that this natural distribution approach more accurately represents real-world noisy situations, indicating that the current models might not be sophisticated enough to account for these complexities without compromising performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker, referred to as PhD F, has been researching speech recognition with a focus on creating a more robust system for word or phone recognition. This involves developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. One of the specific research questions they have been addressing is what types of intermediate categories are needed for classification. Another question relates to the structure of multi-band graphical models, specifically what types of structures should be considered in order to effectively combine evidence from sub-bands.&#10;3. Additionally, PhD F has been investigating how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. To accomplish this, they have proposed a new approach to speech recognition that combines multi-band ideas with an acoustic phonetic approach. This approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features more closely related to the acoustic signal itself." target="1. The project's focus regarding designing acoustics for either a kiosk or a mobile platform is determined by the specific requirements and use cases of the target application. If the application is designed to be stationary and user-specific, like in a kiosk, then it would make more sense to optimize the acoustics for that setting as the physical situation remains relatively stable. In contrast, if the application is intended for a mobile platform with variable acoustic conditions, the system should be designed to adapt to changing acoustics accordingly.&#10;2. The decision may also depend on the complexity and adaptability of the speech processing algorithms being used. More sophisticated models might be able to handle varying acoustic environments better than simpler ones. In this case, designing for a mobile platform could still provide an acceptable user experience if the models are complex enough to absorb additional variability.&#10;3. Lastly, practical considerations such as development time, resources, and budget may influence the decision. Developing a system that works well across multiple platforms might require more effort and resources than focusing on a single platform. However, designing with scalability and adaptability in mind could make it easier to expand the application to other platforms in the future.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker, referred to as PhD F, has been researching speech recognition with a focus on creating a more robust system for word or phone recognition. This involves developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. One of the specific research questions they have been addressing is what types of intermediate categories are needed for classification. Another question relates to the structure of multi-band graphical models, specifically what types of structures should be considered in order to effectively combine evidence from sub-bands.&#10;3. Additionally, PhD F has been investigating how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. To accomplish this, they have proposed a new approach to speech recognition that combines multi-band ideas with an acoustic phonetic approach. This approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features more closely related to the acoustic signal itself." target="1. The speaker, Grad A, is proposing a new approach to speech recognition that combines multi-band ideas with an acoustic-phonetic approach. This approach aims to create a more robust system for word or phone recognition by developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. The new approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features closely related to the acoustic signal itself, within the context of a multi-band model. The hope is that by going multi-band and using these intermediate classifications, the system will be more robust in unseen noisy situations.&#10;3. Grad A plans to investigate several research questions, including what types of intermediate categories are needed for classification, the structure of multi-band graphical models (specifically, what types of structures should be considered to effectively combine evidence from sub-bands), and how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. This new approach is motivated by the need to address real-world noisy situations more accurately, potentially through the introduction of a &quot;natural distribution&quot; of noise rather than simply adding constant or flat noise across all regions. This could lead to a decrease in word error rate and better performance in signal processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker, referred to as PhD F, has been researching speech recognition with a focus on creating a more robust system for word or phone recognition. This involves developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. One of the specific research questions they have been addressing is what types of intermediate categories are needed for classification. Another question relates to the structure of multi-band graphical models, specifically what types of structures should be considered in order to effectively combine evidence from sub-bands.&#10;3. Additionally, PhD F has been investigating how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. To accomplish this, they have proposed a new approach to speech recognition that combines multi-band ideas with an acoustic phonetic approach. This approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features more closely related to the acoustic signal itself." target="1. The speakers discussed introducing a &quot;natural distribution&quot; of noise to the audio data analyzed in the SmartKom project's ASRU paper. This approach aims to make the audio data resemble real-world noisy situations more closely by reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;&#10;2. Two research questions arise from this discussion:&#10;  a. Does this natural distribution approach still work effectively even if the speakers are involved?&#10;  b. This question is cut off in the transcript but presumably relates to testing the effectiveness of the natural distribution approach under different conditions or with various types of noise.">
      <data key="d0">1</data>
    </edge>
    <edge source="} um intermediate classifications , {vocalsound} that we can get a system that 's more robust to {disfmarker} to unseen noises , and situations like that . Um , and so , some of the research issues involved in this are , {vocalsound} um , {vocalsound} {comment} one , what kind of intermediate categories do we need to classify ? Um , another one is {vocalsound} um , what {disfmarker} what other types of structures in these multi - band graphical models should we consider in order to um , combine evidence from {vocalsound} the sub - bands ? And , uh , the third one is how do we {disfmarker} how do we merge all the , uh , information from the individual uh , multi - band classifiers to come up with word {disfmarker} word recognition or {disfmarker} or phone recognition things . Um , so basically that 's {disfmarker} that 's what I 've been doing . And ,&#10;Speaker: PhD F&#10;Content: So you 've got two weeks , huh ?&#10;Speaker: Grad A&#10;Content: I got two weeks to brush up on" target="1. The speaker, Grad A, is proposing a new approach to speech recognition that combines multi-band ideas with an acoustic-phonetic approach. This approach aims to create a more robust system for word or phone recognition by developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. The new approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features closely related to the acoustic signal itself, within the context of a multi-band model. The hope is that by going multi-band and using these intermediate classifications, the system will be more robust in unseen noisy situations.&#10;3. Grad A plans to investigate several research questions, including what types of intermediate categories are needed for classification, the structure of multi-band graphical models (specifically, what types of structures should be considered to effectively combine evidence from sub-bands), and how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. This new approach is motivated by the need to address real-world noisy situations more accurately, potentially through the introduction of a &quot;natural distribution&quot; of noise rather than simply adding constant or flat noise across all regions. This could lead to a decrease in word error rate and better performance in signal processing.">
      <data key="d0">1</data>
    </edge>
    <edge source=" using the neural network , I guess . That 's right ?&#10;Speaker: PhD B&#10;Content: Yeah . We actually trained , uh , the {disfmarker} on the Italian training part .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: We {disfmarker} we had another {vocalsound} system with u&#10;Speaker: PhD E&#10;Content: So it was a f f a phonetic classification system for the Italian Aurora data .&#10;Speaker: PhD B&#10;Content: Yeah . It must be somewhere . Yeah .&#10;Speaker: PhD E&#10;Content: For the Aurora data that it was trained on , it was different . Like , for TI - digits you used a {disfmarker} a previous system that you had , I guess .&#10;Speaker: PhD B&#10;Content: What {disfmarker}  No it {disfmarker} Yeah , yeah . That 's true .&#10;Speaker: PhD E&#10;Content: So the alignments from the different database that are used for training came from different system .&#10;Speaker: PhD B&#10;Content: Syste Yeah .&#10;Speaker: PhD E&#10;Content:" target="1. Hamming or Hanning Window: These window functions can be used to reduce the weight of values at the edges when applying a moving window to a signal. They decrease the impact of abrupt changes between windows and help smooth the signal, which can make the training process more stable. In the context of the &quot;Aurora stuff,&quot; these window functions could be applied when processing audio data as part of a speech recognition system.&#10;2. LDA Filter (Linear Discriminant Analysis filter): The LDA filter is a supervised learning algorithm that can optimize some criterion, such as minimizing within-class variance or maximizing between-class variance. This results in a filter that better separates different classes and improves the performance of the system. In the context of the &quot;Aurora stuff,&quot; an LDA filter could be trained on labeled data to improve speech recognition accuracy.&#10;3. Spectral Subtraction or Wiener Filtering: These techniques reduce noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum, improving the signal-to-noise ratio and making it easier to extract useful information from noisy data. In the context of the &quot;Aurora stuff,&quot; these methods could be applied when processing audio signals to improve speech recognition performance.&#10;&#10;These filters, along with a uniform window, are options for moving window setups in training LDA filters or other signal processing tasks. The choice of filter will depend on the specific application and the characteristics of the data being processed. In the transcript, it is mentioned that spectral subtraction was not effective for certain Aurora tasks, suggesting that a different filter or approach might be needed.">
      <data key="d0">1</data>
    </edge>
    <edge source=" using the neural network , I guess . That 's right ?&#10;Speaker: PhD B&#10;Content: Yeah . We actually trained , uh , the {disfmarker} on the Italian training part .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: We {disfmarker} we had another {vocalsound} system with u&#10;Speaker: PhD E&#10;Content: So it was a f f a phonetic classification system for the Italian Aurora data .&#10;Speaker: PhD B&#10;Content: Yeah . It must be somewhere . Yeah .&#10;Speaker: PhD E&#10;Content: For the Aurora data that it was trained on , it was different . Like , for TI - digits you used a {disfmarker} a previous system that you had , I guess .&#10;Speaker: PhD B&#10;Content: What {disfmarker}  No it {disfmarker} Yeah , yeah . That 's true .&#10;Speaker: PhD E&#10;Content: So the alignments from the different database that are used for training came from different system .&#10;Speaker: PhD B&#10;Content: Syste Yeah .&#10;Speaker: PhD E&#10;Content:" target="Yes, based on the transcript, it appears that the neural network was trained on previously estimated alignments rather than being directly based on the provided label files for the Italian data. Speaker PhD E states that &quot;we trained the neural network on embedded training... That's right?&quot; and later clarifies that &quot;For the Italian data, I think we trained the neural network on re-estimation of the alignment using the neural network.&quot; There is no indication in the transcript that the neural network was directly trained on the provided label files.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable." target=" but it might make a difference . I don't know .&#10;Speaker: Grad C&#10;Content: Uh , yeah , I don't {disfmarker} I don't think the TI - digits data that I have , um , {vocalsound} i is {disfmarker} would be appropriate for that .&#10;Speaker: Professor D&#10;Content: Yeah , probably not . Yeah .&#10;Speaker: Grad C&#10;Content: But what do you {disfmarker} What about if I w I fed it through some kind of , um , speech processing algorithm that changed the speech rate ?&#10;Speaker: Professor D&#10;Content: Yeah , but then you 'll have the degradation of {disfmarker} of , uh , whatever you do uh , added onto that . But maybe . Yeah , maybe if you get something that sounds {disfmarker} that {disfmarker} that 's {disfmarker} does a pretty job at that .&#10;Speaker: Grad C&#10;Content: Yeah . Well , uh , just if you think it 's worth looking into .&#10;Speaker: Professor D&#10;Content: You could imagine that .&#10;Speaker: Grad C&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable." target=" uh , five - percent improvement , and fifty - eight point one . So again , it 's around fifty - six , fifty - seven . Uh {disfmarker}&#10;Speaker: Professor D&#10;Content: Cuz I notice the TI - digits number is exactly the same for these last two ?&#10;Speaker: PhD E&#10;Content: Yeah , because I didn't {disfmarker} For the France Telecom uh , spectral subtraction included in the {disfmarker} our system , the TI - digits number are the right one , but not for the other system because I didn't test it yet {disfmarker} this system , including {disfmarker} with spectral subtraction on the TI - digits data . I just tested it on SpeechDat - Car .&#10;Speaker: Professor D&#10;Content: Ah ! So {disfmarker} so that means the only thing {disfmarker}&#10;Speaker: Professor G&#10;Content: Mm - hmm . So {disfmarker} so {disfmarker} so these numbers are simply {disfmarker}&#10;Speaker: PhD E&#10;Content: This , we have to {disfmarker} Yeah">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable." target=" thirty - five percent on the TI - digits . And {disfmarker} uh , ps the fifty - six percent is like comparable to what the French Telecom gets , but the thirty - five percent is way off .&#10;Speaker: Professor D&#10;Content: I 'm sort of confused but {disfmarker} this {disfmarker} I 'm looking on the second page ,&#10;Speaker: PhD B&#10;Content: Oh , yep .&#10;Speaker: Professor D&#10;Content: and it says &quot; fifty percent &quot; {disfmarker} looking in the lower right - hand corner , &quot; fifty percent relative performance &quot; .&#10;Speaker: Professor G&#10;Content: For the clean training .&#10;Speaker: Professor D&#10;Content: Is that {disfmarker}&#10;Speaker: Professor G&#10;Content: u And if you {disfmarker} if you look {disfmarker}&#10;Speaker: Professor D&#10;Content: is that fifty percent improvement ?&#10;Speaker: PhD B&#10;Content: Yeah . For {disfmarker} that 's for the clean training and the noisy testing for the TI - digits .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable." target="1. The speakers believe that the models are not complex enough to absorb additional variability in the data because they have observed that introducing a &quot;natural distribution&quot; to the audio data, which would make it resemble real-world noisy situations more closely, increases the degradation of the speech processing algorithm. They suggest that this could be due to the models' inability to effectively model and account for the increased variability. Instead, the added noise results in additional errors or degradation in the system's performance.&#10;2. This explanation can be found when PhD B says, &quot;the models are not complex enough to absorb that additional variability that you're introducing&quot; and later reiterated by PhD E, who shares their experience with neural networks being more capable of modeling strange distributions.&#10;3. The speakers also suggest that this natural distribution approach more accurately represents real-world noisy situations, indicating that the current models might not be sophisticated enough to account for these complexities without compromising performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable." target="Based on the transcript, the Spanish government requests that all participants in their program provide a report. The content of this report includes details about the approach and work done by each participant. There is no explicit mention of a decision regarding including digits in the report, but earlier in the conversation, they discuss processing TI-digits data for their system. It can be inferred that the report should cover the process, methods, and results related to working with the TI-digits data as part of their participation in the program.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable." target="1. The project's focus regarding designing acoustics for either a kiosk or a mobile platform is determined by the specific requirements and use cases of the target application. If the application is designed to be stationary and user-specific, like in a kiosk, then it would make more sense to optimize the acoustics for that setting as the physical situation remains relatively stable. In contrast, if the application is intended for a mobile platform with variable acoustic conditions, the system should be designed to adapt to changing acoustics accordingly.&#10;2. The decision may also depend on the complexity and adaptability of the speech processing algorithms being used. More sophisticated models might be able to handle varying acoustic environments better than simpler ones. In this case, designing for a mobile platform could still provide an acceptable user experience if the models are complex enough to absorb additional variability.&#10;3. Lastly, practical considerations such as development time, resources, and budget may influence the decision. Developing a system that works well across multiple platforms might require more effort and resources than focusing on a single platform. However, designing with scalability and adaptability in mind could make it easier to expand the application to other platforms in the future.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable." target="Based on the discussion transcript, the outcome of the discussion regarding delay in compression is that both Company B and Company X have changed their compression schemes altogether. They now have their own compression and decoding schemes, as well as their own Cyclic Redundancy Check (CRC) and error correction mechanisms.&#10;&#10;Company B is able to determine within one more frame if the current frame is in error, reducing the delay to less than one frame. They have also modified the packing of frames to eliminate the delay associated with error correction. Similarly, Company X can also detect errors in one more frame, reducing the delay to less than one frame. They have altered both their compression and packing of frames to accomplish this.&#10;&#10;Therefore, both companies have made significant changes to their previous compression schemes to minimize or completely eliminate delays in compression and error correction.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable." target="Scaling the noise estimate by a factor less than one resulted in improved performance for the TI-digits because, as mentioned in the transcript, there were a lot of zeros in the spectrogram when using the initial approach. By scaling the noise estimate, the speakers aimed to reduce this issue and improve the system's performance.&#10;&#10;The complete results of using this new technique involve applying a noise estimate scaled by a factor of 0.5. This is an ad-hoc method, meaning it is not optimized for any specific goal but serves as an intermediate step in their exploration. The only trend observed from the results was that the current noise estimation or composition scheme worked well for car noise types, while performing less effectively for non-stationary noises like &quot;Babble,&quot; &quot;Subway,&quot; &quot;Street,&quot; or &quot;Restaurant&quot; sounds.&#10;&#10;In summary, scaling the noise estimate by a factor of 0.5 improved performance in this case, but it is an ad-hoc method and not optimized for any specific goal. The complete results show that the technique works well for car noise types but may struggle with non-stationary noises.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, using a speech processing algorithm to change the speech rate of the TI-digits data might not be the most appropriate choice for this task. The speakers mention that this could result in additional degradation, which may negatively impact the performance of their system. They seem to be focusing on reducing word error rates and improving signal processing performance, and adding further degradation through speech rate manipulation could hinder these goals. However, if a suitable algorithm can be found that preserves or enhances the quality of the data while changing the speech rate, it might still be worth considering. Nonetheless, based on the available information, introducing such degradation does not seem advisable." target="1. The speaker, Grad A, is proposing a new approach to speech recognition that combines multi-band ideas with an acoustic-phonetic approach. This approach aims to create a more robust system for word or phone recognition by developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. The new approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features closely related to the acoustic signal itself, within the context of a multi-band model. The hope is that by going multi-band and using these intermediate classifications, the system will be more robust in unseen noisy situations.&#10;3. Grad A plans to investigate several research questions, including what types of intermediate categories are needed for classification, the structure of multi-band graphical models (specifically, what types of structures should be considered to effectively combine evidence from sub-bands), and how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. This new approach is motivated by the need to address real-world noisy situations more accurately, potentially through the introduction of a &quot;natural distribution&quot; of noise rather than simply adding constant or flat noise across all regions. This could lead to a decrease in word error rate and better performance in signal processing.">
      <data key="d0">1</data>
    </edge>
    <edge source=" thirty - five percent on the TI - digits . And {disfmarker} uh , ps the fifty - six percent is like comparable to what the French Telecom gets , but the thirty - five percent is way off .&#10;Speaker: Professor D&#10;Content: I 'm sort of confused but {disfmarker} this {disfmarker} I 'm looking on the second page ,&#10;Speaker: PhD B&#10;Content: Oh , yep .&#10;Speaker: Professor D&#10;Content: and it says &quot; fifty percent &quot; {disfmarker} looking in the lower right - hand corner , &quot; fifty percent relative performance &quot; .&#10;Speaker: Professor G&#10;Content: For the clean training .&#10;Speaker: Professor D&#10;Content: Is that {disfmarker}&#10;Speaker: Professor G&#10;Content: u And if you {disfmarker} if you look {disfmarker}&#10;Speaker: Professor D&#10;Content: is that fifty percent improvement ?&#10;Speaker: PhD B&#10;Content: Yeah . For {disfmarker} that 's for the clean training and the noisy testing for the TI - digits .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;" target="1. There is a mention of a &quot;point three percent hit&quot; in the context of a mismatch between training and testing conditions. This means that there is a decrease in performance or accuracy of around 0.3% when there's a discrepancy between how the model was trained (training link) and what it encounters during testing (testing link).&#10;2. The specific instance of this &quot;point three percent hit&quot; is related to changing the prototype for initializing the HMM (Hidden Markov Model) in order to avoid getting stuck in a local minimum during training. This change resulted in a 50% improvement, from 75.79 to 88, indicating that the mismatch between training and testing conditions had a noticeable impact on performance.&#10;3. The context suggests that this &quot;point three percent hit&quot; is not just for any mismatch but could be specific to certain types of discrepancies between training and testing conditions or specific model configurations. It's important to note that the exact nature and circumstances of this &quot;point three percent hit&quot; aren't explicitly detailed in the transcript, so further investigation would be required to provide a more precise answer.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The professors and PhDs are discussing adjusting the value of the constant added to the signal energy to achieve a desired signal-to-noise ratio (SNR). This constant is determined based on the average speech energy measured from a dataset of one language, such as Italian, and then used as the mean speech energy for all other languages.&#10;2. They are also considering how dependent the performance is on the value of this constant and exploring different values to find the optimal SNR range for the system.&#10;3. Additionally, there is mention of potentially adjusting noise addition or thresholds based on the average energy levels in each language. For example, Finnish has a lower average energy compared to other databases, so the threshold for Finnish should be lower accordingly.&#10;4. Lastly, they touch upon the impracticality of measuring the average energy over an extended period in real-world applications and suggest using a fixed number derived from other methods instead." target="Based on the transcript, the Spanish government requests that all participants in their program provide a report. The content of this report includes details about the approach and work done by each participant. There is no explicit mention of a decision regarding including digits in the report, but earlier in the conversation, they discuss processing TI-digits data for their system. It can be inferred that the report should cover the process, methods, and results related to working with the TI-digits data as part of their participation in the program.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The professors and PhDs are discussing adjusting the value of the constant added to the signal energy to achieve a desired signal-to-noise ratio (SNR). This constant is determined based on the average speech energy measured from a dataset of one language, such as Italian, and then used as the mean speech energy for all other languages.&#10;2. They are also considering how dependent the performance is on the value of this constant and exploring different values to find the optimal SNR range for the system.&#10;3. Additionally, there is mention of potentially adjusting noise addition or thresholds based on the average energy levels in each language. For example, Finnish has a lower average energy compared to other databases, so the threshold for Finnish should be lower accordingly.&#10;4. Lastly, they touch upon the impracticality of measuring the average energy over an extended period in real-world applications and suggest using a fixed number derived from other methods instead." target="1. The project's focus regarding designing acoustics for either a kiosk or a mobile platform is determined by the specific requirements and use cases of the target application. If the application is designed to be stationary and user-specific, like in a kiosk, then it would make more sense to optimize the acoustics for that setting as the physical situation remains relatively stable. In contrast, if the application is intended for a mobile platform with variable acoustic conditions, the system should be designed to adapt to changing acoustics accordingly.&#10;2. The decision may also depend on the complexity and adaptability of the speech processing algorithms being used. More sophisticated models might be able to handle varying acoustic environments better than simpler ones. In this case, designing for a mobile platform could still provide an acceptable user experience if the models are complex enough to absorb additional variability.&#10;3. Lastly, practical considerations such as development time, resources, and budget may influence the decision. Developing a system that works well across multiple platforms might require more effort and resources than focusing on a single platform. However, designing with scalability and adaptability in mind could make it easier to expand the application to other platforms in the future.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The professors and PhDs are discussing adjusting the value of the constant added to the signal energy to achieve a desired signal-to-noise ratio (SNR). This constant is determined based on the average speech energy measured from a dataset of one language, such as Italian, and then used as the mean speech energy for all other languages.&#10;2. They are also considering how dependent the performance is on the value of this constant and exploring different values to find the optimal SNR range for the system.&#10;3. Additionally, there is mention of potentially adjusting noise addition or thresholds based on the average energy levels in each language. For example, Finnish has a lower average energy compared to other databases, so the threshold for Finnish should be lower accordingly.&#10;4. Lastly, they touch upon the impracticality of measuring the average energy over an extended period in real-world applications and suggest using a fixed number derived from other methods instead." target="1. The best performance for the &quot;Car&quot; category was achieved when using an LDA filter trained with clean speech (TI-digits). This resulted in better signal processing performance compared to a filter trained with noisy speech.&#10;2. Although this specific conversation focuses on the improved performance of the clean LDA filter for TI-digits, it is essential to consider that different contexts or datasets might require different LDA filter designs, including those based on noisy speech.&#10;3. In this case, the &quot;Car&quot; category had the best performance in the third column of condition A, but specific numerical values or metrics were not provided in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The professors and PhDs are discussing adjusting the value of the constant added to the signal energy to achieve a desired signal-to-noise ratio (SNR). This constant is determined based on the average speech energy measured from a dataset of one language, such as Italian, and then used as the mean speech energy for all other languages.&#10;2. They are also considering how dependent the performance is on the value of this constant and exploring different values to find the optimal SNR range for the system.&#10;3. Additionally, there is mention of potentially adjusting noise addition or thresholds based on the average energy levels in each language. For example, Finnish has a lower average energy compared to other databases, so the threshold for Finnish should be lower accordingly.&#10;4. Lastly, they touch upon the impracticality of measuring the average energy over an extended period in real-world applications and suggest using a fixed number derived from other methods instead." target="1. The speakers discussed introducing a &quot;natural distribution&quot; of noise to the audio data analyzed in the SmartKom project's ASRU paper. This approach aims to make the audio data resemble real-world noisy situations more closely by reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;&#10;2. Two research questions arise from this discussion:&#10;  a. Does this natural distribution approach still work effectively even if the speakers are involved?&#10;  b. This question is cut off in the transcript but presumably relates to testing the effectiveness of the natural distribution approach under different conditions or with various types of noise.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers believe that the models are not complex enough to absorb additional variability in the data because they have observed that introducing a &quot;natural distribution&quot; to the audio data, which would make it resemble real-world noisy situations more closely, increases the degradation of the speech processing algorithm. They suggest that this could be due to the models' inability to effectively model and account for the increased variability. Instead, the added noise results in additional errors or degradation in the system's performance.&#10;2. This explanation can be found when PhD B says, &quot;the models are not complex enough to absorb that additional variability that you're introducing&quot; and later reiterated by PhD E, who shares their experience with neural networks being more capable of modeling strange distributions.&#10;3. The speakers also suggest that this natural distribution approach more accurately represents real-world noisy situations, indicating that the current models might not be sophisticated enough to account for these complexities without compromising performance." target=" And {disfmarker} and this {disfmarker} this curves are the average over the whole database , so .&#10;Speaker: PhD E&#10;Content: Yeah . Right .&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD E&#10;Content: Um {disfmarker} Yeah , and the different points of the curves are for five uh , thresholds on the probability {comment} uh from point three to point seven .&#10;Speaker: PhD B&#10;Content: So that threshold {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm . Yeah .&#10;Speaker: PhD B&#10;Content: OK . S OK {disfmarker} so d the detection threshold is very {disfmarker}&#10;Speaker: PhD E&#10;Content: So the v&#10;Speaker: PhD B&#10;Content: Yeah , yeah .&#10;Speaker: PhD E&#10;Content: The VAD ? Yeah . There first , a threshold on the probability {comment} @ @ {comment} That puts all the values to zero or one .&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: PhD E&#10;Content: And then the median filtering">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers believe that the models are not complex enough to absorb additional variability in the data because they have observed that introducing a &quot;natural distribution&quot; to the audio data, which would make it resemble real-world noisy situations more closely, increases the degradation of the speech processing algorithm. They suggest that this could be due to the models' inability to effectively model and account for the increased variability. Instead, the added noise results in additional errors or degradation in the system's performance.&#10;2. This explanation can be found when PhD B says, &quot;the models are not complex enough to absorb that additional variability that you're introducing&quot; and later reiterated by PhD E, who shares their experience with neural networks being more capable of modeling strange distributions.&#10;3. The speakers also suggest that this natural distribution approach more accurately represents real-world noisy situations, indicating that the current models might not be sophisticated enough to account for these complexities without compromising performance." target="1. The project's focus regarding designing acoustics for either a kiosk or a mobile platform is determined by the specific requirements and use cases of the target application. If the application is designed to be stationary and user-specific, like in a kiosk, then it would make more sense to optimize the acoustics for that setting as the physical situation remains relatively stable. In contrast, if the application is intended for a mobile platform with variable acoustic conditions, the system should be designed to adapt to changing acoustics accordingly.&#10;2. The decision may also depend on the complexity and adaptability of the speech processing algorithms being used. More sophisticated models might be able to handle varying acoustic environments better than simpler ones. In this case, designing for a mobile platform could still provide an acceptable user experience if the models are complex enough to absorb additional variability.&#10;3. Lastly, practical considerations such as development time, resources, and budget may influence the decision. Developing a system that works well across multiple platforms might require more effort and resources than focusing on a single platform. However, designing with scalability and adaptability in mind could make it easier to expand the application to other platforms in the future.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers believe that the models are not complex enough to absorb additional variability in the data because they have observed that introducing a &quot;natural distribution&quot; to the audio data, which would make it resemble real-world noisy situations more closely, increases the degradation of the speech processing algorithm. They suggest that this could be due to the models' inability to effectively model and account for the increased variability. Instead, the added noise results in additional errors or degradation in the system's performance.&#10;2. This explanation can be found when PhD B says, &quot;the models are not complex enough to absorb that additional variability that you're introducing&quot; and later reiterated by PhD E, who shares their experience with neural networks being more capable of modeling strange distributions.&#10;3. The speakers also suggest that this natural distribution approach more accurately represents real-world noisy situations, indicating that the current models might not be sophisticated enough to account for these complexities without compromising performance." target="1. The group's current position on constraining the latency is that they are not unduly restricting it at this stage because they are still experimenting with improving performance in noisy conditions. There is a minority within the group who argue for further constraining of the latency, but the majority believes that longer latencies provide benefits, so they are continuing to explore the trade-offs between longer and shorter latencies.&#10;2. The individual who argues for further constraint of latency is not explicitly named in the transcript, but it is mentioned that there is a minority within the group who hold this view.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the Spanish government requests that all participants in their program provide a report. The content of this report includes details about the approach and work done by each participant. There is no explicit mention of a decision regarding including digits in the report, but earlier in the conversation, they discuss processing TI-digits data for their system. It can be inferred that the report should cover the process, methods, and results related to working with the TI-digits data as part of their participation in the program." target="} the Spanish government , uh , requires that anyway . They want some kind of report from everybody who 's in the program .&#10;Speaker: PhD H&#10;Content: Mm - hmm .&#10;Speaker: Professor D&#10;Content: So . And of course I 'd {disfmarker} we 'd {disfmarker} we 'd like to see it too . So ,&#10;Speaker: PhD H&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: yeah .&#10;Speaker: PhD F&#10;Content: So , um , what 's {disfmarker} Do you think we , uh , should do the digits or skip it ? Or what are {disfmarker} what do you think ?&#10;Speaker: Professor D&#10;Content: Uh , we have them now ?&#10;Speaker: PhD F&#10;Content: Yeah , got them .&#10;Speaker: Professor D&#10;Content: Uh , why don why don't we do it ?&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: Just {comment} {disfmarker} just take a minute .&#10;Speaker: PhD H&#10;Content: I can send yet .">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the Spanish government requests that all participants in their program provide a report. The content of this report includes details about the approach and work done by each participant. There is no explicit mention of a decision regarding including digits in the report, but earlier in the conversation, they discuss processing TI-digits data for their system. It can be inferred that the report should cover the process, methods, and results related to working with the TI-digits data as part of their participation in the program." target=" final number as {disfmarker} as Sunil did it&#10;Speaker: PhD H&#10;Content: And prepare at the s&#10;Speaker: Professor G&#10;Content: and {vocalsound} um and maybe also to {disfmarker} to write somehow a document where you describe your approach , and what you have done .&#10;Speaker: PhD H&#10;Content: Yeah , I was thinking to do that next week .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: Yeah , I 'll {disfmarker} I 'll borrow the head back and {disfmarker} and agree . Yeah ,&#10;Speaker: PhD H&#10;Content: Yeah , I wi I {disfmarker} I will do that next week .&#10;Speaker: Professor D&#10;Content: that 's {disfmarker} that 's {disfmarker} Right . In fact , actually I g I guess the , uh {disfmarker} the Spanish government , uh , requires that anyway . They want some kind of report from everybody who 's in the program .&#10;Speaker: PhD H">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the Spanish government requests that all participants in their program provide a report. The content of this report includes details about the approach and work done by each participant. There is no explicit mention of a decision regarding including digits in the report, but earlier in the conversation, they discuss processing TI-digits data for their system. It can be inferred that the report should cover the process, methods, and results related to working with the TI-digits data as part of their participation in the program." target="Based on the discussion transcript, the outcome of the discussion regarding delay in compression is that both Company B and Company X have changed their compression schemes altogether. They now have their own compression and decoding schemes, as well as their own Cyclic Redundancy Check (CRC) and error correction mechanisms.&#10;&#10;Company B is able to determine within one more frame if the current frame is in error, reducing the delay to less than one frame. They have also modified the packing of frames to eliminate the delay associated with error correction. Similarly, Company X can also detect errors in one more frame, reducing the delay to less than one frame. They have altered both their compression and packing of frames to accomplish this.&#10;&#10;Therefore, both companies have made significant changes to their previous compression schemes to minimize or completely eliminate delays in compression and error correction.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the Spanish government requests that all participants in their program provide a report. The content of this report includes details about the approach and work done by each participant. There is no explicit mention of a decision regarding including digits in the report, but earlier in the conversation, they discuss processing TI-digits data for their system. It can be inferred that the report should cover the process, methods, and results related to working with the TI-digits data as part of their participation in the program." target="Scaling the noise estimate by a factor less than one resulted in improved performance for the TI-digits because, as mentioned in the transcript, there were a lot of zeros in the spectrogram when using the initial approach. By scaling the noise estimate, the speakers aimed to reduce this issue and improve the system's performance.&#10;&#10;The complete results of using this new technique involve applying a noise estimate scaled by a factor of 0.5. This is an ad-hoc method, meaning it is not optimized for any specific goal but serves as an intermediate step in their exploration. The only trend observed from the results was that the current noise estimation or composition scheme worked well for car noise types, while performing less effectively for non-stationary noises like &quot;Babble,&quot; &quot;Subway,&quot; &quot;Street,&quot; or &quot;Restaurant&quot; sounds.&#10;&#10;In summary, scaling the noise estimate by a factor of 0.5 improved performance in this case, but it is an ad-hoc method and not optimized for any specific goal. The complete results show that the technique works well for car noise types but may struggle with non-stationary noises.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The project's focus regarding designing acoustics for either a kiosk or a mobile platform is determined by the specific requirements and use cases of the target application. If the application is designed to be stationary and user-specific, like in a kiosk, then it would make more sense to optimize the acoustics for that setting as the physical situation remains relatively stable. In contrast, if the application is intended for a mobile platform with variable acoustic conditions, the system should be designed to adapt to changing acoustics accordingly.&#10;2. The decision may also depend on the complexity and adaptability of the speech processing algorithms being used. More sophisticated models might be able to handle varying acoustic environments better than simpler ones. In this case, designing for a mobile platform could still provide an acceptable user experience if the models are complex enough to absorb additional variability.&#10;3. Lastly, practical considerations such as development time, resources, and budget may influence the decision. Developing a system that works well across multiple platforms might require more effort and resources than focusing on a single platform. However, designing with scalability and adaptability in mind could make it easier to expand the application to other platforms in the future." target="disfmarker} Uh , there 's been some discussion about whether the work we 're doing in that project is gonna be for the kiosk or for the mobile or for both . And I think for this kind of discussion it matters . If it 's in the kiosk , then the physical situation is the same . It 's gonna {disfmarker} you know , the exact interaction of the microphone 's gonna differ depending on the person and so forth . But at least the basic acoustics are gonna be the same . So f if it 's really in one kiosk , then I think that you could just chain together and {disfmarker} and you know , as much {disfmarker} as much speech as possible to {disfmarker} because what you 're really trying to get at is the {disfmarker} is the reverberation characteristic .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: But in {disfmarker} in the case of the mobile , uh , {comment} presumably the acoustic 's changing all over the place .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor D">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The project's focus regarding designing acoustics for either a kiosk or a mobile platform is determined by the specific requirements and use cases of the target application. If the application is designed to be stationary and user-specific, like in a kiosk, then it would make more sense to optimize the acoustics for that setting as the physical situation remains relatively stable. In contrast, if the application is intended for a mobile platform with variable acoustic conditions, the system should be designed to adapt to changing acoustics accordingly.&#10;2. The decision may also depend on the complexity and adaptability of the speech processing algorithms being used. More sophisticated models might be able to handle varying acoustic environments better than simpler ones. In this case, designing for a mobile platform could still provide an acceptable user experience if the models are complex enough to absorb additional variability.&#10;3. Lastly, practical considerations such as development time, resources, and budget may influence the decision. Developing a system that works well across multiple platforms might require more effort and resources than focusing on a single platform. However, designing with scalability and adaptability in mind could make it easier to expand the application to other platforms in the future." target="1. The speakers discussed introducing a &quot;natural distribution&quot; of noise to the audio data analyzed in the SmartKom project's ASRU paper. This approach aims to make the audio data resemble real-world noisy situations more closely by reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;&#10;2. Two research questions arise from this discussion:&#10;  a. Does this natural distribution approach still work effectively even if the speakers are involved?&#10;  b. This question is cut off in the transcript but presumably relates to testing the effectiveness of the natural distribution approach under different conditions or with various types of noise.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion transcript, the outcome of the discussion regarding delay in compression is that both Company B and Company X have changed their compression schemes altogether. They now have their own compression and decoding schemes, as well as their own Cyclic Redundancy Check (CRC) and error correction mechanisms.&#10;&#10;Company B is able to determine within one more frame if the current frame is in error, reducing the delay to less than one frame. They have also modified the packing of frames to eliminate the delay associated with error correction. Similarly, Company X can also detect errors in one more frame, reducing the delay to less than one frame. They have altered both their compression and packing of frames to accomplish this.&#10;&#10;Therefore, both companies have made significant changes to their previous compression schemes to minimize or completely eliminate delays in compression and error correction." target="1. The group's current position on constraining the latency is that they are not unduly restricting it at this stage because they are still experimenting with improving performance in noisy conditions. There is a minority within the group who argue for further constraining of the latency, but the majority believes that longer latencies provide benefits, so they are continuing to explore the trade-offs between longer and shorter latencies.&#10;2. The individual who argues for further constraint of latency is not explicitly named in the transcript, but it is mentioned that there is a minority within the group who hold this view.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group's current position on constraining the latency is that they are not unduly restricting it at this stage because they are still experimenting with improving performance in noisy conditions. There is a minority within the group who argue for further constraining of the latency, but the majority believes that longer latencies provide benefits, so they are continuing to explore the trade-offs between longer and shorter latencies.&#10;2. The individual who argues for further constraint of latency is not explicitly named in the transcript, but it is mentioned that there is a minority within the group who hold this view." target="}&#10;Speaker: Professor D&#10;Content: This is {disfmarker} So {disfmarker} So , basically our {disfmarker} our position is {vocalsound} that , um , we shouldn't be unduly constraining the latency at this point because we 're all still experimenting with trying to make the performance better in the presence of noise . Uh , there is a minority in that group who is a arguing {disfmarker} who are arguing for {vocalsound} um , uh , having a further constraining of the latency . So we 're s just continuing to keep aware of what the trade - offs are and , you know , what {disfmarker} what do we gain from having longer or shorter latencies ?&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: Professor D&#10;Content: But since we always seem to at least get something out of longer latencies not being so constrained , we 're tending to go with that if we 're not told we can't do it .&#10;Speaker: PhD F&#10;Content: What {disfmarker} where was the , um {disfmarker} the smallest latency of all">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group's current position on constraining the latency is that they are not unduly restricting it at this stage because they are still experimenting with improving performance in noisy conditions. There is a minority within the group who argue for further constraining of the latency, but the majority believes that longer latencies provide benefits, so they are continuing to explore the trade-offs between longer and shorter latencies.&#10;2. The individual who argues for further constraint of latency is not explicitly named in the transcript, but it is mentioned that there is a minority within the group who hold this view." target=" .&#10;Speaker: PhD E&#10;Content: Well , um , for Italian and Spanish it 's {disfmarker} th this value works good but not necessarily for Finnish . Mmm . But unfortunately there is , like , this forty millisecond latency and , um {disfmarker} Yeah , so I would try to somewhat reduce this @ @ . I already know that if I completely remove this latency , so . {vocalsound} um , {comment} it {disfmarker} um there is a three percent hit on Italian .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: d Does latency {disfmarker}&#10;Speaker: Professor G&#10;Content: i&#10;Speaker: PhD B&#10;Content: Sorry . Go ahead .&#10;Speaker: Professor G&#10;Content: Yeah . Your {disfmarker} your smoothing was @ @ {comment} uh , over this s so to say , the {disfmarker} the factor of the Wiener . And then it 's , uh {disfmarker} What was it ? This {disfmarker}&#10;Speaker: PhD E&#10;Content: M">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group's current position on constraining the latency is that they are not unduly restricting it at this stage because they are still experimenting with improving performance in noisy conditions. There is a minority within the group who argue for further constraining of the latency, but the majority believes that longer latencies provide benefits, so they are continuing to explore the trade-offs between longer and shorter latencies.&#10;2. The individual who argues for further constraint of latency is not explicitly named in the transcript, but it is mentioned that there is a minority within the group who hold this view." target="1. The process of smoothing the gain in this context involves using a non-linear smoothing technique for low gain values, while not applying any smoothing for high gain values. This approach is taken to balance the need for noise reduction with the potential distortion or loss of signal information that can result from aggressive smoothing.&#10;2. The subtraction factor or algorithm's gain is being smoothed in this case, as mentioned by Professor G. This suggests that the group is trying to improve the accuracy and stability of the noise reduction technique by reducing the variability of the gain value over time.&#10;3. The decision on whether to smooth or not, and the choice of smoothing method, depends on various factors such as the desired latency, the characteristics of the signal and noise, the performance trade-offs between different techniques, and the specific requirements of the application.&#10;4. The group's discussion suggests that they are still experimenting with different approaches to smoothing the gain, as there is no consensus on whether further constraining the latency would be beneficial or not. This indicates that the process involves ongoing evaluation and adjustment based on empirical results and theoretical considerations.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of smoothing the gain in this context involves using a non-linear smoothing technique for low gain values, while not applying any smoothing for high gain values. This approach is taken to balance the need for noise reduction with the potential distortion or loss of signal information that can result from aggressive smoothing.&#10;2. The subtraction factor or algorithm's gain is being smoothed in this case, as mentioned by Professor G. This suggests that the group is trying to improve the accuracy and stability of the noise reduction technique by reducing the variability of the gain value over time.&#10;3. The decision on whether to smooth or not, and the choice of smoothing method, depends on various factors such as the desired latency, the characteristics of the signal and noise, the performance trade-offs between different techniques, and the specific requirements of the application.&#10;4. The group's discussion suggests that they are still experimenting with different approaches to smoothing the gain, as there is no consensus on whether further constraining the latency would be beneficial or not. This indicates that the process involves ongoing evaluation and adjustment based on empirical results and theoretical considerations." target="isfmarker} {vocalsound} and later on you smooth also this subtraction factor .&#10;Speaker: PhD E&#10;Content: Uh , no , it 's {disfmarker} it 's just the gain that 's smoothed actually&#10;Speaker: PhD B&#10;Content: Uh , actually I d I do all the smoothing .&#10;Speaker: PhD E&#10;Content: but it 's smoothed {disfmarker}&#10;Speaker: Professor G&#10;Content: Ah . Oh , it w it was you .&#10;Speaker: PhD B&#10;Content: Yeah , yeah .&#10;Speaker: PhD E&#10;Content: Uh {disfmarker} Yeah .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Yeah . No , in this case it 's just the gain .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: And {disfmarker}&#10;Speaker: Professor G&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: But the way it 's done is that um , for low gain , there is this non nonlinear smoothing actually . For low">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of smoothing the gain in this context involves using a non-linear smoothing technique for low gain values, while not applying any smoothing for high gain values. This approach is taken to balance the need for noise reduction with the potential distortion or loss of signal information that can result from aggressive smoothing.&#10;2. The subtraction factor or algorithm's gain is being smoothed in this case, as mentioned by Professor G. This suggests that the group is trying to improve the accuracy and stability of the noise reduction technique by reducing the variability of the gain value over time.&#10;3. The decision on whether to smooth or not, and the choice of smoothing method, depends on various factors such as the desired latency, the characteristics of the signal and noise, the performance trade-offs between different techniques, and the specific requirements of the application.&#10;4. The group's discussion suggests that they are still experimenting with different approaches to smoothing the gain, as there is no consensus on whether further constraining the latency would be beneficial or not. This indicates that the process involves ongoing evaluation and adjustment based on empirical results and theoretical considerations." target=" it 's , uh {disfmarker} What was it ? This {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: this smoothing , it was over the subtraction factor , so to say .&#10;Speaker: PhD E&#10;Content: It 's a smoothing over the {disfmarker} the gain of the subtraction algorithm .&#10;Speaker: Professor G&#10;Content: Was this done {disfmarker} Mm - hmm . And {disfmarker} and you are looking into the future , into the past .&#10;Speaker: PhD E&#10;Content: Right .&#10;Speaker: Professor G&#10;Content: And smoothing .&#10;Speaker: PhD E&#10;Content: So , to smooth this {pause} thing .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Yeah . Um {disfmarker}&#10;Speaker: Professor G&#10;Content: And did {disfmarker} did you try simply to smooth um to smooth the {disfmarker} the {disfmarker} t to {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of smoothing the gain in this context involves using a non-linear smoothing technique for low gain values, while not applying any smoothing for high gain values. This approach is taken to balance the need for noise reduction with the potential distortion or loss of signal information that can result from aggressive smoothing.&#10;2. The subtraction factor or algorithm's gain is being smoothed in this case, as mentioned by Professor G. This suggests that the group is trying to improve the accuracy and stability of the noise reduction technique by reducing the variability of the gain value over time.&#10;3. The decision on whether to smooth or not, and the choice of smoothing method, depends on various factors such as the desired latency, the characteristics of the signal and noise, the performance trade-offs between different techniques, and the specific requirements of the application.&#10;4. The group's discussion suggests that they are still experimenting with different approaches to smoothing the gain, as there is no consensus on whether further constraining the latency would be beneficial or not. This indicates that the process involves ongoing evaluation and adjustment based on empirical results and theoretical considerations." target=" PhD E&#10;Content: But the way it 's done is that um , for low gain , there is this non nonlinear smoothing actually . For low gains um , I use the smoothed sm uh , smoothed version but {disfmarker} for high gain @ @ {comment} it 's {disfmarker} I don't smooth .&#10;Speaker: Professor G&#10;Content: Uh . Mm - hmm . I just , uh {disfmarker} it {disfmarker} Experience shows you , if {disfmarker} if you do the {disfmarker}  The best is to do the smoo smoothing as early as possible .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Professor G&#10;Content: So w when you start up . I mean , you start up with the {disfmarker} with the {disfmarker} somehow with the noisy envelope .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: And , best is to smooth this somehow .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Uh , yeah , I">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of smoothing the gain in this context involves using a non-linear smoothing technique for low gain values, while not applying any smoothing for high gain values. This approach is taken to balance the need for noise reduction with the potential distortion or loss of signal information that can result from aggressive smoothing.&#10;2. The subtraction factor or algorithm's gain is being smoothed in this case, as mentioned by Professor G. This suggests that the group is trying to improve the accuracy and stability of the noise reduction technique by reducing the variability of the gain value over time.&#10;3. The decision on whether to smooth or not, and the choice of smoothing method, depends on various factors such as the desired latency, the characteristics of the signal and noise, the performance trade-offs between different techniques, and the specific requirements of the application.&#10;4. The group's discussion suggests that they are still experimenting with different approaches to smoothing the gain, as there is no consensus on whether further constraining the latency would be beneficial or not. This indicates that the process involves ongoing evaluation and adjustment based on empirical results and theoretical considerations." target="1. Cepstral Mean Subtraction (CMS) is a technique used in short-term window analysis to remove general characteristics from data. It involves calculating the mean of the cepstrum (a transform of the log power spectrum) for each window and subtracting it from the cepstrum of that window. This helps get rid of very general characteristics, such as those caused by the recording device or room acoustics.&#10;2. Flipping frames from the front to create negative values is a technique used in some signal processing applications, including CMS. By taking a certain number of frames from the beginning of the signal and flipping them around, the original frames are replaced with their mirror images. This can help balance the signal by reducing the impact of any abrupt changes at the beginning of the window. In the context of CMS, flipping frames creates negative values in the cepstrum that are then subtracted from the original cepstrum to achieve mean subtraction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These window functions can be used to reduce the weight of values at the edges when applying a moving window to a signal. They decrease the impact of abrupt changes between windows and help smooth the signal, which can make the training process more stable. In the context of the &quot;Aurora stuff,&quot; these window functions could be applied when processing audio data as part of a speech recognition system.&#10;2. LDA Filter (Linear Discriminant Analysis filter): The LDA filter is a supervised learning algorithm that can optimize some criterion, such as minimizing within-class variance or maximizing between-class variance. This results in a filter that better separates different classes and improves the performance of the system. In the context of the &quot;Aurora stuff,&quot; an LDA filter could be trained on labeled data to improve speech recognition accuracy.&#10;3. Spectral Subtraction or Wiener Filtering: These techniques reduce noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum, improving the signal-to-noise ratio and making it easier to extract useful information from noisy data. In the context of the &quot;Aurora stuff,&quot; these methods could be applied when processing audio signals to improve speech recognition performance.&#10;&#10;These filters, along with a uniform window, are options for moving window setups in training LDA filters or other signal processing tasks. The choice of filter will depend on the specific application and the characteristics of the data being processed. In the transcript, it is mentioned that spectral subtraction was not effective for certain Aurora tasks, suggesting that a different filter or approach might be needed." target="}&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: But {disfmarker} Yeah . Then I tried the same {disfmarker} exactly the same spectral subtraction algorithm on these Aurora tasks and it simply doesn't work . It 's even {disfmarker} it , uh , hurts even .&#10;Speaker: Professor G&#10;Content: Hmm .&#10;Speaker: PhD E&#10;Content: So .&#10;Speaker: Professor D&#10;Content: We probably should at some point here try the tandem {disfmarker} the {disfmarker} the {disfmarker} the system - two kind of stuff with this , with the spectral subtraction for that reason .&#10;Speaker: Professor G&#10;Content: Hmm .&#10;Speaker: Professor D&#10;Content: Cuz {vocalsound} again , it should do a transformation to a domain where it maybe {disfmarker} looks more Gaussian .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Hmm . Yeah , y I">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Hamming or Hanning Window: These window functions can be used to reduce the weight of values at the edges when applying a moving window to a signal. They decrease the impact of abrupt changes between windows and help smooth the signal, which can make the training process more stable. In the context of the &quot;Aurora stuff,&quot; these window functions could be applied when processing audio data as part of a speech recognition system.&#10;2. LDA Filter (Linear Discriminant Analysis filter): The LDA filter is a supervised learning algorithm that can optimize some criterion, such as minimizing within-class variance or maximizing between-class variance. This results in a filter that better separates different classes and improves the performance of the system. In the context of the &quot;Aurora stuff,&quot; an LDA filter could be trained on labeled data to improve speech recognition accuracy.&#10;3. Spectral Subtraction or Wiener Filtering: These techniques reduce noise in signals by subtracting an estimate of the noise spectrum from the signal spectrum, improving the signal-to-noise ratio and making it easier to extract useful information from noisy data. In the context of the &quot;Aurora stuff,&quot; these methods could be applied when processing audio signals to improve speech recognition performance.&#10;&#10;These filters, along with a uniform window, are options for moving window setups in training LDA filters or other signal processing tasks. The choice of filter will depend on the specific application and the characteristics of the data being processed. In the transcript, it is mentioned that spectral subtraction was not effective for certain Aurora tasks, suggesting that a different filter or approach might be needed." target="1. Cepstral Mean Subtraction (CMS) is a technique used in short-term window analysis to remove general characteristics from data. It involves calculating the mean of the cepstrum (a transform of the log power spectrum) for each window and subtracting it from the cepstrum of that window. This helps get rid of very general characteristics, such as those caused by the recording device or room acoustics.&#10;2. Flipping frames from the front to create negative values is a technique used in some signal processing applications, including CMS. By taking a certain number of frames from the beginning of the signal and flipping them around, the original frames are replaced with their mirror images. This can help balance the signal by reducing the impact of any abrupt changes at the beginning of the window. In the context of CMS, flipping frames creates negative values in the cepstrum that are then subtracted from the original cepstrum to achieve mean subtraction.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, based on the transcript, it appears that the neural network was trained on previously estimated alignments rather than being directly based on the provided label files for the Italian data. Speaker PhD E states that &quot;we trained the neural network on embedded training... That's right?&quot; and later clarifies that &quot;For the Italian data, I think we trained the neural network on re-estimation of the alignment using the neural network.&quot; There is no indication in the transcript that the neural network was directly trained on the provided label files." target="&#10;Content: Yeah , but {disfmarker} Yes .&#10;Speaker: PhD B&#10;Content: Or {disfmarker} Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Uh , the next , um {disfmarker} Oh , it 's there .&#10;Speaker: Professor G&#10;Content: So is the {disfmarker} is the {disfmarker} is the training {disfmarker} is the training based on these labels files which you take as reference here ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Wh - when you train the neural net y y you {disfmarker}&#10;Speaker: PhD E&#10;Content: No . It 's not . It 's {disfmarker} it was trained on some alignment obtained um , uh {disfmarker} For the Italian data , I think we trained the neural network on {disfmarker} with embedded training . So re - estimation of the alignment using the neural network , I guess . That 's right ?&#10;Speaker: PhD B&#10;Content: Yeah . We actually trained , uh , the {">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, based on the transcript, it appears that the neural network was trained on previously estimated alignments rather than being directly based on the provided label files for the Italian data. Speaker PhD E states that &quot;we trained the neural network on embedded training... That's right?&quot; and later clarifies that &quot;For the Italian data, I think we trained the neural network on re-estimation of the alignment using the neural network.&quot; There is no indication in the transcript that the neural network was directly trained on the provided label files." target="aker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: so the VAD was trained on maybe different set of labels for channel zero and channel one&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: was the alignments were w were different for {disfmarker} s certainly different because they were independently trained .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: We didn't copy the channel zero alignments to channel one .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: But for the new alignments what you generated , you just copied the channel zero to channel one , right ? Yeah .&#10;Speaker: PhD E&#10;Content: Right . Yeah . Um . And eh , h">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, based on the transcript, it appears that the neural network was trained on previously estimated alignments rather than being directly based on the provided label files for the Italian data. Speaker PhD E states that &quot;we trained the neural network on embedded training... That's right?&quot; and later clarifies that &quot;For the Italian data, I think we trained the neural network on re-estimation of the alignment using the neural network.&quot; There is no indication in the transcript that the neural network was directly trained on the provided label files." target=" that are used for training came from different system .&#10;Speaker: PhD B&#10;Content: Syste Yeah .&#10;Speaker: PhD E&#10;Content: Then we put them tog together . Well , you put them together and trained the VAD on them .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Hmm .&#10;Speaker: PhD E&#10;Content: Uh , But did you use channel {disfmarker} did you align channel one also ? Or {disfmarker}&#10;Speaker: PhD B&#10;Content: I just took their entire Italian training part .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: So it was both channel zero plus channel one .&#10;Speaker: PhD E&#10;Content: So di Yeah . So the alignments might be wrong then on channel one , right ?&#10;Speaker: PhD B&#10;Content: On one . Possible .&#10;Speaker: PhD E&#10;Content: So we might ,&#10;Speaker: PhD B&#10;Content: We can do a realignment .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, based on the transcript, it appears that the neural network was trained on previously estimated alignments rather than being directly based on the provided label files for the Italian data. Speaker PhD E states that &quot;we trained the neural network on embedded training... That's right?&quot; and later clarifies that &quot;For the Italian data, I think we trained the neural network on re-estimation of the alignment using the neural network.&quot; There is no indication in the transcript that the neural network was directly trained on the provided label files." target="&#10;Speaker: PhD E&#10;Content: So we might ,&#10;Speaker: PhD B&#10;Content: We can do a realignment .&#10;Speaker: PhD E&#10;Content: yeah ,&#10;Speaker: PhD B&#10;Content: That 's true .&#10;Speaker: PhD E&#10;Content: at least want to retrain on these alignments , which should be better because they come from close - talking microphone .&#10;Speaker: Professor G&#10;Content: Yeah , the {disfmarker} that was my idea . I mean , if {disfmarker} if it ha if it is not the same labeling which is taking the spaces .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Yeah , possible .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: I mean , it {disfmarker} so the system {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: so the VAD was trained on maybe different set of labels for channel">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The advantage of using a fixed filter, such as the RASTA filter, is that it remains unchanged and does not degrade over time, making it a reliable choice for signal processing. This is because it does not need to be retrained or adapted for different conditions, unlike trained filters.&#10;2. In the context of the SmartKom system, using a fixed amount of data for mean subtraction can be beneficial when less than twelve seconds of data are available. By training the models with a fixed amount of data (e.g., six or twelve seconds), the system can ensure stability and consistency in the processing steps that follow.&#10;3. However, if there is not enough data available to estimate a reliable mean value for mean subtraction, concatenating utterances together can help increase the amount of available data. This approach may be more suitable than training a filter specifically for certain conditions, as it allows the system to use all the available data and avoid potential overfitting or poor generalization issues that can arise with trained filters.&#10;4. Overall, using a fixed filter or a fixed amount of data for mean subtraction provides a stable and consistent processing approach, while concatenating utterances together can help increase the amount of available data when there is less than twelve seconds of data in the SmartKom system." target=" SmartKom system to do the mean subtraction . You said in {vocalsound} systems where you use cepstral mean subtraction , they concatenate utterances and , {vocalsound} do you know how they address this issue of , um , testing versus training ? Can {disfmarker}&#10;Speaker: Professor D&#10;Content: Go ahead .&#10;Speaker: Professor G&#10;Content: I think what they do is they do it always on - line , I mean , that you just take what you have from the past , that you calculate the mean of this and subtract the mean .&#10;Speaker: Grad C&#10;Content: OK . Um {disfmarker}&#10;Speaker: Professor G&#10;Content: And then you can {disfmarker} yeah , you {disfmarker} you can increase your window whi while you get {disfmarker} while you are getting more samples .&#10;Speaker: Grad C&#10;Content: OK , um , and , um , so {disfmarker} so in tha in that case , wh what do they do when they 're t um , performing the cepstral mean subtraction on the training data ? So {">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The advantage of using a fixed filter, such as the RASTA filter, is that it remains unchanged and does not degrade over time, making it a reliable choice for signal processing. This is because it does not need to be retrained or adapted for different conditions, unlike trained filters.&#10;2. In the context of the SmartKom system, using a fixed amount of data for mean subtraction can be beneficial when less than twelve seconds of data are available. By training the models with a fixed amount of data (e.g., six or twelve seconds), the system can ensure stability and consistency in the processing steps that follow.&#10;3. However, if there is not enough data available to estimate a reliable mean value for mean subtraction, concatenating utterances together can help increase the amount of available data. This approach may be more suitable than training a filter specifically for certain conditions, as it allows the system to use all the available data and avoid potential overfitting or poor generalization issues that can arise with trained filters.&#10;4. Overall, using a fixed filter or a fixed amount of data for mean subtraction provides a stable and consistent processing approach, while concatenating utterances together can help increase the amount of available data when there is less than twelve seconds of data in the SmartKom system." target=" of what we 're doing in this Aur - Aurora stuff . And , uh , it 's still not clear to me in the long run whether the best thing to do would be to do that or to have some stylized version of the filter that looks like these things you 've trained up , because you always have the problem that it 's trained up for one condition and it isn't quite right for another . So . uh {disfmarker} that 's {disfmarker} that 's why {disfmarker} that 's why RASTA filter has actually ended up lasting a long time , people still using it quite a bit , because y you don't change it . So doesn't get any worse . Uh ,&#10;Speaker: Grad C&#10;Content: Huh .&#10;Speaker: Professor D&#10;Content: Anyway .&#10;Speaker: Grad C&#10;Content: o OK . So , um , a actually I was just thinking about what I was asking about earlier , wi which is about having {vocalsound} less than say twelve seconds in the SmartKom system to do the mean subtraction . You said in {vocalsound} systems where you use cepstral mean subtraction , they">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The advantage of using a fixed filter, such as the RASTA filter, is that it remains unchanged and does not degrade over time, making it a reliable choice for signal processing. This is because it does not need to be retrained or adapted for different conditions, unlike trained filters.&#10;2. In the context of the SmartKom system, using a fixed amount of data for mean subtraction can be beneficial when less than twelve seconds of data are available. By training the models with a fixed amount of data (e.g., six or twelve seconds), the system can ensure stability and consistency in the processing steps that follow.&#10;3. However, if there is not enough data available to estimate a reliable mean value for mean subtraction, concatenating utterances together can help increase the amount of available data. This approach may be more suitable than training a filter specifically for certain conditions, as it allows the system to use all the available data and avoid potential overfitting or poor generalization issues that can arise with trained filters.&#10;4. Overall, using a fixed filter or a fixed amount of data for mean subtraction provides a stable and consistent processing approach, while concatenating utterances together can help increase the amount of available data when there is less than twelve seconds of data in the SmartKom system." target=" , in my tests before with HTK I found it worked {disfmarker} it worked the best with about twelve seconds of data used to estimate the mean , but , we 'll often have less {comment} in the SmartKom system . Um . So I think we 'll use as much data as we have {pause} at a particular time , and we 'll {disfmarker} {vocalsound} we 'll concatenate utterances together , um , to get as much data as we possibly can from the user . But , {vocalsound} um , {vocalsound} there 's a question of how to set up the models . So um , we could train the models . If we think twelve seconds is ideal we could train the models using twelve seconds to calculate the mean , to mean subtract the training data . Or we could , um , use some other amount . So {disfmarker} like I did an experiment where I , um , was using six seconds in test , um , but , for {disfmarker} I tried twelve seconds in train . And I tried , um , um , the same in train {disfmarker} I 'm a I tried six">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Cepstral Mean Subtraction (CMS) is a technique used in short-term window analysis to remove general characteristics from data. It involves calculating the mean of the cepstrum (a transform of the log power spectrum) for each window and subtracting it from the cepstrum of that window. This helps get rid of very general characteristics, such as those caused by the recording device or room acoustics.&#10;2. Flipping frames from the front to create negative values is a technique used in some signal processing applications, including CMS. By taking a certain number of frames from the beginning of the signal and flipping them around, the original frames are replaced with their mirror images. This can help balance the signal by reducing the impact of any abrupt changes at the beginning of the window. In the context of CMS, flipping frames creates negative values in the cepstrum that are then subtracted from the original cepstrum to achieve mean subtraction." target=" . Yeah , people do something . They {disfmarker} they , uh , they have some , um , uh , in {disfmarker} in cepstral mean subtraction , for short - term window {disfmarker} analysis windows , as is usually done , you 're trying to get rid of some very general characteristic . And so , uh , if you have any other information about what a general kind of characteristic would be , then you {disfmarker} you can do it there .&#10;Speaker: PhD F&#10;Content: You can also {disfmarker} you can also reflect the data . So you take , uh {disfmarker} you know , I 'm not sure how many frames you need .&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: But you take that many from the front and flip it around to {disfmarker} a as the negative value .&#10;Speaker: Professor D&#10;Content: Yeah , that 's {disfmarker} Yeah .&#10;Speaker: PhD F&#10;Content: So you can always {disfmarker}&#10;Speaker: Professor D&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="Scaling the noise estimate by a factor less than one resulted in improved performance for the TI-digits because, as mentioned in the transcript, there were a lot of zeros in the spectrogram when using the initial approach. By scaling the noise estimate, the speakers aimed to reduce this issue and improve the system's performance.&#10;&#10;The complete results of using this new technique involve applying a noise estimate scaled by a factor of 0.5. This is an ad-hoc method, meaning it is not optimized for any specific goal but serves as an intermediate step in their exploration. The only trend observed from the results was that the current noise estimation or composition scheme worked well for car noise types, while performing less effectively for non-stationary noises like &quot;Babble,&quot; &quot;Subway,&quot; &quot;Street,&quot; or &quot;Restaurant&quot; sounds.&#10;&#10;In summary, scaling the noise estimate by a factor of 0.5 improved performance in this case, but it is an ad-hoc method and not optimized for any specific goal. The complete results show that the technique works well for car noise types but may struggle with non-stationary noises." target=" , I didn't h I mean , I {disfmarker} I found that the results {disfmarker} I mean , I wasn't getting that r results on the TI - digit . So I was like looking into &quot; why , what is wrong with the TI - digits ? &quot; . Why {disfmarker} why I was not getting it . And I found that , the noise estimation is a reason for the TI - digits to perform worse than the baseline . So , uh , I actually , picked th I mean , the first thing I did was I just scaled the noise estimate by a factor which is less than one to see if that {disfmarker} because I found there are a lot of zeros in the spectrogram for the TI - digits when I used this approach . So the first thing I did was I just scaled the noise estimate . And I found {disfmarker} So the {disfmarker} the results that I 've shown here are the complete results using the new {disfmarker} Well , the n the new technique is nothing but the noise estimate scaled by a factor of point five . So it 's just an ad - hoc {disfmarker} I mean">
      <data key="d0">1</data>
    </edge>
    <edge source="Scaling the noise estimate by a factor less than one resulted in improved performance for the TI-digits because, as mentioned in the transcript, there were a lot of zeros in the spectrogram when using the initial approach. By scaling the noise estimate, the speakers aimed to reduce this issue and improve the system's performance.&#10;&#10;The complete results of using this new technique involve applying a noise estimate scaled by a factor of 0.5. This is an ad-hoc method, meaning it is not optimized for any specific goal but serves as an intermediate step in their exploration. The only trend observed from the results was that the current noise estimation or composition scheme worked well for car noise types, while performing less effectively for non-stationary noises like &quot;Babble,&quot; &quot;Subway,&quot; &quot;Street,&quot; or &quot;Restaurant&quot; sounds.&#10;&#10;In summary, scaling the noise estimate by a factor of 0.5 improved performance in this case, but it is an ad-hoc method and not optimized for any specific goal. The complete results show that the technique works well for car noise types but may struggle with non-stationary noises." target=" technique is nothing but the noise estimate scaled by a factor of point five . So it 's just an ad - hoc {disfmarker} I mean , some intermediate result , because it 's not optimized for anything . So the results {disfmarker} The trend {disfmarker} the only trend I could see from those results was like the {disfmarker} the p the current noise estimation or the , uh , noise composition scheme is working good for like the car noise type of thing . Because I 've {disfmarker} the only {disfmarker} only {disfmarker} p very good result in the TI - digits is the noise {disfmarker} car noise condition for their test - A , which is like the best I could see that uh , for any non - stationary noise like &quot; Babble &quot; or &quot; Subway &quot; or any {disfmarker} &quot; Street &quot; , some &quot; Restaurant &quot; noise , it 's like {disfmarker} it 's not performing w very well . So , the {disfmarker} {vocalsound} So that {disfmarker} that 's the first thing I c uh , I could">
      <data key="d0">1</data>
    </edge>
    <edge source="1. There is a mention of a &quot;point three percent hit&quot; in the context of a mismatch between training and testing conditions. This means that there is a decrease in performance or accuracy of around 0.3% when there's a discrepancy between how the model was trained (training link) and what it encounters during testing (testing link).&#10;2. The specific instance of this &quot;point three percent hit&quot; is related to changing the prototype for initializing the HMM (Hidden Markov Model) in order to avoid getting stuck in a local minimum during training. This change resulted in a 50% improvement, from 75.79 to 88, indicating that the mismatch between training and testing conditions had a noticeable impact on performance.&#10;3. The context suggests that this &quot;point three percent hit&quot; is not just for any mismatch but could be specific to certain types of discrepancies between training and testing conditions or specific model configurations. It's important to note that the exact nature and circumstances of this &quot;point three percent hit&quot; aren't explicitly detailed in the transcript, so further investigation would be required to provide a more precise answer." target=" take a hit .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: i In some cases it might be u better to have a mismatch . Like I think I saw something like {disfmarker} like if you only have two seconds in test , or , um , maybe it was something like four seconds , you actually do a little better if you , um , {vocalsound} train on six seconds than if you train on four seconds .&#10;Speaker: Professor D&#10;Content: Yeah . Right .&#10;Speaker: Grad C&#10;Content: Um , but the case , uh {disfmarker} with the point three percent hit was {vocalsound} using six seconds in test , um , comparing train on twelve seconds {comment} versus train on six seconds .&#10;Speaker: Professor D&#10;Content: And which was worse ?&#10;Speaker: Grad C&#10;Content: The train on twelve seconds .&#10;Speaker: Professor D&#10;Content: OK . But point three percent , uh , w from what to what ? That 's point three percent {disfmarker}&#10;Speaker: Grad C&#10;Content: On {disfmarker} The {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. There is a mention of a &quot;point three percent hit&quot; in the context of a mismatch between training and testing conditions. This means that there is a decrease in performance or accuracy of around 0.3% when there's a discrepancy between how the model was trained (training link) and what it encounters during testing (testing link).&#10;2. The specific instance of this &quot;point three percent hit&quot; is related to changing the prototype for initializing the HMM (Hidden Markov Model) in order to avoid getting stuck in a local minimum during training. This change resulted in a 50% improvement, from 75.79 to 88, indicating that the mismatch between training and testing conditions had a noticeable impact on performance.&#10;3. The context suggests that this &quot;point three percent hit&quot; is not just for any mismatch but could be specific to certain types of discrepancies between training and testing conditions or specific model configurations. It's important to note that the exact nature and circumstances of this &quot;point three percent hit&quot; aren't explicitly detailed in the transcript, so further investigation would be required to provide a more precise answer." target="isfmarker} we changed the proto for initializing the HMM {disfmarker} I mean , this {disfmarker} this is basically because it gets stuck in some local minimum in the training . That seventy - five point seven nine in the Finnish mismatch which is that {disfmarker} the eleven point nine six what we see .&#10;Speaker: Professor D&#10;Content: Uh - huh .&#10;Speaker: Professor G&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor D&#10;Content: So we have to jiggle it somehow ?&#10;Speaker: PhD B&#10;Content: Yeah {disfmarker} so we start with that different proto and it becomes eighty - eight , which is like some fifty percent improvement .&#10;Speaker: Professor D&#10;Content: S Wait a minute . Start with a different what ?&#10;Speaker: PhD B&#10;Content: Different prototype , which is like a different initialization for the , uh , s transition probabilities . It 's just that right now , the initialization is to stay more in the current state , which is point four point six , right ? Yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. There is a mention of a &quot;point three percent hit&quot; in the context of a mismatch between training and testing conditions. This means that there is a decrease in performance or accuracy of around 0.3% when there's a discrepancy between how the model was trained (training link) and what it encounters during testing (testing link).&#10;2. The specific instance of this &quot;point three percent hit&quot; is related to changing the prototype for initializing the HMM (Hidden Markov Model) in order to avoid getting stuck in a local minimum during training. This change resulted in a 50% improvement, from 75.79 to 88, indicating that the mismatch between training and testing conditions had a noticeable impact on performance.&#10;3. The context suggests that this &quot;point three percent hit&quot; is not just for any mismatch but could be specific to certain types of discrepancies between training and testing conditions or specific model configurations. It's important to note that the exact nature and circumstances of this &quot;point three percent hit&quot; aren't explicitly detailed in the transcript, so further investigation would be required to provide a more precise answer." target=" out what 's the optimal way to set this up . So , um , {vocalsound} I 'll try to make the plots and then put some postscript up on my {disfmarker} on my web page . And I 'll mention it in my status report if people wanna take a look .&#10;Speaker: Professor D&#10;Content: You could clarify something for me . You 're saying point three percent , you take a point three percent hit , {vocalsound} when the training and testing links are {disfmarker} don't match or something ?&#10;Speaker: PhD E&#10;Content: Hello .&#10;Speaker: Professor D&#10;Content: Is that what it is ?&#10;Speaker: Grad C&#10;Content: w Well , it c&#10;Speaker: Professor D&#10;Content: Or {disfmarker} ?&#10;Speaker: Grad C&#10;Content: I {disfmarker} I don't think it {disfmarker} it 's {vocalsound} just for any mismatch {vocalsound} you take a hit .&#10;Speaker: Professor D&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: i In some cases it might be u">
      <data key="d0">1</data>
    </edge>
    <edge source="1. There is a mention of a &quot;point three percent hit&quot; in the context of a mismatch between training and testing conditions. This means that there is a decrease in performance or accuracy of around 0.3% when there's a discrepancy between how the model was trained (training link) and what it encounters during testing (testing link).&#10;2. The specific instance of this &quot;point three percent hit&quot; is related to changing the prototype for initializing the HMM (Hidden Markov Model) in order to avoid getting stuck in a local minimum during training. This change resulted in a 50% improvement, from 75.79 to 88, indicating that the mismatch between training and testing conditions had a noticeable impact on performance.&#10;3. The context suggests that this &quot;point three percent hit&quot; is not just for any mismatch but could be specific to certain types of discrepancies between training and testing conditions or specific model configurations. It's important to note that the exact nature and circumstances of this &quot;point three percent hit&quot; aren't explicitly detailed in the transcript, so further investigation would be required to provide a more precise answer." target=" 's point three percent {disfmarker}&#10;Speaker: Grad C&#10;Content: On {disfmarker} The {disfmarker} the {disfmarker} the accuracies {vocalsound} w went from {disfmarker} it was something vaguely like ninety - five point six accuracy , um , improved to ninety - five point nine wh when I {disfmarker}&#10;Speaker: Professor D&#10;Content: So four point four to four point one .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: So {disfmarker} yeah . So about a {disfmarker} about an eight percent , uh , seven or eight percent relative ?&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor D&#10;Content: Uh , Yeah . Well , I think in a p You know , if {disfmarker} if you were going for an evaluation system you 'd care . But if you were doing a live system that people were actually using nobody would notice . It 's {disfmarker} uh , I think the thing is to get something that 's practical , that {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. There is a mention of a &quot;point three percent hit&quot; in the context of a mismatch between training and testing conditions. This means that there is a decrease in performance or accuracy of around 0.3% when there's a discrepancy between how the model was trained (training link) and what it encounters during testing (testing link).&#10;2. The specific instance of this &quot;point three percent hit&quot; is related to changing the prototype for initializing the HMM (Hidden Markov Model) in order to avoid getting stuck in a local minimum during training. This change resulted in a 50% improvement, from 75.79 to 88, indicating that the mismatch between training and testing conditions had a noticeable impact on performance.&#10;3. The context suggests that this &quot;point three percent hit&quot; is not just for any mismatch but could be specific to certain types of discrepancies between training and testing conditions or specific model configurations. It's important to note that the exact nature and circumstances of this &quot;point three percent hit&quot; aren't explicitly detailed in the transcript, so further investigation would be required to provide a more precise answer." target=" Yeah .&#10;Speaker: Professor D&#10;Content: so I 'm confused .&#10;Speaker: PhD B&#10;Content: Uh , actually the noise compensation whatever , uh , we are put in it works very well for the high mismatch condition . I mean , it 's consistent in the SpeechDat - Car and in the clean training also it gives it {disfmarker} But this fifty percent is {disfmarker} is that the {disfmarker} the high mismatch performance {disfmarker} equivalent to the high mismatch performance in the speech .&#10;Speaker: PhD F&#10;Content: So n s So since the high mismatch performance is much worse to begin with , it 's easier to get a better relative improvement .&#10;Speaker: PhD B&#10;Content: Yeah . Yeah . I do . Yeah , yeah . So by putting this noise {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah . Yeah , if we look at the figures on the right , we see that the reference system is very bad .&#10;Speaker: Professor D&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: Yeah . The reference drops like a very fast {disfmarker}&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker, Grad A, is proposing a new approach to speech recognition that combines multi-band ideas with an acoustic-phonetic approach. This approach aims to create a more robust system for word or phone recognition by developing a method that can classify intermediate categories and combine evidence from sub-bands in multi-band graphical models.&#10;2. The new approach involves using graphical models to recognize intermediate categories, such as phonetic features or other features closely related to the acoustic signal itself, within the context of a multi-band model. The hope is that by going multi-band and using these intermediate classifications, the system will be more robust in unseen noisy situations.&#10;3. Grad A plans to investigate several research questions, including what types of intermediate categories are needed for classification, the structure of multi-band graphical models (specifically, what types of structures should be considered to effectively combine evidence from sub-bands), and how to merge information from individual multi-band classifiers to improve word or phone recognition accuracy.&#10;4. This new approach is motivated by the need to address real-world noisy situations more accurately, potentially through the introduction of a &quot;natural distribution&quot; of noise rather than simply adding constant or flat noise across all regions. This could lead to a decrease in word error rate and better performance in signal processing." target="1. The speakers discussed introducing a &quot;natural distribution&quot; of noise to the audio data analyzed in the SmartKom project's ASRU paper. This approach aims to make the audio data resemble real-world noisy situations more closely by reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;&#10;2. Two research questions arise from this discussion:&#10;  a. Does this natural distribution approach still work effectively even if the speakers are involved?&#10;  b. This question is cut off in the transcript but presumably relates to testing the effectiveness of the natural distribution approach under different conditions or with various types of noise.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The best performance for the &quot;Car&quot; category was achieved when using an LDA filter trained with clean speech (TI-digits). This resulted in better signal processing performance compared to a filter trained with noisy speech.&#10;2. Although this specific conversation focuses on the improved performance of the clean LDA filter for TI-digits, it is essential to consider that different contexts or datasets might require different LDA filter designs, including those based on noisy speech.&#10;3. In this case, the &quot;Car&quot; category had the best performance in the third column of condition A, but specific numerical values or metrics were not provided in the conversation." target=" PhD E&#10;Content: Yeah . Uh , ppp . I don't know , you have questions about that , or suggestions ?&#10;Speaker: PhD B&#10;Content: Mmm . S so {disfmarker}&#10;Speaker: PhD E&#10;Content: It seems {disfmarker} the performance seems worse in Finnish , which {disfmarker}&#10;Speaker: PhD B&#10;Content: Well , it 's not trained on Finnish .&#10;Speaker: PhD E&#10;Content: uh {disfmarker}&#10;Speaker: PhD H&#10;Content: It 's worse .&#10;Speaker: PhD E&#10;Content: It 's not trained on Finnish , yeah .&#10;Speaker: Professor D&#10;Content: What 's it trained on ?&#10;Speaker: PhD B&#10;Content: I mean , the MLP 's not trained on Finnish .&#10;Speaker: Professor D&#10;Content: Right , what 's it trained on ?&#10;Speaker: PhD B&#10;Content: Oh {disfmarker} oh . Sorry . Uh , it 's Italian TI - digits .&#10;Speaker: Professor D&#10;Content: Yeah . Oh , it 's trained on Italian ?&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed introducing a &quot;natural distribution&quot; of noise to the audio data analyzed in the SmartKom project's ASRU paper. This approach aims to make the audio data resemble real-world noisy situations more closely by reducing the &quot;floor&quot; of noisy regions, possibly by decreasing the signal-to-noise ratio in those areas.&#10;&#10;2. Two research questions arise from this discussion:&#10;  a. Does this natural distribution approach still work effectively even if the speakers are involved?&#10;  b. This question is cut off in the transcript but presumably relates to testing the effectiveness of the natural distribution approach under different conditions or with various types of noise." target=" nobody would notice . It 's {disfmarker} uh , I think the thing is to get something that 's practical , that {disfmarker} that you could really use .&#10;Speaker: Grad C&#10;Content: Huh . That 's {disfmarker} that 's interesting . Alright , the e uh , I see your point . I guess I was thinking of it as , um , {vocalsound} an interesting research problem . The {disfmarker} how to g I was thinking that for the ASRU paper we could have a section saying , {vocalsound} &quot; For SmartKom , we {disfmarker} we d in {disfmarker} we tried this approach in , uh , {vocalsound} interactive system &quot; , which I don't think has been done before .&#10;Speaker: Professor D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Grad C&#10;Content: And {disfmarker} and then there was two research questions from that .&#10;Speaker: Professor D&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: And one is the k does it still work if">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
