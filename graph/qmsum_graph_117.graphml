<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" />
    <node id=" {disfmarker} i it might not sound very good .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh , and i the degradation from that might {disfmarker} might actually make it even harder , {vocalsound} uh , to understand than the LPC - twelve . So all I 'm saying is that the LPC - twelve {vocalsound} puts in {disfmarker} synthesis puts in some degradation that 's not what we 're used to hearing ,&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: and is , um {disfmarker} It 's not {disfmarker} it 's not just a question of how much information is there , as if you will always take maximum {vocalsound} advantage of any information that 's presented to you .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: In fact , you {vocalsound} hear some things better than others . And so it {disfmarker} it isn't {disfmarker}&#10;Speaker:" />
    <node id=": Professor B&#10;Content: There 's two problems there . I mean {disfmarker} I mean , so {disfmarker} so the first is {vocalsound} that by doing LPC - twelve with synthesized speech w like you 're saying , uh , it 's {disfmarker} {vocalsound} i i you 're {disfmarker} you 're adding other degradation .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Right ? So it 's not just the noise but you 're adding in fact some degradation because it 's only an approximation . Um , and the second thing is {disfmarker} which is m maybe more interesting {disfmarker} is that , um , {comment} {vocalsound} if you do it with whispered speech , you get this number . What if you had {pause} done analysis {comment} re - synthesis and taken the pitch as well ? Alright ? So now you put the pitch in .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: What would the percentage be then ?&#10;Speaker" />
    <node id=" LPC n twelve is pretty crummy &quot; . You know ?&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: So I I I 'm not sure {disfmarker} I 'm not sure how we can conclude from this anything about {disfmarker} that our system is close to {vocalsound} the human performance .&#10;Speaker: PhD D&#10;Content: Ye Yeah . Well , the point is that eh l ey {disfmarker} the point is that , um , {vocalsound} what I {disfmarker} what I listened to when I re - synthesized the LP - the LPC - twelve {pause} spectrum {vocalsound} is in a way what the system , uh , is hearing , cuz @ @ {disfmarker} all the {disfmarker} all the , um , excitation {disfmarker} all the {disfmarker} well , the excitation is {disfmarker} is not taken into account . That 's what we do with our system . And&#10;Speaker: Professor B&#10;Content: Well , you 're not doing the L" />
    <node id="disfmarker} a periodic sound and , {pause} @ @ {comment} uh , unvoiced sound , and the noise&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: which is mostly , {vocalsound} uh , noise . I mean not {pause} periodic . So , {pause} what {disfmarker} what do you mean exactly by putting back the pitch in ? Because {disfmarker}&#10;Speaker: PhD A&#10;Content: In the LPC synthesis ? I think {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . You did LPC re - synthesis {disfmarker}&#10;Speaker: PhD D&#10;Content: I&#10;Speaker: Professor B&#10;Content: L PC re - synthesis .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: So , {vocalsound} uh {disfmarker} and you did it with a noise source , rather than with {disfmarker} with a s periodic source .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor" />
    <node id=": Yeah .&#10;Speaker: PhD D&#10;Content: while our system is currently at seven percent . Um , but what happens also is that if I listen to the , um {disfmarker} {nonvocalsound} a re - synthesized version of the speech and {pause} I re - synthesized this using a white noise that 's filtered by a LPC , uh , filter {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Um , well , you can argue , that , uh {disfmarker} that this is not speech ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: so the ear is not trained to recognize this . But s actually it sound like {pause} whispering , so we are {disfmarker}&#10;Speaker: Professor B&#10;Content: Well , I mean , it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: eh {disfmarker}&#10;Speaker: Professor B&#10;Content: There 's two problems there . I mean {disfmarker} I mean , so {disfmarker} so" />
    <node id=" {disfmarker} with a s periodic source .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Right ? So if you actually did real re - synthesis like you do in an LPC synthesizer , where it 's unvoiced you use noise , where it 's voiced you use , {vocalsound} uh , periodic pulses .&#10;Speaker: PhD D&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD D&#10;Content: Yeah , but it 's neither {pause} purely voiced or purely unvoiced . Esp - especially because there is noise .&#10;Speaker: Professor B&#10;Content: Well , it might be hard to do it&#10;Speaker: PhD D&#10;Content: So {disfmarker}&#10;Speaker: Professor B&#10;Content: but it but {disfmarker} but the thing is that if you {disfmarker} {vocalsound} um , if you detect that there 's periodic {disfmarker} s strong periodic components , then you can use a voiced {disfmarker} voice thing .&#10;Speaker: PhD D&#10;" />
    <node id="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." />
    <node id=" B&#10;Content: So this is a particular human . This is {disfmarker} this i this is Stephane .&#10;Speaker: PhD D&#10;Content: Yeah . So that 's {disfmarker} that 's {disfmarker}&#10;Speaker: Grad E&#10;Content: St - Stephane .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: that 's the {disfmarker} the flaw of the experiment . This is just {disfmarker} i j {comment} {vocalsound} {vocalsound} it 's just one subject ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Getting close .&#10;Speaker: PhD D&#10;Content: but {disfmarker} but still , uh , {vocalsound} what happens is {disfmarker} is that , {vocalsound} uh , the digit error rate on this is around one percent ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: while our system is currently at seven percent . Um , but what happens also is that if I listen" />
    <node id="isfmarker}&#10;Speaker: Professor B&#10;Content: I mean , we {disfmarker} W what 's {disfmarker} what 's some of the lower error rates on {disfmarker} on {disfmarker} on {disfmarker} uh , some of the higher error rates on , uh , {vocalsound} some of these w uh , uh , highly mismatched difficult conditions ? What 's a {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Uh . Yeah , it 's around fifteen to twenty percent .&#10;Speaker: PhD A&#10;Content: Correct ?&#10;Speaker: PhD D&#10;Content: And the baseline , eh {disfmarker}&#10;Speaker: PhD A&#10;Content: Accuracy ?&#10;Speaker: PhD D&#10;Content: Uh , error rate .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Twenty percent error rate ,&#10;Speaker: Professor B&#10;Content: Yeah . So twenty percent error rate on digits .&#10;Speaker: PhD D&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , oh ," />
    <node id="Speaker: PhD D&#10;Content: Yeah , that 's it .&#10;Speaker: Professor B&#10;Content: Yeah . That 's {disfmarker} that 's {disfmarker} I mean , one {disfmarker} one percent is sort of what I would {disfmarker} I would figure . If somebody was paying really close attention , you might get {disfmarker} I would actually think that if , {vocalsound} you looked at people on various times of the day and different amounts of attention , you might actually get up to three or four percent error on digits . Uh , {vocalsound} uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Mm - hmm . Um .&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker} you know , we 're not {disfmarker} we 're not incredibly far off . On the other hand , with any of these numbers except maybe the one percent , it 's st it 's not actually usable in a commercial system with a full telephone number or something .&#10;Speaker: PhD D&#10;Content: Uh - huh . Yeah . At" />
    <node id=" then you 'd say &quot; OK , it 's {disfmarker} it 's in fact having , um , {vocalsound} not just the spectral envelope but also the {disfmarker} also the {disfmarker} the pitch {vocalsound} that , uh , {comment} @ @ {comment} has the information that people can use , anyway . &quot;&#10;Speaker: PhD D&#10;Content: Uh - huh . Mmm .&#10;Speaker: PhD A&#10;Content: But from this it 's pretty safe to say that the system is with either {vocalsound} two to seven percent away from {pause} the performance of a human . Right ? So it 's somewhere in that range .&#10;Speaker: Professor B&#10;Content: Well , or it 's {disfmarker} it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Two {disfmarker} two to six percent .&#10;Speaker: Professor B&#10;Content: Yeah , so {disfmarker} It 's {disfmarker} it 's one point four times , uh , to , uh , seven times the error ,&#10;Speaker:" />
    <node id=" is the system that we have currently . Oh , yes . We have , like , a system that gives sixty - two percent improvement , but {vocalsound} if you want to stick to the {disfmarker} {vocalsound} this latency {disfmarker} Well , it has a latency of two thirty , but {vocalsound} if you want also to stick to the number {vocalsound} of features that {disfmarker} limit it to sixty , {vocalsound} then we go a little bit down but it 's still sixty - one percent . Uh , and if we drop the tandem network , then we have fifty - seven percent .&#10;Speaker: Professor B&#10;Content: Uh , but th the two th two thirty includes the tandem network ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: OK . And i is the tandem network , uh , small enough that it will fit on the terminal size in terms of {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Uh , no , I don't think so .&#10;Speaker: Professor B&#10;Content: No .&#10;Speaker: PhD D&#10;Content:" />
    <node id=" on digits .&#10;Speaker: PhD D&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , oh , on digits .&#10;Speaker: Professor B&#10;Content: So if you 're doing {disfmarker} so if you 're doing ,&#10;Speaker: PhD D&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: On digits .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: you know ,&#10;Speaker: PhD D&#10;Content: And this is so {disfmarker} so {disfmarker} still the baseline .&#10;Speaker: Professor B&#10;Content: sixty - thousand {disfmarker} &#10;Speaker: PhD D&#10;Content: Right ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , and if you 're saying sixty - thousand word recognition , getting sixty percent error on some of these noise condition not at all surprising .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: The baseline" />
    <node id="1. The percentage of having the pitch is significant in terms of people's perception because it can impact the clarity and intelligibility of speech. A lower pitch retention percentage, such as 1%, may result in better understanding and a more positive opinion of LPC-12, while a higher percentage, like near 5%, might lead to degradation and a less favorable view.&#10;2. If the pitch retention stayed at around 5%, it could indicate that the LPC-12 system is not performing well, and users might perceive its quality as &quot;crummy.&quot; Conversely, a lower percentage demonstrates better performance, making it closer to human performance in terms of speech clarity and intelligibility." />
    <node id="&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: What would the percentage be then ?&#10;Speaker: PhD D&#10;Content: Um {disfmarker}&#10;Speaker: Professor B&#10;Content: See , that 's the question . So , you see , if it 's {disfmarker} if it 's {disfmarker} if it 's , uh {disfmarker} Let 's say it 's {pause} back down to one percent again .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: That would say at least for people , having the pitch is really , really important , which would be interesting in itself . Um ,&#10;Speaker: PhD D&#10;Content: Uh , yeah . But {disfmarker}&#10;Speaker: Professor B&#10;Content: if i on the other hand , if it stayed up {pause} near five percent , {vocalsound} then I 'd say &quot; boy , LPC n twelve is pretty crummy &quot; . You know ?&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B" />
    <node id="isfmarker} is it that different , I mean ?&#10;Speaker: Professor B&#10;Content: Um , {vocalsound} I don't know what mel , {pause} uh , based synthesis would sound like ,&#10;Speaker: PhD D&#10;Content: I&#10;Speaker: Professor B&#10;Content: but certainly the spectra are quite different .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Couldn't you t couldn't you , um , test the human performance on just the original {pause} audio ?&#10;Speaker: PhD D&#10;Content: Mm - hmm . This is the one percent number .&#10;Speaker: Professor B&#10;Content: Yeah , it 's one percent . He 's trying to remove the pitch information&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Oh , oh . OK ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: and make it closer to what {disfmarker} to what we 're seeing as the" />
    <node id="Extraction of computing features from a waveform involves processing and manipulating the original data, which can result in loss of some information. However, this process can also highlight or emphasize other important aspects or characteristics of the signal that may be relevant for the specific task a neural net is trying to perform. For example, features such as spectral coefficients or Mel-frequency cepstral coefficients (MFCCs) can capture patterns in the frequency content of speech signals that are meaningful for speech recognition. By focusing on these relevant features, a neural network might be able to better learn and generalize from the data, even if some information has been lost during the feature extraction process. In other words, while some information is lost, what remains may still be more informative and useful for the task at hand. This can lead to better performance compared to using the raw waveform, which might contain irrelevant or noisy information that could negatively impact the learning process in a neural network." />
    <node id=" , you know , neural net functions ,&#10;Speaker: Grad E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: its {disfmarker} {comment} {vocalsound} whatever it finds to be best .&#10;Speaker: Grad C&#10;Content: &#10;Speaker: Professor B&#10;Content: Um , so you could argue that in fact it {disfmarker} But I {disfmarker} I don't actually believe that argument because I know that , um , {vocalsound} you can , uh {disfmarker} computing features is useful , even though {pause} in principle you haven't {pause} {vocalsound} added anything {disfmarker} in fact , you subtracted something , from the original waveform {disfmarker} You know , uh , if you 've {disfmarker} you 've processed it in some way you 've typically lost something {disfmarker} some information . And so , {vocalsound} you 've lost information and yet it does better with {disfmarker} {vocalsound} with features than it does with the waveform . So ," />
    <node id="ve lost information and yet it does better with {disfmarker} {vocalsound} with features than it does with the waveform . So , uh , I {disfmarker} I know that i sometimes it 's useful to {disfmarker} {pause} to constrain things . So that 's {vocalsound} why it really seems like the constraint {disfmarker} in {disfmarker} in all this stuff it 's the constraints that are actually what matters . Because if it wasn't {pause} the constraints that mattered , then we would 've completely solved this problem long ago , because long ago we already knew how to put waveforms into powerful statistical mechanisms . So .&#10;Speaker: PhD D&#10;Content: Yeah . Well , if we had infinite processing power and {pause} data , {comment} I guess , using the waveform could {disfmarker}&#10;Speaker: Grad E&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: Yeah Uh , then it would work . Yeah , I agree . Yeah . There 's the problem .&#10;Speaker: PhD D&#10;Content: So , that 's {disfmarker}&#10;" />
    <node id=" stream ?&#10;Speaker: PhD D&#10;Content: But , um , we {disfmarker} we did a first try with this , and it {disfmarker} it {vocalsound} clearly hurts .&#10;Speaker: Professor B&#10;Content: But , uh , how was the stream combined ?&#10;Speaker: PhD D&#10;Content: Uh . {vocalsound} It was c it was just combined , um , by the acoustic model . So there was , no neural network for the moment .&#10;Speaker: Professor B&#10;Content: Right . So , I mean , if you just had a second stream that was just spectral and had another neural net and combined there , that {disfmarker} that , uh , {vocalsound} might be good .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Yeah . Mm - hmm . Mm - hmm . Mmm . Yeah . Um {disfmarker} Yeah , and the other thing , that noise estimation and th um , maybe try to train {disfmarker} uh , the training data for the t tandem network , right now , is like {disfmarker} i is using the noises" />
    <node id=" um , {vocalsound} {vocalsound} between Michael 's approach to , uh , some {disfmarker} some sort of optimal brain damage or optimal brain surgeon on the neural nets .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: So , like , if we have , um {disfmarker} we have our {disfmarker} we have our RASTA features and {disfmarker} and presumably the neural nets are {disfmarker} are learning some sort of a nonlinear mapping , {vocalsound} uh , from the {disfmarker} the {disfmarker} the features {vocalsound} to {disfmarker} to this {disfmarker} this probability posterior space .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: Right ? And , um {disfmarker} {vocalsound} {vocalsound} and each of the hidden units is learning some sort of {disfmarker} some sort of {disfmarker} some sort" />
    <node id="ifiers , then do recognition , and , um , improve speech recognition in that way . Um , so right now I 'm in {disfmarker} in the phase where {vocalsound} I 'm looking at {disfmarker} at , um , deciding on a initial set of intermediate categories . And {vocalsound} I 'm looking {vocalsound} for data data - driven {pause} methods that can help me find , {vocalsound} um , a set of intermediate categories {vocalsound} of speech that , uh , will help me to discriminate {pause} later down the line . And one of the ideas , {vocalsound} um , that was to take a {disfmarker} take a neural net {disfmarker} train {disfmarker} train an ordinary neural net {vocalsound} to {disfmarker} {vocalsound} uh , to learn the posterior probabilities of phones . And so , um , at the end of the day you have this neural net and it has hidden {disfmarker} {vocalsound} hidden units . And each of these hidden units is {disfmarker} {vocalsound}" />
    <node id="1. Possibilities: If Speaker A (referred to as &quot;he&quot; in the transcript) comes up with a useful contribution, such as a better noise estimation module, he could potentially share it with the team (PhD D and Professor B). They could then integrate this new module into their existing system, if they determine that it would be beneficial.&#10;2. Potential outcomes: The integration of a better noise estimation module could lead to improved performance in detecting and handling noise in their system. This improvement might be significant enough to merit shipping or implementing it. Additionally, Speaker A's contribution could encourage further collaboration and innovation within the team, as they explore new ideas and possibilities for enhancing their system." />
    <node id=" now over this next {pause} period ,&#10;Speaker: PhD D&#10;Content: Uh , I dunno .&#10;Speaker: Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I don't have feedback from him , but&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: I guess he 's gonna , maybe {disfmarker}&#10;Speaker: Professor B&#10;Content: Well , he 's got it anyway , so he can .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So potentially if he came up with something that was useful , like a diff a better noise estimation module or something , he could ship it to you guys u up there&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: and&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: we could put it in .&#10;Speaker: PhD D&#10;Content: Mm - hmm . {vocalsound} Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . Yeah . So" />
    <node id=" detection .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Working on the second stream . Of course we have ideas on this also , but {disfmarker} {vocalsound} w we need to try different things and {disfmarker} Uh , but their noise estimation , um {disfmarker} {vocalsound} uh {disfmarker}&#10;Speaker: Professor B&#10;Content: I mean , back on the second stream , I mean , that 's something we 've talked about for a while . I mean , I think {nonvocalsound} that 's certainly a high hope .&#10;Speaker: PhD D&#10;Content: Yeah . {vocalsound} Mmm .&#10;Speaker: Professor B&#10;Content: Um , so we have this {disfmarker} this default idea about just using some sort of purely spectral thing ?&#10;Speaker: PhD D&#10;Content: Uh , yeah .&#10;Speaker: Professor B&#10;Content: for a second stream ?&#10;Speaker: PhD D&#10;Content: But , um , we {disfmarker} we did a first try with this , and it" />
    <node id=" Professor B&#10;Content: So he just has it all sitting there . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah . Um {disfmarker} Mm - hmm .&#10;Speaker: Professor B&#10;Content: So if he 'll {disfmarker} he might work on improving the noise estimate or on {vocalsound} some histogram things , or {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . I just saw the Eurospeech {disfmarker} We {disfmarker} we didn't talk about it at our meeting but I just saw the {disfmarker} just read the paper . Someone , I forget the name , {comment} and {disfmarker} and Ney , uh , about histogram equalization ? Did you see that one ?&#10;Speaker: PhD D&#10;Content: Um , it was a poster . Or {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . I mean , I just read the paper .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content:" />
    <node id=" D&#10;Content: Um {disfmarker}&#10;Speaker: Professor B&#10;Content: That might be the key , actually .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Cuz you were really thinking about speech versus nonspeech for that .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: That 's a good point .&#10;Speaker: PhD D&#10;Content: Mmm . Uh . Well , there are other things that {vocalsound} we should do but , {vocalsound} um , {vocalsound} it requires time and {disfmarker} {vocalsound} We have ideas , like {disfmarker} so , these things are like hav having a better VAD . Uh , we have some ideas about that . It would {disfmarker} {vocalsound} probably implies working a little bit on features that are more {vocalsound} suited to a voice activity detection .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Working on the second stream ." />
    <node id="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." />
    <node id=" I agree . Yeah . There 's the problem .&#10;Speaker: PhD D&#10;Content: So , that 's {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . Then it would work . But {disfmarker} but , I mean , i it 's {disfmarker} {vocalsound} With finite {pause} of those things {disfmarker} I mean , uh , we {disfmarker} we have done experiments where we literally have put waveforms in and {disfmarker} and {disfmarker} and , uh ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: we kept the number of parameters the same and so forth , and it used a lot of training data . And it {disfmarker} and it {disfmarker} it , uh {disfmarker} not infinite but a lot , and then compared to the number parameters {disfmarker} and it {disfmarker} it , uh {disfmarker} it just doesn't do nearly as well . So , anyway the point is that you want to suppress" />
    <node id=": Professor B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Mmm . Uh . Well , it looks strange , but {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . But {disfmarker}&#10;Speaker: PhD D&#10;Content: but it&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Maybe it 's {disfmarker} has something to do to {vocalsound} the fact that {vocalsound} we don't have infinite training data and {disfmarker}&#10;Speaker: Professor B&#10;Content: We don't ?&#10;Speaker: PhD D&#10;Content: Well ! And so {disfmarker} Well , things are not optimal&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: and {disfmarker} Mmm {disfmarker}&#10;Speaker: Grad E&#10;Content: Are you {disfmarker} you were going to say why {disfmarker} what made you {disfmarker} wh what led you to do that .&#10;Speaker: PhD D&#10;Content: Yeah ." />
    <node id="The important patterns that hidden units in a neural network learn for discriminating between phone classes are patterns related to phone classes or potentially intermediate categories. These patterns can be examined to gain insights into the internal workings of the neural network and possibly identify intermediate categories. However, it is not explicitly stated if this method can definitively lead to discovering new intermediate categories in the given transcripts. Researchers are exploring various data-driven methods to find a set of intermediate categories that can help improve speech recognition, including using hidden patterns learned by neural networks." />
    <node id=" {disfmarker} {vocalsound} hidden units . And each of these hidden units is {disfmarker} {vocalsound} um , is learning some sort of pattern . And so , um , what {disfmarker} what are these patterns ?&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: I don't know . Um , and I 'm gonna to try to {disfmarker} {vocalsound} to look at those patterns {vocalsound} to {disfmarker} to see , {vocalsound} um , {vocalsound} from those patterns {disfmarker} uh , presumably those are important patterns for discriminating between phone classes . And maybe {disfmarker} {vocalsound} maybe some , uh , intermediate categories can come from {vocalsound} just looking at the patterns of {disfmarker} {vocalsound} um , that the neural net learns .&#10;Speaker: Professor B&#10;Content: Be - before you get on the next part l let me just point out that s there 's {disfmarker} there 's a {disfmarker}" />
    <node id=" PhD D&#10;Content: Yeah . So there is the tandem network that e e e estimates the phone probabilities&#10;Speaker: Professor B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD D&#10;Content: and the silence probabilities also .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: And {vocalsound} things get better when , instead of using the silence probability computed by the tandem network , we use the silence probability , uh , given by the VAD network ,&#10;Speaker: Professor B&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: um ,&#10;Speaker: Professor B&#10;Content: The VAD network is {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Which is smaller , but maybe , um {disfmarker} So we have a network for the VAD which has one hundred hidden units , and the tandem network has five hundred . Um . So it 's smaller but th the silence probability {pause} from this network seems , uh , better .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Mmm . Uh . Well , it looks strange , but {disf" />
    <node id=" .&#10;Speaker: Grad C&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: Mm - hmm . Um . Yeah , so {disfmarker} so that 's the {disfmarker} that 's the first part {disfmarker} uh , one {disfmarker} one of the ideas to get at some {disfmarker} {vocalsound} some patterns of intermediate categories . Um , {vocalsound} the other one {pause} was , {vocalsound} um , to , {vocalsound} uh , come up with a {disfmarker} a {disfmarker} a model {disfmarker} {comment} um , a graphical model , {vocalsound} that treats {pause} the intermediate categories {vocalsound} as hidden {disfmarker} hidden variables , latent variables , that we don't know anything about , but that through , {vocalsound} um , s statistical training and the EM algorithm , {vocalsound} um , at the end of the day , {vocalsound} we have , um {disfmarker} we have learned something about these {d" />
    <node id="marker} it , uh {disfmarker} it just doesn't do nearly as well . So , anyway the point is that you want to suppress {disfmarker}&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: it 's not just having the maximum information , you want to suppress , {vocalsound} uh , the aspects of the input signal that are not helpful for {disfmarker} for the discrimination you 're trying to make . So . So maybe just briefly , uh {disfmarker}&#10;Speaker: Grad E&#10;Content: Well , that sort of segues into {pause} what {disfmarker} what I 'm doing .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Um , {vocalsound} so , uh , the big picture is k um , {vocalsound} come up with a set of , {vocalsound} uh , intermediate categories , then build intermediate category classifiers , then do recognition , and , um , improve speech recognition in that way . Um , so right now I 'm in {disfmarker}" />
    <node id="vocalsound} and each of the hidden units is learning some sort of {disfmarker} some sort of {disfmarker} some sort of pattern . Right ? And it could be , like {disfmarker} {vocalsound} like these , um {disfmarker} these auditory patterns that Michael {pause} is looking at . And then when you 're looking at the {disfmarker} {vocalsound} the , uh , {pause} um , {vocalsound} the best features , {vocalsound} you know , you can take out {disfmarker} you can do the {disfmarker} do this , uh , brain surgery by taking out , {vocalsound} um , hidden units that don't really help at all .&#10;Speaker: Professor B&#10;Content: Mm - hmm . Or the {disfmarker} or features .&#10;Speaker: Grad E&#10;Content: And this is k sorta like {disfmarker}&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean , y actually , you" />
    <node id="The computational system computes features based on Gabor functions, which are analysis functions with a certain extent in time and frequency. These functions are used to sample the signal in a time-frequency representation, allowing for multi-scale analysis. By optimizing the parameters of these basis functions, the system can extract different possible features that may be relevant for the specific task it is performing. This process involves analyzing the input signal using wavelet basis functions and Gabor functions, which sample the time-frequency plane to capture patterns in the frequency content of the signal. The resulting features can help the neural network learn and generalize from the data more effectively, potentially leading to better performance compared to using raw waveform data." />
    <node id=" It 's , um {disfmarker} {vocalsound} {vocalsound} it computes features which are , {vocalsound} um , {vocalsound} based on {disfmarker} sort of like based on diffe different w um , wavelet basis functions {vocalsound} used to analyze {vocalsound} the input .  So th he uses analysis functions called {vocalsound} Gabor functions , um , {vocalsound} which have a certain {vocalsound} extent , um , {vocalsound} in time and in frequency . And {vocalsound} the idea is these are used to sample , {vocalsound} um , the signal in a represented as a time - frequency representation . So you 're {pause} sampling some piece of this time - frequency plane . And , um , {vocalsound} that , {vocalsound} um , is {disfmarker} is interesting , cuz , {vocalsound} @ @ for {disfmarker} for one thing , you could use it , {vocalsound} um , in a {disfmarker} a multi - scale way . You could have" />
    <node id="&#10;Content: Right ?&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean , y actually , you make me think a {disfmarker} a very important point here is that , um , {vocalsound} if we a again try to look at how is this different from what we 're already doing , {vocalsound} uh , there 's a {disfmarker} a , uh {disfmarker} {vocalsound} a nasty argument that could be made th that it 's {disfmarker} it 's not different at {disfmarker} at all , because , uh {disfmarker} if you ignore the {disfmarker} the selection part because we are going into a {disfmarker} a very powerful , {vocalsound} uh , nonlinearity that , uh , in fact is combining over time and frequency , and is coming up with its own {disfmarker} you know , better than Gabor functions its , you know , neural net functions ,&#10;Speaker: Grad E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content:" />
    <node id=" one thing , you could use it , {vocalsound} um , in a {disfmarker} a multi - scale way . You could have these {disfmarker} instead of having everything {disfmarker} like we use a twenty - five millisecond or so analysis window , {vocalsound} typically , um , and that 's our time scale for features , but you could {disfmarker} {vocalsound} using this , um , basis function idea , you could have some basis functions which have a lot longer time scale and , um , some which have a lot shorter , and {vocalsound} so it would be like {pause} a set of multi - scale features . So he 's interested in , um {disfmarker} Th - this is {disfmarker} because it 's , um {disfmarker} there are these different parameters for the shape of these {vocalsound} basis functions , {vocalsound} um {disfmarker} {vocalsound} there are a lot of different possible basis functions . And so he {disfmarker} {vocalsound} he actually does {vocalsound} an optimization" />
    <node id="1. The Aurora group had a morning conference call to discuss and make a decision about a latency problem. The call lasted for around two hours, and they decided to reduce the current latency of 230 to 220, which is an eighty-millisecond reduction.&#10;2. They discussed the use of noise in the system's training data, which might be similar to the noises used for testing. This commonality could potentially lead to controversy, as some participants might argue that it provides an unfair advantage during testing. Suggestions were made to replace the current noise with different noise samples or add other noise types to maintain a fair testing environment.&#10;3. There was no clear information about the Wall Street Journal part of the test, such as how it would be run or if there would be any specific arrangements for running the systems at certain sites. Some participants mentioned that some sites might not have sufficient compute resources for running the Wall Street Journal tests, and discussions took place regarding having Mississippi State handle the testing for those sites instead.&#10;4. A LVCSR (Large Vocabulary Continuous Speech Recognition) system was downloaded from Mississippi State, along with their software and scripts to make it easier to run on Wall Street Journal data. The whole system description was obtained, though there was uncertainty regarding whether it's an open-source or proprietary system, as well as how its features are combined (Gaussians, clustering, etc.).&#10;5. There is no concrete timeline for when the revised system with a 220 latency should be available; it is expected to be ready in approximately six weeks." />
    <node id="aker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: That 's good .&#10;Speaker: PhD D&#10;Content: Um , Yeah . Well , {vocalsound} maybe we can start with this . Mmm .&#10;Speaker: Professor B&#10;Content: All today , huh ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: Um . Yeah . So there was this conference call this morning , um , and the only topic on the agenda was just to discuss a and to come at {disfmarker} uh , to get a decision about this latency problem .&#10;Speaker: Professor B&#10;Content: No , this {disfmarker} I 'm sorry , this is a conference call between different Aurora people or just {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Uh , yeah . It 's the conference call between the Aurora , {vocalsound} uh , group .&#10;Speaker: Professor B&#10;Content: It 's the main conference call . OK .&#10;Speaker: PhD D&#10;Content: Uh , yeah . There were like" />
    <node id="s not actually usable in a commercial system with a full telephone number or something .&#10;Speaker: PhD D&#10;Content: Uh - huh . Yeah . At these noise levels .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: Well , yeah . These numbers , I mean . Mmm .&#10;Speaker: Professor B&#10;Content: Good . Um , while we 're still on Aurora stuff {pause} maybe you can talk a little about the status with the , uh , {vocalsound} Wall Street Journal {vocalsound} things for it .&#10;Speaker: PhD A&#10;Content: So I 've , um , downloaded , uh , a couple of things from Mississippi State . Um , one is their {vocalsound} software {disfmarker} their , uh , LVCSR system . Downloaded the latest version of that . Got it compiled and everything . Um , downloaded the scripts . They wrote some scripts that sort of make it easy to run {vocalsound} the system on the Wall Street Journal , uh , data . Um" />
    <node id=" share across everything ? Or {disfmarker} {vocalsound} or if it 's {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Yeah , th I have {disfmarker} I {disfmarker} I {disfmarker} I don't have it up here but I have a {disfmarker} {pause} the whole system description , that describes exactly what their {pause} system is&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: and I {disfmarker} I 'm not sure . But , um {disfmarker}&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: It 's some kind of a mixture of Gaussians and , {vocalsound} uh , clustering and , uh {disfmarker} They 're {disfmarker} they 're trying to put in sort of all of the standard features that people use nowadays .&#10;Speaker: Grad E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So the other , uh , Aurora thing maybe is {d" />
    <node id=": Professor B&#10;Content: It 's the main conference call . OK .&#10;Speaker: PhD D&#10;Content: Uh , yeah . There were like two hours of {pause} discussions , and then suddenly , {vocalsound} uh , people were tired , I guess , and they decided on {nonvocalsound} a number , two hundred and twenty , um , included e including everything . Uh , it means that it 's like eighty milliseconds {pause} less than before .&#10;Speaker: Professor B&#10;Content: And what are we sitting at currently ?&#10;Speaker: PhD D&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: So , currently d uh , we have system that has two hundred and thirty . So , that 's fine .&#10;Speaker: Professor B&#10;Content: Two thirty .&#10;Speaker: PhD D&#10;Content: Yeah . So that 's the system that 's described on the second point of {pause} this {vocalsound} document .&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker} we have to reduce it by ten milliseconds somehow .&#10;Speaker: PhD D" />
    <node id=" will be in six weeks or something . So . Is that about right {pause} you think ?&#10;Speaker: PhD D&#10;Content: Uh , we don't know yet , I {disfmarker} I think .&#10;Speaker: Professor B&#10;Content: Really , we don't know ?&#10;Speaker: PhD D&#10;Content: Uh - huh . Um .&#10;Speaker: PhD A&#10;Content: It wasn't on the conference call this morning ?&#10;Speaker: Professor B&#10;Content: Hmm .&#10;Speaker: PhD D&#10;Content: No .&#10;Speaker: PhD A&#10;Content: Hmm . Did they say anything on the conference call {pause} about , um , how the {pause} Wall Street Journal part of the test was going to be {pause} run ? Because I {disfmarker} I thought I remembered hearing that some sites {vocalsound} were saying that they didn't have the compute to be able to run the Wall Street Journal stuff at their place ,&#10;Speaker: PhD D&#10;Content: No . Mmm .&#10;Speaker: PhD A&#10;Content: so there was some talk about having Mississippi State run {pause} the systems for them . And I {disf" />
    <node id="disfmarker} uh , the training data for the t tandem network , right now , is like {disfmarker} i is using the noises from the Aurora task and {vocalsound} {vocalsound} I think that people might , {vocalsound} um , try to argue about that because {vocalsound} then in some cases we have the same noises in {disfmarker} for training the network {pause} than the noises that are used for testing ,&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: and {disfmarker} So we have t n uh , to try to get rid of these {disfmarker} {vocalsound} this problem .&#10;Speaker: Professor B&#10;Content: Yeah . Maybe you just put in some other noise , something that 's different .&#10;Speaker: PhD D&#10;Content: Mm - hmm . {vocalsound} Yeah .&#10;Speaker: Professor B&#10;Content: I mean , it {disfmarker} it 's probably helpful to have {disfmarker} have a little noise there . But it may be something else&#10;Speaker: PhD D" />
    <node id="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance." />
    <node id="&#10;Speaker: PhD A&#10;Content: Well , see , I was a little confused because on this table , I 'm {disfmarker} the they 're showing word error rate . But on this one , I {disfmarker} I don't know if these are word error rates because they 're really big . So , {vocalsound} under condition one here it 's ten percent . Then under three it goes to sixty - four point six percent .&#10;Speaker: Professor B&#10;Content: Yeah , that 's probably Aurora .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean {disfmarker}&#10;Speaker: PhD A&#10;Content: So m I guess maybe they 're error rates but they 're , uh {disfmarker} they 're really high .&#10;Speaker: Professor B&#10;Content: I {disfmarker} I {disfmarker} I don't find that surpri&#10;Speaker: PhD A&#10;Content: So {disfmarker}&#10;Speaker: Professor B&#10;Content: I mean , we {disfmarker} W what 's {disfmarker" />
    <node id="1. The histograms in the paper are related to different frequency bins or bands.&#10;2. Based on the conversation, it is suggested that there is &quot;one histogram per frequency bin&quot; or &quot;one per critical band/frequency band.&quot; However, the exact number of histograms per frequency bin or band is not explicitly mentioned in the transcript provided. The speakers are discussing and trying to recall details from the paper, but they do not come to a clear conclusion.&#10;&#10;To summarize, there are multiple histograms related to different frequency bins or bands, with potentially one histogram per frequency bin or band, but the exact number is not specified in the transcript." />
    <node id=" D&#10;Content: I think they have , yeah , different histograms . I uh {disfmarker} Something like one per {pause} frequency band ,&#10;Speaker: Professor B&#10;Content: One {disfmarker}&#10;Speaker: PhD A&#10;Content: So , one histogram per frequency bin .&#10;Speaker: Professor B&#10;Content: One per critical {disfmarker}&#10;Speaker: PhD D&#10;Content: or {disfmarker} But I did {disfmarker} Yeah , I guess .&#10;Speaker: PhD A&#10;Content: And that 's {disfmarker}&#10;Speaker: PhD D&#10;Content: But I should read the paper . I just went {pause} through the poster quickly ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So th&#10;Speaker: Professor B&#10;Content: And I don't remember whether it was {pause} filter bank things&#10;Speaker: PhD A&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: and I didn't {disfmarker}&#10;Speaker: Professor B&#10;Content: or whether it was FFT bins&#10;Speaker:" />
    <node id="} adjusting it for {disfmarker} for a lot of different levels . And then they have s they have some kind of , {vocalsound} uh , {pause} a floor or something ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: so if it gets too low you don't {disfmarker} don't do it .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: And they {disfmarker} they claimed very nice results ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So is this a histogram across different frequency bins ?&#10;Speaker: Professor B&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Or {disfmarker} ?&#10;Speaker: Professor B&#10;Content: Um , I think this i You know , I don't remember that . Do you remember {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I think they have , yeah , different histograms . I uh {disfmarker} Something like one per {pause} frequency band" />
    <node id="Content: and I didn't {disfmarker}&#10;Speaker: Professor B&#10;Content: or whether it was FFT bins&#10;Speaker: PhD A&#10;Content: Huh .&#10;Speaker: Professor B&#10;Content: or {disfmarker}&#10;Speaker: PhD A&#10;Content: And {disfmarker} and that {disfmarker} that , um , {pause} histogram represents {pause} the {pause} different energy levels that have been seen at that {pause} frequency ?&#10;Speaker: Professor B&#10;Content: I don't remember that . And how often they {disfmarker} you 've seen them . Yeah .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Yeah . And they do {disfmarker} they said that they could do it for the test {disfmarker} So you don't have to change the training . You just do a measurement over the training . And then , uh , for testing , uh , you can do it for one per utterance . Even relatively short utterances . And they claim it {disfmark" />
    <node id=" that the histogram of the new data looks like the old data .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: You do this kind of {vocalsound} piece - wise linear or , {vocalsound} uh , some kind of piece - wise approximation . They did a {disfmarker} uh one version that was piece - wise linear and another that had a power law thing between them {disfmarker} {vocalsound} between the {pause} points . And , uh , they said they s they sort of see it in a way as s for the speech case {comment} {disfmarker} as being kind of a generalization of spectral subtraction in a way , because , you know , in spectral subtraction you 're trying to {vocalsound} get rid of this excess energy . Uh , you know , it 's not supposed to be there . Uh {disfmarker} {vocalsound} and , uh , this is sort of {pause} {vocalsound} adjusting it for {disfmarker} for a lot of different levels . And then they have s they have some kind of , {vocalsound" />
    <node id="1. The decision of whether to use noise or periodic pulses in speech synthesis is determined by the nature of the sound being synthesized: unvoiced sounds are produced using noise, while voiced sounds are generated using periodic pulses.&#10;2. Yes, it is possible to detect strong periodic components for voiced sound generation in speech synthesis. This can be achieved through LPC (Linear Predictive Coding) re-synthesis, where the presence of strong periodic components indicates that a voiced sound should be used, while the absence of such components suggests an unvoiced sound, which would require noise for synthesis.&#10;&#10;In summary, analyzing the signal to detect periodic components helps determine whether to use noise or periodic pulses in speech synthesis. This information can then be utilized to create more accurate and natural-sounding synthetic speech." />
    <node id=" 're talking about .&#10;Speaker: Grad C&#10;Content: OK . So Michael Kleinschmidt , who 's a PHD student from Germany , {vocalsound} showed up this week . He 'll be here for about six months . And he 's done some work using {vocalsound} an auditory model {pause} of , um , {vocalsound} human hearing , and {pause} using that f uh , to generate speech recognition features . And {pause} he did {vocalsound} work back in Germany {vocalsound} with , um , a toy recognition system {vocalsound} using , um , isolated {vocalsound} digit recognition {vocalsound} as the task . It was actually just a single - layer neural network {vocalsound} that classified words {disfmarker} classified digits , {vocalsound} in fact . Um , and {pause} he tried that on {disfmarker} I think on some Aurora data and got results that he thought {pause} seemed respectable . And he w he 's coming here to u u use it on a {vocalsound} uh , a real speech recognition system . So I 'll be working" />
    <node id="Professionor B and PhD D are discussing the possibility that the key factor for achieving a better outcome could be the integration of multiple sources of knowledge and different features in their process. They suggest this might provide a more accurate or effective system. This idea is based on the observation that relying solely on one source, like the 'VAD' (Voice Activity Detection) method, may not yield optimal results. The potential benefits of combining various methods could lead to improved performance and further collaboration within their team." />
    <node id="aker: Professor B&#10;Content: And this is {disfmarker} and rather than just taking one minus that to get the other , which is essentially what 's happening , you have this other source of knowledge that you 're putting in there . So you make use of both of them {vocalsound} in {disfmarker} in {pause} what you 're ending up with . Maybe it 's better .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Anyway , you can probably justify anything if what 's use&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: And {disfmarker} and the features are different also . I mean , the VAD doesn't use the same features there are .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Oh !&#10;Speaker: PhD D&#10;Content: Um {disfmarker}&#10;Speaker: Professor B&#10;Content: That might be the key , actually .&#10;Speaker:" />
    <node id="Speaker: PhD A&#10;Content: That 's ri&#10;Speaker: Professor B&#10;Content: We do the digits and then we get our treats .&#10;Speaker: Grad E&#10;Content: Oops .&#10;Speaker: PhD A&#10;Content: OK ." />
    <node id="1. The speakers are discussing the potential benefits of using log probabilities in their models, as they have a more Gaussian shape than regular probabilities, making them easier to model. They consider whether adjusting the word insertion penalty could have a positive effect on the system's performance.&#10;2. The speakers seem uncertain about whether they can change the word insertion penalty based on the lack of a &quot;ruling&quot; or clear decision. They express concerns about not knowing what the impact would be at the &quot;other end&quot; due to the complexity of their methods.&#10;&#10;In summary, the speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty given their current situation." />
    <node id="disfmarker} I mean , they 're not used as probabilities , but the log probabilities {disfmarker} we 're taking advantage of the fact that something like log probabilities has more of a Gaussian shape than Gaus - than {vocalsound} probabilities , and so we can model them better . So , {pause} in a way we 're taking advantage of the fact that they 're probabilities , because they 're this quantity that looks kind of Gaussian when you take it 's log . So , {comment} {vocalsound} uh , maybe {disfmarker} maybe it would have a {disfmarker} a reasonable effect to do that .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: I d I don't know . But , {pause} I mean , I guess we still haven't had a {disfmarker} {vocalsound} a ruling back on this . And we may end up being in a situation where we just you know really can't change the {vocalsound} word insertion penalty . But the other thing we could do {vocalsound} is {disfmarker}" />
    <node id=" into the rest of it and then that 's used as observations . So it 's {disfmarker} it 's , {vocalsound} um , another way to do it .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm . But , these values are not directly used as probabilities anyway .&#10;Speaker: Professor B&#10;Content: I know they 're not .&#10;Speaker: PhD D&#10;Content: So there are {disfmarker} there is {disfmarker}&#10;Speaker: Professor B&#10;Content: I know they 're not . But {disfmarker} but , you know {disfmarker} So because what we 're doing is pretty strange and complicated , we don't really know what the effect is {pause} at the other end .&#10;Speaker: PhD D&#10;Content: Uh - huh . Mm - hmm .&#10;Speaker: Professor B&#10;Content: So , {vocalsound} um , {pause} my thought was maybe {disfmarker} I mean , they 're not used as probabilities , but the log probabilities {disfmarker} we 're taking advantage of" />
    <node id="The purpose of adjusting a new set of data to resemble the distribution of a previous set, as discussed in the context of creating a histogram of amplitudes or levels of brightness, is to improve the performance of a model or system. This technique is applied in image processing and can be useful for speech processing as well.&#10;&#10;The method involves comparing the histograms of the new and old data. If the new data's histogram does not match the old data's histogram, adjustments are made to the new data so that its histogram becomes more similar to the old data's histogram. This process can be done piece-wise linearly or through some other kind of approximation. The speakers in the conversation mention a piece-wise linear approach and an approach using a power law between points.&#10;&#10;The underlying idea is that by making the new data's distribution (histogram) resemble the old data's distribution, the system can perform better, as demonstrated in the example where a human subject (Stephane) achieved a lower error rate than the system. However, it is essential to note that simply increasing the amount of training data may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." />
    <node id="isfmarker} you have the distributions from the training set ,&#10;Speaker: PhD D&#10;Content: N&#10;Speaker: Professor B&#10;Content: and then , uh {disfmarker} So this is just a {disfmarker} a histogram of {disfmarker} of {vocalsound} the amplitudes , I guess . Right ? And then {disfmarker} {vocalsound} Um , people do this in image processing some .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: You have this kind of {disfmarker} {vocalsound} of histogram of {disfmarker} of levels of brightness or whatever . And {disfmarker} and {disfmarker} and then , {vocalsound} when you get a new {disfmarker} new thing that you {disfmarker} you want to adjust to be {pause} better in some way , {vocalsound} you adjust it so that the histogram of the new data looks like the old data .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;" />
    <node id="1. The conclusion that the system's performance is close to human performance cannot be drawn based on the LPC-12 method, as pointed out by PhD D. This is because the degradation from LPC-12 synthesis may introduce issues not typically found in human speech due to its approximation method. Furthermore, excitation, which plays a significant role in what the system hears, is not taken into account in the LPC-12 system, making it difficult to accurately assess the system's performance compared to human performance." />
    <node id="Based on the supporting materials provided, making an accurate assessment of a system's performance compared to human performance using the LPC-12 method has limitations. The degradation from LPC-12 synthesis and the absence of excitation consideration in this method can introduce issues that are not typically found in human speech, making it challenging to draw definitive conclusions about the system's accuracy. Therefore, any comparison to the human-recreated LPC twelve spectrum should be approached with caution, as it may not provide a complete or accurate representation of the system's performance relative to human performance." />
    <node id="1. Doctor D is planning to go to OGI (Oregon Health &amp; Science University) not next week, but maybe the week after that.&#10;2. Yes, both Birger and Michael Kleinschmidt (Michael is referred to as just Michael in the transcript) are planning to join the meeting two weeks from the date of the current meeting." />
    <node id="Speaker: PhD A&#10;Content: Eh , we should be going .&#10;Speaker: Professor B&#10;Content: So ne next week we 'll have , uh , both Birger {pause} and , uh , Mike {disfmarker} Michael {disfmarker} Michael Kleinschmidt and Birger Kollmeier will join us .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Um , and you 're {disfmarker} {vocalsound} you 're probably gonna go up in a couple {disfmarker} three weeks or so ? When d when are you thinking of going up to , uh , OGI ?&#10;Speaker: PhD D&#10;Content: Yeah , like , uh , not next week but maybe the week after .&#10;Speaker: Professor B&#10;Content: OK . Good . So at least we 'll have one meeting with {vocalsound} yo with you still around , and {disfmarker} and {disfmarker}&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: That 's good .&#10;Speaker: PhD D&#10;" />
    <node id=": Mm - hmm . {vocalsound} Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . Yeah . So , that 's good . So , why don't we just , uh , um {disfmarker} I think starting {disfmarker} {pause} starting a w couple weeks from now , especially if you 're not gonna be around for a while , we 'll {disfmarker} we 'll be shifting more over to some other {disfmarker} {vocalsound} other territory . But , uh , uh , {comment} uh , n not {disfmarker} not so much in this meeting about Aurora , but {disfmarker} but , uh , uh , maybe just , uh , quickly today about {disfmarker} maybe you could just say a little bit about what you 've been talking about with Michael . And {disfmarker} and then Barry can say something about {pause} what {comment} {disfmarker} what we 're talking about .&#10;Speaker: Grad C&#10;Content: OK . So Michael Kleinschmidt , who 's a PHD student from Germany" />
    <node id="1. The visualization being discussed here is a histogram, which is a type of bar graph used to display the distribution of data by showing the number of data points that fall within specified ranges or &quot;bins.&quot; In this context, the histograms are related to different frequency bins or bands in the field of signal processing.&#10;2. The exact type of histogram (single or multiple) is not explicitly stated in the transcript provided, but there are suggestions that it could be either one histogram per frequency bin or band or multiple separate histograms for different frequency bands. This information is based on the conversation between the participants, who are trying to recall details from the paper they are discussing." />
    <node id="1. Alternative Approach: Professor B suggests using a two-source system, where one source is noise and the other is periodic pulses, as an alternative to using a voiced &quot;voice thing&quot; when dealing with strong periodic components. This approach allows for more accurate representation of various sound types in speech synthesis.&#10;2. Preference for Two-Source System: Professor B prefers a two-source system over a one-source system because it offers greater flexibility in handling different types of sounds, such as unvoiced and voiced sounds. By utilizing both noise and periodic pulses, the synthetic speech can more closely mimic natural human speech, resulting in improved quality and naturalness." />
    <node id="disfmarker} s strong periodic components , then you can use a voiced {disfmarker} voice thing .&#10;Speaker: PhD D&#10;Content: Oh . Uh - huh . Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . I mean , it 's probably not worth your time . It 's {disfmarker} it 's a side thing and {disfmarker} and {disfmarker} and there 's a lot to do .&#10;Speaker: PhD D&#10;Content: Uh - huh , yeah .&#10;Speaker: Professor B&#10;Content: But I 'm {disfmarker} I 'm just saying , at least as a thought experiment , {vocalsound} that 's what I would wanna test .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Uh , I wan would wanna drive it with a {disfmarker} a {disfmarker} a two - source system rather than a {disfmarker} than a one - source system .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm .&#10;Spe" />
    <node id="er} than a one - source system .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: And then that would tell you whether in fact it 's {disfmarker} Cuz we 've talked about , like , this harmonic tunneling or {vocalsound} other things that people have done based on pitch , maybe that 's really a key element . Maybe {disfmarker} maybe , uh , {vocalsound} uh , without that , it 's {disfmarker} it 's not possible to do a whole lot better than we 're doing . That {disfmarker} that could be .&#10;Speaker: PhD D&#10;Content: Yeah . That 's what I was thinking by doing this es experiment ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: like {disfmarker} Mmm . {vocalsound} Evi&#10;Speaker: Professor B&#10;Content: But , I mean , other than that , I don't think it 's {disfmarker} I mean , other than the pitch de information ," />
    <node id="The optimization procedure being used here is called &quot;recursive feature elimination&quot; or a variation of it. The speaker starts with a set of candidate basis functions and classifies using each possible subset of this set, which is M possible subsets for a set of M candidates. The feature that doesn't contribute to the classification performance is considered useless and eliminated. This process is repeated by randomly selecting another feature from the remaining pool of possible basis functions until an optimal set is obtained." />
    <node id=" a lot of different possible basis functions . And so he {disfmarker} {vocalsound} he actually does {vocalsound} an optimization procedure to choose an {disfmarker} {vocalsound} an optimal set of basis functions out of all the possible ones .&#10;Speaker: PhD A&#10;Content: Hmm . H What does he do to choose those ?&#10;Speaker: Grad C&#10;Content: The method he uses is kind of funny {disfmarker} is , {comment} {vocalsound} um , {vocalsound} he starts with {disfmarker} he has a set of M of them . Um , he {disfmarker} and then {pause} he uses that to classify {disfmarker} I mean , he t he tries , um , {vocalsound} using {pause} just M minus one of them . So there are M possible subsets of this {vocalsound} length - M vector . He tries classifying , using each of the M {vocalsound} possible sub - vectors .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Grad C&#10;Content: Whichever sub - vector , {voc" />
    <node id=" sub - vectors .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Grad C&#10;Content: Whichever sub - vector , {vocalsound} um , works the {disfmarker} the best , I guess , he says {disfmarker} {vocalsound} the {disfmarker} the fe feature that didn't use was the most useless feature ,&#10;Speaker: Professor B&#10;Content: Y yeah . Gets thrown out . Yeah .&#10;Speaker: Grad C&#10;Content: so we 'll throw it out and we 're gonna randomly select another feature {pause} from the set of possible basis functions .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So it 's a {disfmarker}&#10;Speaker: Professor B&#10;Content: So i so it 's actuall&#10;Speaker: PhD A&#10;Content: it 's a little bit like a genetic algorithm or something in a way .&#10;Speaker: Professor B&#10;Content: Well , it 's {disfmarker} it 's much simpler .&#10;Speaker: Grad E&#10;Content" />
    <node id="Content: Uh , no , I don't think so .&#10;Speaker: Professor B&#10;Content: No .&#10;Speaker: PhD D&#10;Content: No .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: It 's still {disfmarker} in terms of computation , if we use , like , their way of computing the {disfmarker} the maps {disfmarker} the {disfmarker} the MIPs , {vocalsound} I think it fits ,&#10;Speaker: Professor B&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD D&#10;Content: but it 's , uh , m mainly a problem of memory .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: Um , and I don't know how much {pause} this can be discussed or not , because it 's {disfmarker} it could be in ROM , so it 's maybe not that expensive . But {disfmarker}&#10;Speaker: Professor B&#10;Content: Ho - how much memory d ? H how many {disfmarker}" />
    <node id="1. The re-synthesized version of speech filtered by an LPC (Linear Predictive Coding) filter may not be immediately recognizable as speech because it is generated using a white noise source that has been filtered, rather than a periodic sound source. This can result in a less natural sounding &quot;whispering&quot; quality to the synthesized speech.&#10;2. The LPC re-synthesis method introduces degradation not typically found in human speech due to its approximation technique, which further complicates the assessment of the system's performance compared to human performance.&#10;3. Excitation, a crucial factor in what the system hears, is not taken into account by the LPC-12 system. This makes it challenging to determine if the system's performance is close to human performance." />
    <node id="1. Yes, there was discussion about having Mississippi State run the systems for some sites that might not have sufficient compute resources for running the Wall Street Journal tests. However, this was not decided during today's meeting and is still under consideration.&#10;2. Based on the conversation, it seems that updates about the large-vocabulary task were not shared in the most recent emails, as one of the participants mentioned not having read them and being unsure if the system from Mississippi State is open-source or proprietary." />
    <node id="&#10;Speaker: PhD A&#10;Content: so there was some talk about having Mississippi State run {pause} the systems for them . And I {disfmarker} Did {disfmarker} did that come up at all ?&#10;Speaker: PhD D&#10;Content: Uh , no . Well , this {disfmarker} first , this was not the point at all of this {disfmarker} the meeting today&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD D&#10;Content: and ,&#10;Speaker: Professor B&#10;Content: Some&#10;Speaker: PhD D&#10;Content: uh , frankly , I don't know because I d {comment} didn't read also the {pause} most recent mails about {vocalsound} the large - vocabulary task . But , {vocalsound} uh , did you {disfmarker} do you still , uh , get the mails ? You 're not on the mailing list or what ?&#10;Speaker: PhD A&#10;Content: Hmm - mm . The only , um , mail I get is from Mississippi State {disfmarker}&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;" />
    <node id=" The only , um , mail I get is from Mississippi State {disfmarker}&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: so {disfmarker}&#10;Speaker: PhD D&#10;Content: Oh , yeah . So we should have a look at this .&#10;Speaker: PhD A&#10;Content: about their system . I {disfmarker} I don't get any {pause} mail about {disfmarker}&#10;Speaker: Professor B&#10;Content: I have to say , there 's uh something funny - sounding about saying that one of these big companies doesn't have enough cup compute power do that , so they 're having to have it done by Mississippi State .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: It just {disfmarker} {vocalsound} just sounds funny .&#10;Speaker: PhD A&#10;Content: Yeah . It does .&#10;Speaker: Professor B&#10;Content: But ,&#10;Speaker: PhD A&#10;Content: Yeah . I 'm {disfmarker} I 'm wondering about that&#10;Speaker: Professor B&#10;Content: anyway" />
    <node id=": PhD A&#10;Content: I did , but Joe {pause} said , you know , &quot; what you 're saying makes sense&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: and {pause} I don't know &quot; . So he doesn't know what the answer is .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: I mean , that 's th We had this back and forth a little bit about , {vocalsound} you know , are sites gonna {disfmarker} are you gonna run this data for different sites ? And , well , if {disfmarker} if Mississippi State runs it , then maybe they 'll do a little optimization on that {pause} parameter , and , uh {disfmarker} But then he wasn't asked to run it for anybody . So i it 's {disfmarker} it 's just not clear yet what 's gonna happen .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Uh , he 's been putting this stuff out on their web site and {disf" />
    <node id="1. The different conditions that the speakers are referring to are likely the various noise conditions and mismatched conditions in which the experiment was conducted. They mention that high error rates under these difficult conditions are not surprising, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;2. It is challenging to determine whether the numbers represent accuracy or error rate because the values discussed are not clearly defined as either. When looking at the numbers, it's unclear if a higher value indicates better performance (accuracy) or worse performance (error rate). This confusion arises from the way the numbers are presented or discussed in the conversation, making it difficult for the listeners to interpret the results accurately." />
    <node id=" some of these noise condition not at all surprising .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: The baseline is sixty percent also on digits ,&#10;Speaker: PhD A&#10;Content: Oh , is it ?&#10;Speaker: PhD D&#10;Content: on the m more {pause} mismatched conditions .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: So .&#10;Speaker: PhD A&#10;Content: So , yeah , that 's probably what it is then . Yeah . So they have a lot of different conditions that they 're gonna be filling out .&#10;Speaker: Professor B&#10;Content: It 's a bad sign when you {disfmarker} looking at the numbers , you can't tell whether it 's accuracy or error rate .&#10;Speaker: PhD A&#10;Content: Yeah . Yeah . It 's {disfmarker} it 's gonna be hard . Um , they 're {disfmarker} I I 'm still waiting for them to {pause} release the , um , {vocalsound} multi - CPU version of their" />
    <node id="1. Extracting the pitch from actual speech and putting it back in might help in improving the performance of the model that hears some things better than others. This is because, as per Professor B, removing pitch information makes the audio closer to a one-source system, which could negatively impact the performance. By adding the pitch back, it might be possible to restore some of the lost information and improve the model's ability to distinguish different sound sources or speech components.&#10;2. However, there are potential challenges in implementing this idea. Professor B mentioned that synthesizing speech with added pitch information could introduce other degradations due to the approximation method used. Furthermore, it is unclear how mel-based synthesis would perform compared to the current approach. Therefore, careful experimentation and evaluation would be necessary to determine the effectiveness of this method.&#10;&#10;In summary, adding the extracted pitch back into the speech might improve the model's performance by restoring some lost information; however, it is crucial to consider potential degradations and other factors that could impact the overall outcome." />
    <node id="alsound} hear some things better than others . And so it {disfmarker} it isn't {disfmarker}&#10;Speaker: PhD A&#10;Content: But {disfmarker}&#10;Speaker: Professor B&#10;Content: But , {vocalsound} I agree that it says that , uh , the kind of information that we 're feeding it is probably , {vocalsound} um , um , a little bit , um , minimal . There 's definitely some things that we 've thrown away . And that 's why I was saying it might be interesting if you {disfmarker} {vocalsound} an interesting test of this would be if you {disfmarker} if you actually put the pitch back in . So , you just extract it from the actual speech and put it back in , and see does that {disfmarker} is that {disfmarker} does that make the difference ? If that {disfmarker} if that takes it down to one percent again , {vocalsound} then you 'd say &quot; OK , it 's {disfmarker} it 's in fact having , um , {vocalsound} not" />
    <node id="Based on the transcript, it is not explicitly stated what specific activity Professor B, Grad E, and PhD A are planning to do that involves &quot;digging for treats like rats.&quot; The phrase appears to be a metaphor or simile used by Professor B to describe their upcoming conference call or meeting. It's possible that the treats refer to some kind of reward or positive outcome associated with the meeting, while the digging for the treats may represent the effort and work required to achieve that outcome. However, without further context or information, it is difficult to determine the exact meaning or nature of this activity." />
    <node id=" um , at the end of the day , {vocalsound} we have , um {disfmarker} we have learned something about these {disfmarker} these latent , um {disfmarker} latent variables which happen to correspond to {vocalsound} intermediate categories . Um . {vocalsound} {nonvocalsound} Yeah , and so those are the {disfmarker} the two directions that I 'm {disfmarker} I 'm looking into right now . And , uh , {vocalsound} um {disfmarker} {vocalsound} {vocalsound} Yeah . I guess that 's {disfmarker} that 's it .&#10;Speaker: Professor B&#10;Content: OK . Should we do our digits and get ou get our treats ?&#10;Speaker: Grad E&#10;Content: Oh , tea time ?&#10;Speaker: Professor B&#10;Content: Yeah . It 's kind of like , you know , the little rats with the little thing dropping down to them .&#10;Speaker: PhD A&#10;Content: That 's ri&#10;Speaker: Professor B&#10;Content: We do the digits and then we get our treats" />
    <node id="Based on the transcript, the specific knob or scaling factor that should be used to change the scale of likelihoods, not observations, when determining a scaling factor in their process is not explicitly mentioned. Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose." />
    <node id=" Yeah .&#10;Speaker: PhD A&#10;Content: And then we just use that to determine some scaling factor that we use .&#10;Speaker: Professor B&#10;Content: Yeah . So I mean , I I think that that 's a reasonable thing to do and the only question is what 's the actual knob that we use ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And the knob that we use should {disfmarker} uh , uh , unfortunately , like I say , I don't know the analytic solution to this cuz what we really want to do is change the scale of the likelihoods ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: not the cha not the scale of the {disfmarker} {vocalsound} the {pause} observations . But {disfmarker} but , uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Out of curiosity , what {disfmarker} what kind of recogn" />
    <node id="1. If the pitch information is extracted from actual speech and added back into the current input of the system, it could potentially help restore some lost information, as mentioned by Professor B. This might improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate.&#10;2. However, there are potential challenges with this approach. Synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.&#10;3. It is also unclear how mel-based synthesis would perform compared to the current approach, so thorough testing would be required before any conclusions can be drawn about its effectiveness in reducing error rates." />
    <node id="1. The values being discussed in the transcript are candidate basis functions or features used for classification in an optimization procedure called recursive feature elimination (or a variation of it). In this process, they start with a set of M candidate basis functions and classify using each possible subset of this set, recursively eliminating features that do not contribute to the classification performance until an optimal set is obtained.&#10;   &#10;2. Log probabilities are being utilized in the process despite not using the values as probabilities because log probabilities have a more Gaussian shape than regular probabilities, making them easier to model and work with in complex methods like the ones being used by the speakers. They mentioned the benefits of taking advantage of their Gaussian shape and the fact that these values are not directly used as probabilities but rather their log probabilities.&#10;&#10;In summary, the values in this context refer to candidate basis functions or features used for classification, and log probabilities are being utilized for modeling purposes due to their more Gaussian shape despite not being used directly as probabilities." />
    <node id="The significant difference in word error rates being discussed by the speakers refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is concerning and raises questions about the system's performance. The high error rates might be due to the software named Aurora, as suggested by Professor B. They imply that Aurora might be contributing to the high error rates based on their experience and familiarity with its performance, but they do not provide specific reasons for this conclusion.&#10;&#10;The speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty due to the complexity of their methods and not knowing what the impact would be at the &quot;other end.&quot; The different conditions they refer to are likely various noise conditions and mismatched conditions in which the experiment was conducted. High error rates under these difficult conditions are expected, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose. The numbers discussed during the conversation do not make it clear if a higher value indicates better performance (accuracy) or worse performance (error rate), causing confusion for the listeners.&#10;&#10;Lastly, Professor B suggests that adding pitch information extracted from actual speech back into the current input of the system could potentially help restore lost information and improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate. However, synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors." />
    <edge source="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" target=" {disfmarker} i it might not sound very good .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh , and i the degradation from that might {disfmarker} might actually make it even harder , {vocalsound} uh , to understand than the LPC - twelve . So all I 'm saying is that the LPC - twelve {vocalsound} puts in {disfmarker} synthesis puts in some degradation that 's not what we 're used to hearing ,&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: and is , um {disfmarker} It 's not {disfmarker} it 's not just a question of how much information is there , as if you will always take maximum {vocalsound} advantage of any information that 's presented to you .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: In fact , you {vocalsound} hear some things better than others . And so it {disfmarker} it isn't {disfmarker}&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" target=": Professor B&#10;Content: There 's two problems there . I mean {disfmarker} I mean , so {disfmarker} so the first is {vocalsound} that by doing LPC - twelve with synthesized speech w like you 're saying , uh , it 's {disfmarker} {vocalsound} i i you 're {disfmarker} you 're adding other degradation .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Right ? So it 's not just the noise but you 're adding in fact some degradation because it 's only an approximation . Um , and the second thing is {disfmarker} which is m maybe more interesting {disfmarker} is that , um , {comment} {vocalsound} if you do it with whispered speech , you get this number . What if you had {pause} done analysis {comment} re - synthesis and taken the pitch as well ? Alright ? So now you put the pitch in .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: What would the percentage be then ?&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" target=" LPC n twelve is pretty crummy &quot; . You know ?&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: So I I I 'm not sure {disfmarker} I 'm not sure how we can conclude from this anything about {disfmarker} that our system is close to {vocalsound} the human performance .&#10;Speaker: PhD D&#10;Content: Ye Yeah . Well , the point is that eh l ey {disfmarker} the point is that , um , {vocalsound} what I {disfmarker} what I listened to when I re - synthesized the LP - the LPC - twelve {pause} spectrum {vocalsound} is in a way what the system , uh , is hearing , cuz @ @ {disfmarker} all the {disfmarker} all the , um , excitation {disfmarker} all the {disfmarker} well , the excitation is {disfmarker} is not taken into account . That 's what we do with our system . And&#10;Speaker: Professor B&#10;Content: Well , you 're not doing the L">
      <data key="d0">1</data>
    </edge>
    <edge source="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" target="disfmarker} a periodic sound and , {pause} @ @ {comment} uh , unvoiced sound , and the noise&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: which is mostly , {vocalsound} uh , noise . I mean not {pause} periodic . So , {pause} what {disfmarker} what do you mean exactly by putting back the pitch in ? Because {disfmarker}&#10;Speaker: PhD A&#10;Content: In the LPC synthesis ? I think {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . You did LPC re - synthesis {disfmarker}&#10;Speaker: PhD D&#10;Content: I&#10;Speaker: Professor B&#10;Content: L PC re - synthesis .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: So , {vocalsound} uh {disfmarker} and you did it with a noise source , rather than with {disfmarker} with a s periodic source .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor">
      <data key="d0">1</data>
    </edge>
    <edge source="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" target=": Yeah .&#10;Speaker: PhD D&#10;Content: while our system is currently at seven percent . Um , but what happens also is that if I listen to the , um {disfmarker} {nonvocalsound} a re - synthesized version of the speech and {pause} I re - synthesized this using a white noise that 's filtered by a LPC , uh , filter {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Um , well , you can argue , that , uh {disfmarker} that this is not speech ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: so the ear is not trained to recognize this . But s actually it sound like {pause} whispering , so we are {disfmarker}&#10;Speaker: Professor B&#10;Content: Well , I mean , it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: eh {disfmarker}&#10;Speaker: Professor B&#10;Content: There 's two problems there . I mean {disfmarker} I mean , so {disfmarker} so">
      <data key="d0">1</data>
    </edge>
    <edge source="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" target=" {disfmarker} with a s periodic source .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Right ? So if you actually did real re - synthesis like you do in an LPC synthesizer , where it 's unvoiced you use noise , where it 's voiced you use , {vocalsound} uh , periodic pulses .&#10;Speaker: PhD D&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD D&#10;Content: Yeah , but it 's neither {pause} purely voiced or purely unvoiced . Esp - especially because there is noise .&#10;Speaker: Professor B&#10;Content: Well , it might be hard to do it&#10;Speaker: PhD D&#10;Content: So {disfmarker}&#10;Speaker: Professor B&#10;Content: but it but {disfmarker} but the thing is that if you {disfmarker} {vocalsound} um , if you detect that there 's periodic {disfmarker} s strong periodic components , then you can use a voiced {disfmarker} voice thing .&#10;Speaker: PhD D&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" target="1. The percentage of having the pitch is significant in terms of people's perception because it can impact the clarity and intelligibility of speech. A lower pitch retention percentage, such as 1%, may result in better understanding and a more positive opinion of LPC-12, while a higher percentage, like near 5%, might lead to degradation and a less favorable view.&#10;2. If the pitch retention stayed at around 5%, it could indicate that the LPC-12 system is not performing well, and users might perceive its quality as &quot;crummy.&quot; Conversely, a lower percentage demonstrates better performance, making it closer to human performance in terms of speech clarity and intelligibility.">
      <data key="d0">1</data>
    </edge>
    <edge source="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" target="Extraction of computing features from a waveform involves processing and manipulating the original data, which can result in loss of some information. However, this process can also highlight or emphasize other important aspects or characteristics of the signal that may be relevant for the specific task a neural net is trying to perform. For example, features such as spectral coefficients or Mel-frequency cepstral coefficients (MFCCs) can capture patterns in the frequency content of speech signals that are meaningful for speech recognition. By focusing on these relevant features, a neural network might be able to better learn and generalize from the data, even if some information has been lost during the feature extraction process. In other words, while some information is lost, what remains may still be more informative and useful for the task at hand. This can lead to better performance compared to using the raw waveform, which might contain irrelevant or noisy information that could negatively impact the learning process in a neural network.">
      <data key="d0">1</data>
    </edge>
    <edge source="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" target="1. The decision of whether to use noise or periodic pulses in speech synthesis is determined by the nature of the sound being synthesized: unvoiced sounds are produced using noise, while voiced sounds are generated using periodic pulses.&#10;2. Yes, it is possible to detect strong periodic components for voiced sound generation in speech synthesis. This can be achieved through LPC (Linear Predictive Coding) re-synthesis, where the presence of strong periodic components indicates that a voiced sound should be used, while the absence of such components suggests an unvoiced sound, which would require noise for synthesis.&#10;&#10;In summary, analyzing the signal to detect periodic components helps determine whether to use noise or periodic pulses in speech synthesis. This information can then be utilized to create more accurate and natural-sounding synthetic speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" target="1. The conclusion that the system's performance is close to human performance cannot be drawn based on the LPC-12 method, as pointed out by PhD D. This is because the degradation from LPC-12 synthesis may introduce issues not typically found in human speech due to its approximation method. Furthermore, excitation, which plays a significant role in what the system hears, is not taken into account in the LPC-12 system, making it difficult to accurately assess the system's performance compared to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" target="Based on the supporting materials provided, making an accurate assessment of a system's performance compared to human performance using the LPC-12 method has limitations. The degradation from LPC-12 synthesis and the absence of excitation consideration in this method can introduce issues that are not typically found in human speech, making it challenging to draw definitive conclusions about the system's accuracy. Therefore, any comparison to the human-recreated LPC twelve spectrum should be approached with caution, as it may not provide a complete or accurate representation of the system's performance relative to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="The degradation from LPC-12 synthesis might make it harder to understand than other methods because it introduces a type of degradation that is not typically present in human speech. This could be due to the approximation method used in LPC-1" target="1. The re-synthesized version of speech filtered by an LPC (Linear Predictive Coding) filter may not be immediately recognizable as speech because it is generated using a white noise source that has been filtered, rather than a periodic sound source. This can result in a less natural sounding &quot;whispering&quot; quality to the synthesized speech.&#10;2. The LPC re-synthesis method introduces degradation not typically found in human speech due to its approximation technique, which further complicates the assessment of the system's performance compared to human performance.&#10;3. Excitation, a crucial factor in what the system hears, is not taken into account by the LPC-12 system. This makes it challenging to determine if the system's performance is close to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Professor B&#10;Content: There 's two problems there . I mean {disfmarker} I mean , so {disfmarker} so the first is {vocalsound} that by doing LPC - twelve with synthesized speech w like you 're saying , uh , it 's {disfmarker} {vocalsound} i i you 're {disfmarker} you 're adding other degradation .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Right ? So it 's not just the noise but you 're adding in fact some degradation because it 's only an approximation . Um , and the second thing is {disfmarker} which is m maybe more interesting {disfmarker} is that , um , {comment} {vocalsound} if you do it with whispered speech , you get this number . What if you had {pause} done analysis {comment} re - synthesis and taken the pitch as well ? Alright ? So now you put the pitch in .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: What would the percentage be then ?&#10;Speaker" target="1. The percentage of having the pitch is significant in terms of people's perception because it can impact the clarity and intelligibility of speech. A lower pitch retention percentage, such as 1%, may result in better understanding and a more positive opinion of LPC-12, while a higher percentage, like near 5%, might lead to degradation and a less favorable view.&#10;2. If the pitch retention stayed at around 5%, it could indicate that the LPC-12 system is not performing well, and users might perceive its quality as &quot;crummy.&quot; Conversely, a lower percentage demonstrates better performance, making it closer to human performance in terms of speech clarity and intelligibility.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Professor B&#10;Content: There 's two problems there . I mean {disfmarker} I mean , so {disfmarker} so the first is {vocalsound} that by doing LPC - twelve with synthesized speech w like you 're saying , uh , it 's {disfmarker} {vocalsound} i i you 're {disfmarker} you 're adding other degradation .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Right ? So it 's not just the noise but you 're adding in fact some degradation because it 's only an approximation . Um , and the second thing is {disfmarker} which is m maybe more interesting {disfmarker} is that , um , {comment} {vocalsound} if you do it with whispered speech , you get this number . What if you had {pause} done analysis {comment} re - synthesis and taken the pitch as well ? Alright ? So now you put the pitch in .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: What would the percentage be then ?&#10;Speaker" target="1. The decision of whether to use noise or periodic pulses in speech synthesis is determined by the nature of the sound being synthesized: unvoiced sounds are produced using noise, while voiced sounds are generated using periodic pulses.&#10;2. Yes, it is possible to detect strong periodic components for voiced sound generation in speech synthesis. This can be achieved through LPC (Linear Predictive Coding) re-synthesis, where the presence of strong periodic components indicates that a voiced sound should be used, while the absence of such components suggests an unvoiced sound, which would require noise for synthesis.&#10;&#10;In summary, analyzing the signal to detect periodic components helps determine whether to use noise or periodic pulses in speech synthesis. This information can then be utilized to create more accurate and natural-sounding synthetic speech.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Professor B&#10;Content: There 's two problems there . I mean {disfmarker} I mean , so {disfmarker} so the first is {vocalsound} that by doing LPC - twelve with synthesized speech w like you 're saying , uh , it 's {disfmarker} {vocalsound} i i you 're {disfmarker} you 're adding other degradation .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Right ? So it 's not just the noise but you 're adding in fact some degradation because it 's only an approximation . Um , and the second thing is {disfmarker} which is m maybe more interesting {disfmarker} is that , um , {comment} {vocalsound} if you do it with whispered speech , you get this number . What if you had {pause} done analysis {comment} re - synthesis and taken the pitch as well ? Alright ? So now you put the pitch in .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: What would the percentage be then ?&#10;Speaker" target="1. Alternative Approach: Professor B suggests using a two-source system, where one source is noise and the other is periodic pulses, as an alternative to using a voiced &quot;voice thing&quot; when dealing with strong periodic components. This approach allows for more accurate representation of various sound types in speech synthesis.&#10;2. Preference for Two-Source System: Professor B prefers a two-source system over a one-source system because it offers greater flexibility in handling different types of sounds, such as unvoiced and voiced sounds. By utilizing both noise and periodic pulses, the synthetic speech can more closely mimic natural human speech, resulting in improved quality and naturalness.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Professor B&#10;Content: There 's two problems there . I mean {disfmarker} I mean , so {disfmarker} so the first is {vocalsound} that by doing LPC - twelve with synthesized speech w like you 're saying , uh , it 's {disfmarker} {vocalsound} i i you 're {disfmarker} you 're adding other degradation .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Right ? So it 's not just the noise but you 're adding in fact some degradation because it 's only an approximation . Um , and the second thing is {disfmarker} which is m maybe more interesting {disfmarker} is that , um , {comment} {vocalsound} if you do it with whispered speech , you get this number . What if you had {pause} done analysis {comment} re - synthesis and taken the pitch as well ? Alright ? So now you put the pitch in .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: What would the percentage be then ?&#10;Speaker" target="1. The re-synthesized version of speech filtered by an LPC (Linear Predictive Coding) filter may not be immediately recognizable as speech because it is generated using a white noise source that has been filtered, rather than a periodic sound source. This can result in a less natural sounding &quot;whispering&quot; quality to the synthesized speech.&#10;2. The LPC re-synthesis method introduces degradation not typically found in human speech due to its approximation technique, which further complicates the assessment of the system's performance compared to human performance.&#10;3. Excitation, a crucial factor in what the system hears, is not taken into account by the LPC-12 system. This makes it challenging to determine if the system's performance is close to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Professor B&#10;Content: There 's two problems there . I mean {disfmarker} I mean , so {disfmarker} so the first is {vocalsound} that by doing LPC - twelve with synthesized speech w like you 're saying , uh , it 's {disfmarker} {vocalsound} i i you 're {disfmarker} you 're adding other degradation .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Right ? So it 's not just the noise but you 're adding in fact some degradation because it 's only an approximation . Um , and the second thing is {disfmarker} which is m maybe more interesting {disfmarker} is that , um , {comment} {vocalsound} if you do it with whispered speech , you get this number . What if you had {pause} done analysis {comment} re - synthesis and taken the pitch as well ? Alright ? So now you put the pitch in .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: What would the percentage be then ?&#10;Speaker" target="1. Extracting the pitch from actual speech and putting it back in might help in improving the performance of the model that hears some things better than others. This is because, as per Professor B, removing pitch information makes the audio closer to a one-source system, which could negatively impact the performance. By adding the pitch back, it might be possible to restore some of the lost information and improve the model's ability to distinguish different sound sources or speech components.&#10;2. However, there are potential challenges in implementing this idea. Professor B mentioned that synthesizing speech with added pitch information could introduce other degradations due to the approximation method used. Furthermore, it is unclear how mel-based synthesis would perform compared to the current approach. Therefore, careful experimentation and evaluation would be necessary to determine the effectiveness of this method.&#10;&#10;In summary, adding the extracted pitch back into the speech might improve the model's performance by restoring some lost information; however, it is crucial to consider potential degradations and other factors that could impact the overall outcome.">
      <data key="d0">1</data>
    </edge>
    <edge source=" LPC n twelve is pretty crummy &quot; . You know ?&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: So I I I 'm not sure {disfmarker} I 'm not sure how we can conclude from this anything about {disfmarker} that our system is close to {vocalsound} the human performance .&#10;Speaker: PhD D&#10;Content: Ye Yeah . Well , the point is that eh l ey {disfmarker} the point is that , um , {vocalsound} what I {disfmarker} what I listened to when I re - synthesized the LP - the LPC - twelve {pause} spectrum {vocalsound} is in a way what the system , uh , is hearing , cuz @ @ {disfmarker} all the {disfmarker} all the , um , excitation {disfmarker} all the {disfmarker} well , the excitation is {disfmarker} is not taken into account . That 's what we do with our system . And&#10;Speaker: Professor B&#10;Content: Well , you 're not doing the L" target="1. The percentage of having the pitch is significant in terms of people's perception because it can impact the clarity and intelligibility of speech. A lower pitch retention percentage, such as 1%, may result in better understanding and a more positive opinion of LPC-12, while a higher percentage, like near 5%, might lead to degradation and a less favorable view.&#10;2. If the pitch retention stayed at around 5%, it could indicate that the LPC-12 system is not performing well, and users might perceive its quality as &quot;crummy.&quot; Conversely, a lower percentage demonstrates better performance, making it closer to human performance in terms of speech clarity and intelligibility.">
      <data key="d0">1</data>
    </edge>
    <edge source=" LPC n twelve is pretty crummy &quot; . You know ?&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: So I I I 'm not sure {disfmarker} I 'm not sure how we can conclude from this anything about {disfmarker} that our system is close to {vocalsound} the human performance .&#10;Speaker: PhD D&#10;Content: Ye Yeah . Well , the point is that eh l ey {disfmarker} the point is that , um , {vocalsound} what I {disfmarker} what I listened to when I re - synthesized the LP - the LPC - twelve {pause} spectrum {vocalsound} is in a way what the system , uh , is hearing , cuz @ @ {disfmarker} all the {disfmarker} all the , um , excitation {disfmarker} all the {disfmarker} well , the excitation is {disfmarker} is not taken into account . That 's what we do with our system . And&#10;Speaker: Professor B&#10;Content: Well , you 're not doing the L" target="1. The conclusion that the system's performance is close to human performance cannot be drawn based on the LPC-12 method, as pointed out by PhD D. This is because the degradation from LPC-12 synthesis may introduce issues not typically found in human speech due to its approximation method. Furthermore, excitation, which plays a significant role in what the system hears, is not taken into account in the LPC-12 system, making it difficult to accurately assess the system's performance compared to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="disfmarker} a periodic sound and , {pause} @ @ {comment} uh , unvoiced sound , and the noise&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: which is mostly , {vocalsound} uh , noise . I mean not {pause} periodic . So , {pause} what {disfmarker} what do you mean exactly by putting back the pitch in ? Because {disfmarker}&#10;Speaker: PhD A&#10;Content: In the LPC synthesis ? I think {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . You did LPC re - synthesis {disfmarker}&#10;Speaker: PhD D&#10;Content: I&#10;Speaker: Professor B&#10;Content: L PC re - synthesis .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: So , {vocalsound} uh {disfmarker} and you did it with a noise source , rather than with {disfmarker} with a s periodic source .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor" target="1. Possibilities: If Speaker A (referred to as &quot;he&quot; in the transcript) comes up with a useful contribution, such as a better noise estimation module, he could potentially share it with the team (PhD D and Professor B). They could then integrate this new module into their existing system, if they determine that it would be beneficial.&#10;2. Potential outcomes: The integration of a better noise estimation module could lead to improved performance in detecting and handling noise in their system. This improvement might be significant enough to merit shipping or implementing it. Additionally, Speaker A's contribution could encourage further collaboration and innovation within the team, as they explore new ideas and possibilities for enhancing their system.">
      <data key="d0">1</data>
    </edge>
    <edge source="disfmarker} a periodic sound and , {pause} @ @ {comment} uh , unvoiced sound , and the noise&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: which is mostly , {vocalsound} uh , noise . I mean not {pause} periodic . So , {pause} what {disfmarker} what do you mean exactly by putting back the pitch in ? Because {disfmarker}&#10;Speaker: PhD A&#10;Content: In the LPC synthesis ? I think {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . You did LPC re - synthesis {disfmarker}&#10;Speaker: PhD D&#10;Content: I&#10;Speaker: Professor B&#10;Content: L PC re - synthesis .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: So , {vocalsound} uh {disfmarker} and you did it with a noise source , rather than with {disfmarker} with a s periodic source .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor" target="1. The decision of whether to use noise or periodic pulses in speech synthesis is determined by the nature of the sound being synthesized: unvoiced sounds are produced using noise, while voiced sounds are generated using periodic pulses.&#10;2. Yes, it is possible to detect strong periodic components for voiced sound generation in speech synthesis. This can be achieved through LPC (Linear Predictive Coding) re-synthesis, where the presence of strong periodic components indicates that a voiced sound should be used, while the absence of such components suggests an unvoiced sound, which would require noise for synthesis.&#10;&#10;In summary, analyzing the signal to detect periodic components helps determine whether to use noise or periodic pulses in speech synthesis. This information can then be utilized to create more accurate and natural-sounding synthetic speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="disfmarker} a periodic sound and , {pause} @ @ {comment} uh , unvoiced sound , and the noise&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: which is mostly , {vocalsound} uh , noise . I mean not {pause} periodic . So , {pause} what {disfmarker} what do you mean exactly by putting back the pitch in ? Because {disfmarker}&#10;Speaker: PhD A&#10;Content: In the LPC synthesis ? I think {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . You did LPC re - synthesis {disfmarker}&#10;Speaker: PhD D&#10;Content: I&#10;Speaker: Professor B&#10;Content: L PC re - synthesis .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: So , {vocalsound} uh {disfmarker} and you did it with a noise source , rather than with {disfmarker} with a s periodic source .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor" target="1. Alternative Approach: Professor B suggests using a two-source system, where one source is noise and the other is periodic pulses, as an alternative to using a voiced &quot;voice thing&quot; when dealing with strong periodic components. This approach allows for more accurate representation of various sound types in speech synthesis.&#10;2. Preference for Two-Source System: Professor B prefers a two-source system over a one-source system because it offers greater flexibility in handling different types of sounds, such as unvoiced and voiced sounds. By utilizing both noise and periodic pulses, the synthetic speech can more closely mimic natural human speech, resulting in improved quality and naturalness.">
      <data key="d0">1</data>
    </edge>
    <edge source="disfmarker} a periodic sound and , {pause} @ @ {comment} uh , unvoiced sound , and the noise&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: which is mostly , {vocalsound} uh , noise . I mean not {pause} periodic . So , {pause} what {disfmarker} what do you mean exactly by putting back the pitch in ? Because {disfmarker}&#10;Speaker: PhD A&#10;Content: In the LPC synthesis ? I think {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . You did LPC re - synthesis {disfmarker}&#10;Speaker: PhD D&#10;Content: I&#10;Speaker: Professor B&#10;Content: L PC re - synthesis .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: So , {vocalsound} uh {disfmarker} and you did it with a noise source , rather than with {disfmarker} with a s periodic source .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor" target="1. The re-synthesized version of speech filtered by an LPC (Linear Predictive Coding) filter may not be immediately recognizable as speech because it is generated using a white noise source that has been filtered, rather than a periodic sound source. This can result in a less natural sounding &quot;whispering&quot; quality to the synthesized speech.&#10;2. The LPC re-synthesis method introduces degradation not typically found in human speech due to its approximation technique, which further complicates the assessment of the system's performance compared to human performance.&#10;3. Excitation, a crucial factor in what the system hears, is not taken into account by the LPC-12 system. This makes it challenging to determine if the system's performance is close to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: PhD D&#10;Content: while our system is currently at seven percent . Um , but what happens also is that if I listen to the , um {disfmarker} {nonvocalsound} a re - synthesized version of the speech and {pause} I re - synthesized this using a white noise that 's filtered by a LPC , uh , filter {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Um , well , you can argue , that , uh {disfmarker} that this is not speech ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: so the ear is not trained to recognize this . But s actually it sound like {pause} whispering , so we are {disfmarker}&#10;Speaker: Professor B&#10;Content: Well , I mean , it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: eh {disfmarker}&#10;Speaker: Professor B&#10;Content: There 's two problems there . I mean {disfmarker} I mean , so {disfmarker} so" target="1. The re-synthesized version of speech filtered by an LPC (Linear Predictive Coding) filter may not be immediately recognizable as speech because it is generated using a white noise source that has been filtered, rather than a periodic sound source. This can result in a less natural sounding &quot;whispering&quot; quality to the synthesized speech.&#10;2. The LPC re-synthesis method introduces degradation not typically found in human speech due to its approximation technique, which further complicates the assessment of the system's performance compared to human performance.&#10;3. Excitation, a crucial factor in what the system hears, is not taken into account by the LPC-12 system. This makes it challenging to determine if the system's performance is close to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker} with a s periodic source .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Right ? So if you actually did real re - synthesis like you do in an LPC synthesizer , where it 's unvoiced you use noise , where it 's voiced you use , {vocalsound} uh , periodic pulses .&#10;Speaker: PhD D&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD D&#10;Content: Yeah , but it 's neither {pause} purely voiced or purely unvoiced . Esp - especially because there is noise .&#10;Speaker: Professor B&#10;Content: Well , it might be hard to do it&#10;Speaker: PhD D&#10;Content: So {disfmarker}&#10;Speaker: Professor B&#10;Content: but it but {disfmarker} but the thing is that if you {disfmarker} {vocalsound} um , if you detect that there 's periodic {disfmarker} s strong periodic components , then you can use a voiced {disfmarker} voice thing .&#10;Speaker: PhD D&#10;" target="1. The decision of whether to use noise or periodic pulses in speech synthesis is determined by the nature of the sound being synthesized: unvoiced sounds are produced using noise, while voiced sounds are generated using periodic pulses.&#10;2. Yes, it is possible to detect strong periodic components for voiced sound generation in speech synthesis. This can be achieved through LPC (Linear Predictive Coding) re-synthesis, where the presence of strong periodic components indicates that a voiced sound should be used, while the absence of such components suggests an unvoiced sound, which would require noise for synthesis.&#10;&#10;In summary, analyzing the signal to detect periodic components helps determine whether to use noise or periodic pulses in speech synthesis. This information can then be utilized to create more accurate and natural-sounding synthetic speech.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker} with a s periodic source .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Right ? So if you actually did real re - synthesis like you do in an LPC synthesizer , where it 's unvoiced you use noise , where it 's voiced you use , {vocalsound} uh , periodic pulses .&#10;Speaker: PhD D&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD D&#10;Content: Yeah , but it 's neither {pause} purely voiced or purely unvoiced . Esp - especially because there is noise .&#10;Speaker: Professor B&#10;Content: Well , it might be hard to do it&#10;Speaker: PhD D&#10;Content: So {disfmarker}&#10;Speaker: Professor B&#10;Content: but it but {disfmarker} but the thing is that if you {disfmarker} {vocalsound} um , if you detect that there 's periodic {disfmarker} s strong periodic components , then you can use a voiced {disfmarker} voice thing .&#10;Speaker: PhD D&#10;" target="1. Alternative Approach: Professor B suggests using a two-source system, where one source is noise and the other is periodic pulses, as an alternative to using a voiced &quot;voice thing&quot; when dealing with strong periodic components. This approach allows for more accurate representation of various sound types in speech synthesis.&#10;2. Preference for Two-Source System: Professor B prefers a two-source system over a one-source system because it offers greater flexibility in handling different types of sounds, such as unvoiced and voiced sounds. By utilizing both noise and periodic pulses, the synthetic speech can more closely mimic natural human speech, resulting in improved quality and naturalness.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target=" B&#10;Content: So this is a particular human . This is {disfmarker} this i this is Stephane .&#10;Speaker: PhD D&#10;Content: Yeah . So that 's {disfmarker} that 's {disfmarker}&#10;Speaker: Grad E&#10;Content: St - Stephane .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: that 's the {disfmarker} the flaw of the experiment . This is just {disfmarker} i j {comment} {vocalsound} {vocalsound} it 's just one subject ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Getting close .&#10;Speaker: PhD D&#10;Content: but {disfmarker} but still , uh , {vocalsound} what happens is {disfmarker} is that , {vocalsound} uh , the digit error rate on this is around one percent ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: while our system is currently at seven percent . Um , but what happens also is that if I listen">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="isfmarker}&#10;Speaker: Professor B&#10;Content: I mean , we {disfmarker} W what 's {disfmarker} what 's some of the lower error rates on {disfmarker} on {disfmarker} on {disfmarker} uh , some of the higher error rates on , uh , {vocalsound} some of these w uh , uh , highly mismatched difficult conditions ? What 's a {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Uh . Yeah , it 's around fifteen to twenty percent .&#10;Speaker: PhD A&#10;Content: Correct ?&#10;Speaker: PhD D&#10;Content: And the baseline , eh {disfmarker}&#10;Speaker: PhD A&#10;Content: Accuracy ?&#10;Speaker: PhD D&#10;Content: Uh , error rate .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Twenty percent error rate ,&#10;Speaker: Professor B&#10;Content: Yeah . So twenty percent error rate on digits .&#10;Speaker: PhD D&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , oh ,">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="Speaker: PhD D&#10;Content: Yeah , that 's it .&#10;Speaker: Professor B&#10;Content: Yeah . That 's {disfmarker} that 's {disfmarker} I mean , one {disfmarker} one percent is sort of what I would {disfmarker} I would figure . If somebody was paying really close attention , you might get {disfmarker} I would actually think that if , {vocalsound} you looked at people on various times of the day and different amounts of attention , you might actually get up to three or four percent error on digits . Uh , {vocalsound} uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Mm - hmm . Um .&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker} you know , we 're not {disfmarker} we 're not incredibly far off . On the other hand , with any of these numbers except maybe the one percent , it 's st it 's not actually usable in a commercial system with a full telephone number or something .&#10;Speaker: PhD D&#10;Content: Uh - huh . Yeah . At">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target=" then you 'd say &quot; OK , it 's {disfmarker} it 's in fact having , um , {vocalsound} not just the spectral envelope but also the {disfmarker} also the {disfmarker} the pitch {vocalsound} that , uh , {comment} @ @ {comment} has the information that people can use , anyway . &quot;&#10;Speaker: PhD D&#10;Content: Uh - huh . Mmm .&#10;Speaker: PhD A&#10;Content: But from this it 's pretty safe to say that the system is with either {vocalsound} two to seven percent away from {pause} the performance of a human . Right ? So it 's somewhere in that range .&#10;Speaker: Professor B&#10;Content: Well , or it 's {disfmarker} it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Two {disfmarker} two to six percent .&#10;Speaker: Professor B&#10;Content: Yeah , so {disfmarker} It 's {disfmarker} it 's one point four times , uh , to , uh , seven times the error ,&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target=" is the system that we have currently . Oh , yes . We have , like , a system that gives sixty - two percent improvement , but {vocalsound} if you want to stick to the {disfmarker} {vocalsound} this latency {disfmarker} Well , it has a latency of two thirty , but {vocalsound} if you want also to stick to the number {vocalsound} of features that {disfmarker} limit it to sixty , {vocalsound} then we go a little bit down but it 's still sixty - one percent . Uh , and if we drop the tandem network , then we have fifty - seven percent .&#10;Speaker: Professor B&#10;Content: Uh , but th the two th two thirty includes the tandem network ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: OK . And i is the tandem network , uh , small enough that it will fit on the terminal size in terms of {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Uh , no , I don't think so .&#10;Speaker: Professor B&#10;Content: No .&#10;Speaker: PhD D&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target=" on digits .&#10;Speaker: PhD D&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , oh , on digits .&#10;Speaker: Professor B&#10;Content: So if you 're doing {disfmarker} so if you 're doing ,&#10;Speaker: PhD D&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: On digits .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: you know ,&#10;Speaker: PhD D&#10;Content: And this is so {disfmarker} so {disfmarker} still the baseline .&#10;Speaker: Professor B&#10;Content: sixty - thousand {disfmarker} &#10;Speaker: PhD D&#10;Content: Right ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , and if you 're saying sixty - thousand word recognition , getting sixty percent error on some of these noise condition not at all surprising .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: The baseline">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="1. The percentage of having the pitch is significant in terms of people's perception because it can impact the clarity and intelligibility of speech. A lower pitch retention percentage, such as 1%, may result in better understanding and a more positive opinion of LPC-12, while a higher percentage, like near 5%, might lead to degradation and a less favorable view.&#10;2. If the pitch retention stayed at around 5%, it could indicate that the LPC-12 system is not performing well, and users might perceive its quality as &quot;crummy.&quot; Conversely, a lower percentage demonstrates better performance, making it closer to human performance in terms of speech clarity and intelligibility.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="The computational system computes features based on Gabor functions, which are analysis functions with a certain extent in time and frequency. These functions are used to sample the signal in a time-frequency representation, allowing for multi-scale analysis. By optimizing the parameters of these basis functions, the system can extract different possible features that may be relevant for the specific task it is performing. This process involves analyzing the input signal using wavelet basis functions and Gabor functions, which sample the time-frequency plane to capture patterns in the frequency content of the signal. The resulting features can help the neural network learn and generalize from the data more effectively, potentially leading to better performance compared to using raw waveform data.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="The purpose of adjusting a new set of data to resemble the distribution of a previous set, as discussed in the context of creating a histogram of amplitudes or levels of brightness, is to improve the performance of a model or system. This technique is applied in image processing and can be useful for speech processing as well.&#10;&#10;The method involves comparing the histograms of the new and old data. If the new data's histogram does not match the old data's histogram, adjustments are made to the new data so that its histogram becomes more similar to the old data's histogram. This process can be done piece-wise linearly or through some other kind of approximation. The speakers in the conversation mention a piece-wise linear approach and an approach using a power law between points.&#10;&#10;The underlying idea is that by making the new data's distribution (histogram) resemble the old data's distribution, the system can perform better, as demonstrated in the example where a human subject (Stephane) achieved a lower error rate than the system. However, it is essential to note that simply increasing the amount of training data may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="1. The conclusion that the system's performance is close to human performance cannot be drawn based on the LPC-12 method, as pointed out by PhD D. This is because the degradation from LPC-12 synthesis may introduce issues not typically found in human speech due to its approximation method. Furthermore, excitation, which plays a significant role in what the system hears, is not taken into account in the LPC-12 system, making it difficult to accurately assess the system's performance compared to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="Based on the supporting materials provided, making an accurate assessment of a system's performance compared to human performance using the LPC-12 method has limitations. The degradation from LPC-12 synthesis and the absence of excitation consideration in this method can introduce issues that are not typically found in human speech, making it challenging to draw definitive conclusions about the system's accuracy. Therefore, any comparison to the human-recreated LPC twelve spectrum should be approached with caution, as it may not provide a complete or accurate representation of the system's performance relative to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="1. The different conditions that the speakers are referring to are likely the various noise conditions and mismatched conditions in which the experiment was conducted. They mention that high error rates under these difficult conditions are not surprising, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;2. It is challenging to determine whether the numbers represent accuracy or error rate because the values discussed are not clearly defined as either. When looking at the numbers, it's unclear if a higher value indicates better performance (accuracy) or worse performance (error rate). This confusion arises from the way the numbers are presented or discussed in the conversation, making it difficult for the listeners to interpret the results accurately.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="Based on the transcript, the specific knob or scaling factor that should be used to change the scale of likelihoods, not observations, when determining a scaling factor in their process is not explicitly mentioned. Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="1. If the pitch information is extracted from actual speech and added back into the current input of the system, it could potentially help restore some lost information, as mentioned by Professor B. This might improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate.&#10;2. However, there are potential challenges with this approach. Synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.&#10;3. It is also unclear how mel-based synthesis would perform compared to the current approach, so thorough testing would be required before any conclusions can be drawn about its effectiveness in reducing error rates.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference in digit error rate between the experiment with a single subject named Stephane, which has an error rate of one percent, and a system currently being used, which has a seven percent error rate, is six percentage points. The system's error rate is significantly higher than that of the human subject in this case." target="1. The values being discussed in the transcript are candidate basis functions or features used for classification in an optimization procedure called recursive feature elimination (or a variation of it). In this process, they start with a set of M candidate basis functions and classify using each possible subset of this set, recursively eliminating features that do not contribute to the classification performance until an optimal set is obtained.&#10;   &#10;2. Log probabilities are being utilized in the process despite not using the values as probabilities because log probabilities have a more Gaussian shape than regular probabilities, making them easier to model and work with in complex methods like the ones being used by the speakers. They mentioned the benefits of taking advantage of their Gaussian shape and the fact that these values are not directly used as probabilities but rather their log probabilities.&#10;&#10;In summary, the values in this context refer to candidate basis functions or features used for classification, and log probabilities are being utilized for modeling purposes due to their more Gaussian shape despite not being used directly as probabilities.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker}&#10;Speaker: Professor B&#10;Content: I mean , we {disfmarker} W what 's {disfmarker} what 's some of the lower error rates on {disfmarker} on {disfmarker} on {disfmarker} uh , some of the higher error rates on , uh , {vocalsound} some of these w uh , uh , highly mismatched difficult conditions ? What 's a {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Uh . Yeah , it 's around fifteen to twenty percent .&#10;Speaker: PhD A&#10;Content: Correct ?&#10;Speaker: PhD D&#10;Content: And the baseline , eh {disfmarker}&#10;Speaker: PhD A&#10;Content: Accuracy ?&#10;Speaker: PhD D&#10;Content: Uh , error rate .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Twenty percent error rate ,&#10;Speaker: Professor B&#10;Content: Yeah . So twenty percent error rate on digits .&#10;Speaker: PhD D&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , oh ," target="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker}&#10;Speaker: Professor B&#10;Content: I mean , we {disfmarker} W what 's {disfmarker} what 's some of the lower error rates on {disfmarker} on {disfmarker} on {disfmarker} uh , some of the higher error rates on , uh , {vocalsound} some of these w uh , uh , highly mismatched difficult conditions ? What 's a {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Uh . Yeah , it 's around fifteen to twenty percent .&#10;Speaker: PhD A&#10;Content: Correct ?&#10;Speaker: PhD D&#10;Content: And the baseline , eh {disfmarker}&#10;Speaker: PhD A&#10;Content: Accuracy ?&#10;Speaker: PhD D&#10;Content: Uh , error rate .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Twenty percent error rate ,&#10;Speaker: Professor B&#10;Content: Yeah . So twenty percent error rate on digits .&#10;Speaker: PhD D&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , oh ," target="1. The speakers are discussing the potential benefits of using log probabilities in their models, as they have a more Gaussian shape than regular probabilities, making them easier to model. They consider whether adjusting the word insertion penalty could have a positive effect on the system's performance.&#10;2. The speakers seem uncertain about whether they can change the word insertion penalty based on the lack of a &quot;ruling&quot; or clear decision. They express concerns about not knowing what the impact would be at the &quot;other end&quot; due to the complexity of their methods.&#10;&#10;In summary, the speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty given their current situation.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker}&#10;Speaker: Professor B&#10;Content: I mean , we {disfmarker} W what 's {disfmarker} what 's some of the lower error rates on {disfmarker} on {disfmarker} on {disfmarker} uh , some of the higher error rates on , uh , {vocalsound} some of these w uh , uh , highly mismatched difficult conditions ? What 's a {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Uh . Yeah , it 's around fifteen to twenty percent .&#10;Speaker: PhD A&#10;Content: Correct ?&#10;Speaker: PhD D&#10;Content: And the baseline , eh {disfmarker}&#10;Speaker: PhD A&#10;Content: Accuracy ?&#10;Speaker: PhD D&#10;Content: Uh , error rate .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Twenty percent error rate ,&#10;Speaker: Professor B&#10;Content: Yeah . So twenty percent error rate on digits .&#10;Speaker: PhD D&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , oh ," target="1. The different conditions that the speakers are referring to are likely the various noise conditions and mismatched conditions in which the experiment was conducted. They mention that high error rates under these difficult conditions are not surprising, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;2. It is challenging to determine whether the numbers represent accuracy or error rate because the values discussed are not clearly defined as either. When looking at the numbers, it's unclear if a higher value indicates better performance (accuracy) or worse performance (error rate). This confusion arises from the way the numbers are presented or discussed in the conversation, making it difficult for the listeners to interpret the results accurately.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker}&#10;Speaker: Professor B&#10;Content: I mean , we {disfmarker} W what 's {disfmarker} what 's some of the lower error rates on {disfmarker} on {disfmarker} on {disfmarker} uh , some of the higher error rates on , uh , {vocalsound} some of these w uh , uh , highly mismatched difficult conditions ? What 's a {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Uh . Yeah , it 's around fifteen to twenty percent .&#10;Speaker: PhD A&#10;Content: Correct ?&#10;Speaker: PhD D&#10;Content: And the baseline , eh {disfmarker}&#10;Speaker: PhD A&#10;Content: Accuracy ?&#10;Speaker: PhD D&#10;Content: Uh , error rate .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Twenty percent error rate ,&#10;Speaker: Professor B&#10;Content: Yeah . So twenty percent error rate on digits .&#10;Speaker: PhD D&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , oh ," target="The significant difference in word error rates being discussed by the speakers refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is concerning and raises questions about the system's performance. The high error rates might be due to the software named Aurora, as suggested by Professor B. They imply that Aurora might be contributing to the high error rates based on their experience and familiarity with its performance, but they do not provide specific reasons for this conclusion.&#10;&#10;The speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty due to the complexity of their methods and not knowing what the impact would be at the &quot;other end.&quot; The different conditions they refer to are likely various noise conditions and mismatched conditions in which the experiment was conducted. High error rates under these difficult conditions are expected, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose. The numbers discussed during the conversation do not make it clear if a higher value indicates better performance (accuracy) or worse performance (error rate), causing confusion for the listeners.&#10;&#10;Lastly, Professor B suggests that adding pitch information extracted from actual speech back into the current input of the system could potentially help restore lost information and improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate. However, synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.">
      <data key="d0">1</data>
    </edge>
    <edge source=" then you 'd say &quot; OK , it 's {disfmarker} it 's in fact having , um , {vocalsound} not just the spectral envelope but also the {disfmarker} also the {disfmarker} the pitch {vocalsound} that , uh , {comment} @ @ {comment} has the information that people can use , anyway . &quot;&#10;Speaker: PhD D&#10;Content: Uh - huh . Mmm .&#10;Speaker: PhD A&#10;Content: But from this it 's pretty safe to say that the system is with either {vocalsound} two to seven percent away from {pause} the performance of a human . Right ? So it 's somewhere in that range .&#10;Speaker: Professor B&#10;Content: Well , or it 's {disfmarker} it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Two {disfmarker} two to six percent .&#10;Speaker: Professor B&#10;Content: Yeah , so {disfmarker} It 's {disfmarker} it 's one point four times , uh , to , uh , seven times the error ,&#10;Speaker:" target="1. Possibilities: If Speaker A (referred to as &quot;he&quot; in the transcript) comes up with a useful contribution, such as a better noise estimation module, he could potentially share it with the team (PhD D and Professor B). They could then integrate this new module into their existing system, if they determine that it would be beneficial.&#10;2. Potential outcomes: The integration of a better noise estimation module could lead to improved performance in detecting and handling noise in their system. This improvement might be significant enough to merit shipping or implementing it. Additionally, Speaker A's contribution could encourage further collaboration and innovation within the team, as they explore new ideas and possibilities for enhancing their system.">
      <data key="d0">1</data>
    </edge>
    <edge source=" then you 'd say &quot; OK , it 's {disfmarker} it 's in fact having , um , {vocalsound} not just the spectral envelope but also the {disfmarker} also the {disfmarker} the pitch {vocalsound} that , uh , {comment} @ @ {comment} has the information that people can use , anyway . &quot;&#10;Speaker: PhD D&#10;Content: Uh - huh . Mmm .&#10;Speaker: PhD A&#10;Content: But from this it 's pretty safe to say that the system is with either {vocalsound} two to seven percent away from {pause} the performance of a human . Right ? So it 's somewhere in that range .&#10;Speaker: Professor B&#10;Content: Well , or it 's {disfmarker} it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Two {disfmarker} two to six percent .&#10;Speaker: Professor B&#10;Content: Yeah , so {disfmarker} It 's {disfmarker} it 's one point four times , uh , to , uh , seven times the error ,&#10;Speaker:" target="1. If the pitch information is extracted from actual speech and added back into the current input of the system, it could potentially help restore some lost information, as mentioned by Professor B. This might improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate.&#10;2. However, there are potential challenges with this approach. Synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.&#10;3. It is also unclear how mel-based synthesis would perform compared to the current approach, so thorough testing would be required before any conclusions can be drawn about its effectiveness in reducing error rates.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The percentage of having the pitch is significant in terms of people's perception because it can impact the clarity and intelligibility of speech. A lower pitch retention percentage, such as 1%, may result in better understanding and a more positive opinion of LPC-12, while a higher percentage, like near 5%, might lead to degradation and a less favorable view.&#10;2. If the pitch retention stayed at around 5%, it could indicate that the LPC-12 system is not performing well, and users might perceive its quality as &quot;crummy.&quot; Conversely, a lower percentage demonstrates better performance, making it closer to human performance in terms of speech clarity and intelligibility." target="&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: What would the percentage be then ?&#10;Speaker: PhD D&#10;Content: Um {disfmarker}&#10;Speaker: Professor B&#10;Content: See , that 's the question . So , you see , if it 's {disfmarker} if it 's {disfmarker} if it 's , uh {disfmarker} Let 's say it 's {pause} back down to one percent again .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: That would say at least for people , having the pitch is really , really important , which would be interesting in itself . Um ,&#10;Speaker: PhD D&#10;Content: Uh , yeah . But {disfmarker}&#10;Speaker: Professor B&#10;Content: if i on the other hand , if it stayed up {pause} near five percent , {vocalsound} then I 'd say &quot; boy , LPC n twelve is pretty crummy &quot; . You know ?&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The percentage of having the pitch is significant in terms of people's perception because it can impact the clarity and intelligibility of speech. A lower pitch retention percentage, such as 1%, may result in better understanding and a more positive opinion of LPC-12, while a higher percentage, like near 5%, might lead to degradation and a less favorable view.&#10;2. If the pitch retention stayed at around 5%, it could indicate that the LPC-12 system is not performing well, and users might perceive its quality as &quot;crummy.&quot; Conversely, a lower percentage demonstrates better performance, making it closer to human performance in terms of speech clarity and intelligibility." target="isfmarker} is it that different , I mean ?&#10;Speaker: Professor B&#10;Content: Um , {vocalsound} I don't know what mel , {pause} uh , based synthesis would sound like ,&#10;Speaker: PhD D&#10;Content: I&#10;Speaker: Professor B&#10;Content: but certainly the spectra are quite different .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Couldn't you t couldn't you , um , test the human performance on just the original {pause} audio ?&#10;Speaker: PhD D&#10;Content: Mm - hmm . This is the one percent number .&#10;Speaker: Professor B&#10;Content: Yeah , it 's one percent . He 's trying to remove the pitch information&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Oh , oh . OK ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: and make it closer to what {disfmarker} to what we 're seeing as the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The percentage of having the pitch is significant in terms of people's perception because it can impact the clarity and intelligibility of speech. A lower pitch retention percentage, such as 1%, may result in better understanding and a more positive opinion of LPC-12, while a higher percentage, like near 5%, might lead to degradation and a less favorable view.&#10;2. If the pitch retention stayed at around 5%, it could indicate that the LPC-12 system is not performing well, and users might perceive its quality as &quot;crummy.&quot; Conversely, a lower percentage demonstrates better performance, making it closer to human performance in terms of speech clarity and intelligibility." target="1. The conclusion that the system's performance is close to human performance cannot be drawn based on the LPC-12 method, as pointed out by PhD D. This is because the degradation from LPC-12 synthesis may introduce issues not typically found in human speech due to its approximation method. Furthermore, excitation, which plays a significant role in what the system hears, is not taken into account in the LPC-12 system, making it difficult to accurately assess the system's performance compared to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The percentage of having the pitch is significant in terms of people's perception because it can impact the clarity and intelligibility of speech. A lower pitch retention percentage, such as 1%, may result in better understanding and a more positive opinion of LPC-12, while a higher percentage, like near 5%, might lead to degradation and a less favorable view.&#10;2. If the pitch retention stayed at around 5%, it could indicate that the LPC-12 system is not performing well, and users might perceive its quality as &quot;crummy.&quot; Conversely, a lower percentage demonstrates better performance, making it closer to human performance in terms of speech clarity and intelligibility." target="Based on the supporting materials provided, making an accurate assessment of a system's performance compared to human performance using the LPC-12 method has limitations. The degradation from LPC-12 synthesis and the absence of excitation consideration in this method can introduce issues that are not typically found in human speech, making it challenging to draw definitive conclusions about the system's accuracy. Therefore, any comparison to the human-recreated LPC twelve spectrum should be approached with caution, as it may not provide a complete or accurate representation of the system's performance relative to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The percentage of having the pitch is significant in terms of people's perception because it can impact the clarity and intelligibility of speech. A lower pitch retention percentage, such as 1%, may result in better understanding and a more positive opinion of LPC-12, while a higher percentage, like near 5%, might lead to degradation and a less favorable view.&#10;2. If the pitch retention stayed at around 5%, it could indicate that the LPC-12 system is not performing well, and users might perceive its quality as &quot;crummy.&quot; Conversely, a lower percentage demonstrates better performance, making it closer to human performance in terms of speech clarity and intelligibility." target="1. If the pitch information is extracted from actual speech and added back into the current input of the system, it could potentially help restore some lost information, as mentioned by Professor B. This might improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate.&#10;2. However, there are potential challenges with this approach. Synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.&#10;3. It is also unclear how mel-based synthesis would perform compared to the current approach, so thorough testing would be required before any conclusions can be drawn about its effectiveness in reducing error rates.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker} is it that different , I mean ?&#10;Speaker: Professor B&#10;Content: Um , {vocalsound} I don't know what mel , {pause} uh , based synthesis would sound like ,&#10;Speaker: PhD D&#10;Content: I&#10;Speaker: Professor B&#10;Content: but certainly the spectra are quite different .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Couldn't you t couldn't you , um , test the human performance on just the original {pause} audio ?&#10;Speaker: PhD D&#10;Content: Mm - hmm . This is the one percent number .&#10;Speaker: Professor B&#10;Content: Yeah , it 's one percent . He 's trying to remove the pitch information&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Oh , oh . OK ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: and make it closer to what {disfmarker} to what we 're seeing as the" target="The optimization procedure being used here is called &quot;recursive feature elimination&quot; or a variation of it. The speaker starts with a set of candidate basis functions and classifies using each possible subset of this set, which is M possible subsets for a set of M candidates. The feature that doesn't contribute to the classification performance is considered useless and eliminated. This process is repeated by randomly selecting another feature from the remaining pool of possible basis functions until an optimal set is obtained.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker} is it that different , I mean ?&#10;Speaker: Professor B&#10;Content: Um , {vocalsound} I don't know what mel , {pause} uh , based synthesis would sound like ,&#10;Speaker: PhD D&#10;Content: I&#10;Speaker: Professor B&#10;Content: but certainly the spectra are quite different .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Couldn't you t couldn't you , um , test the human performance on just the original {pause} audio ?&#10;Speaker: PhD D&#10;Content: Mm - hmm . This is the one percent number .&#10;Speaker: Professor B&#10;Content: Yeah , it 's one percent . He 's trying to remove the pitch information&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Oh , oh . OK ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: and make it closer to what {disfmarker} to what we 're seeing as the" target="1. Extracting the pitch from actual speech and putting it back in might help in improving the performance of the model that hears some things better than others. This is because, as per Professor B, removing pitch information makes the audio closer to a one-source system, which could negatively impact the performance. By adding the pitch back, it might be possible to restore some of the lost information and improve the model's ability to distinguish different sound sources or speech components.&#10;2. However, there are potential challenges in implementing this idea. Professor B mentioned that synthesizing speech with added pitch information could introduce other degradations due to the approximation method used. Furthermore, it is unclear how mel-based synthesis would perform compared to the current approach. Therefore, careful experimentation and evaluation would be necessary to determine the effectiveness of this method.&#10;&#10;In summary, adding the extracted pitch back into the speech might improve the model's performance by restoring some lost information; however, it is crucial to consider potential degradations and other factors that could impact the overall outcome.">
      <data key="d0">1</data>
    </edge>
    <edge source="Extraction of computing features from a waveform involves processing and manipulating the original data, which can result in loss of some information. However, this process can also highlight or emphasize other important aspects or characteristics of the signal that may be relevant for the specific task a neural net is trying to perform. For example, features such as spectral coefficients or Mel-frequency cepstral coefficients (MFCCs) can capture patterns in the frequency content of speech signals that are meaningful for speech recognition. By focusing on these relevant features, a neural network might be able to better learn and generalize from the data, even if some information has been lost during the feature extraction process. In other words, while some information is lost, what remains may still be more informative and useful for the task at hand. This can lead to better performance compared to using the raw waveform, which might contain irrelevant or noisy information that could negatively impact the learning process in a neural network." target=" , you know , neural net functions ,&#10;Speaker: Grad E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: its {disfmarker} {comment} {vocalsound} whatever it finds to be best .&#10;Speaker: Grad C&#10;Content: &#10;Speaker: Professor B&#10;Content: Um , so you could argue that in fact it {disfmarker} But I {disfmarker} I don't actually believe that argument because I know that , um , {vocalsound} you can , uh {disfmarker} computing features is useful , even though {pause} in principle you haven't {pause} {vocalsound} added anything {disfmarker} in fact , you subtracted something , from the original waveform {disfmarker} You know , uh , if you 've {disfmarker} you 've processed it in some way you 've typically lost something {disfmarker} some information . And so , {vocalsound} you 've lost information and yet it does better with {disfmarker} {vocalsound} with features than it does with the waveform . So ,">
      <data key="d0">1</data>
    </edge>
    <edge source="Extraction of computing features from a waveform involves processing and manipulating the original data, which can result in loss of some information. However, this process can also highlight or emphasize other important aspects or characteristics of the signal that may be relevant for the specific task a neural net is trying to perform. For example, features such as spectral coefficients or Mel-frequency cepstral coefficients (MFCCs) can capture patterns in the frequency content of speech signals that are meaningful for speech recognition. By focusing on these relevant features, a neural network might be able to better learn and generalize from the data, even if some information has been lost during the feature extraction process. In other words, while some information is lost, what remains may still be more informative and useful for the task at hand. This can lead to better performance compared to using the raw waveform, which might contain irrelevant or noisy information that could negatively impact the learning process in a neural network." target="ve lost information and yet it does better with {disfmarker} {vocalsound} with features than it does with the waveform . So , uh , I {disfmarker} I know that i sometimes it 's useful to {disfmarker} {pause} to constrain things . So that 's {vocalsound} why it really seems like the constraint {disfmarker} in {disfmarker} in all this stuff it 's the constraints that are actually what matters . Because if it wasn't {pause} the constraints that mattered , then we would 've completely solved this problem long ago , because long ago we already knew how to put waveforms into powerful statistical mechanisms . So .&#10;Speaker: PhD D&#10;Content: Yeah . Well , if we had infinite processing power and {pause} data , {comment} I guess , using the waveform could {disfmarker}&#10;Speaker: Grad E&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: Yeah Uh , then it would work . Yeah , I agree . Yeah . There 's the problem .&#10;Speaker: PhD D&#10;Content: So , that 's {disfmarker}&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="Extraction of computing features from a waveform involves processing and manipulating the original data, which can result in loss of some information. However, this process can also highlight or emphasize other important aspects or characteristics of the signal that may be relevant for the specific task a neural net is trying to perform. For example, features such as spectral coefficients or Mel-frequency cepstral coefficients (MFCCs) can capture patterns in the frequency content of speech signals that are meaningful for speech recognition. By focusing on these relevant features, a neural network might be able to better learn and generalize from the data, even if some information has been lost during the feature extraction process. In other words, while some information is lost, what remains may still be more informative and useful for the task at hand. This can lead to better performance compared to using the raw waveform, which might contain irrelevant or noisy information that could negatively impact the learning process in a neural network." target=" stream ?&#10;Speaker: PhD D&#10;Content: But , um , we {disfmarker} we did a first try with this , and it {disfmarker} it {vocalsound} clearly hurts .&#10;Speaker: Professor B&#10;Content: But , uh , how was the stream combined ?&#10;Speaker: PhD D&#10;Content: Uh . {vocalsound} It was c it was just combined , um , by the acoustic model . So there was , no neural network for the moment .&#10;Speaker: Professor B&#10;Content: Right . So , I mean , if you just had a second stream that was just spectral and had another neural net and combined there , that {disfmarker} that , uh , {vocalsound} might be good .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Yeah . Mm - hmm . Mm - hmm . Mmm . Yeah . Um {disfmarker} Yeah , and the other thing , that noise estimation and th um , maybe try to train {disfmarker} uh , the training data for the t tandem network , right now , is like {disfmarker} i is using the noises">
      <data key="d0">1</data>
    </edge>
    <edge source="Extraction of computing features from a waveform involves processing and manipulating the original data, which can result in loss of some information. However, this process can also highlight or emphasize other important aspects or characteristics of the signal that may be relevant for the specific task a neural net is trying to perform. For example, features such as spectral coefficients or Mel-frequency cepstral coefficients (MFCCs) can capture patterns in the frequency content of speech signals that are meaningful for speech recognition. By focusing on these relevant features, a neural network might be able to better learn and generalize from the data, even if some information has been lost during the feature extraction process. In other words, while some information is lost, what remains may still be more informative and useful for the task at hand. This can lead to better performance compared to using the raw waveform, which might contain irrelevant or noisy information that could negatively impact the learning process in a neural network." target=" um , {vocalsound} {vocalsound} between Michael 's approach to , uh , some {disfmarker} some sort of optimal brain damage or optimal brain surgeon on the neural nets .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: So , like , if we have , um {disfmarker} we have our {disfmarker} we have our RASTA features and {disfmarker} and presumably the neural nets are {disfmarker} are learning some sort of a nonlinear mapping , {vocalsound} uh , from the {disfmarker} the {disfmarker} the features {vocalsound} to {disfmarker} to this {disfmarker} this probability posterior space .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: Right ? And , um {disfmarker} {vocalsound} {vocalsound} and each of the hidden units is learning some sort of {disfmarker} some sort of {disfmarker} some sort">
      <data key="d0">1</data>
    </edge>
    <edge source="Extraction of computing features from a waveform involves processing and manipulating the original data, which can result in loss of some information. However, this process can also highlight or emphasize other important aspects or characteristics of the signal that may be relevant for the specific task a neural net is trying to perform. For example, features such as spectral coefficients or Mel-frequency cepstral coefficients (MFCCs) can capture patterns in the frequency content of speech signals that are meaningful for speech recognition. By focusing on these relevant features, a neural network might be able to better learn and generalize from the data, even if some information has been lost during the feature extraction process. In other words, while some information is lost, what remains may still be more informative and useful for the task at hand. This can lead to better performance compared to using the raw waveform, which might contain irrelevant or noisy information that could negatively impact the learning process in a neural network." target="ifiers , then do recognition , and , um , improve speech recognition in that way . Um , so right now I 'm in {disfmarker} in the phase where {vocalsound} I 'm looking at {disfmarker} at , um , deciding on a initial set of intermediate categories . And {vocalsound} I 'm looking {vocalsound} for data data - driven {pause} methods that can help me find , {vocalsound} um , a set of intermediate categories {vocalsound} of speech that , uh , will help me to discriminate {pause} later down the line . And one of the ideas , {vocalsound} um , that was to take a {disfmarker} take a neural net {disfmarker} train {disfmarker} train an ordinary neural net {vocalsound} to {disfmarker} {vocalsound} uh , to learn the posterior probabilities of phones . And so , um , at the end of the day you have this neural net and it has hidden {disfmarker} {vocalsound} hidden units . And each of these hidden units is {disfmarker} {vocalsound}">
      <data key="d0">1</data>
    </edge>
    <edge source="Extraction of computing features from a waveform involves processing and manipulating the original data, which can result in loss of some information. However, this process can also highlight or emphasize other important aspects or characteristics of the signal that may be relevant for the specific task a neural net is trying to perform. For example, features such as spectral coefficients or Mel-frequency cepstral coefficients (MFCCs) can capture patterns in the frequency content of speech signals that are meaningful for speech recognition. By focusing on these relevant features, a neural network might be able to better learn and generalize from the data, even if some information has been lost during the feature extraction process. In other words, while some information is lost, what remains may still be more informative and useful for the task at hand. This can lead to better performance compared to using the raw waveform, which might contain irrelevant or noisy information that could negatively impact the learning process in a neural network." target="The computational system computes features based on Gabor functions, which are analysis functions with a certain extent in time and frequency. These functions are used to sample the signal in a time-frequency representation, allowing for multi-scale analysis. By optimizing the parameters of these basis functions, the system can extract different possible features that may be relevant for the specific task it is performing. This process involves analyzing the input signal using wavelet basis functions and Gabor functions, which sample the time-frequency plane to capture patterns in the frequency content of the signal. The resulting features can help the neural network learn and generalize from the data more effectively, potentially leading to better performance compared to using raw waveform data.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , you know , neural net functions ,&#10;Speaker: Grad E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: its {disfmarker} {comment} {vocalsound} whatever it finds to be best .&#10;Speaker: Grad C&#10;Content: &#10;Speaker: Professor B&#10;Content: Um , so you could argue that in fact it {disfmarker} But I {disfmarker} I don't actually believe that argument because I know that , um , {vocalsound} you can , uh {disfmarker} computing features is useful , even though {pause} in principle you haven't {pause} {vocalsound} added anything {disfmarker} in fact , you subtracted something , from the original waveform {disfmarker} You know , uh , if you 've {disfmarker} you 've processed it in some way you 've typically lost something {disfmarker} some information . And so , {vocalsound} you 've lost information and yet it does better with {disfmarker} {vocalsound} with features than it does with the waveform . So ," target="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source="ve lost information and yet it does better with {disfmarker} {vocalsound} with features than it does with the waveform . So , uh , I {disfmarker} I know that i sometimes it 's useful to {disfmarker} {pause} to constrain things . So that 's {vocalsound} why it really seems like the constraint {disfmarker} in {disfmarker} in all this stuff it 's the constraints that are actually what matters . Because if it wasn't {pause} the constraints that mattered , then we would 've completely solved this problem long ago , because long ago we already knew how to put waveforms into powerful statistical mechanisms . So .&#10;Speaker: PhD D&#10;Content: Yeah . Well , if we had infinite processing power and {pause} data , {comment} I guess , using the waveform could {disfmarker}&#10;Speaker: Grad E&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: Yeah Uh , then it would work . Yeah , I agree . Yeah . There 's the problem .&#10;Speaker: PhD D&#10;Content: So , that 's {disfmarker}&#10;" target="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source=" stream ?&#10;Speaker: PhD D&#10;Content: But , um , we {disfmarker} we did a first try with this , and it {disfmarker} it {vocalsound} clearly hurts .&#10;Speaker: Professor B&#10;Content: But , uh , how was the stream combined ?&#10;Speaker: PhD D&#10;Content: Uh . {vocalsound} It was c it was just combined , um , by the acoustic model . So there was , no neural network for the moment .&#10;Speaker: Professor B&#10;Content: Right . So , I mean , if you just had a second stream that was just spectral and had another neural net and combined there , that {disfmarker} that , uh , {vocalsound} might be good .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Yeah . Mm - hmm . Mm - hmm . Mmm . Yeah . Um {disfmarker} Yeah , and the other thing , that noise estimation and th um , maybe try to train {disfmarker} uh , the training data for the t tandem network , right now , is like {disfmarker} i is using the noises" target="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source=" stream ?&#10;Speaker: PhD D&#10;Content: But , um , we {disfmarker} we did a first try with this , and it {disfmarker} it {vocalsound} clearly hurts .&#10;Speaker: Professor B&#10;Content: But , uh , how was the stream combined ?&#10;Speaker: PhD D&#10;Content: Uh . {vocalsound} It was c it was just combined , um , by the acoustic model . So there was , no neural network for the moment .&#10;Speaker: Professor B&#10;Content: Right . So , I mean , if you just had a second stream that was just spectral and had another neural net and combined there , that {disfmarker} that , uh , {vocalsound} might be good .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Yeah . Mm - hmm . Mm - hmm . Mmm . Yeah . Um {disfmarker} Yeah , and the other thing , that noise estimation and th um , maybe try to train {disfmarker} uh , the training data for the t tandem network , right now , is like {disfmarker} i is using the noises" target="1. Extracting the pitch from actual speech and putting it back in might help in improving the performance of the model that hears some things better than others. This is because, as per Professor B, removing pitch information makes the audio closer to a one-source system, which could negatively impact the performance. By adding the pitch back, it might be possible to restore some of the lost information and improve the model's ability to distinguish different sound sources or speech components.&#10;2. However, there are potential challenges in implementing this idea. Professor B mentioned that synthesizing speech with added pitch information could introduce other degradations due to the approximation method used. Furthermore, it is unclear how mel-based synthesis would perform compared to the current approach. Therefore, careful experimentation and evaluation would be necessary to determine the effectiveness of this method.&#10;&#10;In summary, adding the extracted pitch back into the speech might improve the model's performance by restoring some lost information; however, it is crucial to consider potential degradations and other factors that could impact the overall outcome.">
      <data key="d0">1</data>
    </edge>
    <edge source=" um , {vocalsound} {vocalsound} between Michael 's approach to , uh , some {disfmarker} some sort of optimal brain damage or optimal brain surgeon on the neural nets .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: So , like , if we have , um {disfmarker} we have our {disfmarker} we have our RASTA features and {disfmarker} and presumably the neural nets are {disfmarker} are learning some sort of a nonlinear mapping , {vocalsound} uh , from the {disfmarker} the {disfmarker} the features {vocalsound} to {disfmarker} to this {disfmarker} this probability posterior space .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: Right ? And , um {disfmarker} {vocalsound} {vocalsound} and each of the hidden units is learning some sort of {disfmarker} some sort of {disfmarker} some sort" target="The optimization procedure being used here is called &quot;recursive feature elimination&quot; or a variation of it. The speaker starts with a set of candidate basis functions and classifies using each possible subset of this set, which is M possible subsets for a set of M candidates. The feature that doesn't contribute to the classification performance is considered useless and eliminated. This process is repeated by randomly selecting another feature from the remaining pool of possible basis functions until an optimal set is obtained.">
      <data key="d0">1</data>
    </edge>
    <edge source="ifiers , then do recognition , and , um , improve speech recognition in that way . Um , so right now I 'm in {disfmarker} in the phase where {vocalsound} I 'm looking at {disfmarker} at , um , deciding on a initial set of intermediate categories . And {vocalsound} I 'm looking {vocalsound} for data data - driven {pause} methods that can help me find , {vocalsound} um , a set of intermediate categories {vocalsound} of speech that , uh , will help me to discriminate {pause} later down the line . And one of the ideas , {vocalsound} um , that was to take a {disfmarker} take a neural net {disfmarker} train {disfmarker} train an ordinary neural net {vocalsound} to {disfmarker} {vocalsound} uh , to learn the posterior probabilities of phones . And so , um , at the end of the day you have this neural net and it has hidden {disfmarker} {vocalsound} hidden units . And each of these hidden units is {disfmarker} {vocalsound}" target="The important patterns that hidden units in a neural network learn for discriminating between phone classes are patterns related to phone classes or potentially intermediate categories. These patterns can be examined to gain insights into the internal workings of the neural network and possibly identify intermediate categories. However, it is not explicitly stated if this method can definitively lead to discovering new intermediate categories in the given transcripts. Researchers are exploring various data-driven methods to find a set of intermediate categories that can help improve speech recognition, including using hidden patterns learned by neural networks.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Possibilities: If Speaker A (referred to as &quot;he&quot; in the transcript) comes up with a useful contribution, such as a better noise estimation module, he could potentially share it with the team (PhD D and Professor B). They could then integrate this new module into their existing system, if they determine that it would be beneficial.&#10;2. Potential outcomes: The integration of a better noise estimation module could lead to improved performance in detecting and handling noise in their system. This improvement might be significant enough to merit shipping or implementing it. Additionally, Speaker A's contribution could encourage further collaboration and innovation within the team, as they explore new ideas and possibilities for enhancing their system." target=" now over this next {pause} period ,&#10;Speaker: PhD D&#10;Content: Uh , I dunno .&#10;Speaker: Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I don't have feedback from him , but&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: I guess he 's gonna , maybe {disfmarker}&#10;Speaker: Professor B&#10;Content: Well , he 's got it anyway , so he can .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So potentially if he came up with something that was useful , like a diff a better noise estimation module or something , he could ship it to you guys u up there&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: and&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: we could put it in .&#10;Speaker: PhD D&#10;Content: Mm - hmm . {vocalsound} Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . Yeah . So">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Possibilities: If Speaker A (referred to as &quot;he&quot; in the transcript) comes up with a useful contribution, such as a better noise estimation module, he could potentially share it with the team (PhD D and Professor B). They could then integrate this new module into their existing system, if they determine that it would be beneficial.&#10;2. Potential outcomes: The integration of a better noise estimation module could lead to improved performance in detecting and handling noise in their system. This improvement might be significant enough to merit shipping or implementing it. Additionally, Speaker A's contribution could encourage further collaboration and innovation within the team, as they explore new ideas and possibilities for enhancing their system." target=" detection .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Working on the second stream . Of course we have ideas on this also , but {disfmarker} {vocalsound} w we need to try different things and {disfmarker} Uh , but their noise estimation , um {disfmarker} {vocalsound} uh {disfmarker}&#10;Speaker: Professor B&#10;Content: I mean , back on the second stream , I mean , that 's something we 've talked about for a while . I mean , I think {nonvocalsound} that 's certainly a high hope .&#10;Speaker: PhD D&#10;Content: Yeah . {vocalsound} Mmm .&#10;Speaker: Professor B&#10;Content: Um , so we have this {disfmarker} this default idea about just using some sort of purely spectral thing ?&#10;Speaker: PhD D&#10;Content: Uh , yeah .&#10;Speaker: Professor B&#10;Content: for a second stream ?&#10;Speaker: PhD D&#10;Content: But , um , we {disfmarker} we did a first try with this , and it">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Possibilities: If Speaker A (referred to as &quot;he&quot; in the transcript) comes up with a useful contribution, such as a better noise estimation module, he could potentially share it with the team (PhD D and Professor B). They could then integrate this new module into their existing system, if they determine that it would be beneficial.&#10;2. Potential outcomes: The integration of a better noise estimation module could lead to improved performance in detecting and handling noise in their system. This improvement might be significant enough to merit shipping or implementing it. Additionally, Speaker A's contribution could encourage further collaboration and innovation within the team, as they explore new ideas and possibilities for enhancing their system." target=" Professor B&#10;Content: So he just has it all sitting there . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah . Um {disfmarker} Mm - hmm .&#10;Speaker: Professor B&#10;Content: So if he 'll {disfmarker} he might work on improving the noise estimate or on {vocalsound} some histogram things , or {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . I just saw the Eurospeech {disfmarker} We {disfmarker} we didn't talk about it at our meeting but I just saw the {disfmarker} just read the paper . Someone , I forget the name , {comment} and {disfmarker} and Ney , uh , about histogram equalization ? Did you see that one ?&#10;Speaker: PhD D&#10;Content: Um , it was a poster . Or {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . I mean , I just read the paper .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Possibilities: If Speaker A (referred to as &quot;he&quot; in the transcript) comes up with a useful contribution, such as a better noise estimation module, he could potentially share it with the team (PhD D and Professor B). They could then integrate this new module into their existing system, if they determine that it would be beneficial.&#10;2. Potential outcomes: The integration of a better noise estimation module could lead to improved performance in detecting and handling noise in their system. This improvement might be significant enough to merit shipping or implementing it. Additionally, Speaker A's contribution could encourage further collaboration and innovation within the team, as they explore new ideas and possibilities for enhancing their system." target=" D&#10;Content: Um {disfmarker}&#10;Speaker: Professor B&#10;Content: That might be the key , actually .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Cuz you were really thinking about speech versus nonspeech for that .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: That 's a good point .&#10;Speaker: PhD D&#10;Content: Mmm . Uh . Well , there are other things that {vocalsound} we should do but , {vocalsound} um , {vocalsound} it requires time and {disfmarker} {vocalsound} We have ideas , like {disfmarker} so , these things are like hav having a better VAD . Uh , we have some ideas about that . It would {disfmarker} {vocalsound} probably implies working a little bit on features that are more {vocalsound} suited to a voice activity detection .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Working on the second stream .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Possibilities: If Speaker A (referred to as &quot;he&quot; in the transcript) comes up with a useful contribution, such as a better noise estimation module, he could potentially share it with the team (PhD D and Professor B). They could then integrate this new module into their existing system, if they determine that it would be beneficial.&#10;2. Potential outcomes: The integration of a better noise estimation module could lead to improved performance in detecting and handling noise in their system. This improvement might be significant enough to merit shipping or implementing it. Additionally, Speaker A's contribution could encourage further collaboration and innovation within the team, as they explore new ideas and possibilities for enhancing their system." target="Professionor B and PhD D are discussing the possibility that the key factor for achieving a better outcome could be the integration of multiple sources of knowledge and different features in their process. They suggest this might provide a more accurate or effective system. This idea is based on the observation that relying solely on one source, like the 'VAD' (Voice Activity Detection) method, may not yield optimal results. The potential benefits of combining various methods could lead to improved performance and further collaboration within their team.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Possibilities: If Speaker A (referred to as &quot;he&quot; in the transcript) comes up with a useful contribution, such as a better noise estimation module, he could potentially share it with the team (PhD D and Professor B). They could then integrate this new module into their existing system, if they determine that it would be beneficial.&#10;2. Potential outcomes: The integration of a better noise estimation module could lead to improved performance in detecting and handling noise in their system. This improvement might be significant enough to merit shipping or implementing it. Additionally, Speaker A's contribution could encourage further collaboration and innovation within the team, as they explore new ideas and possibilities for enhancing their system." target="1. If the pitch information is extracted from actual speech and added back into the current input of the system, it could potentially help restore some lost information, as mentioned by Professor B. This might improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate.&#10;2. However, there are potential challenges with this approach. Synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.&#10;3. It is also unclear how mel-based synthesis would perform compared to the current approach, so thorough testing would be required before any conclusions can be drawn about its effectiveness in reducing error rates.">
      <data key="d0">1</data>
    </edge>
    <edge source=" now over this next {pause} period ,&#10;Speaker: PhD D&#10;Content: Uh , I dunno .&#10;Speaker: Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I don't have feedback from him , but&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: I guess he 's gonna , maybe {disfmarker}&#10;Speaker: Professor B&#10;Content: Well , he 's got it anyway , so he can .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So potentially if he came up with something that was useful , like a diff a better noise estimation module or something , he could ship it to you guys u up there&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: and&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: we could put it in .&#10;Speaker: PhD D&#10;Content: Mm - hmm . {vocalsound} Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . Yeah . So" target="The optimization procedure being used here is called &quot;recursive feature elimination&quot; or a variation of it. The speaker starts with a set of candidate basis functions and classifies using each possible subset of this set, which is M possible subsets for a set of M candidates. The feature that doesn't contribute to the classification performance is considered useless and eliminated. This process is repeated by randomly selecting another feature from the remaining pool of possible basis functions until an optimal set is obtained.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Professor B&#10;Content: So he just has it all sitting there . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah . Um {disfmarker} Mm - hmm .&#10;Speaker: Professor B&#10;Content: So if he 'll {disfmarker} he might work on improving the noise estimate or on {vocalsound} some histogram things , or {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . I just saw the Eurospeech {disfmarker} We {disfmarker} we didn't talk about it at our meeting but I just saw the {disfmarker} just read the paper . Someone , I forget the name , {comment} and {disfmarker} and Ney , uh , about histogram equalization ? Did you see that one ?&#10;Speaker: PhD D&#10;Content: Um , it was a poster . Or {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . I mean , I just read the paper .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content:" target="1. The histograms in the paper are related to different frequency bins or bands.&#10;2. Based on the conversation, it is suggested that there is &quot;one histogram per frequency bin&quot; or &quot;one per critical band/frequency band.&quot; However, the exact number of histograms per frequency bin or band is not explicitly mentioned in the transcript provided. The speakers are discussing and trying to recall details from the paper, but they do not come to a clear conclusion.&#10;&#10;To summarize, there are multiple histograms related to different frequency bins or bands, with potentially one histogram per frequency bin or band, but the exact number is not specified in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Professor B&#10;Content: So he just has it all sitting there . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah . Um {disfmarker} Mm - hmm .&#10;Speaker: Professor B&#10;Content: So if he 'll {disfmarker} he might work on improving the noise estimate or on {vocalsound} some histogram things , or {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . I just saw the Eurospeech {disfmarker} We {disfmarker} we didn't talk about it at our meeting but I just saw the {disfmarker} just read the paper . Someone , I forget the name , {comment} and {disfmarker} and Ney , uh , about histogram equalization ? Did you see that one ?&#10;Speaker: PhD D&#10;Content: Um , it was a poster . Or {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . I mean , I just read the paper .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content:" target="1. Doctor D is planning to go to OGI (Oregon Health &amp; Science University) not next week, but maybe the week after that.&#10;2. Yes, both Birger and Michael Kleinschmidt (Michael is referred to as just Michael in the transcript) are planning to join the meeting two weeks from the date of the current meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Professor B&#10;Content: So he just has it all sitting there . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah . Um {disfmarker} Mm - hmm .&#10;Speaker: Professor B&#10;Content: So if he 'll {disfmarker} he might work on improving the noise estimate or on {vocalsound} some histogram things , or {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . I just saw the Eurospeech {disfmarker} We {disfmarker} we didn't talk about it at our meeting but I just saw the {disfmarker} just read the paper . Someone , I forget the name , {comment} and {disfmarker} and Ney , uh , about histogram equalization ? Did you see that one ?&#10;Speaker: PhD D&#10;Content: Um , it was a poster . Or {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . I mean , I just read the paper .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content:" target="1. The visualization being discussed here is a histogram, which is a type of bar graph used to display the distribution of data by showing the number of data points that fall within specified ranges or &quot;bins.&quot; In this context, the histograms are related to different frequency bins or bands in the field of signal processing.&#10;2. The exact type of histogram (single or multiple) is not explicitly stated in the transcript provided, but there are suggestions that it could be either one histogram per frequency bin or band or multiple separate histograms for different frequency bands. This information is based on the conversation between the participants, who are trying to recall details from the paper they are discussing.">
      <data key="d0">1</data>
    </edge>
    <edge source=" D&#10;Content: Um {disfmarker}&#10;Speaker: Professor B&#10;Content: That might be the key , actually .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Cuz you were really thinking about speech versus nonspeech for that .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: That 's a good point .&#10;Speaker: PhD D&#10;Content: Mmm . Uh . Well , there are other things that {vocalsound} we should do but , {vocalsound} um , {vocalsound} it requires time and {disfmarker} {vocalsound} We have ideas , like {disfmarker} so , these things are like hav having a better VAD . Uh , we have some ideas about that . It would {disfmarker} {vocalsound} probably implies working a little bit on features that are more {vocalsound} suited to a voice activity detection .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Working on the second stream ." target="1. Extracting the pitch from actual speech and putting it back in might help in improving the performance of the model that hears some things better than others. This is because, as per Professor B, removing pitch information makes the audio closer to a one-source system, which could negatively impact the performance. By adding the pitch back, it might be possible to restore some of the lost information and improve the model's ability to distinguish different sound sources or speech components.&#10;2. However, there are potential challenges in implementing this idea. Professor B mentioned that synthesizing speech with added pitch information could introduce other degradations due to the approximation method used. Furthermore, it is unclear how mel-based synthesis would perform compared to the current approach. Therefore, careful experimentation and evaluation would be necessary to determine the effectiveness of this method.&#10;&#10;In summary, adding the extracted pitch back into the speech might improve the model's performance by restoring some lost information; however, it is crucial to consider potential degradations and other factors that could impact the overall outcome.">
      <data key="d0">1</data>
    </edge>
    <edge source=" D&#10;Content: Um {disfmarker}&#10;Speaker: Professor B&#10;Content: That might be the key , actually .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Cuz you were really thinking about speech versus nonspeech for that .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: That 's a good point .&#10;Speaker: PhD D&#10;Content: Mmm . Uh . Well , there are other things that {vocalsound} we should do but , {vocalsound} um , {vocalsound} it requires time and {disfmarker} {vocalsound} We have ideas , like {disfmarker} so , these things are like hav having a better VAD . Uh , we have some ideas about that . It would {disfmarker} {vocalsound} probably implies working a little bit on features that are more {vocalsound} suited to a voice activity detection .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Working on the second stream ." target="Based on the transcript, it is not explicitly stated what specific activity Professor B, Grad E, and PhD A are planning to do that involves &quot;digging for treats like rats.&quot; The phrase appears to be a metaphor or simile used by Professor B to describe their upcoming conference call or meeting. It's possible that the treats refer to some kind of reward or positive outcome associated with the meeting, while the digging for the treats may represent the effort and work required to achieve that outcome. However, without further context or information, it is difficult to determine the exact meaning or nature of this activity.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." target=" I agree . Yeah . There 's the problem .&#10;Speaker: PhD D&#10;Content: So , that 's {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . Then it would work . But {disfmarker} but , I mean , i it 's {disfmarker} {vocalsound} With finite {pause} of those things {disfmarker} I mean , uh , we {disfmarker} we have done experiments where we literally have put waveforms in and {disfmarker} and {disfmarker} and , uh ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: we kept the number of parameters the same and so forth , and it used a lot of training data . And it {disfmarker} and it {disfmarker} it , uh {disfmarker} not infinite but a lot , and then compared to the number parameters {disfmarker} and it {disfmarker} it , uh {disfmarker} it just doesn't do nearly as well . So , anyway the point is that you want to suppress">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." target=": Professor B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Mmm . Uh . Well , it looks strange , but {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . But {disfmarker}&#10;Speaker: PhD D&#10;Content: but it&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Maybe it 's {disfmarker} has something to do to {vocalsound} the fact that {vocalsound} we don't have infinite training data and {disfmarker}&#10;Speaker: Professor B&#10;Content: We don't ?&#10;Speaker: PhD D&#10;Content: Well ! And so {disfmarker} Well , things are not optimal&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: and {disfmarker} Mmm {disfmarker}&#10;Speaker: Grad E&#10;Content: Are you {disfmarker} you were going to say why {disfmarker} what made you {disfmarker} wh what led you to do that .&#10;Speaker: PhD D&#10;Content: Yeah .">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." target="The computational system computes features based on Gabor functions, which are analysis functions with a certain extent in time and frequency. These functions are used to sample the signal in a time-frequency representation, allowing for multi-scale analysis. By optimizing the parameters of these basis functions, the system can extract different possible features that may be relevant for the specific task it is performing. This process involves analyzing the input signal using wavelet basis functions and Gabor functions, which sample the time-frequency plane to capture patterns in the frequency content of the signal. The resulting features can help the neural network learn and generalize from the data more effectively, potentially leading to better performance compared to using raw waveform data.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." target="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." target="1. The histograms in the paper are related to different frequency bins or bands.&#10;2. Based on the conversation, it is suggested that there is &quot;one histogram per frequency bin&quot; or &quot;one per critical band/frequency band.&quot; However, the exact number of histograms per frequency bin or band is not explicitly mentioned in the transcript provided. The speakers are discussing and trying to recall details from the paper, but they do not come to a clear conclusion.&#10;&#10;To summarize, there are multiple histograms related to different frequency bins or bands, with potentially one histogram per frequency bin or band, but the exact number is not specified in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." target="Professionor B and PhD D are discussing the possibility that the key factor for achieving a better outcome could be the integration of multiple sources of knowledge and different features in their process. They suggest this might provide a more accurate or effective system. This idea is based on the observation that relying solely on one source, like the 'VAD' (Voice Activity Detection) method, may not yield optimal results. The potential benefits of combining various methods could lead to improved performance and further collaboration within their team.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." target="The purpose of adjusting a new set of data to resemble the distribution of a previous set, as discussed in the context of creating a histogram of amplitudes or levels of brightness, is to improve the performance of a model or system. This technique is applied in image processing and can be useful for speech processing as well.&#10;&#10;The method involves comparing the histograms of the new and old data. If the new data's histogram does not match the old data's histogram, adjustments are made to the new data so that its histogram becomes more similar to the old data's histogram. This process can be done piece-wise linearly or through some other kind of approximation. The speakers in the conversation mention a piece-wise linear approach and an approach using a power law between points.&#10;&#10;The underlying idea is that by making the new data's distribution (histogram) resemble the old data's distribution, the system can perform better, as demonstrated in the example where a human subject (Stephane) achieved a lower error rate than the system. However, it is essential to note that simply increasing the amount of training data may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." target="1. The different conditions that the speakers are referring to are likely the various noise conditions and mismatched conditions in which the experiment was conducted. They mention that high error rates under these difficult conditions are not surprising, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;2. It is challenging to determine whether the numbers represent accuracy or error rate because the values discussed are not clearly defined as either. When looking at the numbers, it's unclear if a higher value indicates better performance (accuracy) or worse performance (error rate). This confusion arises from the way the numbers are presented or discussed in the conversation, making it difficult for the listeners to interpret the results accurately.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the discussion revolves around comparing the performance of a model when increasing the amount of training data while keeping the number of parameters constant. The speakers conducted an experiment where they put waveforms into the model and found that it did not perform as well as expected. Specifically, a system used in the experiment had a 7% error rate, while a single human subject, Stephane, achieved only a 1% error rate.&#10;&#10;The conversation suggests that having more training data might not directly lead to better performance if the model is not designed or constrained properly. This concept is highlighted when one of the speakers, PhD D, mentioned that &quot;the constraints that matter&quot; play an essential role in solving the problem. In other words, simply increasing the amount of training data alone may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." target="Based on the transcript, the specific knob or scaling factor that should be used to change the scale of likelihoods, not observations, when determining a scaling factor in their process is not explicitly mentioned. Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Professor B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Mmm . Uh . Well , it looks strange , but {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . But {disfmarker}&#10;Speaker: PhD D&#10;Content: but it&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Maybe it 's {disfmarker} has something to do to {vocalsound} the fact that {vocalsound} we don't have infinite training data and {disfmarker}&#10;Speaker: Professor B&#10;Content: We don't ?&#10;Speaker: PhD D&#10;Content: Well ! And so {disfmarker} Well , things are not optimal&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: and {disfmarker} Mmm {disfmarker}&#10;Speaker: Grad E&#10;Content: Are you {disfmarker} you were going to say why {disfmarker} what made you {disfmarker} wh what led you to do that .&#10;Speaker: PhD D&#10;Content: Yeah ." target="Professionor B and PhD D are discussing the possibility that the key factor for achieving a better outcome could be the integration of multiple sources of knowledge and different features in their process. They suggest this might provide a more accurate or effective system. This idea is based on the observation that relying solely on one source, like the 'VAD' (Voice Activity Detection) method, may not yield optimal results. The potential benefits of combining various methods could lead to improved performance and further collaboration within their team.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Professor B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Mmm . Uh . Well , it looks strange , but {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . But {disfmarker}&#10;Speaker: PhD D&#10;Content: but it&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Maybe it 's {disfmarker} has something to do to {vocalsound} the fact that {vocalsound} we don't have infinite training data and {disfmarker}&#10;Speaker: Professor B&#10;Content: We don't ?&#10;Speaker: PhD D&#10;Content: Well ! And so {disfmarker} Well , things are not optimal&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: and {disfmarker} Mmm {disfmarker}&#10;Speaker: Grad E&#10;Content: Are you {disfmarker} you were going to say why {disfmarker} what made you {disfmarker} wh what led you to do that .&#10;Speaker: PhD D&#10;Content: Yeah ." target="Based on the transcript, it is not explicitly stated what specific activity Professor B, Grad E, and PhD A are planning to do that involves &quot;digging for treats like rats.&quot; The phrase appears to be a metaphor or simile used by Professor B to describe their upcoming conference call or meeting. It's possible that the treats refer to some kind of reward or positive outcome associated with the meeting, while the digging for the treats may represent the effort and work required to achieve that outcome. However, without further context or information, it is difficult to determine the exact meaning or nature of this activity.">
      <data key="d0">1</data>
    </edge>
    <edge source="The important patterns that hidden units in a neural network learn for discriminating between phone classes are patterns related to phone classes or potentially intermediate categories. These patterns can be examined to gain insights into the internal workings of the neural network and possibly identify intermediate categories. However, it is not explicitly stated if this method can definitively lead to discovering new intermediate categories in the given transcripts. Researchers are exploring various data-driven methods to find a set of intermediate categories that can help improve speech recognition, including using hidden patterns learned by neural networks." target=" {disfmarker} {vocalsound} hidden units . And each of these hidden units is {disfmarker} {vocalsound} um , is learning some sort of pattern . And so , um , what {disfmarker} what are these patterns ?&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: I don't know . Um , and I 'm gonna to try to {disfmarker} {vocalsound} to look at those patterns {vocalsound} to {disfmarker} to see , {vocalsound} um , {vocalsound} from those patterns {disfmarker} uh , presumably those are important patterns for discriminating between phone classes . And maybe {disfmarker} {vocalsound} maybe some , uh , intermediate categories can come from {vocalsound} just looking at the patterns of {disfmarker} {vocalsound} um , that the neural net learns .&#10;Speaker: Professor B&#10;Content: Be - before you get on the next part l let me just point out that s there 's {disfmarker} there 's a {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="The important patterns that hidden units in a neural network learn for discriminating between phone classes are patterns related to phone classes or potentially intermediate categories. These patterns can be examined to gain insights into the internal workings of the neural network and possibly identify intermediate categories. However, it is not explicitly stated if this method can definitively lead to discovering new intermediate categories in the given transcripts. Researchers are exploring various data-driven methods to find a set of intermediate categories that can help improve speech recognition, including using hidden patterns learned by neural networks." target=" PhD D&#10;Content: Yeah . So there is the tandem network that e e e estimates the phone probabilities&#10;Speaker: Professor B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD D&#10;Content: and the silence probabilities also .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: And {vocalsound} things get better when , instead of using the silence probability computed by the tandem network , we use the silence probability , uh , given by the VAD network ,&#10;Speaker: Professor B&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: um ,&#10;Speaker: Professor B&#10;Content: The VAD network is {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Which is smaller , but maybe , um {disfmarker} So we have a network for the VAD which has one hundred hidden units , and the tandem network has five hundred . Um . So it 's smaller but th the silence probability {pause} from this network seems , uh , better .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Mmm . Uh . Well , it looks strange , but {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="The important patterns that hidden units in a neural network learn for discriminating between phone classes are patterns related to phone classes or potentially intermediate categories. These patterns can be examined to gain insights into the internal workings of the neural network and possibly identify intermediate categories. However, it is not explicitly stated if this method can definitively lead to discovering new intermediate categories in the given transcripts. Researchers are exploring various data-driven methods to find a set of intermediate categories that can help improve speech recognition, including using hidden patterns learned by neural networks." target=" .&#10;Speaker: Grad C&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: Mm - hmm . Um . Yeah , so {disfmarker} so that 's the {disfmarker} that 's the first part {disfmarker} uh , one {disfmarker} one of the ideas to get at some {disfmarker} {vocalsound} some patterns of intermediate categories . Um , {vocalsound} the other one {pause} was , {vocalsound} um , to , {vocalsound} uh , come up with a {disfmarker} a {disfmarker} a model {disfmarker} {comment} um , a graphical model , {vocalsound} that treats {pause} the intermediate categories {vocalsound} as hidden {disfmarker} hidden variables , latent variables , that we don't know anything about , but that through , {vocalsound} um , s statistical training and the EM algorithm , {vocalsound} um , at the end of the day , {vocalsound} we have , um {disfmarker} we have learned something about these {d">
      <data key="d0">1</data>
    </edge>
    <edge source="The important patterns that hidden units in a neural network learn for discriminating between phone classes are patterns related to phone classes or potentially intermediate categories. These patterns can be examined to gain insights into the internal workings of the neural network and possibly identify intermediate categories. However, it is not explicitly stated if this method can definitively lead to discovering new intermediate categories in the given transcripts. Researchers are exploring various data-driven methods to find a set of intermediate categories that can help improve speech recognition, including using hidden patterns learned by neural networks." target="marker} it , uh {disfmarker} it just doesn't do nearly as well . So , anyway the point is that you want to suppress {disfmarker}&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: it 's not just having the maximum information , you want to suppress , {vocalsound} uh , the aspects of the input signal that are not helpful for {disfmarker} for the discrimination you 're trying to make . So . So maybe just briefly , uh {disfmarker}&#10;Speaker: Grad E&#10;Content: Well , that sort of segues into {pause} what {disfmarker} what I 'm doing .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Um , {vocalsound} so , uh , the big picture is k um , {vocalsound} come up with a set of , {vocalsound} uh , intermediate categories , then build intermediate category classifiers , then do recognition , and , um , improve speech recognition in that way . Um , so right now I 'm in {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="The important patterns that hidden units in a neural network learn for discriminating between phone classes are patterns related to phone classes or potentially intermediate categories. These patterns can be examined to gain insights into the internal workings of the neural network and possibly identify intermediate categories. However, it is not explicitly stated if this method can definitively lead to discovering new intermediate categories in the given transcripts. Researchers are exploring various data-driven methods to find a set of intermediate categories that can help improve speech recognition, including using hidden patterns learned by neural networks." target="vocalsound} and each of the hidden units is learning some sort of {disfmarker} some sort of {disfmarker} some sort of pattern . Right ? And it could be , like {disfmarker} {vocalsound} like these , um {disfmarker} these auditory patterns that Michael {pause} is looking at . And then when you 're looking at the {disfmarker} {vocalsound} the , uh , {pause} um , {vocalsound} the best features , {vocalsound} you know , you can take out {disfmarker} you can do the {disfmarker} do this , uh , brain surgery by taking out , {vocalsound} um , hidden units that don't really help at all .&#10;Speaker: Professor B&#10;Content: Mm - hmm . Or the {disfmarker} or features .&#10;Speaker: Grad E&#10;Content: And this is k sorta like {disfmarker}&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean , y actually , you">
      <data key="d0">1</data>
    </edge>
    <edge source="The important patterns that hidden units in a neural network learn for discriminating between phone classes are patterns related to phone classes or potentially intermediate categories. These patterns can be examined to gain insights into the internal workings of the neural network and possibly identify intermediate categories. However, it is not explicitly stated if this method can definitively lead to discovering new intermediate categories in the given transcripts. Researchers are exploring various data-driven methods to find a set of intermediate categories that can help improve speech recognition, including using hidden patterns learned by neural networks." target="1. The decision of whether to use noise or periodic pulses in speech synthesis is determined by the nature of the sound being synthesized: unvoiced sounds are produced using noise, while voiced sounds are generated using periodic pulses.&#10;2. Yes, it is possible to detect strong periodic components for voiced sound generation in speech synthesis. This can be achieved through LPC (Linear Predictive Coding) re-synthesis, where the presence of strong periodic components indicates that a voiced sound should be used, while the absence of such components suggests an unvoiced sound, which would require noise for synthesis.&#10;&#10;In summary, analyzing the signal to detect periodic components helps determine whether to use noise or periodic pulses in speech synthesis. This information can then be utilized to create more accurate and natural-sounding synthetic speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="The computational system computes features based on Gabor functions, which are analysis functions with a certain extent in time and frequency. These functions are used to sample the signal in a time-frequency representation, allowing for multi-scale analysis. By optimizing the parameters of these basis functions, the system can extract different possible features that may be relevant for the specific task it is performing. This process involves analyzing the input signal using wavelet basis functions and Gabor functions, which sample the time-frequency plane to capture patterns in the frequency content of the signal. The resulting features can help the neural network learn and generalize from the data more effectively, potentially leading to better performance compared to using raw waveform data." target=" It 's , um {disfmarker} {vocalsound} {vocalsound} it computes features which are , {vocalsound} um , {vocalsound} based on {disfmarker} sort of like based on diffe different w um , wavelet basis functions {vocalsound} used to analyze {vocalsound} the input .  So th he uses analysis functions called {vocalsound} Gabor functions , um , {vocalsound} which have a certain {vocalsound} extent , um , {vocalsound} in time and in frequency . And {vocalsound} the idea is these are used to sample , {vocalsound} um , the signal in a represented as a time - frequency representation . So you 're {pause} sampling some piece of this time - frequency plane . And , um , {vocalsound} that , {vocalsound} um , is {disfmarker} is interesting , cuz , {vocalsound} @ @ for {disfmarker} for one thing , you could use it , {vocalsound} um , in a {disfmarker} a multi - scale way . You could have">
      <data key="d0">1</data>
    </edge>
    <edge source="The computational system computes features based on Gabor functions, which are analysis functions with a certain extent in time and frequency. These functions are used to sample the signal in a time-frequency representation, allowing for multi-scale analysis. By optimizing the parameters of these basis functions, the system can extract different possible features that may be relevant for the specific task it is performing. This process involves analyzing the input signal using wavelet basis functions and Gabor functions, which sample the time-frequency plane to capture patterns in the frequency content of the signal. The resulting features can help the neural network learn and generalize from the data more effectively, potentially leading to better performance compared to using raw waveform data." target="&#10;Content: Right ?&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean , y actually , you make me think a {disfmarker} a very important point here is that , um , {vocalsound} if we a again try to look at how is this different from what we 're already doing , {vocalsound} uh , there 's a {disfmarker} a , uh {disfmarker} {vocalsound} a nasty argument that could be made th that it 's {disfmarker} it 's not different at {disfmarker} at all , because , uh {disfmarker} if you ignore the {disfmarker} the selection part because we are going into a {disfmarker} a very powerful , {vocalsound} uh , nonlinearity that , uh , in fact is combining over time and frequency , and is coming up with its own {disfmarker} you know , better than Gabor functions its , you know , neural net functions ,&#10;Speaker: Grad E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="The computational system computes features based on Gabor functions, which are analysis functions with a certain extent in time and frequency. These functions are used to sample the signal in a time-frequency representation, allowing for multi-scale analysis. By optimizing the parameters of these basis functions, the system can extract different possible features that may be relevant for the specific task it is performing. This process involves analyzing the input signal using wavelet basis functions and Gabor functions, which sample the time-frequency plane to capture patterns in the frequency content of the signal. The resulting features can help the neural network learn and generalize from the data more effectively, potentially leading to better performance compared to using raw waveform data." target=" one thing , you could use it , {vocalsound} um , in a {disfmarker} a multi - scale way . You could have these {disfmarker} instead of having everything {disfmarker} like we use a twenty - five millisecond or so analysis window , {vocalsound} typically , um , and that 's our time scale for features , but you could {disfmarker} {vocalsound} using this , um , basis function idea , you could have some basis functions which have a lot longer time scale and , um , some which have a lot shorter , and {vocalsound} so it would be like {pause} a set of multi - scale features . So he 's interested in , um {disfmarker} Th - this is {disfmarker} because it 's , um {disfmarker} there are these different parameters for the shape of these {vocalsound} basis functions , {vocalsound} um {disfmarker} {vocalsound} there are a lot of different possible basis functions . And so he {disfmarker} {vocalsound} he actually does {vocalsound} an optimization">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The Aurora group had a morning conference call to discuss and make a decision about a latency problem. The call lasted for around two hours, and they decided to reduce the current latency of 230 to 220, which is an eighty-millisecond reduction.&#10;2. They discussed the use of noise in the system's training data, which might be similar to the noises used for testing. This commonality could potentially lead to controversy, as some participants might argue that it provides an unfair advantage during testing. Suggestions were made to replace the current noise with different noise samples or add other noise types to maintain a fair testing environment.&#10;3. There was no clear information about the Wall Street Journal part of the test, such as how it would be run or if there would be any specific arrangements for running the systems at certain sites. Some participants mentioned that some sites might not have sufficient compute resources for running the Wall Street Journal tests, and discussions took place regarding having Mississippi State handle the testing for those sites instead.&#10;4. A LVCSR (Large Vocabulary Continuous Speech Recognition) system was downloaded from Mississippi State, along with their software and scripts to make it easier to run on Wall Street Journal data. The whole system description was obtained, though there was uncertainty regarding whether it's an open-source or proprietary system, as well as how its features are combined (Gaussians, clustering, etc.).&#10;5. There is no concrete timeline for when the revised system with a 220 latency should be available; it is expected to be ready in approximately six weeks." target="aker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: That 's good .&#10;Speaker: PhD D&#10;Content: Um , Yeah . Well , {vocalsound} maybe we can start with this . Mmm .&#10;Speaker: Professor B&#10;Content: All today , huh ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: Um . Yeah . So there was this conference call this morning , um , and the only topic on the agenda was just to discuss a and to come at {disfmarker} uh , to get a decision about this latency problem .&#10;Speaker: Professor B&#10;Content: No , this {disfmarker} I 'm sorry , this is a conference call between different Aurora people or just {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Uh , yeah . It 's the conference call between the Aurora , {vocalsound} uh , group .&#10;Speaker: Professor B&#10;Content: It 's the main conference call . OK .&#10;Speaker: PhD D&#10;Content: Uh , yeah . There were like">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The Aurora group had a morning conference call to discuss and make a decision about a latency problem. The call lasted for around two hours, and they decided to reduce the current latency of 230 to 220, which is an eighty-millisecond reduction.&#10;2. They discussed the use of noise in the system's training data, which might be similar to the noises used for testing. This commonality could potentially lead to controversy, as some participants might argue that it provides an unfair advantage during testing. Suggestions were made to replace the current noise with different noise samples or add other noise types to maintain a fair testing environment.&#10;3. There was no clear information about the Wall Street Journal part of the test, such as how it would be run or if there would be any specific arrangements for running the systems at certain sites. Some participants mentioned that some sites might not have sufficient compute resources for running the Wall Street Journal tests, and discussions took place regarding having Mississippi State handle the testing for those sites instead.&#10;4. A LVCSR (Large Vocabulary Continuous Speech Recognition) system was downloaded from Mississippi State, along with their software and scripts to make it easier to run on Wall Street Journal data. The whole system description was obtained, though there was uncertainty regarding whether it's an open-source or proprietary system, as well as how its features are combined (Gaussians, clustering, etc.).&#10;5. There is no concrete timeline for when the revised system with a 220 latency should be available; it is expected to be ready in approximately six weeks." target="s not actually usable in a commercial system with a full telephone number or something .&#10;Speaker: PhD D&#10;Content: Uh - huh . Yeah . At these noise levels .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: Well , yeah . These numbers , I mean . Mmm .&#10;Speaker: Professor B&#10;Content: Good . Um , while we 're still on Aurora stuff {pause} maybe you can talk a little about the status with the , uh , {vocalsound} Wall Street Journal {vocalsound} things for it .&#10;Speaker: PhD A&#10;Content: So I 've , um , downloaded , uh , a couple of things from Mississippi State . Um , one is their {vocalsound} software {disfmarker} their , uh , LVCSR system . Downloaded the latest version of that . Got it compiled and everything . Um , downloaded the scripts . They wrote some scripts that sort of make it easy to run {vocalsound} the system on the Wall Street Journal , uh , data . Um">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The Aurora group had a morning conference call to discuss and make a decision about a latency problem. The call lasted for around two hours, and they decided to reduce the current latency of 230 to 220, which is an eighty-millisecond reduction.&#10;2. They discussed the use of noise in the system's training data, which might be similar to the noises used for testing. This commonality could potentially lead to controversy, as some participants might argue that it provides an unfair advantage during testing. Suggestions were made to replace the current noise with different noise samples or add other noise types to maintain a fair testing environment.&#10;3. There was no clear information about the Wall Street Journal part of the test, such as how it would be run or if there would be any specific arrangements for running the systems at certain sites. Some participants mentioned that some sites might not have sufficient compute resources for running the Wall Street Journal tests, and discussions took place regarding having Mississippi State handle the testing for those sites instead.&#10;4. A LVCSR (Large Vocabulary Continuous Speech Recognition) system was downloaded from Mississippi State, along with their software and scripts to make it easier to run on Wall Street Journal data. The whole system description was obtained, though there was uncertainty regarding whether it's an open-source or proprietary system, as well as how its features are combined (Gaussians, clustering, etc.).&#10;5. There is no concrete timeline for when the revised system with a 220 latency should be available; it is expected to be ready in approximately six weeks." target=" share across everything ? Or {disfmarker} {vocalsound} or if it 's {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Yeah , th I have {disfmarker} I {disfmarker} I {disfmarker} I don't have it up here but I have a {disfmarker} {pause} the whole system description , that describes exactly what their {pause} system is&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: and I {disfmarker} I 'm not sure . But , um {disfmarker}&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: It 's some kind of a mixture of Gaussians and , {vocalsound} uh , clustering and , uh {disfmarker} They 're {disfmarker} they 're trying to put in sort of all of the standard features that people use nowadays .&#10;Speaker: Grad E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So the other , uh , Aurora thing maybe is {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The Aurora group had a morning conference call to discuss and make a decision about a latency problem. The call lasted for around two hours, and they decided to reduce the current latency of 230 to 220, which is an eighty-millisecond reduction.&#10;2. They discussed the use of noise in the system's training data, which might be similar to the noises used for testing. This commonality could potentially lead to controversy, as some participants might argue that it provides an unfair advantage during testing. Suggestions were made to replace the current noise with different noise samples or add other noise types to maintain a fair testing environment.&#10;3. There was no clear information about the Wall Street Journal part of the test, such as how it would be run or if there would be any specific arrangements for running the systems at certain sites. Some participants mentioned that some sites might not have sufficient compute resources for running the Wall Street Journal tests, and discussions took place regarding having Mississippi State handle the testing for those sites instead.&#10;4. A LVCSR (Large Vocabulary Continuous Speech Recognition) system was downloaded from Mississippi State, along with their software and scripts to make it easier to run on Wall Street Journal data. The whole system description was obtained, though there was uncertainty regarding whether it's an open-source or proprietary system, as well as how its features are combined (Gaussians, clustering, etc.).&#10;5. There is no concrete timeline for when the revised system with a 220 latency should be available; it is expected to be ready in approximately six weeks." target=": Professor B&#10;Content: It 's the main conference call . OK .&#10;Speaker: PhD D&#10;Content: Uh , yeah . There were like two hours of {pause} discussions , and then suddenly , {vocalsound} uh , people were tired , I guess , and they decided on {nonvocalsound} a number , two hundred and twenty , um , included e including everything . Uh , it means that it 's like eighty milliseconds {pause} less than before .&#10;Speaker: Professor B&#10;Content: And what are we sitting at currently ?&#10;Speaker: PhD D&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: So , currently d uh , we have system that has two hundred and thirty . So , that 's fine .&#10;Speaker: Professor B&#10;Content: Two thirty .&#10;Speaker: PhD D&#10;Content: Yeah . So that 's the system that 's described on the second point of {pause} this {vocalsound} document .&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker} we have to reduce it by ten milliseconds somehow .&#10;Speaker: PhD D">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The Aurora group had a morning conference call to discuss and make a decision about a latency problem. The call lasted for around two hours, and they decided to reduce the current latency of 230 to 220, which is an eighty-millisecond reduction.&#10;2. They discussed the use of noise in the system's training data, which might be similar to the noises used for testing. This commonality could potentially lead to controversy, as some participants might argue that it provides an unfair advantage during testing. Suggestions were made to replace the current noise with different noise samples or add other noise types to maintain a fair testing environment.&#10;3. There was no clear information about the Wall Street Journal part of the test, such as how it would be run or if there would be any specific arrangements for running the systems at certain sites. Some participants mentioned that some sites might not have sufficient compute resources for running the Wall Street Journal tests, and discussions took place regarding having Mississippi State handle the testing for those sites instead.&#10;4. A LVCSR (Large Vocabulary Continuous Speech Recognition) system was downloaded from Mississippi State, along with their software and scripts to make it easier to run on Wall Street Journal data. The whole system description was obtained, though there was uncertainty regarding whether it's an open-source or proprietary system, as well as how its features are combined (Gaussians, clustering, etc.).&#10;5. There is no concrete timeline for when the revised system with a 220 latency should be available; it is expected to be ready in approximately six weeks." target=" will be in six weeks or something . So . Is that about right {pause} you think ?&#10;Speaker: PhD D&#10;Content: Uh , we don't know yet , I {disfmarker} I think .&#10;Speaker: Professor B&#10;Content: Really , we don't know ?&#10;Speaker: PhD D&#10;Content: Uh - huh . Um .&#10;Speaker: PhD A&#10;Content: It wasn't on the conference call this morning ?&#10;Speaker: Professor B&#10;Content: Hmm .&#10;Speaker: PhD D&#10;Content: No .&#10;Speaker: PhD A&#10;Content: Hmm . Did they say anything on the conference call {pause} about , um , how the {pause} Wall Street Journal part of the test was going to be {pause} run ? Because I {disfmarker} I thought I remembered hearing that some sites {vocalsound} were saying that they didn't have the compute to be able to run the Wall Street Journal stuff at their place ,&#10;Speaker: PhD D&#10;Content: No . Mmm .&#10;Speaker: PhD A&#10;Content: so there was some talk about having Mississippi State run {pause} the systems for them . And I {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The Aurora group had a morning conference call to discuss and make a decision about a latency problem. The call lasted for around two hours, and they decided to reduce the current latency of 230 to 220, which is an eighty-millisecond reduction.&#10;2. They discussed the use of noise in the system's training data, which might be similar to the noises used for testing. This commonality could potentially lead to controversy, as some participants might argue that it provides an unfair advantage during testing. Suggestions were made to replace the current noise with different noise samples or add other noise types to maintain a fair testing environment.&#10;3. There was no clear information about the Wall Street Journal part of the test, such as how it would be run or if there would be any specific arrangements for running the systems at certain sites. Some participants mentioned that some sites might not have sufficient compute resources for running the Wall Street Journal tests, and discussions took place regarding having Mississippi State handle the testing for those sites instead.&#10;4. A LVCSR (Large Vocabulary Continuous Speech Recognition) system was downloaded from Mississippi State, along with their software and scripts to make it easier to run on Wall Street Journal data. The whole system description was obtained, though there was uncertainty regarding whether it's an open-source or proprietary system, as well as how its features are combined (Gaussians, clustering, etc.).&#10;5. There is no concrete timeline for when the revised system with a 220 latency should be available; it is expected to be ready in approximately six weeks." target="disfmarker} uh , the training data for the t tandem network , right now , is like {disfmarker} i is using the noises from the Aurora task and {vocalsound} {vocalsound} I think that people might , {vocalsound} um , try to argue about that because {vocalsound} then in some cases we have the same noises in {disfmarker} for training the network {pause} than the noises that are used for testing ,&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: and {disfmarker} So we have t n uh , to try to get rid of these {disfmarker} {vocalsound} this problem .&#10;Speaker: Professor B&#10;Content: Yeah . Maybe you just put in some other noise , something that 's different .&#10;Speaker: PhD D&#10;Content: Mm - hmm . {vocalsound} Yeah .&#10;Speaker: Professor B&#10;Content: I mean , it {disfmarker} it 's probably helpful to have {disfmarker} have a little noise there . But it may be something else&#10;Speaker: PhD D">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The Aurora group had a morning conference call to discuss and make a decision about a latency problem. The call lasted for around two hours, and they decided to reduce the current latency of 230 to 220, which is an eighty-millisecond reduction.&#10;2. They discussed the use of noise in the system's training data, which might be similar to the noises used for testing. This commonality could potentially lead to controversy, as some participants might argue that it provides an unfair advantage during testing. Suggestions were made to replace the current noise with different noise samples or add other noise types to maintain a fair testing environment.&#10;3. There was no clear information about the Wall Street Journal part of the test, such as how it would be run or if there would be any specific arrangements for running the systems at certain sites. Some participants mentioned that some sites might not have sufficient compute resources for running the Wall Street Journal tests, and discussions took place regarding having Mississippi State handle the testing for those sites instead.&#10;4. A LVCSR (Large Vocabulary Continuous Speech Recognition) system was downloaded from Mississippi State, along with their software and scripts to make it easier to run on Wall Street Journal data. The whole system description was obtained, though there was uncertainty regarding whether it's an open-source or proprietary system, as well as how its features are combined (Gaussians, clustering, etc.).&#10;5. There is no concrete timeline for when the revised system with a 220 latency should be available; it is expected to be ready in approximately six weeks." target="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The Aurora group had a morning conference call to discuss and make a decision about a latency problem. The call lasted for around two hours, and they decided to reduce the current latency of 230 to 220, which is an eighty-millisecond reduction.&#10;2. They discussed the use of noise in the system's training data, which might be similar to the noises used for testing. This commonality could potentially lead to controversy, as some participants might argue that it provides an unfair advantage during testing. Suggestions were made to replace the current noise with different noise samples or add other noise types to maintain a fair testing environment.&#10;3. There was no clear information about the Wall Street Journal part of the test, such as how it would be run or if there would be any specific arrangements for running the systems at certain sites. Some participants mentioned that some sites might not have sufficient compute resources for running the Wall Street Journal tests, and discussions took place regarding having Mississippi State handle the testing for those sites instead.&#10;4. A LVCSR (Large Vocabulary Continuous Speech Recognition) system was downloaded from Mississippi State, along with their software and scripts to make it easier to run on Wall Street Journal data. The whole system description was obtained, though there was uncertainty regarding whether it's an open-source or proprietary system, as well as how its features are combined (Gaussians, clustering, etc.).&#10;5. There is no concrete timeline for when the revised system with a 220 latency should be available; it is expected to be ready in approximately six weeks." target="1. Yes, there was discussion about having Mississippi State run the systems for some sites that might not have sufficient compute resources for running the Wall Street Journal tests. However, this was not decided during today's meeting and is still under consideration.&#10;2. Based on the conversation, it seems that updates about the large-vocabulary task were not shared in the most recent emails, as one of the participants mentioned not having read them and being unsure if the system from Mississippi State is open-source or proprietary.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: That 's good .&#10;Speaker: PhD D&#10;Content: Um , Yeah . Well , {vocalsound} maybe we can start with this . Mmm .&#10;Speaker: Professor B&#10;Content: All today , huh ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: Um . Yeah . So there was this conference call this morning , um , and the only topic on the agenda was just to discuss a and to come at {disfmarker} uh , to get a decision about this latency problem .&#10;Speaker: Professor B&#10;Content: No , this {disfmarker} I 'm sorry , this is a conference call between different Aurora people or just {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Uh , yeah . It 's the conference call between the Aurora , {vocalsound} uh , group .&#10;Speaker: Professor B&#10;Content: It 's the main conference call . OK .&#10;Speaker: PhD D&#10;Content: Uh , yeah . There were like" target="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: That 's good .&#10;Speaker: PhD D&#10;Content: Um , Yeah . Well , {vocalsound} maybe we can start with this . Mmm .&#10;Speaker: Professor B&#10;Content: All today , huh ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: Um . Yeah . So there was this conference call this morning , um , and the only topic on the agenda was just to discuss a and to come at {disfmarker} uh , to get a decision about this latency problem .&#10;Speaker: Professor B&#10;Content: No , this {disfmarker} I 'm sorry , this is a conference call between different Aurora people or just {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Uh , yeah . It 's the conference call between the Aurora , {vocalsound} uh , group .&#10;Speaker: Professor B&#10;Content: It 's the main conference call . OK .&#10;Speaker: PhD D&#10;Content: Uh , yeah . There were like" target="1. Doctor D is planning to go to OGI (Oregon Health &amp; Science University) not next week, but maybe the week after that.&#10;2. Yes, both Birger and Michael Kleinschmidt (Michael is referred to as just Michael in the transcript) are planning to join the meeting two weeks from the date of the current meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Professor B&#10;Content: It 's the main conference call . OK .&#10;Speaker: PhD D&#10;Content: Uh , yeah . There were like two hours of {pause} discussions , and then suddenly , {vocalsound} uh , people were tired , I guess , and they decided on {nonvocalsound} a number , two hundred and twenty , um , included e including everything . Uh , it means that it 's like eighty milliseconds {pause} less than before .&#10;Speaker: Professor B&#10;Content: And what are we sitting at currently ?&#10;Speaker: PhD D&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: So , currently d uh , we have system that has two hundred and thirty . So , that 's fine .&#10;Speaker: Professor B&#10;Content: Two thirty .&#10;Speaker: PhD D&#10;Content: Yeah . So that 's the system that 's described on the second point of {pause} this {vocalsound} document .&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker} we have to reduce it by ten milliseconds somehow .&#10;Speaker: PhD D" target="1. Doctor D is planning to go to OGI (Oregon Health &amp; Science University) not next week, but maybe the week after that.&#10;2. Yes, both Birger and Michael Kleinschmidt (Michael is referred to as just Michael in the transcript) are planning to join the meeting two weeks from the date of the current meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Professor B&#10;Content: It 's the main conference call . OK .&#10;Speaker: PhD D&#10;Content: Uh , yeah . There were like two hours of {pause} discussions , and then suddenly , {vocalsound} uh , people were tired , I guess , and they decided on {nonvocalsound} a number , two hundred and twenty , um , included e including everything . Uh , it means that it 's like eighty milliseconds {pause} less than before .&#10;Speaker: Professor B&#10;Content: And what are we sitting at currently ?&#10;Speaker: PhD D&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: So , currently d uh , we have system that has two hundred and thirty . So , that 's fine .&#10;Speaker: Professor B&#10;Content: Two thirty .&#10;Speaker: PhD D&#10;Content: Yeah . So that 's the system that 's described on the second point of {pause} this {vocalsound} document .&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker} we have to reduce it by ten milliseconds somehow .&#10;Speaker: PhD D" target="Based on the transcript, it is not explicitly stated what specific activity Professor B, Grad E, and PhD A are planning to do that involves &quot;digging for treats like rats.&quot; The phrase appears to be a metaphor or simile used by Professor B to describe their upcoming conference call or meeting. It's possible that the treats refer to some kind of reward or positive outcome associated with the meeting, while the digging for the treats may represent the effort and work required to achieve that outcome. However, without further context or information, it is difficult to determine the exact meaning or nature of this activity.">
      <data key="d0">1</data>
    </edge>
    <edge source=" will be in six weeks or something . So . Is that about right {pause} you think ?&#10;Speaker: PhD D&#10;Content: Uh , we don't know yet , I {disfmarker} I think .&#10;Speaker: Professor B&#10;Content: Really , we don't know ?&#10;Speaker: PhD D&#10;Content: Uh - huh . Um .&#10;Speaker: PhD A&#10;Content: It wasn't on the conference call this morning ?&#10;Speaker: Professor B&#10;Content: Hmm .&#10;Speaker: PhD D&#10;Content: No .&#10;Speaker: PhD A&#10;Content: Hmm . Did they say anything on the conference call {pause} about , um , how the {pause} Wall Street Journal part of the test was going to be {pause} run ? Because I {disfmarker} I thought I remembered hearing that some sites {vocalsound} were saying that they didn't have the compute to be able to run the Wall Street Journal stuff at their place ,&#10;Speaker: PhD D&#10;Content: No . Mmm .&#10;Speaker: PhD A&#10;Content: so there was some talk about having Mississippi State run {pause} the systems for them . And I {disf" target="1. Yes, there was discussion about having Mississippi State run the systems for some sites that might not have sufficient compute resources for running the Wall Street Journal tests. However, this was not decided during today's meeting and is still under consideration.&#10;2. Based on the conversation, it seems that updates about the large-vocabulary task were not shared in the most recent emails, as one of the participants mentioned not having read them and being unsure if the system from Mississippi State is open-source or proprietary.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance." target="&#10;Speaker: PhD A&#10;Content: Well , see , I was a little confused because on this table , I 'm {disfmarker} the they 're showing word error rate . But on this one , I {disfmarker} I don't know if these are word error rates because they 're really big . So , {vocalsound} under condition one here it 's ten percent . Then under three it goes to sixty - four point six percent .&#10;Speaker: Professor B&#10;Content: Yeah , that 's probably Aurora .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean {disfmarker}&#10;Speaker: PhD A&#10;Content: So m I guess maybe they 're error rates but they 're , uh {disfmarker} they 're really high .&#10;Speaker: Professor B&#10;Content: I {disfmarker} I {disfmarker} I don't find that surpri&#10;Speaker: PhD A&#10;Content: So {disfmarker}&#10;Speaker: Professor B&#10;Content: I mean , we {disfmarker} W what 's {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance." target="Professionor B and PhD D are discussing the possibility that the key factor for achieving a better outcome could be the integration of multiple sources of knowledge and different features in their process. They suggest this might provide a more accurate or effective system. This idea is based on the observation that relying solely on one source, like the 'VAD' (Voice Activity Detection) method, may not yield optimal results. The potential benefits of combining various methods could lead to improved performance and further collaboration within their team.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance." target="1. The speakers are discussing the potential benefits of using log probabilities in their models, as they have a more Gaussian shape than regular probabilities, making them easier to model. They consider whether adjusting the word insertion penalty could have a positive effect on the system's performance.&#10;2. The speakers seem uncertain about whether they can change the word insertion penalty based on the lack of a &quot;ruling&quot; or clear decision. They express concerns about not knowing what the impact would be at the &quot;other end&quot; due to the complexity of their methods.&#10;&#10;In summary, the speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty given their current situation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance." target="The purpose of adjusting a new set of data to resemble the distribution of a previous set, as discussed in the context of creating a histogram of amplitudes or levels of brightness, is to improve the performance of a model or system. This technique is applied in image processing and can be useful for speech processing as well.&#10;&#10;The method involves comparing the histograms of the new and old data. If the new data's histogram does not match the old data's histogram, adjustments are made to the new data so that its histogram becomes more similar to the old data's histogram. This process can be done piece-wise linearly or through some other kind of approximation. The speakers in the conversation mention a piece-wise linear approach and an approach using a power law between points.&#10;&#10;The underlying idea is that by making the new data's distribution (histogram) resemble the old data's distribution, the system can perform better, as demonstrated in the example where a human subject (Stephane) achieved a lower error rate than the system. However, it is essential to note that simply increasing the amount of training data may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance." target="1. The conclusion that the system's performance is close to human performance cannot be drawn based on the LPC-12 method, as pointed out by PhD D. This is because the degradation from LPC-12 synthesis may introduce issues not typically found in human speech due to its approximation method. Furthermore, excitation, which plays a significant role in what the system hears, is not taken into account in the LPC-12 system, making it difficult to accurately assess the system's performance compared to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance." target="Based on the supporting materials provided, making an accurate assessment of a system's performance compared to human performance using the LPC-12 method has limitations. The degradation from LPC-12 synthesis and the absence of excitation consideration in this method can introduce issues that are not typically found in human speech, making it challenging to draw definitive conclusions about the system's accuracy. Therefore, any comparison to the human-recreated LPC twelve spectrum should be approached with caution, as it may not provide a complete or accurate representation of the system's performance relative to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance." target="1. The different conditions that the speakers are referring to are likely the various noise conditions and mismatched conditions in which the experiment was conducted. They mention that high error rates under these difficult conditions are not surprising, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;2. It is challenging to determine whether the numbers represent accuracy or error rate because the values discussed are not clearly defined as either. When looking at the numbers, it's unclear if a higher value indicates better performance (accuracy) or worse performance (error rate). This confusion arises from the way the numbers are presented or discussed in the conversation, making it difficult for the listeners to interpret the results accurately.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significant difference in word error rates being discussed in the transcript refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is substantial and raises concerns about the system's performance.&#10;&#10;2. The high error rates might be due to the software named Aurora, as suggested by Professor B. However, it is essential to note that this conclusion is inferred from the conversation and not explicitly stated. When comparing the higher error rate of sixty-four point six percent to a fifteen to twenty percent range mentioned later in the discussion, it becomes clear that the Aurora software could be contributing to the high error rates. The speakers do not provide specific reasons for the high error rates related to Aurora; they merely imply that Aurora might be responsible based on their experience and familiarity with its performance." target="The significant difference in word error rates being discussed by the speakers refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is concerning and raises questions about the system's performance. The high error rates might be due to the software named Aurora, as suggested by Professor B. They imply that Aurora might be contributing to the high error rates based on their experience and familiarity with its performance, but they do not provide specific reasons for this conclusion.&#10;&#10;The speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty due to the complexity of their methods and not knowing what the impact would be at the &quot;other end.&quot; The different conditions they refer to are likely various noise conditions and mismatched conditions in which the experiment was conducted. High error rates under these difficult conditions are expected, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose. The numbers discussed during the conversation do not make it clear if a higher value indicates better performance (accuracy) or worse performance (error rate), causing confusion for the listeners.&#10;&#10;Lastly, Professor B suggests that adding pitch information extracted from actual speech back into the current input of the system could potentially help restore lost information and improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate. However, synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The histograms in the paper are related to different frequency bins or bands.&#10;2. Based on the conversation, it is suggested that there is &quot;one histogram per frequency bin&quot; or &quot;one per critical band/frequency band.&quot; However, the exact number of histograms per frequency bin or band is not explicitly mentioned in the transcript provided. The speakers are discussing and trying to recall details from the paper, but they do not come to a clear conclusion.&#10;&#10;To summarize, there are multiple histograms related to different frequency bins or bands, with potentially one histogram per frequency bin or band, but the exact number is not specified in the transcript." target=" D&#10;Content: I think they have , yeah , different histograms . I uh {disfmarker} Something like one per {pause} frequency band ,&#10;Speaker: Professor B&#10;Content: One {disfmarker}&#10;Speaker: PhD A&#10;Content: So , one histogram per frequency bin .&#10;Speaker: Professor B&#10;Content: One per critical {disfmarker}&#10;Speaker: PhD D&#10;Content: or {disfmarker} But I did {disfmarker} Yeah , I guess .&#10;Speaker: PhD A&#10;Content: And that 's {disfmarker}&#10;Speaker: PhD D&#10;Content: But I should read the paper . I just went {pause} through the poster quickly ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So th&#10;Speaker: Professor B&#10;Content: And I don't remember whether it was {pause} filter bank things&#10;Speaker: PhD A&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: and I didn't {disfmarker}&#10;Speaker: Professor B&#10;Content: or whether it was FFT bins&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The histograms in the paper are related to different frequency bins or bands.&#10;2. Based on the conversation, it is suggested that there is &quot;one histogram per frequency bin&quot; or &quot;one per critical band/frequency band.&quot; However, the exact number of histograms per frequency bin or band is not explicitly mentioned in the transcript provided. The speakers are discussing and trying to recall details from the paper, but they do not come to a clear conclusion.&#10;&#10;To summarize, there are multiple histograms related to different frequency bins or bands, with potentially one histogram per frequency bin or band, but the exact number is not specified in the transcript." target="} adjusting it for {disfmarker} for a lot of different levels . And then they have s they have some kind of , {vocalsound} uh , {pause} a floor or something ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: so if it gets too low you don't {disfmarker} don't do it .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: And they {disfmarker} they claimed very nice results ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So is this a histogram across different frequency bins ?&#10;Speaker: Professor B&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Or {disfmarker} ?&#10;Speaker: Professor B&#10;Content: Um , I think this i You know , I don't remember that . Do you remember {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I think they have , yeah , different histograms . I uh {disfmarker} Something like one per {pause} frequency band">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The histograms in the paper are related to different frequency bins or bands.&#10;2. Based on the conversation, it is suggested that there is &quot;one histogram per frequency bin&quot; or &quot;one per critical band/frequency band.&quot; However, the exact number of histograms per frequency bin or band is not explicitly mentioned in the transcript provided. The speakers are discussing and trying to recall details from the paper, but they do not come to a clear conclusion.&#10;&#10;To summarize, there are multiple histograms related to different frequency bins or bands, with potentially one histogram per frequency bin or band, but the exact number is not specified in the transcript." target="Content: and I didn't {disfmarker}&#10;Speaker: Professor B&#10;Content: or whether it was FFT bins&#10;Speaker: PhD A&#10;Content: Huh .&#10;Speaker: Professor B&#10;Content: or {disfmarker}&#10;Speaker: PhD A&#10;Content: And {disfmarker} and that {disfmarker} that , um , {pause} histogram represents {pause} the {pause} different energy levels that have been seen at that {pause} frequency ?&#10;Speaker: Professor B&#10;Content: I don't remember that . And how often they {disfmarker} you 've seen them . Yeah .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Yeah . And they do {disfmarker} they said that they could do it for the test {disfmarker} So you don't have to change the training . You just do a measurement over the training . And then , uh , for testing , uh , you can do it for one per utterance . Even relatively short utterances . And they claim it {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The histograms in the paper are related to different frequency bins or bands.&#10;2. Based on the conversation, it is suggested that there is &quot;one histogram per frequency bin&quot; or &quot;one per critical band/frequency band.&quot; However, the exact number of histograms per frequency bin or band is not explicitly mentioned in the transcript provided. The speakers are discussing and trying to recall details from the paper, but they do not come to a clear conclusion.&#10;&#10;To summarize, there are multiple histograms related to different frequency bins or bands, with potentially one histogram per frequency bin or band, but the exact number is not specified in the transcript." target=" that the histogram of the new data looks like the old data .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: You do this kind of {vocalsound} piece - wise linear or , {vocalsound} uh , some kind of piece - wise approximation . They did a {disfmarker} uh one version that was piece - wise linear and another that had a power law thing between them {disfmarker} {vocalsound} between the {pause} points . And , uh , they said they s they sort of see it in a way as s for the speech case {comment} {disfmarker} as being kind of a generalization of spectral subtraction in a way , because , you know , in spectral subtraction you 're trying to {vocalsound} get rid of this excess energy . Uh , you know , it 's not supposed to be there . Uh {disfmarker} {vocalsound} and , uh , this is sort of {pause} {vocalsound} adjusting it for {disfmarker} for a lot of different levels . And then they have s they have some kind of , {vocalsound">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The histograms in the paper are related to different frequency bins or bands.&#10;2. Based on the conversation, it is suggested that there is &quot;one histogram per frequency bin&quot; or &quot;one per critical band/frequency band.&quot; However, the exact number of histograms per frequency bin or band is not explicitly mentioned in the transcript provided. The speakers are discussing and trying to recall details from the paper, but they do not come to a clear conclusion.&#10;&#10;To summarize, there are multiple histograms related to different frequency bins or bands, with potentially one histogram per frequency bin or band, but the exact number is not specified in the transcript." target="1. The speakers are discussing the potential benefits of using log probabilities in their models, as they have a more Gaussian shape than regular probabilities, making them easier to model. They consider whether adjusting the word insertion penalty could have a positive effect on the system's performance.&#10;2. The speakers seem uncertain about whether they can change the word insertion penalty based on the lack of a &quot;ruling&quot; or clear decision. They express concerns about not knowing what the impact would be at the &quot;other end&quot; due to the complexity of their methods.&#10;&#10;In summary, the speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty given their current situation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The histograms in the paper are related to different frequency bins or bands.&#10;2. Based on the conversation, it is suggested that there is &quot;one histogram per frequency bin&quot; or &quot;one per critical band/frequency band.&quot; However, the exact number of histograms per frequency bin or band is not explicitly mentioned in the transcript provided. The speakers are discussing and trying to recall details from the paper, but they do not come to a clear conclusion.&#10;&#10;To summarize, there are multiple histograms related to different frequency bins or bands, with potentially one histogram per frequency bin or band, but the exact number is not specified in the transcript." target="The purpose of adjusting a new set of data to resemble the distribution of a previous set, as discussed in the context of creating a histogram of amplitudes or levels of brightness, is to improve the performance of a model or system. This technique is applied in image processing and can be useful for speech processing as well.&#10;&#10;The method involves comparing the histograms of the new and old data. If the new data's histogram does not match the old data's histogram, adjustments are made to the new data so that its histogram becomes more similar to the old data's histogram. This process can be done piece-wise linearly or through some other kind of approximation. The speakers in the conversation mention a piece-wise linear approach and an approach using a power law between points.&#10;&#10;The underlying idea is that by making the new data's distribution (histogram) resemble the old data's distribution, the system can perform better, as demonstrated in the example where a human subject (Stephane) achieved a lower error rate than the system. However, it is essential to note that simply increasing the amount of training data may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The histograms in the paper are related to different frequency bins or bands.&#10;2. Based on the conversation, it is suggested that there is &quot;one histogram per frequency bin&quot; or &quot;one per critical band/frequency band.&quot; However, the exact number of histograms per frequency bin or band is not explicitly mentioned in the transcript provided. The speakers are discussing and trying to recall details from the paper, but they do not come to a clear conclusion.&#10;&#10;To summarize, there are multiple histograms related to different frequency bins or bands, with potentially one histogram per frequency bin or band, but the exact number is not specified in the transcript." target="1. The visualization being discussed here is a histogram, which is a type of bar graph used to display the distribution of data by showing the number of data points that fall within specified ranges or &quot;bins.&quot; In this context, the histograms are related to different frequency bins or bands in the field of signal processing.&#10;2. The exact type of histogram (single or multiple) is not explicitly stated in the transcript provided, but there are suggestions that it could be either one histogram per frequency bin or band or multiple separate histograms for different frequency bands. This information is based on the conversation between the participants, who are trying to recall details from the paper they are discussing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The histograms in the paper are related to different frequency bins or bands.&#10;2. Based on the conversation, it is suggested that there is &quot;one histogram per frequency bin&quot; or &quot;one per critical band/frequency band.&quot; However, the exact number of histograms per frequency bin or band is not explicitly mentioned in the transcript provided. The speakers are discussing and trying to recall details from the paper, but they do not come to a clear conclusion.&#10;&#10;To summarize, there are multiple histograms related to different frequency bins or bands, with potentially one histogram per frequency bin or band, but the exact number is not specified in the transcript." target="1. The different conditions that the speakers are referring to are likely the various noise conditions and mismatched conditions in which the experiment was conducted. They mention that high error rates under these difficult conditions are not surprising, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;2. It is challenging to determine whether the numbers represent accuracy or error rate because the values discussed are not clearly defined as either. When looking at the numbers, it's unclear if a higher value indicates better performance (accuracy) or worse performance (error rate). This confusion arises from the way the numbers are presented or discussed in the conversation, making it difficult for the listeners to interpret the results accurately.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The histograms in the paper are related to different frequency bins or bands.&#10;2. Based on the conversation, it is suggested that there is &quot;one histogram per frequency bin&quot; or &quot;one per critical band/frequency band.&quot; However, the exact number of histograms per frequency bin or band is not explicitly mentioned in the transcript provided. The speakers are discussing and trying to recall details from the paper, but they do not come to a clear conclusion.&#10;&#10;To summarize, there are multiple histograms related to different frequency bins or bands, with potentially one histogram per frequency bin or band, but the exact number is not specified in the transcript." target="Based on the transcript, the specific knob or scaling factor that should be used to change the scale of likelihoods, not observations, when determining a scaling factor in their process is not explicitly mentioned. Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source=" D&#10;Content: I think they have , yeah , different histograms . I uh {disfmarker} Something like one per {pause} frequency band ,&#10;Speaker: Professor B&#10;Content: One {disfmarker}&#10;Speaker: PhD A&#10;Content: So , one histogram per frequency bin .&#10;Speaker: Professor B&#10;Content: One per critical {disfmarker}&#10;Speaker: PhD D&#10;Content: or {disfmarker} But I did {disfmarker} Yeah , I guess .&#10;Speaker: PhD A&#10;Content: And that 's {disfmarker}&#10;Speaker: PhD D&#10;Content: But I should read the paper . I just went {pause} through the poster quickly ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So th&#10;Speaker: Professor B&#10;Content: And I don't remember whether it was {pause} filter bank things&#10;Speaker: PhD A&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: and I didn't {disfmarker}&#10;Speaker: Professor B&#10;Content: or whether it was FFT bins&#10;Speaker:" target="1. The visualization being discussed here is a histogram, which is a type of bar graph used to display the distribution of data by showing the number of data points that fall within specified ranges or &quot;bins.&quot; In this context, the histograms are related to different frequency bins or bands in the field of signal processing.&#10;2. The exact type of histogram (single or multiple) is not explicitly stated in the transcript provided, but there are suggestions that it could be either one histogram per frequency bin or band or multiple separate histograms for different frequency bands. This information is based on the conversation between the participants, who are trying to recall details from the paper they are discussing.">
      <data key="d0">1</data>
    </edge>
    <edge source="} adjusting it for {disfmarker} for a lot of different levels . And then they have s they have some kind of , {vocalsound} uh , {pause} a floor or something ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: so if it gets too low you don't {disfmarker} don't do it .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: And they {disfmarker} they claimed very nice results ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So is this a histogram across different frequency bins ?&#10;Speaker: Professor B&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Or {disfmarker} ?&#10;Speaker: Professor B&#10;Content: Um , I think this i You know , I don't remember that . Do you remember {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I think they have , yeah , different histograms . I uh {disfmarker} Something like one per {pause} frequency band" target="1. The visualization being discussed here is a histogram, which is a type of bar graph used to display the distribution of data by showing the number of data points that fall within specified ranges or &quot;bins.&quot; In this context, the histograms are related to different frequency bins or bands in the field of signal processing.&#10;2. The exact type of histogram (single or multiple) is not explicitly stated in the transcript provided, but there are suggestions that it could be either one histogram per frequency bin or band or multiple separate histograms for different frequency bands. This information is based on the conversation between the participants, who are trying to recall details from the paper they are discussing.">
      <data key="d0">1</data>
    </edge>
    <edge source="} adjusting it for {disfmarker} for a lot of different levels . And then they have s they have some kind of , {vocalsound} uh , {pause} a floor or something ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: so if it gets too low you don't {disfmarker} don't do it .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: And they {disfmarker} they claimed very nice results ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So is this a histogram across different frequency bins ?&#10;Speaker: Professor B&#10;Content: and {disfmarker}&#10;Speaker: PhD A&#10;Content: Or {disfmarker} ?&#10;Speaker: Professor B&#10;Content: Um , I think this i You know , I don't remember that . Do you remember {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I think they have , yeah , different histograms . I uh {disfmarker} Something like one per {pause} frequency band" target="Based on the transcript, it is not explicitly stated what specific activity Professor B, Grad E, and PhD A are planning to do that involves &quot;digging for treats like rats.&quot; The phrase appears to be a metaphor or simile used by Professor B to describe their upcoming conference call or meeting. It's possible that the treats refer to some kind of reward or positive outcome associated with the meeting, while the digging for the treats may represent the effort and work required to achieve that outcome. However, without further context or information, it is difficult to determine the exact meaning or nature of this activity.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that the histogram of the new data looks like the old data .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: You do this kind of {vocalsound} piece - wise linear or , {vocalsound} uh , some kind of piece - wise approximation . They did a {disfmarker} uh one version that was piece - wise linear and another that had a power law thing between them {disfmarker} {vocalsound} between the {pause} points . And , uh , they said they s they sort of see it in a way as s for the speech case {comment} {disfmarker} as being kind of a generalization of spectral subtraction in a way , because , you know , in spectral subtraction you 're trying to {vocalsound} get rid of this excess energy . Uh , you know , it 's not supposed to be there . Uh {disfmarker} {vocalsound} and , uh , this is sort of {pause} {vocalsound} adjusting it for {disfmarker} for a lot of different levels . And then they have s they have some kind of , {vocalsound" target="The purpose of adjusting a new set of data to resemble the distribution of a previous set, as discussed in the context of creating a histogram of amplitudes or levels of brightness, is to improve the performance of a model or system. This technique is applied in image processing and can be useful for speech processing as well.&#10;&#10;The method involves comparing the histograms of the new and old data. If the new data's histogram does not match the old data's histogram, adjustments are made to the new data so that its histogram becomes more similar to the old data's histogram. This process can be done piece-wise linearly or through some other kind of approximation. The speakers in the conversation mention a piece-wise linear approach and an approach using a power law between points.&#10;&#10;The underlying idea is that by making the new data's distribution (histogram) resemble the old data's distribution, the system can perform better, as demonstrated in the example where a human subject (Stephane) achieved a lower error rate than the system. However, it is essential to note that simply increasing the amount of training data may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that the histogram of the new data looks like the old data .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: You do this kind of {vocalsound} piece - wise linear or , {vocalsound} uh , some kind of piece - wise approximation . They did a {disfmarker} uh one version that was piece - wise linear and another that had a power law thing between them {disfmarker} {vocalsound} between the {pause} points . And , uh , they said they s they sort of see it in a way as s for the speech case {comment} {disfmarker} as being kind of a generalization of spectral subtraction in a way , because , you know , in spectral subtraction you 're trying to {vocalsound} get rid of this excess energy . Uh , you know , it 's not supposed to be there . Uh {disfmarker} {vocalsound} and , uh , this is sort of {pause} {vocalsound} adjusting it for {disfmarker} for a lot of different levels . And then they have s they have some kind of , {vocalsound" target="1. The visualization being discussed here is a histogram, which is a type of bar graph used to display the distribution of data by showing the number of data points that fall within specified ranges or &quot;bins.&quot; In this context, the histograms are related to different frequency bins or bands in the field of signal processing.&#10;2. The exact type of histogram (single or multiple) is not explicitly stated in the transcript provided, but there are suggestions that it could be either one histogram per frequency bin or band or multiple separate histograms for different frequency bands. This information is based on the conversation between the participants, who are trying to recall details from the paper they are discussing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The decision of whether to use noise or periodic pulses in speech synthesis is determined by the nature of the sound being synthesized: unvoiced sounds are produced using noise, while voiced sounds are generated using periodic pulses.&#10;2. Yes, it is possible to detect strong periodic components for voiced sound generation in speech synthesis. This can be achieved through LPC (Linear Predictive Coding) re-synthesis, where the presence of strong periodic components indicates that a voiced sound should be used, while the absence of such components suggests an unvoiced sound, which would require noise for synthesis.&#10;&#10;In summary, analyzing the signal to detect periodic components helps determine whether to use noise or periodic pulses in speech synthesis. This information can then be utilized to create more accurate and natural-sounding synthetic speech." target=" 're talking about .&#10;Speaker: Grad C&#10;Content: OK . So Michael Kleinschmidt , who 's a PHD student from Germany , {vocalsound} showed up this week . He 'll be here for about six months . And he 's done some work using {vocalsound} an auditory model {pause} of , um , {vocalsound} human hearing , and {pause} using that f uh , to generate speech recognition features . And {pause} he did {vocalsound} work back in Germany {vocalsound} with , um , a toy recognition system {vocalsound} using , um , isolated {vocalsound} digit recognition {vocalsound} as the task . It was actually just a single - layer neural network {vocalsound} that classified words {disfmarker} classified digits , {vocalsound} in fact . Um , and {pause} he tried that on {disfmarker} I think on some Aurora data and got results that he thought {pause} seemed respectable . And he w he 's coming here to u u use it on a {vocalsound} uh , a real speech recognition system . So I 'll be working">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The decision of whether to use noise or periodic pulses in speech synthesis is determined by the nature of the sound being synthesized: unvoiced sounds are produced using noise, while voiced sounds are generated using periodic pulses.&#10;2. Yes, it is possible to detect strong periodic components for voiced sound generation in speech synthesis. This can be achieved through LPC (Linear Predictive Coding) re-synthesis, where the presence of strong periodic components indicates that a voiced sound should be used, while the absence of such components suggests an unvoiced sound, which would require noise for synthesis.&#10;&#10;In summary, analyzing the signal to detect periodic components helps determine whether to use noise or periodic pulses in speech synthesis. This information can then be utilized to create more accurate and natural-sounding synthetic speech." target="1. The conclusion that the system's performance is close to human performance cannot be drawn based on the LPC-12 method, as pointed out by PhD D. This is because the degradation from LPC-12 synthesis may introduce issues not typically found in human speech due to its approximation method. Furthermore, excitation, which plays a significant role in what the system hears, is not taken into account in the LPC-12 system, making it difficult to accurately assess the system's performance compared to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The decision of whether to use noise or periodic pulses in speech synthesis is determined by the nature of the sound being synthesized: unvoiced sounds are produced using noise, while voiced sounds are generated using periodic pulses.&#10;2. Yes, it is possible to detect strong periodic components for voiced sound generation in speech synthesis. This can be achieved through LPC (Linear Predictive Coding) re-synthesis, where the presence of strong periodic components indicates that a voiced sound should be used, while the absence of such components suggests an unvoiced sound, which would require noise for synthesis.&#10;&#10;In summary, analyzing the signal to detect periodic components helps determine whether to use noise or periodic pulses in speech synthesis. This information can then be utilized to create more accurate and natural-sounding synthetic speech." target="Based on the supporting materials provided, making an accurate assessment of a system's performance compared to human performance using the LPC-12 method has limitations. The degradation from LPC-12 synthesis and the absence of excitation consideration in this method can introduce issues that are not typically found in human speech, making it challenging to draw definitive conclusions about the system's accuracy. Therefore, any comparison to the human-recreated LPC twelve spectrum should be approached with caution, as it may not provide a complete or accurate representation of the system's performance relative to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The decision of whether to use noise or periodic pulses in speech synthesis is determined by the nature of the sound being synthesized: unvoiced sounds are produced using noise, while voiced sounds are generated using periodic pulses.&#10;2. Yes, it is possible to detect strong periodic components for voiced sound generation in speech synthesis. This can be achieved through LPC (Linear Predictive Coding) re-synthesis, where the presence of strong periodic components indicates that a voiced sound should be used, while the absence of such components suggests an unvoiced sound, which would require noise for synthesis.&#10;&#10;In summary, analyzing the signal to detect periodic components helps determine whether to use noise or periodic pulses in speech synthesis. This information can then be utilized to create more accurate and natural-sounding synthetic speech." target="1. Alternative Approach: Professor B suggests using a two-source system, where one source is noise and the other is periodic pulses, as an alternative to using a voiced &quot;voice thing&quot; when dealing with strong periodic components. This approach allows for more accurate representation of various sound types in speech synthesis.&#10;2. Preference for Two-Source System: Professor B prefers a two-source system over a one-source system because it offers greater flexibility in handling different types of sounds, such as unvoiced and voiced sounds. By utilizing both noise and periodic pulses, the synthetic speech can more closely mimic natural human speech, resulting in improved quality and naturalness.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The decision of whether to use noise or periodic pulses in speech synthesis is determined by the nature of the sound being synthesized: unvoiced sounds are produced using noise, while voiced sounds are generated using periodic pulses.&#10;2. Yes, it is possible to detect strong periodic components for voiced sound generation in speech synthesis. This can be achieved through LPC (Linear Predictive Coding) re-synthesis, where the presence of strong periodic components indicates that a voiced sound should be used, while the absence of such components suggests an unvoiced sound, which would require noise for synthesis.&#10;&#10;In summary, analyzing the signal to detect periodic components helps determine whether to use noise or periodic pulses in speech synthesis. This information can then be utilized to create more accurate and natural-sounding synthetic speech." target="1. The re-synthesized version of speech filtered by an LPC (Linear Predictive Coding) filter may not be immediately recognizable as speech because it is generated using a white noise source that has been filtered, rather than a periodic sound source. This can result in a less natural sounding &quot;whispering&quot; quality to the synthesized speech.&#10;2. The LPC re-synthesis method introduces degradation not typically found in human speech due to its approximation technique, which further complicates the assessment of the system's performance compared to human performance.&#10;3. Excitation, a crucial factor in what the system hears, is not taken into account by the LPC-12 system. This makes it challenging to determine if the system's performance is close to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source=" 're talking about .&#10;Speaker: Grad C&#10;Content: OK . So Michael Kleinschmidt , who 's a PHD student from Germany , {vocalsound} showed up this week . He 'll be here for about six months . And he 's done some work using {vocalsound} an auditory model {pause} of , um , {vocalsound} human hearing , and {pause} using that f uh , to generate speech recognition features . And {pause} he did {vocalsound} work back in Germany {vocalsound} with , um , a toy recognition system {vocalsound} using , um , isolated {vocalsound} digit recognition {vocalsound} as the task . It was actually just a single - layer neural network {vocalsound} that classified words {disfmarker} classified digits , {vocalsound} in fact . Um , and {pause} he tried that on {disfmarker} I think on some Aurora data and got results that he thought {pause} seemed respectable . And he w he 's coming here to u u use it on a {vocalsound} uh , a real speech recognition system . So I 'll be working" target="1. Doctor D is planning to go to OGI (Oregon Health &amp; Science University) not next week, but maybe the week after that.&#10;2. Yes, both Birger and Michael Kleinschmidt (Michael is referred to as just Michael in the transcript) are planning to join the meeting two weeks from the date of the current meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professionor B and PhD D are discussing the possibility that the key factor for achieving a better outcome could be the integration of multiple sources of knowledge and different features in their process. They suggest this might provide a more accurate or effective system. This idea is based on the observation that relying solely on one source, like the 'VAD' (Voice Activity Detection) method, may not yield optimal results. The potential benefits of combining various methods could lead to improved performance and further collaboration within their team." target="aker: Professor B&#10;Content: And this is {disfmarker} and rather than just taking one minus that to get the other , which is essentially what 's happening , you have this other source of knowledge that you 're putting in there . So you make use of both of them {vocalsound} in {disfmarker} in {pause} what you 're ending up with . Maybe it 's better .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Anyway , you can probably justify anything if what 's use&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: And {disfmarker} and the features are different also . I mean , the VAD doesn't use the same features there are .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Oh !&#10;Speaker: PhD D&#10;Content: Um {disfmarker}&#10;Speaker: Professor B&#10;Content: That might be the key , actually .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="Professionor B and PhD D are discussing the possibility that the key factor for achieving a better outcome could be the integration of multiple sources of knowledge and different features in their process. They suggest this might provide a more accurate or effective system. This idea is based on the observation that relying solely on one source, like the 'VAD' (Voice Activity Detection) method, may not yield optimal results. The potential benefits of combining various methods could lead to improved performance and further collaboration within their team." target="Speaker: PhD A&#10;Content: That 's ri&#10;Speaker: Professor B&#10;Content: We do the digits and then we get our treats .&#10;Speaker: Grad E&#10;Content: Oops .&#10;Speaker: PhD A&#10;Content: OK .">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD A&#10;Content: That 's ri&#10;Speaker: Professor B&#10;Content: We do the digits and then we get our treats .&#10;Speaker: Grad E&#10;Content: Oops .&#10;Speaker: PhD A&#10;Content: OK ." target="1. The speakers are discussing the potential benefits of using log probabilities in their models, as they have a more Gaussian shape than regular probabilities, making them easier to model. They consider whether adjusting the word insertion penalty could have a positive effect on the system's performance.&#10;2. The speakers seem uncertain about whether they can change the word insertion penalty based on the lack of a &quot;ruling&quot; or clear decision. They express concerns about not knowing what the impact would be at the &quot;other end&quot; due to the complexity of their methods.&#10;&#10;In summary, the speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty given their current situation.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD A&#10;Content: That 's ri&#10;Speaker: Professor B&#10;Content: We do the digits and then we get our treats .&#10;Speaker: Grad E&#10;Content: Oops .&#10;Speaker: PhD A&#10;Content: OK ." target="Based on the transcript, it is not explicitly stated what specific activity Professor B, Grad E, and PhD A are planning to do that involves &quot;digging for treats like rats.&quot; The phrase appears to be a metaphor or simile used by Professor B to describe their upcoming conference call or meeting. It's possible that the treats refer to some kind of reward or positive outcome associated with the meeting, while the digging for the treats may represent the effort and work required to achieve that outcome. However, without further context or information, it is difficult to determine the exact meaning or nature of this activity.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the potential benefits of using log probabilities in their models, as they have a more Gaussian shape than regular probabilities, making them easier to model. They consider whether adjusting the word insertion penalty could have a positive effect on the system's performance.&#10;2. The speakers seem uncertain about whether they can change the word insertion penalty based on the lack of a &quot;ruling&quot; or clear decision. They express concerns about not knowing what the impact would be at the &quot;other end&quot; due to the complexity of their methods.&#10;&#10;In summary, the speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty given their current situation." target="disfmarker} I mean , they 're not used as probabilities , but the log probabilities {disfmarker} we 're taking advantage of the fact that something like log probabilities has more of a Gaussian shape than Gaus - than {vocalsound} probabilities , and so we can model them better . So , {pause} in a way we 're taking advantage of the fact that they 're probabilities , because they 're this quantity that looks kind of Gaussian when you take it 's log . So , {comment} {vocalsound} uh , maybe {disfmarker} maybe it would have a {disfmarker} a reasonable effect to do that .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: I d I don't know . But , {pause} I mean , I guess we still haven't had a {disfmarker} {vocalsound} a ruling back on this . And we may end up being in a situation where we just you know really can't change the {vocalsound} word insertion penalty . But the other thing we could do {vocalsound} is {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the potential benefits of using log probabilities in their models, as they have a more Gaussian shape than regular probabilities, making them easier to model. They consider whether adjusting the word insertion penalty could have a positive effect on the system's performance.&#10;2. The speakers seem uncertain about whether they can change the word insertion penalty based on the lack of a &quot;ruling&quot; or clear decision. They express concerns about not knowing what the impact would be at the &quot;other end&quot; due to the complexity of their methods.&#10;&#10;In summary, the speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty given their current situation." target=" into the rest of it and then that 's used as observations . So it 's {disfmarker} it 's , {vocalsound} um , another way to do it .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm . But , these values are not directly used as probabilities anyway .&#10;Speaker: Professor B&#10;Content: I know they 're not .&#10;Speaker: PhD D&#10;Content: So there are {disfmarker} there is {disfmarker}&#10;Speaker: Professor B&#10;Content: I know they 're not . But {disfmarker} but , you know {disfmarker} So because what we 're doing is pretty strange and complicated , we don't really know what the effect is {pause} at the other end .&#10;Speaker: PhD D&#10;Content: Uh - huh . Mm - hmm .&#10;Speaker: Professor B&#10;Content: So , {vocalsound} um , {pause} my thought was maybe {disfmarker} I mean , they 're not used as probabilities , but the log probabilities {disfmarker} we 're taking advantage of">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the potential benefits of using log probabilities in their models, as they have a more Gaussian shape than regular probabilities, making them easier to model. They consider whether adjusting the word insertion penalty could have a positive effect on the system's performance.&#10;2. The speakers seem uncertain about whether they can change the word insertion penalty based on the lack of a &quot;ruling&quot; or clear decision. They express concerns about not knowing what the impact would be at the &quot;other end&quot; due to the complexity of their methods.&#10;&#10;In summary, the speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty given their current situation." target="Based on the transcript, the specific knob or scaling factor that should be used to change the scale of likelihoods, not observations, when determining a scaling factor in their process is not explicitly mentioned. Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the potential benefits of using log probabilities in their models, as they have a more Gaussian shape than regular probabilities, making them easier to model. They consider whether adjusting the word insertion penalty could have a positive effect on the system's performance.&#10;2. The speakers seem uncertain about whether they can change the word insertion penalty based on the lack of a &quot;ruling&quot; or clear decision. They express concerns about not knowing what the impact would be at the &quot;other end&quot; due to the complexity of their methods.&#10;&#10;In summary, the speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty given their current situation." target="1. The values being discussed in the transcript are candidate basis functions or features used for classification in an optimization procedure called recursive feature elimination (or a variation of it). In this process, they start with a set of M candidate basis functions and classify using each possible subset of this set, recursively eliminating features that do not contribute to the classification performance until an optimal set is obtained.&#10;   &#10;2. Log probabilities are being utilized in the process despite not using the values as probabilities because log probabilities have a more Gaussian shape than regular probabilities, making them easier to model and work with in complex methods like the ones being used by the speakers. They mentioned the benefits of taking advantage of their Gaussian shape and the fact that these values are not directly used as probabilities but rather their log probabilities.&#10;&#10;In summary, the values in this context refer to candidate basis functions or features used for classification, and log probabilities are being utilized for modeling purposes due to their more Gaussian shape despite not being used directly as probabilities.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing the potential benefits of using log probabilities in their models, as they have a more Gaussian shape than regular probabilities, making them easier to model. They consider whether adjusting the word insertion penalty could have a positive effect on the system's performance.&#10;2. The speakers seem uncertain about whether they can change the word insertion penalty based on the lack of a &quot;ruling&quot; or clear decision. They express concerns about not knowing what the impact would be at the &quot;other end&quot; due to the complexity of their methods.&#10;&#10;In summary, the speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty given their current situation." target="The significant difference in word error rates being discussed by the speakers refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is concerning and raises questions about the system's performance. The high error rates might be due to the software named Aurora, as suggested by Professor B. They imply that Aurora might be contributing to the high error rates based on their experience and familiarity with its performance, but they do not provide specific reasons for this conclusion.&#10;&#10;The speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty due to the complexity of their methods and not knowing what the impact would be at the &quot;other end.&quot; The different conditions they refer to are likely various noise conditions and mismatched conditions in which the experiment was conducted. High error rates under these difficult conditions are expected, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose. The numbers discussed during the conversation do not make it clear if a higher value indicates better performance (accuracy) or worse performance (error rate), causing confusion for the listeners.&#10;&#10;Lastly, Professor B suggests that adding pitch information extracted from actual speech back into the current input of the system could potentially help restore lost information and improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate. However, synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.">
      <data key="d0">1</data>
    </edge>
    <edge source=" into the rest of it and then that 's used as observations . So it 's {disfmarker} it 's , {vocalsound} um , another way to do it .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm . But , these values are not directly used as probabilities anyway .&#10;Speaker: Professor B&#10;Content: I know they 're not .&#10;Speaker: PhD D&#10;Content: So there are {disfmarker} there is {disfmarker}&#10;Speaker: Professor B&#10;Content: I know they 're not . But {disfmarker} but , you know {disfmarker} So because what we 're doing is pretty strange and complicated , we don't really know what the effect is {pause} at the other end .&#10;Speaker: PhD D&#10;Content: Uh - huh . Mm - hmm .&#10;Speaker: Professor B&#10;Content: So , {vocalsound} um , {pause} my thought was maybe {disfmarker} I mean , they 're not used as probabilities , but the log probabilities {disfmarker} we 're taking advantage of" target="1. The values being discussed in the transcript are candidate basis functions or features used for classification in an optimization procedure called recursive feature elimination (or a variation of it). In this process, they start with a set of M candidate basis functions and classify using each possible subset of this set, recursively eliminating features that do not contribute to the classification performance until an optimal set is obtained.&#10;   &#10;2. Log probabilities are being utilized in the process despite not using the values as probabilities because log probabilities have a more Gaussian shape than regular probabilities, making them easier to model and work with in complex methods like the ones being used by the speakers. They mentioned the benefits of taking advantage of their Gaussian shape and the fact that these values are not directly used as probabilities but rather their log probabilities.&#10;&#10;In summary, the values in this context refer to candidate basis functions or features used for classification, and log probabilities are being utilized for modeling purposes due to their more Gaussian shape despite not being used directly as probabilities.">
      <data key="d0">1</data>
    </edge>
    <edge source="The purpose of adjusting a new set of data to resemble the distribution of a previous set, as discussed in the context of creating a histogram of amplitudes or levels of brightness, is to improve the performance of a model or system. This technique is applied in image processing and can be useful for speech processing as well.&#10;&#10;The method involves comparing the histograms of the new and old data. If the new data's histogram does not match the old data's histogram, adjustments are made to the new data so that its histogram becomes more similar to the old data's histogram. This process can be done piece-wise linearly or through some other kind of approximation. The speakers in the conversation mention a piece-wise linear approach and an approach using a power law between points.&#10;&#10;The underlying idea is that by making the new data's distribution (histogram) resemble the old data's distribution, the system can perform better, as demonstrated in the example where a human subject (Stephane) achieved a lower error rate than the system. However, it is essential to note that simply increasing the amount of training data may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." target="isfmarker} you have the distributions from the training set ,&#10;Speaker: PhD D&#10;Content: N&#10;Speaker: Professor B&#10;Content: and then , uh {disfmarker} So this is just a {disfmarker} a histogram of {disfmarker} of {vocalsound} the amplitudes , I guess . Right ? And then {disfmarker} {vocalsound} Um , people do this in image processing some .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: You have this kind of {disfmarker} {vocalsound} of histogram of {disfmarker} of levels of brightness or whatever . And {disfmarker} and {disfmarker} and then , {vocalsound} when you get a new {disfmarker} new thing that you {disfmarker} you want to adjust to be {pause} better in some way , {vocalsound} you adjust it so that the histogram of the new data looks like the old data .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The purpose of adjusting a new set of data to resemble the distribution of a previous set, as discussed in the context of creating a histogram of amplitudes or levels of brightness, is to improve the performance of a model or system. This technique is applied in image processing and can be useful for speech processing as well.&#10;&#10;The method involves comparing the histograms of the new and old data. If the new data's histogram does not match the old data's histogram, adjustments are made to the new data so that its histogram becomes more similar to the old data's histogram. This process can be done piece-wise linearly or through some other kind of approximation. The speakers in the conversation mention a piece-wise linear approach and an approach using a power law between points.&#10;&#10;The underlying idea is that by making the new data's distribution (histogram) resemble the old data's distribution, the system can perform better, as demonstrated in the example where a human subject (Stephane) achieved a lower error rate than the system. However, it is essential to note that simply increasing the amount of training data may not be sufficient for improving the model's performance; proper design, including constraining the model and processing power, is also necessary." target="1. The visualization being discussed here is a histogram, which is a type of bar graph used to display the distribution of data by showing the number of data points that fall within specified ranges or &quot;bins.&quot; In this context, the histograms are related to different frequency bins or bands in the field of signal processing.&#10;2. The exact type of histogram (single or multiple) is not explicitly stated in the transcript provided, but there are suggestions that it could be either one histogram per frequency bin or band or multiple separate histograms for different frequency bands. This information is based on the conversation between the participants, who are trying to recall details from the paper they are discussing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The conclusion that the system's performance is close to human performance cannot be drawn based on the LPC-12 method, as pointed out by PhD D. This is because the degradation from LPC-12 synthesis may introduce issues not typically found in human speech due to its approximation method. Furthermore, excitation, which plays a significant role in what the system hears, is not taken into account in the LPC-12 system, making it difficult to accurately assess the system's performance compared to human performance." target="Based on the supporting materials provided, making an accurate assessment of a system's performance compared to human performance using the LPC-12 method has limitations. The degradation from LPC-12 synthesis and the absence of excitation consideration in this method can introduce issues that are not typically found in human speech, making it challenging to draw definitive conclusions about the system's accuracy. Therefore, any comparison to the human-recreated LPC twelve spectrum should be approached with caution, as it may not provide a complete or accurate representation of the system's performance relative to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The conclusion that the system's performance is close to human performance cannot be drawn based on the LPC-12 method, as pointed out by PhD D. This is because the degradation from LPC-12 synthesis may introduce issues not typically found in human speech due to its approximation method. Furthermore, excitation, which plays a significant role in what the system hears, is not taken into account in the LPC-12 system, making it difficult to accurately assess the system's performance compared to human performance." target="1. The re-synthesized version of speech filtered by an LPC (Linear Predictive Coding) filter may not be immediately recognizable as speech because it is generated using a white noise source that has been filtered, rather than a periodic sound source. This can result in a less natural sounding &quot;whispering&quot; quality to the synthesized speech.&#10;2. The LPC re-synthesis method introduces degradation not typically found in human speech due to its approximation technique, which further complicates the assessment of the system's performance compared to human performance.&#10;3. Excitation, a crucial factor in what the system hears, is not taken into account by the LPC-12 system. This makes it challenging to determine if the system's performance is close to human performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Doctor D is planning to go to OGI (Oregon Health &amp; Science University) not next week, but maybe the week after that.&#10;2. Yes, both Birger and Michael Kleinschmidt (Michael is referred to as just Michael in the transcript) are planning to join the meeting two weeks from the date of the current meeting." target="Speaker: PhD A&#10;Content: Eh , we should be going .&#10;Speaker: Professor B&#10;Content: So ne next week we 'll have , uh , both Birger {pause} and , uh , Mike {disfmarker} Michael {disfmarker} Michael Kleinschmidt and Birger Kollmeier will join us .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Um , and you 're {disfmarker} {vocalsound} you 're probably gonna go up in a couple {disfmarker} three weeks or so ? When d when are you thinking of going up to , uh , OGI ?&#10;Speaker: PhD D&#10;Content: Yeah , like , uh , not next week but maybe the week after .&#10;Speaker: Professor B&#10;Content: OK . Good . So at least we 'll have one meeting with {vocalsound} yo with you still around , and {disfmarker} and {disfmarker}&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: That 's good .&#10;Speaker: PhD D&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Doctor D is planning to go to OGI (Oregon Health &amp; Science University) not next week, but maybe the week after that.&#10;2. Yes, both Birger and Michael Kleinschmidt (Michael is referred to as just Michael in the transcript) are planning to join the meeting two weeks from the date of the current meeting." target=": Mm - hmm . {vocalsound} Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . Yeah . So , that 's good . So , why don't we just , uh , um {disfmarker} I think starting {disfmarker} {pause} starting a w couple weeks from now , especially if you 're not gonna be around for a while , we 'll {disfmarker} we 'll be shifting more over to some other {disfmarker} {vocalsound} other territory . But , uh , uh , {comment} uh , n not {disfmarker} not so much in this meeting about Aurora , but {disfmarker} but , uh , uh , maybe just , uh , quickly today about {disfmarker} maybe you could just say a little bit about what you 've been talking about with Michael . And {disfmarker} and then Barry can say something about {pause} what {comment} {disfmarker} what we 're talking about .&#10;Speaker: Grad C&#10;Content: OK . So Michael Kleinschmidt , who 's a PHD student from Germany">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Doctor D is planning to go to OGI (Oregon Health &amp; Science University) not next week, but maybe the week after that.&#10;2. Yes, both Birger and Michael Kleinschmidt (Michael is referred to as just Michael in the transcript) are planning to join the meeting two weeks from the date of the current meeting." target="1. Yes, there was discussion about having Mississippi State run the systems for some sites that might not have sufficient compute resources for running the Wall Street Journal tests. However, this was not decided during today's meeting and is still under consideration.&#10;2. Based on the conversation, it seems that updates about the large-vocabulary task were not shared in the most recent emails, as one of the participants mentioned not having read them and being unsure if the system from Mississippi State is open-source or proprietary.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The visualization being discussed here is a histogram, which is a type of bar graph used to display the distribution of data by showing the number of data points that fall within specified ranges or &quot;bins.&quot; In this context, the histograms are related to different frequency bins or bands in the field of signal processing.&#10;2. The exact type of histogram (single or multiple) is not explicitly stated in the transcript provided, but there are suggestions that it could be either one histogram per frequency bin or band or multiple separate histograms for different frequency bands. This information is based on the conversation between the participants, who are trying to recall details from the paper they are discussing." target="Based on the transcript, the specific knob or scaling factor that should be used to change the scale of likelihoods, not observations, when determining a scaling factor in their process is not explicitly mentioned. Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The visualization being discussed here is a histogram, which is a type of bar graph used to display the distribution of data by showing the number of data points that fall within specified ranges or &quot;bins.&quot; In this context, the histograms are related to different frequency bins or bands in the field of signal processing.&#10;2. The exact type of histogram (single or multiple) is not explicitly stated in the transcript provided, but there are suggestions that it could be either one histogram per frequency bin or band or multiple separate histograms for different frequency bands. This information is based on the conversation between the participants, who are trying to recall details from the paper they are discussing." target="1. The values being discussed in the transcript are candidate basis functions or features used for classification in an optimization procedure called recursive feature elimination (or a variation of it). In this process, they start with a set of M candidate basis functions and classify using each possible subset of this set, recursively eliminating features that do not contribute to the classification performance until an optimal set is obtained.&#10;   &#10;2. Log probabilities are being utilized in the process despite not using the values as probabilities because log probabilities have a more Gaussian shape than regular probabilities, making them easier to model and work with in complex methods like the ones being used by the speakers. They mentioned the benefits of taking advantage of their Gaussian shape and the fact that these values are not directly used as probabilities but rather their log probabilities.&#10;&#10;In summary, the values in this context refer to candidate basis functions or features used for classification, and log probabilities are being utilized for modeling purposes due to their more Gaussian shape despite not being used directly as probabilities.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Alternative Approach: Professor B suggests using a two-source system, where one source is noise and the other is periodic pulses, as an alternative to using a voiced &quot;voice thing&quot; when dealing with strong periodic components. This approach allows for more accurate representation of various sound types in speech synthesis.&#10;2. Preference for Two-Source System: Professor B prefers a two-source system over a one-source system because it offers greater flexibility in handling different types of sounds, such as unvoiced and voiced sounds. By utilizing both noise and periodic pulses, the synthetic speech can more closely mimic natural human speech, resulting in improved quality and naturalness." target="disfmarker} s strong periodic components , then you can use a voiced {disfmarker} voice thing .&#10;Speaker: PhD D&#10;Content: Oh . Uh - huh . Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . I mean , it 's probably not worth your time . It 's {disfmarker} it 's a side thing and {disfmarker} and {disfmarker} and there 's a lot to do .&#10;Speaker: PhD D&#10;Content: Uh - huh , yeah .&#10;Speaker: Professor B&#10;Content: But I 'm {disfmarker} I 'm just saying , at least as a thought experiment , {vocalsound} that 's what I would wanna test .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Uh , I wan would wanna drive it with a {disfmarker} a {disfmarker} a two - source system rather than a {disfmarker} than a one - source system .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Alternative Approach: Professor B suggests using a two-source system, where one source is noise and the other is periodic pulses, as an alternative to using a voiced &quot;voice thing&quot; when dealing with strong periodic components. This approach allows for more accurate representation of various sound types in speech synthesis.&#10;2. Preference for Two-Source System: Professor B prefers a two-source system over a one-source system because it offers greater flexibility in handling different types of sounds, such as unvoiced and voiced sounds. By utilizing both noise and periodic pulses, the synthetic speech can more closely mimic natural human speech, resulting in improved quality and naturalness." target="er} than a one - source system .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: And then that would tell you whether in fact it 's {disfmarker} Cuz we 've talked about , like , this harmonic tunneling or {vocalsound} other things that people have done based on pitch , maybe that 's really a key element . Maybe {disfmarker} maybe , uh , {vocalsound} uh , without that , it 's {disfmarker} it 's not possible to do a whole lot better than we 're doing . That {disfmarker} that could be .&#10;Speaker: PhD D&#10;Content: Yeah . That 's what I was thinking by doing this es experiment ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: like {disfmarker} Mmm . {vocalsound} Evi&#10;Speaker: Professor B&#10;Content: But , I mean , other than that , I don't think it 's {disfmarker} I mean , other than the pitch de information ,">
      <data key="d0">1</data>
    </edge>
    <edge source="er} than a one - source system .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: And then that would tell you whether in fact it 's {disfmarker} Cuz we 've talked about , like , this harmonic tunneling or {vocalsound} other things that people have done based on pitch , maybe that 's really a key element . Maybe {disfmarker} maybe , uh , {vocalsound} uh , without that , it 's {disfmarker} it 's not possible to do a whole lot better than we 're doing . That {disfmarker} that could be .&#10;Speaker: PhD D&#10;Content: Yeah . That 's what I was thinking by doing this es experiment ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: like {disfmarker} Mmm . {vocalsound} Evi&#10;Speaker: Professor B&#10;Content: But , I mean , other than that , I don't think it 's {disfmarker} I mean , other than the pitch de information ," target="1. Extracting the pitch from actual speech and putting it back in might help in improving the performance of the model that hears some things better than others. This is because, as per Professor B, removing pitch information makes the audio closer to a one-source system, which could negatively impact the performance. By adding the pitch back, it might be possible to restore some of the lost information and improve the model's ability to distinguish different sound sources or speech components.&#10;2. However, there are potential challenges in implementing this idea. Professor B mentioned that synthesizing speech with added pitch information could introduce other degradations due to the approximation method used. Furthermore, it is unclear how mel-based synthesis would perform compared to the current approach. Therefore, careful experimentation and evaluation would be necessary to determine the effectiveness of this method.&#10;&#10;In summary, adding the extracted pitch back into the speech might improve the model's performance by restoring some lost information; however, it is crucial to consider potential degradations and other factors that could impact the overall outcome.">
      <data key="d0">1</data>
    </edge>
    <edge source="The optimization procedure being used here is called &quot;recursive feature elimination&quot; or a variation of it. The speaker starts with a set of candidate basis functions and classifies using each possible subset of this set, which is M possible subsets for a set of M candidates. The feature that doesn't contribute to the classification performance is considered useless and eliminated. This process is repeated by randomly selecting another feature from the remaining pool of possible basis functions until an optimal set is obtained." target=" a lot of different possible basis functions . And so he {disfmarker} {vocalsound} he actually does {vocalsound} an optimization procedure to choose an {disfmarker} {vocalsound} an optimal set of basis functions out of all the possible ones .&#10;Speaker: PhD A&#10;Content: Hmm . H What does he do to choose those ?&#10;Speaker: Grad C&#10;Content: The method he uses is kind of funny {disfmarker} is , {comment} {vocalsound} um , {vocalsound} he starts with {disfmarker} he has a set of M of them . Um , he {disfmarker} and then {pause} he uses that to classify {disfmarker} I mean , he t he tries , um , {vocalsound} using {pause} just M minus one of them . So there are M possible subsets of this {vocalsound} length - M vector . He tries classifying , using each of the M {vocalsound} possible sub - vectors .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Grad C&#10;Content: Whichever sub - vector , {voc">
      <data key="d0">1</data>
    </edge>
    <edge source="The optimization procedure being used here is called &quot;recursive feature elimination&quot; or a variation of it. The speaker starts with a set of candidate basis functions and classifies using each possible subset of this set, which is M possible subsets for a set of M candidates. The feature that doesn't contribute to the classification performance is considered useless and eliminated. This process is repeated by randomly selecting another feature from the remaining pool of possible basis functions until an optimal set is obtained." target=" sub - vectors .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Grad C&#10;Content: Whichever sub - vector , {vocalsound} um , works the {disfmarker} the best , I guess , he says {disfmarker} {vocalsound} the {disfmarker} the fe feature that didn't use was the most useless feature ,&#10;Speaker: Professor B&#10;Content: Y yeah . Gets thrown out . Yeah .&#10;Speaker: Grad C&#10;Content: so we 'll throw it out and we 're gonna randomly select another feature {pause} from the set of possible basis functions .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So it 's a {disfmarker}&#10;Speaker: Professor B&#10;Content: So i so it 's actuall&#10;Speaker: PhD A&#10;Content: it 's a little bit like a genetic algorithm or something in a way .&#10;Speaker: Professor B&#10;Content: Well , it 's {disfmarker} it 's much simpler .&#10;Speaker: Grad E&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="The optimization procedure being used here is called &quot;recursive feature elimination&quot; or a variation of it. The speaker starts with a set of candidate basis functions and classifies using each possible subset of this set, which is M possible subsets for a set of M candidates. The feature that doesn't contribute to the classification performance is considered useless and eliminated. This process is repeated by randomly selecting another feature from the remaining pool of possible basis functions until an optimal set is obtained." target="Content: Uh , no , I don't think so .&#10;Speaker: Professor B&#10;Content: No .&#10;Speaker: PhD D&#10;Content: No .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: It 's still {disfmarker} in terms of computation , if we use , like , their way of computing the {disfmarker} the maps {disfmarker} the {disfmarker} the MIPs , {vocalsound} I think it fits ,&#10;Speaker: Professor B&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD D&#10;Content: but it 's , uh , m mainly a problem of memory .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: Um , and I don't know how much {pause} this can be discussed or not , because it 's {disfmarker} it could be in ROM , so it 's maybe not that expensive . But {disfmarker}&#10;Speaker: Professor B&#10;Content: Ho - how much memory d ? H how many {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="The optimization procedure being used here is called &quot;recursive feature elimination&quot; or a variation of it. The speaker starts with a set of candidate basis functions and classifies using each possible subset of this set, which is M possible subsets for a set of M candidates. The feature that doesn't contribute to the classification performance is considered useless and eliminated. This process is repeated by randomly selecting another feature from the remaining pool of possible basis functions until an optimal set is obtained." target="1. The values being discussed in the transcript are candidate basis functions or features used for classification in an optimization procedure called recursive feature elimination (or a variation of it). In this process, they start with a set of M candidate basis functions and classify using each possible subset of this set, recursively eliminating features that do not contribute to the classification performance until an optimal set is obtained.&#10;   &#10;2. Log probabilities are being utilized in the process despite not using the values as probabilities because log probabilities have a more Gaussian shape than regular probabilities, making them easier to model and work with in complex methods like the ones being used by the speakers. They mentioned the benefits of taking advantage of their Gaussian shape and the fact that these values are not directly used as probabilities but rather their log probabilities.&#10;&#10;In summary, the values in this context refer to candidate basis functions or features used for classification, and log probabilities are being utilized for modeling purposes due to their more Gaussian shape despite not being used directly as probabilities.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Yes, there was discussion about having Mississippi State run the systems for some sites that might not have sufficient compute resources for running the Wall Street Journal tests. However, this was not decided during today's meeting and is still under consideration.&#10;2. Based on the conversation, it seems that updates about the large-vocabulary task were not shared in the most recent emails, as one of the participants mentioned not having read them and being unsure if the system from Mississippi State is open-source or proprietary." target="&#10;Speaker: PhD A&#10;Content: so there was some talk about having Mississippi State run {pause} the systems for them . And I {disfmarker} Did {disfmarker} did that come up at all ?&#10;Speaker: PhD D&#10;Content: Uh , no . Well , this {disfmarker} first , this was not the point at all of this {disfmarker} the meeting today&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD D&#10;Content: and ,&#10;Speaker: Professor B&#10;Content: Some&#10;Speaker: PhD D&#10;Content: uh , frankly , I don't know because I d {comment} didn't read also the {pause} most recent mails about {vocalsound} the large - vocabulary task . But , {vocalsound} uh , did you {disfmarker} do you still , uh , get the mails ? You 're not on the mailing list or what ?&#10;Speaker: PhD A&#10;Content: Hmm - mm . The only , um , mail I get is from Mississippi State {disfmarker}&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Yes, there was discussion about having Mississippi State run the systems for some sites that might not have sufficient compute resources for running the Wall Street Journal tests. However, this was not decided during today's meeting and is still under consideration.&#10;2. Based on the conversation, it seems that updates about the large-vocabulary task were not shared in the most recent emails, as one of the participants mentioned not having read them and being unsure if the system from Mississippi State is open-source or proprietary." target=" The only , um , mail I get is from Mississippi State {disfmarker}&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: so {disfmarker}&#10;Speaker: PhD D&#10;Content: Oh , yeah . So we should have a look at this .&#10;Speaker: PhD A&#10;Content: about their system . I {disfmarker} I don't get any {pause} mail about {disfmarker}&#10;Speaker: Professor B&#10;Content: I have to say , there 's uh something funny - sounding about saying that one of these big companies doesn't have enough cup compute power do that , so they 're having to have it done by Mississippi State .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: It just {disfmarker} {vocalsound} just sounds funny .&#10;Speaker: PhD A&#10;Content: Yeah . It does .&#10;Speaker: Professor B&#10;Content: But ,&#10;Speaker: PhD A&#10;Content: Yeah . I 'm {disfmarker} I 'm wondering about that&#10;Speaker: Professor B&#10;Content: anyway">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Yes, there was discussion about having Mississippi State run the systems for some sites that might not have sufficient compute resources for running the Wall Street Journal tests. However, this was not decided during today's meeting and is still under consideration.&#10;2. Based on the conversation, it seems that updates about the large-vocabulary task were not shared in the most recent emails, as one of the participants mentioned not having read them and being unsure if the system from Mississippi State is open-source or proprietary." target=": PhD A&#10;Content: I did , but Joe {pause} said , you know , &quot; what you 're saying makes sense&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: and {pause} I don't know &quot; . So he doesn't know what the answer is .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: I mean , that 's th We had this back and forth a little bit about , {vocalsound} you know , are sites gonna {disfmarker} are you gonna run this data for different sites ? And , well , if {disfmarker} if Mississippi State runs it , then maybe they 'll do a little optimization on that {pause} parameter , and , uh {disfmarker} But then he wasn't asked to run it for anybody . So i it 's {disfmarker} it 's just not clear yet what 's gonna happen .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Uh , he 's been putting this stuff out on their web site and {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The different conditions that the speakers are referring to are likely the various noise conditions and mismatched conditions in which the experiment was conducted. They mention that high error rates under these difficult conditions are not surprising, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;2. It is challenging to determine whether the numbers represent accuracy or error rate because the values discussed are not clearly defined as either. When looking at the numbers, it's unclear if a higher value indicates better performance (accuracy) or worse performance (error rate). This confusion arises from the way the numbers are presented or discussed in the conversation, making it difficult for the listeners to interpret the results accurately." target=" some of these noise condition not at all surprising .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: The baseline is sixty percent also on digits ,&#10;Speaker: PhD A&#10;Content: Oh , is it ?&#10;Speaker: PhD D&#10;Content: on the m more {pause} mismatched conditions .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: So .&#10;Speaker: PhD A&#10;Content: So , yeah , that 's probably what it is then . Yeah . So they have a lot of different conditions that they 're gonna be filling out .&#10;Speaker: Professor B&#10;Content: It 's a bad sign when you {disfmarker} looking at the numbers , you can't tell whether it 's accuracy or error rate .&#10;Speaker: PhD A&#10;Content: Yeah . Yeah . It 's {disfmarker} it 's gonna be hard . Um , they 're {disfmarker} I I 'm still waiting for them to {pause} release the , um , {vocalsound} multi - CPU version of their">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The different conditions that the speakers are referring to are likely the various noise conditions and mismatched conditions in which the experiment was conducted. They mention that high error rates under these difficult conditions are not surprising, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;2. It is challenging to determine whether the numbers represent accuracy or error rate because the values discussed are not clearly defined as either. When looking at the numbers, it's unclear if a higher value indicates better performance (accuracy) or worse performance (error rate). This confusion arises from the way the numbers are presented or discussed in the conversation, making it difficult for the listeners to interpret the results accurately." target="The significant difference in word error rates being discussed by the speakers refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is concerning and raises questions about the system's performance. The high error rates might be due to the software named Aurora, as suggested by Professor B. They imply that Aurora might be contributing to the high error rates based on their experience and familiarity with its performance, but they do not provide specific reasons for this conclusion.&#10;&#10;The speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty due to the complexity of their methods and not knowing what the impact would be at the &quot;other end.&quot; The different conditions they refer to are likely various noise conditions and mismatched conditions in which the experiment was conducted. High error rates under these difficult conditions are expected, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose. The numbers discussed during the conversation do not make it clear if a higher value indicates better performance (accuracy) or worse performance (error rate), causing confusion for the listeners.&#10;&#10;Lastly, Professor B suggests that adding pitch information extracted from actual speech back into the current input of the system could potentially help restore lost information and improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate. However, synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Extracting the pitch from actual speech and putting it back in might help in improving the performance of the model that hears some things better than others. This is because, as per Professor B, removing pitch information makes the audio closer to a one-source system, which could negatively impact the performance. By adding the pitch back, it might be possible to restore some of the lost information and improve the model's ability to distinguish different sound sources or speech components.&#10;2. However, there are potential challenges in implementing this idea. Professor B mentioned that synthesizing speech with added pitch information could introduce other degradations due to the approximation method used. Furthermore, it is unclear how mel-based synthesis would perform compared to the current approach. Therefore, careful experimentation and evaluation would be necessary to determine the effectiveness of this method.&#10;&#10;In summary, adding the extracted pitch back into the speech might improve the model's performance by restoring some lost information; however, it is crucial to consider potential degradations and other factors that could impact the overall outcome." target="alsound} hear some things better than others . And so it {disfmarker} it isn't {disfmarker}&#10;Speaker: PhD A&#10;Content: But {disfmarker}&#10;Speaker: Professor B&#10;Content: But , {vocalsound} I agree that it says that , uh , the kind of information that we 're feeding it is probably , {vocalsound} um , um , a little bit , um , minimal . There 's definitely some things that we 've thrown away . And that 's why I was saying it might be interesting if you {disfmarker} {vocalsound} an interesting test of this would be if you {disfmarker} if you actually put the pitch back in . So , you just extract it from the actual speech and put it back in , and see does that {disfmarker} is that {disfmarker} does that make the difference ? If that {disfmarker} if that takes it down to one percent again , {vocalsound} then you 'd say &quot; OK , it 's {disfmarker} it 's in fact having , um , {vocalsound} not">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Extracting the pitch from actual speech and putting it back in might help in improving the performance of the model that hears some things better than others. This is because, as per Professor B, removing pitch information makes the audio closer to a one-source system, which could negatively impact the performance. By adding the pitch back, it might be possible to restore some of the lost information and improve the model's ability to distinguish different sound sources or speech components.&#10;2. However, there are potential challenges in implementing this idea. Professor B mentioned that synthesizing speech with added pitch information could introduce other degradations due to the approximation method used. Furthermore, it is unclear how mel-based synthesis would perform compared to the current approach. Therefore, careful experimentation and evaluation would be necessary to determine the effectiveness of this method.&#10;&#10;In summary, adding the extracted pitch back into the speech might improve the model's performance by restoring some lost information; however, it is crucial to consider potential degradations and other factors that could impact the overall outcome." target="1. If the pitch information is extracted from actual speech and added back into the current input of the system, it could potentially help restore some lost information, as mentioned by Professor B. This might improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate.&#10;2. However, there are potential challenges with this approach. Synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.&#10;3. It is also unclear how mel-based synthesis would perform compared to the current approach, so thorough testing would be required before any conclusions can be drawn about its effectiveness in reducing error rates.">
      <data key="d0">1</data>
    </edge>
    <edge source="alsound} hear some things better than others . And so it {disfmarker} it isn't {disfmarker}&#10;Speaker: PhD A&#10;Content: But {disfmarker}&#10;Speaker: Professor B&#10;Content: But , {vocalsound} I agree that it says that , uh , the kind of information that we 're feeding it is probably , {vocalsound} um , um , a little bit , um , minimal . There 's definitely some things that we 've thrown away . And that 's why I was saying it might be interesting if you {disfmarker} {vocalsound} an interesting test of this would be if you {disfmarker} if you actually put the pitch back in . So , you just extract it from the actual speech and put it back in , and see does that {disfmarker} is that {disfmarker} does that make the difference ? If that {disfmarker} if that takes it down to one percent again , {vocalsound} then you 'd say &quot; OK , it 's {disfmarker} it 's in fact having , um , {vocalsound} not" target="1. If the pitch information is extracted from actual speech and added back into the current input of the system, it could potentially help restore some lost information, as mentioned by Professor B. This might improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate.&#10;2. However, there are potential challenges with this approach. Synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.&#10;3. It is also unclear how mel-based synthesis would perform compared to the current approach, so thorough testing would be required before any conclusions can be drawn about its effectiveness in reducing error rates.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it is not explicitly stated what specific activity Professor B, Grad E, and PhD A are planning to do that involves &quot;digging for treats like rats.&quot; The phrase appears to be a metaphor or simile used by Professor B to describe their upcoming conference call or meeting. It's possible that the treats refer to some kind of reward or positive outcome associated with the meeting, while the digging for the treats may represent the effort and work required to achieve that outcome. However, without further context or information, it is difficult to determine the exact meaning or nature of this activity." target=" um , at the end of the day , {vocalsound} we have , um {disfmarker} we have learned something about these {disfmarker} these latent , um {disfmarker} latent variables which happen to correspond to {vocalsound} intermediate categories . Um . {vocalsound} {nonvocalsound} Yeah , and so those are the {disfmarker} the two directions that I 'm {disfmarker} I 'm looking into right now . And , uh , {vocalsound} um {disfmarker} {vocalsound} {vocalsound} Yeah . I guess that 's {disfmarker} that 's it .&#10;Speaker: Professor B&#10;Content: OK . Should we do our digits and get ou get our treats ?&#10;Speaker: Grad E&#10;Content: Oh , tea time ?&#10;Speaker: Professor B&#10;Content: Yeah . It 's kind of like , you know , the little rats with the little thing dropping down to them .&#10;Speaker: PhD A&#10;Content: That 's ri&#10;Speaker: Professor B&#10;Content: We do the digits and then we get our treats">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the specific knob or scaling factor that should be used to change the scale of likelihoods, not observations, when determining a scaling factor in their process is not explicitly mentioned. Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose." target=" Yeah .&#10;Speaker: PhD A&#10;Content: And then we just use that to determine some scaling factor that we use .&#10;Speaker: Professor B&#10;Content: Yeah . So I mean , I I think that that 's a reasonable thing to do and the only question is what 's the actual knob that we use ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And the knob that we use should {disfmarker} uh , uh , unfortunately , like I say , I don't know the analytic solution to this cuz what we really want to do is change the scale of the likelihoods ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: not the cha not the scale of the {disfmarker} {vocalsound} the {pause} observations . But {disfmarker} but , uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Out of curiosity , what {disfmarker} what kind of recogn">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the specific knob or scaling factor that should be used to change the scale of likelihoods, not observations, when determining a scaling factor in their process is not explicitly mentioned. Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose." target="1. The values being discussed in the transcript are candidate basis functions or features used for classification in an optimization procedure called recursive feature elimination (or a variation of it). In this process, they start with a set of M candidate basis functions and classify using each possible subset of this set, recursively eliminating features that do not contribute to the classification performance until an optimal set is obtained.&#10;   &#10;2. Log probabilities are being utilized in the process despite not using the values as probabilities because log probabilities have a more Gaussian shape than regular probabilities, making them easier to model and work with in complex methods like the ones being used by the speakers. They mentioned the benefits of taking advantage of their Gaussian shape and the fact that these values are not directly used as probabilities but rather their log probabilities.&#10;&#10;In summary, the values in this context refer to candidate basis functions or features used for classification, and log probabilities are being utilized for modeling purposes due to their more Gaussian shape despite not being used directly as probabilities.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the specific knob or scaling factor that should be used to change the scale of likelihoods, not observations, when determining a scaling factor in their process is not explicitly mentioned. Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose." target="The significant difference in word error rates being discussed by the speakers refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is concerning and raises questions about the system's performance. The high error rates might be due to the software named Aurora, as suggested by Professor B. They imply that Aurora might be contributing to the high error rates based on their experience and familiarity with its performance, but they do not provide specific reasons for this conclusion.&#10;&#10;The speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty due to the complexity of their methods and not knowing what the impact would be at the &quot;other end.&quot; The different conditions they refer to are likely various noise conditions and mismatched conditions in which the experiment was conducted. High error rates under these difficult conditions are expected, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose. The numbers discussed during the conversation do not make it clear if a higher value indicates better performance (accuracy) or worse performance (error rate), causing confusion for the listeners.&#10;&#10;Lastly, Professor B suggests that adding pitch information extracted from actual speech back into the current input of the system could potentially help restore lost information and improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate. However, synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. If the pitch information is extracted from actual speech and added back into the current input of the system, it could potentially help restore some lost information, as mentioned by Professor B. This might improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate.&#10;2. However, there are potential challenges with this approach. Synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.&#10;3. It is also unclear how mel-based synthesis would perform compared to the current approach, so thorough testing would be required before any conclusions can be drawn about its effectiveness in reducing error rates." target="The significant difference in word error rates being discussed by the speakers refers to the comparison between an error rate of ten percent under certain conditions and a much higher error rate of sixty-four point six percent under different conditions. This difference is concerning and raises questions about the system's performance. The high error rates might be due to the software named Aurora, as suggested by Professor B. They imply that Aurora might be contributing to the high error rates based on their experience and familiarity with its performance, but they do not provide specific reasons for this conclusion.&#10;&#10;The speakers are contemplating the use of log probabilities and whether adjustments to the word insertion penalty could improve performance, but they face uncertainty regarding the possibility of changing the penalty due to the complexity of their methods and not knowing what the impact would be at the &quot;other end.&quot; The different conditions they refer to are likely various noise conditions and mismatched conditions in which the experiment was conducted. High error rates under these difficult conditions are expected, implying that there might be several distinct sets of parameters or scenarios being evaluated. However, the exact details of these conditions are not provided in the transcript.&#10;&#10;Speaker Professor B alludes to wanting to change the scale of likelihoods but expresses uncertainty about the analytic solution and the exact &quot;knob&quot; to use for this purpose. The numbers discussed during the conversation do not make it clear if a higher value indicates better performance (accuracy) or worse performance (error rate), causing confusion for the listeners.&#10;&#10;Lastly, Professor B suggests that adding pitch information extracted from actual speech back into the current input of the system could potentially help restore lost information and improve the model's performance in distinguishing different sound sources or speech components, possibly reducing the error rate. However, synthesizing speech with added pitch information may introduce degradations due to approximation methods used during implementation. Careful experimentation and evaluation would be necessary to determine if these potential issues outweigh the benefits of reduced errors.">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
