<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="In the transcript provided, PhD A mentions that the new system (proposal two) is &quot;a little worse&quot; when compared to proposal one. This statement is made in the context of comparing the relative improvements between the two proposals. Specifically, they mention that for some cases things are &quot;somewhat better&quot; in proposal two, but for other cases they are &quot;somewhat worse.&quot;&#10;&#10;Later in the transcript, PhD A clarifies what they mean by &quot;worse.&quot; When comparing proposal two to proposal one, the new system is not as good as proposal one when a neural network is added. However, even though it is worse than proposal one in this specific aspect, PhD A still considers the new system to be better overall because of its shorter latencies and other improvements made in the transition from proposal one to proposal two.&#10;&#10;In summary, PhD A considers proposal two to be &quot;a little worse&quot; than proposal one in the specific context of comparing relative improvements and when a neural network is added to the new system. However, they still consider the new system to be better overall due to its shorter latencies and other improvements." />
    <node id="&#10;Content: Yes . Uh {disfmarker}&#10;Speaker: Professor B&#10;Content: But what you 're saying is that when you do these {disfmarker} So let me try to understand . When {disfmarker} when you do these same improvements {vocalsound} to proposal - one ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: that , uh , on the {disfmarker} i things are somewhat better , uh , in proposal - two for the well - matched case and somewhat worse for the other two cases .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So does , uh {disfmarker} when you say , uh {disfmarker} So {disfmarker} The th now that these other things are in there , is it the case maybe that the additions of proposal - two over proposal - one are {pause} less im important ?&#10;Speaker: PhD A&#10;Content: Yeah . Probably , yeah .&#10;Speaker: Professor B&#10;Content: I get it .&#10;Speaker: PhD A&#10;Content: Um {disf" />
    <node id=" it on the other conditions , when you say it 's a little worse ?&#10;Speaker: PhD A&#10;Content: It 's like , uh , fff , fff {comment} {vocalsound} {pause} um , {comment} {vocalsound} {vocalsound} {pause} ten percent relative . Yeah .&#10;Speaker: Professor B&#10;Content: OK . Um .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: But it has the , uh {disfmarker} the latencies are much shorter . That 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Uh - y w when I say it 's worse , it 's not {disfmarker} it 's when I {disfmarker} I {disfmarker} uh , compare proposal - two to proposal - one , so , r uh , y putting neural network {vocalsound} compared to n not having any neural network . I mean , this new system is {disfmarker} is {disfmarker} is better ,&#10;Speaker: Professor B&#10;Content: Uh - huh ." />
    <node id=" proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much better . So , yeah . I asked {vocalsound} Sunil for more information about that , but , uh , I don't know yet . Um . And what 's happened here is that we {disfmarker} so we have this kind of new , um , reference system which {vocalsound} use a nice {disfmarker} a {disfmarker} a clean downsampling - upsampling , which use a new filter {vocalsound} that 's much shorter and which also cuts the frequency below sixty - four hertz ,&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: which was not done on our first proposal .&#10;Speaker: Professor B&#10;Content: When you say &quot; we have that &quot; , does Sunil have it now , too ,&#10;Speaker: PhD A&#10;Content: I No .&#10;Speaker: Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Professor B&#10;Content" />
    <node id=" {disfmarker} it 's like if you say what 's the {disfmarker} what 's the best way to do an average , an arithmetic average or a geometric average ?&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: It depends what you wanna show .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Each {disfmarker} each one is gonna have a different characteristic .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So {disfmarker}&#10;Speaker: PhD C&#10;Content: Well , it seems like they should do , like , the percentage improvement or something , rather than the {pause} absolute improvement .&#10;Speaker: PhD A&#10;Content: Tha - that 's what they do .&#10;Speaker: Professor B&#10;Content: Well , they are doing that .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: No , that is relative . But the question is , do you average the relative improvements {pause} or do you average the error" />
    <node id=" use the whole recognizer to do it {vocalsound} but {disfmarker} rather than {vocalsound} a separate thing , but {disfmarker} {vocalsound} but they 'll have it on some level . So , um .&#10;Speaker: PhD C&#10;Content: It seems like whatever they choose they shouldn't , {vocalsound} you know , purposefully brain - damage a part of the system to {pause} make a worse baseline , or {disfmarker}&#10;Speaker: Professor B&#10;Content: Well , I think people just had&#10;Speaker: PhD C&#10;Content: You know ?&#10;Speaker: Professor B&#10;Content: it wasn't that they purposely brain - damaged it . I think people hadn't really thought through {vocalsound} about the , uh {disfmarker} the VAD issue .&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And {disfmarker} and then when the {disfmarker} the {disfmarker} the proposals actually came in and half of them had V" />
    <node id="aker: PhD C&#10;Content: And when you have your final thing , we go back to this .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: So , um , and it 's a real simple change to make . I mean , it 's like one little text file you edit and change those numbers , and you don't do anything else .&#10;Speaker: PhD F&#10;Content: Oh , this is a {disfmarker}&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: And then you just run .&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: PhD C&#10;Content: So it 's a very simple change to make and it doesn't seem to hurt all that much .&#10;Speaker: PhD A&#10;Content: So you {disfmarker} you run with three , two , two , five ? That 's a&#10;Speaker: PhD C&#10;Content: So I {disfmarker} Uh , I {disfmarker} I have to look to see what the exact numbers were .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Spe" />
    <node id="Based on the transcript, Speaker PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. Specifically, she mentions that her speech might be problematic for the unvoiced part of the recognizer. Additionally, she seems to have a stronger accent or use different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause." />
    <node id=": Professor B&#10;Content: Uh , first order for speech recognition , you say &quot; I don't care about the source &quot; .&#10;Speaker: PhD F&#10;Content: For {disfmarker} Yeah .&#10;Speaker: PhD A&#10;Content: Well , I mean for the {disfmarker} the energy .&#10;Speaker: PhD F&#10;Content: I have the mean .&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: And so you just want to find out what the filters are .&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: The filters {vocalsound} roughly act like a , um {disfmarker} {vocalsound} a , uh {disfmarker} {vocalsound} a an overall resonant {disfmarker} you know , f some resonances and so forth that th that 's processing excitation .&#10;Speaker: PhD F&#10;Content: Here .&#10;Speaker: PhD A&#10;Content: They should be more close .&#10;Speaker: PhD" />
    <node id="alsound} these {disfmarker} {vocalsound} he 's {disfmarker} he 's , uh {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: This is {disfmarker} this by the way a bad thing . We 're trying to get , um , m more female voices in this record as well . So . Make sur make sure Carmen {vocalsound} talks as well . Uh , but has he pretty much been talking about what you 're doing also , and {disfmarker} ?&#10;Speaker: PhD F&#10;Content: Oh , I {disfmarker} I am doing this .&#10;Speaker: Professor B&#10;Content: Yes .&#10;Speaker: PhD F&#10;Content: Yeah , yeah . I don't know . I 'm sorry , but I think that for the recognizer for the meeting recorder that it 's better that I don't speak .&#10;Speaker: Professor B&#10;Content: Yeah , well .&#10;Speaker: PhD F&#10;Content: Because {disfmarker}&#10;Speaker: Professor B&#10;Content: You know , uh , we '" />
    <node id="Content: Oh .&#10;Speaker: PhD F&#10;Content: for unvoiced .&#10;Speaker: Professor B&#10;Content: Yeah . So , you know , all {disfmarker}&#10;Speaker: PhD F&#10;Content: I 'm sorry .&#10;Speaker: PhD A&#10;Content: But {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Yeah . This is the {disfmarker} between {disfmarker}&#10;Speaker: PhD A&#10;Content: This is another voiced example . Yeah .&#10;Speaker: PhD F&#10;Content: No . But it 's this ,&#10;Speaker: PhD A&#10;Content: Oh , yeah . This is {disfmarker}&#10;Speaker: PhD F&#10;Content: but between the frequency that we are considered for the excitation {disfmarker}&#10;Speaker: PhD A&#10;Content: Right . Mm - hmm .&#10;Speaker: PhD F&#10;Content: for the difference and this is the difference .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: This is the difference . OK .&#10;" />
    <node id=" no , no . But th the kind of robustness to noise {disfmarker}&#10;Speaker: PhD F&#10;Content: Oh !&#10;Speaker: PhD A&#10;Content: So if {disfmarker} if you take this frame , {vocalsound} uh , from the noisy utterance and the same frame from the clean utterance {disfmarker}&#10;Speaker: PhD F&#10;Content: Hmm .&#10;Speaker: PhD C&#10;Content: You end up with a similar difference&#10;Speaker: PhD A&#10;Content: Y y y yeah . We end up with {disfmarker}&#10;Speaker: PhD C&#10;Content: over here ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: OK . Cool !&#10;Speaker: PhD F&#10;Content: I have here the same frame for the {pause} clean speech {disfmarker}&#10;Speaker: PhD C&#10;Content: Oh , that 's clean .&#10;Speaker: PhD F&#10;Content: the same cle&#10;Speaker: PhD C&#10;Content: Oh , OK&#10;Speaker: PhD F&#10;Content: But they are a difference .&#10;Speaker: PhD A" />
    <node id=" .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: I can't see you {comment} now .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: I don't have .&#10;Speaker: PhD C&#10;Content: And so you said this is pretty {disfmarker} doing this kind of thing is pretty robust to noise ?&#10;Speaker: PhD A&#10;Content: It seems , yeah . Um ,&#10;Speaker: PhD C&#10;Content: Huh .&#10;Speaker: PhD F&#10;Content: Pfft . Oops . The mean is different {vocalsound} with it , because the {disfmarker} {vocalsound} the histogram for the {disfmarker} {vocalsound} the classifica&#10;Speaker: PhD A&#10;Content: No , no , no . But th the kind of robustness to noise {disfmarker}&#10;Speaker: PhD F&#10;Content: Oh !&#10;Spe" />
    <node id="PHP A and their team obtained the voiced and unvoiced truth data used in their analysis by utilizing TIMIT, a speech recognition database, and applying canonical mappings between the phones. This information was revealed when PhD C asked how they got their voiced and unvoiced truth data, to which PhD A responded by explaining their use of TIMIT and the application of canonical mappings between the phones." />
    <node id="aker: PhD A&#10;Content: Yeah , that 's right .&#10;Speaker: Professor B&#10;Content: Yeah , yeah .&#10;Speaker: PhD A&#10;Content: Um {disfmarker} Yeah .&#10;Speaker: PhD F&#10;Content: We have here some histogram ,&#10;Speaker: PhD A&#10;Content: E yeah ,&#10;Speaker: PhD F&#10;Content: but they have a lot of overlap .&#10;Speaker: PhD A&#10;Content: but it 's {disfmarker} it 's still {disfmarker} Yeah . So , well , for unvoiced portion we have something tha {vocalsound} that has a mean around O point three , and for voiced portion the mean is O point fifty - nine . But the variance seem quite {vocalsound} high .&#10;Speaker: PhD C&#10;Content: How do you know {disfmarker} ?&#10;Speaker: PhD A&#10;Content: So {disfmarker} Mmm .&#10;Speaker: PhD C&#10;Content: How did you get your {pause} voiced and unvoiced truth data ?&#10;Speaker: PhD A&#10;Content: We used , uh , TIMIT and we used" />
    <node id=" get your {pause} voiced and unvoiced truth data ?&#10;Speaker: PhD A&#10;Content: We used , uh , TIMIT and we used canonical mappings between the phones&#10;Speaker: PhD F&#10;Content: Yeah . We , uh , use {pause} TIMIT on this ,&#10;Speaker: PhD A&#10;Content: and&#10;Speaker: PhD F&#10;Content: for {disfmarker}&#10;Speaker: PhD A&#10;Content: th Yeah .&#10;Speaker: PhD F&#10;Content: But if we look at it in one sentence , it {disfmarker} apparently it 's good , I think .&#10;Speaker: PhD A&#10;Content: Yeah , but {disfmarker} Yeah . Uh , so it 's noisy TIMIT . That 's right . Yeah .&#10;Speaker: Grad E&#10;Content: It 's noisy TIMIT .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: It seems quite robust to noise , so when we take {disfmarker} we draw its parameters across time for a clean sentence and then nois the same noisy sentence , it 's very close .&#10;Speaker: Professor B" />
    <node id=": OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Oh ! OK . Yeah .&#10;Speaker: PhD A&#10;Content: and that 's {disfmarker} that should be flat for {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: I see . So do you have a picture that sh ?&#10;Speaker: PhD A&#10;Content: So - It 's {disfmarker} Y&#10;Speaker: PhD C&#10;Content: Is this for a voiced segment ,&#10;Speaker: PhD A&#10;Content: yeah .&#10;Speaker: PhD C&#10;Content: this picture ? What does it look like for unvoiced ?&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: You have several {disfmarker} some unvoiced ?&#10;Speaker: PhD F&#10;Content: The dif No . Unvoiced , I don't have&#10;Speaker: PhD A&#10;Content: Oh .&#10;Speaker: PhD F&#10;Content: for unvoiced .&#10;Speaker: Professor B&#10;Content: Yeah . So , you" />
    <node id=" , um {disfmarker} well , to {disfmarker} to obtain a statistical model on these features and to {disfmarker} or just to use a neural network and hopefully these features w would help {disfmarker}&#10;Speaker: PhD C&#10;Content: Because it seems like what you said about the mean of the {disfmarker} the voiced and the unvoiced {disfmarker} {comment} {vocalsound} that seemed pretty encouraging .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Well , yeah , except the variance was big .&#10;Speaker: PhD C&#10;Content: Right ?&#10;Speaker: PhD A&#10;Content: Yeah . Except the variance is quite high .&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD C&#10;Content: Well , y&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Well , y I {disfmarker} I don't know that I would trust that so much because you 're doing these canonical mappings from TIMIT labellings .&#10;Speaker: PhD A&#10;" />
    <node id="1. Professor B's argument about voiced phonemes giving us intuition: During the discussion, Professor B suggests that the concept of voiced phonemes helps shape our intuition regarding speech recognition (e.g., &quot;what I'm arguing is that that gives you your intuition&quot;). This implies that understanding the difference between voiced and unvoiced sounds contributes to forming a basic framework for analyzing and comprehending speech signals.&#10;2. Complexities in speech and their acceptance: Professor B argues that in reality, there are overlaps and complexities in speech that make it difficult to perfectly separate voiced from unvoiced sounds (&quot;But in reality, it's all of this overlap and so forth&quot;). Despite these challenges, the professor suggests that this might be acceptable for a few reasons. First, what we gain from the analysis is not necessarily a clear-cut distinction between voiced and unvoiced sounds but rather a characterization driven by our intuition. Second, imperfect data may affect the accuracy of any speech recognition system. Consequently, relying on this basic intuition might still provide valuable insights in understanding &quot;almost raw data&quot; instead of relying solely on refined and smoothed versions of the signal." />
    <node id=" it gives you just information about if it 's voiced or not voiced , ma mainly , I mean . But {disfmarker} So ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: this is why we {disfmarker} we started to look {pause} by having sort of voiced phonemes&#10;Speaker: Professor B&#10;Content: Well , that 's the rea w w what I 'm arguing is that 's Yeah . I mean , uh , what I 'm arguing is that that {disfmarker} that 's givi you {disfmarker} gives you your intuition .&#10;Speaker: PhD A&#10;Content: and {disfmarker} Mm - hmm .&#10;Speaker: Professor B&#10;Content: But in {disfmarker} in reality , it 's {disfmarker} you know , there 's all of this {disfmarker} this overlap and so forth ,&#10;Speaker: Grad E&#10;Content: Oh , sorry .&#10;Speaker: Professor B&#10;Content: and {disfmarker} But what I 'm saying is that may be OK , because" />
    <node id=" .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Two hundred and fifty thousand .&#10;Speaker: PhD A&#10;Content: Fifteen hundred . Because {disfmarker} Yeah .&#10;Speaker: PhD F&#10;Content: Yeah . Two thousand and fifteen hundred .&#10;Speaker: PhD A&#10;Content: Above , um {disfmarker} {vocalsound} it seems that {disfmarker} Well , some voiced sound can have also , {vocalsound} like , a noisy {pause} part on high frequencies , and {disfmarker} But {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Well , it 's just {disfmarker}&#10;Speaker: Professor B&#10;Content: No , it 's {disfmarker} makes sense to look at {pause} low frequencies .&#10;Speaker: PhD C&#10;Content: So this is {disfmarker} uh , basically this is comparing {vocalsound} an original version of the signal to a smoothed version of the same signal ?&#10;Speaker: PhD F&#10;Content: Yeah" />
    <node id=" , sorry .&#10;Speaker: Professor B&#10;Content: and {disfmarker} But what I 'm saying is that may be OK , because what you 're really getting is not actually voiced versus unvoiced , both for the fac the reason of the overlap and {disfmarker} and then , uh , th you know , structural reasons , uh , uh , like the one that Chuck said , that {disfmarker} that in fact , well , the data itself is {disfmarker} {vocalsound} that you 're working with is not perfect .&#10;Speaker: PhD A&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor B&#10;Content: So , what I 'm saying is maybe that 's not a killer because you 're just getting some characterization , one that 's driven by your intuition about voiced - unvoiced certainly ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: but it 's just some characterization {vocalsound} of something back in the {disfmarker} in the {disfmarker} in the almost raw data , rather than the smooth version" />
    <node id=" the world 's best voiced - unvoiced , uh , uh , classifier ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: but it 's more that , {vocalsound} you know , uh , uh , try some different statistical characterizations of that difference back to the raw data&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: and {disfmarker} and m maybe there 's something there that {pause} the system can use .&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: Yeah . Yeah , but ther more obvious is that {disfmarker} Yeah . The {disfmarker} the more obvious is that {disfmarker} that {disfmarker} well , using the {disfmarker} th the FFT , um , {vocalsound} you just {disfmarker} it gives you just information about if it 's voiced or not voiced , ma mainly , I mean . But {disfmarker} So ,&#10;Spe" />
    <node id=" Professor B&#10;Content: And {disfmarker} and , uh , the {disfmarker} the MSG features were sort of built up {vocalsound} with this notion {disfmarker}&#10;Speaker: Grad E&#10;Content: Yeah . Right .&#10;Speaker: Professor B&#10;Content: But , I guess , I thought you had brought this up in the context of , um , targets somehow .&#10;Speaker: Grad E&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: But i m&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: Professor B&#10;Content: i it 's not {disfmarker} I mean , they 're sort of not in the same kind of category as , say , a phonetic target or a syllabic target&#10;Speaker: Grad E&#10;Content: Mmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: or a {disfmarker}&#10;Speaker: Grad E&#10;Content: Um , I was thinking more like using them as {disfmarker} as the inputs to {disfmarker} to the detectors .&#10;Speaker: Professor" />
    <node id="1. Using Multiple Numbers: Professor B suggests using two or three numbers instead of relying on a single number to provide a more comprehensive understanding of the situation (content: &quot;I think one thing to do is to just not rely on a single number to maybe have two or three numbers&quot;).&#10;2. Weighted Average: Both professors discuss the idea of using a weighted average, which takes into account different factors and their significance in the given situation (content: &quot;And the thing is it's not just a pure average because there are these weightings. It's a weighted average.&quot;).&#10;3. Relative Improvement or Error Rates: Professor B raises the question of whether to average relative improvements or error rates, suggesting that this decision might be influenced by the specific context and requirements (content: &quot;But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that?&quot;).&#10;4. Analyzing Scores Instead of Improvements: PhD C proposes analyzing scores directly instead of focusing on improvements, which might help in finding a more suitable way to combine different factors (content: &quot;Why don't they not look at improvements but just look at your av your scores? You know, figure out how to combine the scores&quot;)." />
    <node id="Speaker: Professor B&#10;Content: Yeah . Yeah .&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So maybe , {vocalsound} le&#10;Speaker: PhD C&#10;Content: Should we do digits ?&#10;Speaker: Professor B&#10;Content: let 's do digits . Let you {disfmarker} you start .&#10;Speaker: Grad D&#10;Content: Oh , OK .&#10;Speaker: Grad E&#10;Content: L fifty .&#10;Speaker: PhD A&#10;Content: Right ." />
    <node id="disfmarker}&#10;Speaker: Professor B&#10;Content: Well , no {disfmarker} well , no . I mean , {vocalsound} it isn't the operating theater . I mean , they don they {disfmarker} they don't {disfmarker} they don't really {pause} know , I think .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean , I th&#10;Speaker: PhD C&#10;Content: So if {disfmarker} if they don't know , doesn't that suggest the way for them to go ? Uh , you assume everything 's equal . I mean , y y I mean , you {disfmarker}&#10;Speaker: Professor B&#10;Content: Well , I mean , I {disfmarker} I think one thing to do is to just not rely on a single number {disfmarker} to maybe have two or three numbers ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: you know ,&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: and {disfmark" />
    <node id=": Professor B&#10;Content: No , that is relative . But the question is , do you average the relative improvements {pause} or do you average the error rates and take the relative improvement maybe of that ?&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Professor B&#10;Content: And the thing is it 's not just a pure average because there are these weightings .&#10;Speaker: PhD C&#10;Content: Oh .&#10;Speaker: Professor B&#10;Content: It 's a weighted average . Um .&#10;Speaker: PhD A&#10;Content: Yeah . And so when you average the {disfmarker} the relative improvement it tends to {disfmarker} {vocalsound} to give a lot of {disfmarker} of , um , {vocalsound} importance to the well - matched case because {pause} the baseline is already very good and , um , i it 's {disfmarker}&#10;Speaker: PhD C&#10;Content: Why don't they not look at improvements but just look at your av your scores ? You know , figure out how to combine the scores&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Spe" />
    <node id=" A&#10;Content: Mmm , I just {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , you don't know . OK .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Alright .&#10;Speaker: PhD A&#10;Content: Um , yeah . So the points were the {disfmarker} the weights {disfmarker} how to weight the different error rates {vocalsound} that are obtained from different language and {disfmarker} and conditions . Um , it 's not clear that they will keep the same kind of weighting . Right now it 's a weighting on {disfmarker} on improvement .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Some people are arguing that it would be better to have weights on uh {disfmarker} well , to {disfmarker} to combine error rates {pause} before computing improvement . Uh , and the fact is that for {disfmarker} right now for {pause} the English , they have weights {disfmarker} they {disfmarker} they combine" />
    <node id="1. First, the signal's FFT (Fast Fourier Transform) spectrum is computed to obtain a frequency-domain representation of the signal.&#10;2. Then, the mel filter bank is applied to the signal. A mel filter bank is a set of triangular filters spaced on the mel scale, which approximately corresponds to the human auditory system's frequency perception. This process results in a new spectrum that emphasizes the important features for human hearing.&#10;3. The mel filter bank spectrum is then subtracted from the original FFT spectrum. This difference represents the excitation signal, which carries information about the speech production mechanism (vocal cords for voiced sounds and noise for unvoiced sounds).&#10;4. The resulting excitation signal may be used as a feature for various speech processing tasks, such as voice activity detection, pitch estimation, or speaker identification. It's essential to note that this excitation isn't the actual physiological excitation but an approximation that conveys information about it." />
    <node id=" the excitation , as I recall , is you 're subtracting the {disfmarker} the , um {disfmarker} the mel {disfmarker} mel {disfmarker} {vocalsound} mel filter , uh , spectrum from the FFT spectrum .&#10;Speaker: PhD A&#10;Content: e That 's right . Yeah . So {disfmarker}&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So we have the mel f filter bank , we have the FFT , so we {pause} just {disfmarker}&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker} it 's not really an excitation ,&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Professor B&#10;Content: but it 's something that hopefully tells you something about the excitation .&#10;Speaker: PhD A&#10;Content: Yeah , that 's right .&#10;Speaker: Professor B&#10;Content: Yeah , yeah .&#10;Speaker: PhD" />
    <node id=" an FFT , it may be {disfmarker} it may be pushing things .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: And {disfmarker} and , uh {disfmarker}&#10;Speaker: PhD C&#10;Content: Would you {disfmarker} would you wanna do this kind of , uh , difference thing {vocalsound} after you do spectral subtraction ?&#10;Speaker: PhD A&#10;Content: Uh , {vocalsound} maybe .&#10;Speaker: PhD F&#10;Content: No . Maybe we can do that .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Hmm . The spectral subtraction is being done at what level ? Is it being done at the level of FFT bins or at the level of , uh , mel spectrum or something ?&#10;Speaker: PhD A&#10;Content: Um , I guess it depends .&#10;Speaker: Professor B&#10;Content: I mean , how are they doing it ?&#10;Speaker: PhD A&#10;Content: How they 're doing it ? Yeah . Um , I guess Ericsson is on the , um , filter bank ," />
    <node id="isfmarker}&#10;Speaker: PhD C&#10;Content: what sorts of features are you looking at ?&#10;Speaker: PhD F&#10;Content: We have some {disfmarker}&#10;Speaker: PhD A&#10;Content: So we would be looking at , um , the {pause} variance of the spectrum of the excitation ,&#10;Speaker: PhD F&#10;Content: uh , um , this , this , and this .&#10;Speaker: PhD A&#10;Content: something like this , which is {disfmarker} should be high for voiced sounds . Uh , we {disfmarker}&#10;Speaker: PhD C&#10;Content: Wait a minute . I {disfmarker} what does that mean ? The variance of the spectrum of excitation .&#10;Speaker: PhD A&#10;Content: Yeah . So the {disfmarker} So basically the spectrum of the excitation {vocalsound} for a purely periodic sig signal shou sh&#10;Speaker: Professor B&#10;Content: OK . Yeah , w what yo what you 're calling the excitation , as I recall , is you 're subtracting the {disfmarker} the , um {disfmarker} the mel" />
    <node id="&#10;Speaker: PhD A&#10;Content: And the way to do this {vocalsound} is that {disfmarker} well , we have the {disfmarker} we have the FFT because it 's computed in {disfmarker} in the {disfmarker} in the system , and we have {vocalsound} the mel {vocalsound} filter banks ,&#10;Speaker: PhD C&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD A&#10;Content: and so if we {disfmarker} if we , like , remove the mel filter bank from the FFT , {vocalsound} we have something that 's {pause} close to the {pause} excitation signal .&#10;Speaker: Grad E&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: It 's something that 's like {vocalsound} a {disfmarker} a a train of p a pulse train for voiced sound&#10;Speaker: PhD C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Oh ! OK . Yeah .&#10;Speaker" />
    <node id=" a smooth spectral envelope , so there must be something else that you get {pause} in return for that {disfmarker}&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: that , uh {disfmarker} uh {disfmarker} So .&#10;Speaker: PhD C&#10;Content: So how does {disfmarker} uh , maybe I 'm going in too much detail , but {vocalsound} how exactly do you make the difference between the FFT and the smoothed {pause} spectral envelope ? Wha - wh i i uh , how is that , uh {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Um , we just {disfmarker} How did we do it up again ?&#10;Speaker: PhD F&#10;Content: Uh , we distend the {disfmarker} we have the twenty - three coefficient af after the mel f {vocalsound} filter ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: and we extend these coefficient between the {disfmarker} all the frequency range .&#10;" />
    <node id=" um , the mel cepstru mel {pause} spectrum , mel cepstrum , {vocalsound} any of these variants , um , give you the smooth spectrum . It 's the spectral envelope . By going back to the FFT , {vocalsound} you 're getting something that is {pause} more like the raw data . So the question is , what characterization {disfmarker} and you 're playing around with this {disfmarker} another way of looking at it is what characterization {vocalsound} of the difference between {pause} the raw data {pause} and this smooth version {pause} is something that you 're missing that could help ? So , I mean , looking at different statistical measures of that difference , coming up with some things and just trying them out and seeing if you add them onto the feature vector does that make things better or worse in noise , where you 're really just i i the way I 'm looking at it is not so much you 're trying to f find the best {disfmarker} the world 's best voiced - unvoiced , uh , uh , classifier ,&#10;Speaker: PhD C&#10;Content: Mm - hmm" />
    <node id="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." />
    <node id=" the code .&#10;Speaker: Professor B&#10;Content: Oh , OK . Yeah . I mean , it 's {disfmarker} um , certainly in a short {disfmarker} short - term this just sounds easier .&#10;Speaker: Grad D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Yeah . I mean , longer - term if it 's {disfmarker} {vocalsound} if it turns out to be useful , one {disfmarker} one might want to do something else ,&#10;Speaker: Grad D&#10;Content: Right . That 's true .&#10;Speaker: Professor B&#10;Content: but {disfmarker} Uh , uh , I mean , in {disfmarker} in other words , you {disfmarker} you may be putting other kinds of errors in {pause} from the re - synthesis process .&#10;Speaker: Grad D&#10;Content: But {disfmarker} e u From the re - synthesis ? Um ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad D&#10;Content: O - OK . I don't know anything about re - synthesis . Uh ," />
    <node id=" Professor B&#10;Content: Yeah .&#10;Speaker: Grad D&#10;Content: O - OK . I don't know anything about re - synthesis . Uh , how likely do you think that is ?&#10;Speaker: Professor B&#10;Content: Uh , it depends what you {disfmarker} what you do . I mean , it 's {disfmarker} it 's {disfmarker} it 's , uh , um {disfmarker} Don't know . But anyway it sounds like a reasonable way to go for a {disfmarker} for an initial thing , and we can look at {disfmarker} {vocalsound} at exactly what you end up doing and {disfmarker} and then figure out if there 's some {disfmarker} {vocalsound} something that could be {disfmarker} be hurt by the end part of the process .&#10;Speaker: Grad D&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: OK . So that 's {disfmarker} That was it , huh ?&#10;Speaker: Grad D&#10;Content: That {disfmarker} Yeah , e That 's" />
    <node id=" seven iterations of re - estimation in each of those three . And so , you know , that 's part of what takes so long to train the {disfmarker} the {disfmarker} the back - end for this .&#10;Speaker: Professor B&#10;Content: I 'm sorry , I didn't quite get that . There 's {disfmarker} there 's four and there 's seven and {disfmarker} I {disfmarker} I 'm sorry .&#10;Speaker: PhD C&#10;Content: Yeah . Uh , maybe I should write it on the board . So , {vocalsound} there 's four rounds of training . Um , I g I g I guess you could say iterations . The first one is three , then seven , seven , and seven . And what these numbers refer to is the number of times that the , uh , HMM re - estimation is run . It 's this program called H E&#10;Speaker: Professor B&#10;Content: But in HTK , what 's the difference between , uh , a {disfmarker} an inner loop and an outer loop in these iterations ?&#10;Speaker: PhD C&#10;Content: OK ." />
    <node id="er} I realized to use it I 'd need to have these short analysis frames get plugged directly into the feature computation somehow&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Grad D&#10;Content: and right now I think our feature computation is set to up to , um , {vocalsound} take , um , audio as input , in general . So I decided that I {disfmarker} I 'll do the reverberation removal on the long analysis windows and then just re - synthesize audio and then send that .&#10;Speaker: Professor B&#10;Content: This is in order to use the SRI system or something . Right ?&#10;Speaker: Grad D&#10;Content: Um , or {disfmarker} or even if I 'm using our system , I was thinking it might be easier to just re - synthesize the audio ,&#10;Speaker: Professor B&#10;Content: Yeah ?&#10;Speaker: Grad D&#10;Content: because then I could just feacalc as is and I wouldn't have to change the code .&#10;Speaker: Professor B&#10;Content: Oh , OK . Yeah . I mean , it 's {disfmarker} um ," />
    <node id="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements." />
    <node id=" what they were doing wasn't so bad at all .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: But , um .&#10;Speaker: PhD C&#10;Content: Yeah . It seems like you should try to make your baseline as good as possible . And if it turns out that {pause} you can't improve on that , well , I mean , then , you know , nobody wins and you just use MFCC . Right ?&#10;Speaker: Professor B&#10;Content: Yeah . I mean , it seems like , uh , it should include sort of the current state of the art {vocalsound} that you want {disfmarker} are trying to improve , and MFCC 's , you know , or PLP or something {disfmarker} it seems like {vocalsound} reasonable baseline for the features , and anybody doing this task , {vocalsound} uh , is gonna have some sort of voice activity detection at some level , in some way . They might use the whole recognizer to do it {vocalsound} but {disfmarker} rather than {vocalsound} a separate thing , but" />
    <node id=" suppose you 've {disfmarker} that {vocalsound} what you really wanna do is put a good pitch detector on there and if it gets an unambiguous {disfmarker}&#10;Speaker: PhD C&#10;Content: Oh , oh . I see .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: if it gets an unambiguous result then you 're definitely in a {disfmarker} in a {disfmarker} in a voice in a , uh , s region with speech . Uh .&#10;Speaker: PhD C&#10;Content: So there 's this assumption that the v the voice activity detector can only use the MFCC ?&#10;Speaker: PhD A&#10;Content: That 's not clear , but this {disfmarker} {vocalsound} e&#10;Speaker: Professor B&#10;Content: Well , for the baseline .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So {disfmarker} so if you use other features then y But it 's just a question of what is your baseline . Right ? What is it that you 're supposed" />
    <node id=" , just after this . So I think spectral subtraction , LDA filtering , and on - line normalization , so which is similar to {vocalsound} the pro proposal - one , but with {pause} spectral subtraction in addition , and it seems that on - line normalization doesn't help further when you have spectral subtraction .&#10;Speaker: PhD C&#10;Content: Is this related to the issue that you brought up a couple of meetings ago with the {disfmarker} the {vocalsound} musical tones&#10;Speaker: PhD A&#10;Content: I {disfmarker}&#10;Speaker: PhD C&#10;Content: and {disfmarker} ?&#10;Speaker: PhD A&#10;Content: I have no idea , because the issue I brought up was with a very simple spectral subtraction approach ,&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: and the one that {vocalsound} they use at OGI is one from {disfmarker} from {vocalsound} the proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much" />
    <node id=" . So you really need to put more {disfmarker} more in the {disfmarker} in {disfmarker} in the front - end .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So i&#10;Speaker: Professor B&#10;Content: Um ,&#10;Speaker: PhD A&#10;Content: S&#10;Speaker: Professor B&#10;Content: sure . But i bu&#10;Speaker: PhD C&#10;Content: Wait a minute . I {disfmarker} I 'm confused .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Wha - what do you mean ?&#10;Speaker: PhD A&#10;Content: Yeah , if i&#10;Speaker: Professor B&#10;Content: So y so you m s Yeah , but {disfmarker} Well , let 's say for ins see , MFCC for instance doesn't have anything in it , uh , related to the pitch . So just {disfmarker} just for example . So suppose you 've {disfmarker} that {vocalsound} what you really wanna do is put a good pitch detector on there and if it" />
    <node id=" anything {vocalsound} ac {vocalsound} make any {comment} policy change because {vocalsound} {vocalsound} everybody has {disfmarker} has , uh , their own opinion&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: and {disfmarker} I don't know .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh , so {disfmarker} Yeah . Yeah , but there is probably a {disfmarker} a big change that will {vocalsound} be made is that the {disfmarker} the baseline {disfmarker} th they want to have a new baseline , perhaps , which is , um , MFCC but with {vocalsound} a voice activity detector . And apparently , {vocalsound} uh , some people are pushing to still keep this fifty percent number . So they want {vocalsound} to have at least fifty percent improvement on the baseline , but w which would be a much better baseline .&#10;Speaker: Professor B&#10;Content: M" />
    <node id="The frequency range used for computing the variance in the example being discussed by the speakers is from 200 Hz to 1500 Hz. One of the speakers later corrects this range to 250 thousand Hz, but it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." />
    <node id=" . I have here one example if you {disfmarker} if you want see something like that .&#10;Speaker: PhD A&#10;Content: Then we compute the difference .&#10;Speaker: PhD C&#10;Content: and ,&#10;Speaker: PhD A&#10;Content: Yeah . Uh - huh .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD C&#10;Content: uh , sum the differences ?&#10;Speaker: PhD A&#10;Content: So . And I think the variance is computed only from , like , two hundred hertz to {pause} one {disfmarker} to fifteen hundred .&#10;Speaker: PhD C&#10;Content: Oh ! OK .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Two thou two {disfmarker} {comment} fifteen hundred ?&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Because {disfmarker}&#10;Speaker: PhD F&#10;Content: No .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Two hundred and fifty thousand .&#10;Speaker: PhD" />
    <node id="} we draw its parameters across time for a clean sentence and then nois the same noisy sentence , it 's very close .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Yeah . So there are {disfmarker} there is this . There could be also the , um {disfmarker} {vocalsound} something like the maximum of the auto - correlation function or {disfmarker} which {disfmarker}&#10;Speaker: PhD C&#10;Content: Is this a {disfmarker} a s a trained system ? Or is it a system where you just pick some thresholds ? Ho - how does it work ?&#10;Speaker: PhD A&#10;Content: Right now we just are trying to find some features . And ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: uh {disfmarker} Yeah . Hopefully , I think what we want to have is to put these features in s some kind of , um {disfmarker} well , to {disfmarker} to obtain a statistical model on these features and to {disfmarker" />
    <node id=" - hmm .&#10;Speaker: PhD F&#10;Content: and we extend these coefficient between the {disfmarker} all the frequency range .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: And i the interpolation i between the point {vocalsound} is {disfmarker} give for the triang triangular filter , the value of the triangular filter and of this way we obtained this mode this model speech .&#10;Speaker: PhD A&#10;Content: S&#10;Speaker: Professor B&#10;Content: So you essentially take the values that {disfmarker} th that you get from the triangular filter and extend them to sor sort of like a rectangle , that 's at that m value .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Yeah . I think we have linear interpolation .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So we have {disfmarker} we have one point for {disfmarker} one energy for each filter bank ,&#10;Speaker: PhD F&#10;Content: mmm Yeah , it" />
    <node id="1. Benefits of smooth representation for generalization: A smooth representation generally leads to better generalization because it reduces overfitting by averaging out fine details and noise in the data. This can help improve the model's performance on unseen data, as the model is less likely to learn spurious patterns that only exist in the training set.&#10;&#10;2. Potential drawbacks of losing details during smoothing: Smoothing inevitably leads to loss of information, which might include valuable details about the original signal. This could potentially make the model miss important features or characteristics of the data, leading to reduced accuracy or suboptimal performance in certain cases. In speech recognition, for example, excessive smoothing might cause the loss of crucial information related to phoneme distinction or speaker identity.&#10;&#10;In summary, while smoothing offers benefits such as improved generalization and reduced overfitting, it can also result in the loss of important details that may be necessary for accurate analysis or modeling. Balancing these trade-offs is essential for developing effective speech recognition systems." />
    <node id="er} so , stand standing back from that , you sort of say there 's this very detailed representation .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: You go to a smooth representation .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: You go to a smooth representation cuz this typically generalizes better .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um , but whenever you smooth you lose something , so the question is have you lost something you can you use ?&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: Um , probably you wouldn't want to go to the extreme of just ta saying &quot; OK , our feature set will be the FFT &quot; , cuz we really think we do gain something in robustness from going to something smoother , but maybe there 's something that we missed .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So what is it ?&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor" />
    <node id="In the meeting transcript, PhD C reported on their experiments related to the HTK (Hidden Markov Model Toolkit) back-end models. They focused on the number of training iterations in different rounds of the model training process. Specifically, there are four rounds of training, and the number of iterations for each round is as follows:&#10;&#10;1. First round: 3 iterations&#10;2. Second to fourth rounds: 7 iterations for each round&#10;&#10;These iterations refer to the number of times the HMM (Hidden Markov Model) re-estimation is run during each training round. The higher number of iterations in later rounds indicates that more computational resources are devoted to refining the models in those stages." />
    <node id=" doo .&#10;Speaker: Professor B&#10;Content: So .&#10;Speaker: PhD C&#10;Content: I have something just fairly brief to report on .&#10;Speaker: Professor B&#10;Content: Mmm .&#10;Speaker: PhD C&#10;Content: Um , I did some {pause} experim uh , uh , just a few more experiments before I had to , {vocalsound} uh , go away for the w well , that week .&#10;Speaker: Professor B&#10;Content: Great !&#10;Speaker: PhD C&#10;Content: Was it last week or whenever ? Um , so what I was started playing with was the {disfmarker} th again , this is the HTK back - end . And , um , I was curious because the way that they train up the models , {vocalsound} they go through about four sort of rounds of {disfmarker} of training . And in the first round they do {disfmarker} uh , I think it 's three iterations , and for the last three rounds e e they do seven iterations of re - estimation in each of those three . And so , you know , that 's part of what takes so long to train the {d" />
    <node id=" Grad E&#10;Content: Hmm .&#10;Speaker: PhD C&#10;Content: So we could do a lot more experiments and throw a lot more stuff in there .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: That 's great .&#10;Speaker: PhD C&#10;Content: Um . Oh , the other thing that I did was , um , {vocalsound} I compiled {pause} the HTK stuff for the Linux boxes . So we have this big thing that we got from IBM , which is a five - processor machine . Really fast , but it 's running Linux . So , you can now run your experiments on that machine and you can run five at a time and it runs , {vocalsound} uh , as fast as , you know , uh , five different machines .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , um , I 've forgotten now what the name of that machine is but I can {disfmarker} I can send email around about it .&#10;Speaker: PhD A&#10;Content: Yeah ." />
    <node id="aker: Professor B&#10;Content: And you also can choose your features in such a way as to prove {disfmarker} improve recognition . They may not be the same thing .&#10;Speaker: PhD C&#10;Content: But it seems like you should do both .&#10;Speaker: Professor B&#10;Content: You should do both&#10;Speaker: PhD C&#10;Content: Right ?&#10;Speaker: Professor B&#10;Content: and {disfmarker} and I {disfmarker} I think that this still makes {disfmarker} I still think this makes sense as a baseline . It 's just saying , as a baseline , we know {disfmarker}&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: you know , we had the MFCC 's before , lots of people have done voice activity detectors ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: you might as well pick some voice activity detector and make that the baseline , just like you picked some version of HTK and made that the baseline .&#10;Speaker: PhD A&#10;Content: Yeah . Right .&#10;Speaker" />
    <node id=" , just like you picked some version of HTK and made that the baseline .&#10;Speaker: PhD A&#10;Content: Yeah . Right .&#10;Speaker: Professor B&#10;Content: And then {pause} let 's try and make everything better . Um , and if one of the ways you make it better is by having your features {pause} be better features for the VAD then that 's {disfmarker} so be it .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: But , uh , uh , uh , at least you have a starting point that 's {disfmarker} um , cuz i i some of {disfmarker} the some of the people didn't have a VAD at all , I guess . Right ? And {disfmarker} and&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: then they {disfmarker} they looked pretty bad and {disfmarker} and in fact what they were doing wasn't so bad at all .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mm - hmm ." />
    <node id="The frequency range used for computing the variance in the example discussed by the speakers is from 200 Hz to 1500 Hz. This is stated explicitly by speaker A, and although professor F mistakenly gives a higher frequency range, it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." />
    <node id="1. The voiced-unvoiced (VU) detector's purpose is to determine if a given segment of speech is voiced or unvoiced. Voiced sounds are produced when the vocal cords vibrate, such as vowel sounds, whereas unvoiced sounds result from airflow through the mouth without vocal cord vibration, like consonant sounds (e.g., 'f' or 's').&#10;2. In this conversation, the VU detector is intended to be used as a feature extractor for speech processing tasks, not part of a typical voiced-unvoiced detection system. The idea is that it could be incorporated into either a separate neural network designed specifically for VU detection or combined with a larger neural network responsible for phoneme classification.&#10;3. Additionally, there is discussion about using the VU detector as part of an improved front-end processing system to enhance pitch detection, which is crucial for better speech processing performance since Mel Frequency Cepstral Coefficients (MFCCs) do not include any pitch-related information. By implementing a good pitch detection algorithm, they aim to improve voice activity detection and other speech processing tasks' accuracy and robustness.&#10;&#10;In summary, the voiced-unvoiced detector is intended for use as a feature extractor in speech processing tasks and potentially improving pitch detection algorithms within front-end processing systems." />
    <node id=" at a few different {vocalsound} ways of look of characterizing that difference and , uh , you could have one of them but {disfmarker} and {disfmarker} and see , you know , which of them helps .&#10;Speaker: PhD A&#10;Content: Mm - hmm . OK .&#10;Speaker: PhD C&#10;Content: So i is the idea that you 're going to take {pause} whatever features you develop and {disfmarker} and just add them onto the future vector ? Or , what 's the use of the {disfmarker} the voiced - unvoiced detector ?&#10;Speaker: PhD A&#10;Content: Uh , I guess we don't know exactly yet . But , {vocalsound} um {disfmarker} Yeah . Th&#10;Speaker: PhD C&#10;Content: It 's not part of a VAD system that you 're doing ?&#10;Speaker: PhD F&#10;Content: No .&#10;Speaker: PhD A&#10;Content: Uh , no . No .&#10;Speaker: PhD C&#10;Content: Oh , OK .&#10;Speaker: PhD A&#10;Content: No , the idea was , I guess , to" />
    <node id="&#10;Speaker: PhD C&#10;Content: Oh , OK .&#10;Speaker: PhD A&#10;Content: No , the idea was , I guess , to {disfmarker} to use them as {disfmarker} as features .&#10;Speaker: PhD C&#10;Content: Features . I see .&#10;Speaker: PhD A&#10;Content: Uh {disfmarker} Yeah , it could be , uh {disfmarker} it could be {vocalsound} a neural network that does voiced and unvoiced detection ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: but it could be in the {disfmarker} also the big neural network that does phoneme classification .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mmm . Yeah .&#10;Speaker: Professor B&#10;Content: But each one of the mixture components {disfmarker} I mean , you have , uh , uh , variance only , so it 's kind of like you 're just multiplying together these , um , probabilities from the individual features {pause} within each mixture . So it" />
    <node id="1. Advanced processing techniques: If re-synthesis turns out to be useful in the long term, more advanced or customized processing techniques can be applied to the signal. (Potential long-term benefits of using re-synthesis)&#10;2. Robustness: Even with the sacrifice of a smooth spectral envelope, combining probabilities of individual features within each mixture might lead to more robust results, as suggested by Professor B and PhD A when discussing the comparison between raw data and smooth versions of the spectrum.&#10;3. Exploration of statistical measures: By analyzing the difference between raw data and the smooth version, new statistical measures can be explored and added to feature vectors to improve performance in noisy environments. This approach is mentioned by PhD as a way of finding something that might have been missed in the smoothed spectrum but could contribute positively to the final result.&#10;4. Potential improvements through spectral subtraction and LDA filtering: As suggested by Professor B, combining probabilities within each mixture along with spectral subtraction and LDA filtering might yield better results than simpler approaches, as mentioned in a previous meeting. However, this depends on the specific implementation of spectral subtraction and its compatibility with online normalization.&#10;5. Addressing musical tones issue: Although not explicitly stated, combining probabilities within each mixture might help address issues related to musical tones raised by PhD A in a previous meeting. However, further investigation is needed to confirm this potential advantage." />
    <node id=" , so it 's kind of like you 're just multiplying together these , um , probabilities from the individual features {pause} within each mixture . So it 's {disfmarker} so , uh , it seems l you know {disfmarker}&#10;Speaker: PhD C&#10;Content: I think it 's a neat thing . Uh , it seems like a good idea .&#10;Speaker: Professor B&#10;Content: Yeah . Um . Yeah . I mean , {vocalsound} I know that , um , people doing some robustness things a ways back were {disfmarker} were just doing {disfmarker} just being gross and just throwing in the FFT and actually it wasn't {disfmarker} wasn't {disfmarker} wasn't so bad . Uh , so it would s and {disfmarker} and you know that i it 's gotta hurt you a little bit to not have a {disfmarker} {vocalsound} a spectral , uh {disfmarker} a s a smooth spectral envelope , so there must be something else that you get {pause} in return for that {disfmarker}&#10;Speaker: PhD" />
    <node id=" .&#10;Speaker: PhD F&#10;Content: Here .&#10;Speaker: PhD A&#10;Content: They should be more close .&#10;Speaker: PhD F&#10;Content: Ah , no . This is this ? More close . Is this ? And this .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So they are {disfmarker} this is {disfmarker} there is less difference .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So if you look at the spectral envelope , just the very smooth properties of it , {vocalsound} you get something closer to that .&#10;Speaker: PhD A&#10;Content: This is less {disfmarker} it 's less robust .&#10;Speaker: PhD F&#10;Content: Less robust . Yeah .&#10;Speaker: PhD A&#10;Content: Oh , yeah .&#10;Speaker: Professor B&#10;Content: And the notion is if you have the full spectrum , with all the little nitty -" />
    <node id="1. The process for drawing a system's parameters across time for a clean sentence and a noisy sentence involves analyzing the characteristics of speech signals in both scenarios. This likely includes examining various attributes such as frequency content, amplitude, and duration of sounds within the sentences. By comparing these parameters between clean and noisy sentences, researchers can gain insights into how noise affects speech perception and develop methods to mitigate its impact.&#10;&#10;2. Features are being used in an attempt to create a statistical model for this process by extracting relevant characteristics from the speech signals that highlight differences between clean and noisy sentences. In this context, features might include measures related to voiced vs. unvoiced sounds, pitch detection, or other spectral properties. The goal is to build a statistical model using these features that can accurately differentiate between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;The discussion revolves around finding suitable features for this task, such as the maximum of the auto-correlation function, which could help characterize voiced phonemes more effectively. The team aims to use these features to create a statistical model that captures the differences between clean and noisy sentences, ultimately leading to better speech recognition performance." />
    <node id=" details and all the ways which is {disfmarker} that these are complete lies . Uh , the {disfmarker} the {disfmarker} you know , what you 're doing in feature extraction for speech recognition is you have , {vocalsound} uh , in your head a {disfmarker} a {disfmarker} a {disfmarker} a simplified production model for speech ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: in which you have a periodic or aperiodic source that 's driving some filters .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Yeah . This is the {disfmarker} the auto - correlation {disfmarker} the R - zero energy .&#10;Speaker: PhD A&#10;Content: Do you have the mean {disfmarker} do you have the mean for the auto - correlation {disfmarker} ?&#10;Speaker: Professor B&#10;Content: Uh , first order for speech recognition , you say &quot; I don't care about the source &quot; .&#10;Speaker: PhD F" />
    <node id="1. Choosing a weak baseline: If the baseline is not robust or representative of the current state-of-the-art, comparing improvements against it might not provide accurate insights into the performance of new methods. This could lead to underestimating the true potential of novel approaches.&#10;&#10;2. Limited comparative evaluation: Reducing efforts in feature design may result in fewer comparisons with alternative features or methods, which could be crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;3. Incomplete VAD design: A simpler feature design might not capture all aspects required for a good Voice Activity Detection (VAD) system. Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments.&#10;&#10;4. Relying solely on the recognizer for VAD: While it is possible to use the whole recognizer to perform VAD, this approach may not always provide optimal results compared to dedicated VAD systems tailored to specific applications and signal types.&#10;&#10;5. Fairness concerns: It might be considered unfair to compare a simplified feature design to more complex approaches if additional baseline features are required to create a good VAD system. This could lead to an inaccurate assessment of the performance of novel methods, potentially favoring simpler techniques that do not fully address the problem at hand.&#10;&#10;To ensure fairness and accurate comparisons, it is essential to:&#10;- Choose a robust and representative baseline.&#10;- Perform thorough evaluations against various alternatives.&#10;- Consider all aspects required for a good VAD system, including complexity and real-time requirements." />
    <node id="} so if you use other features then y But it 's just a question of what is your baseline . Right ? What is it that you 're supposed to do better than ?&#10;Speaker: PhD C&#10;Content: I g Yeah .&#10;Speaker: Professor B&#10;Content: And so having the baseline be the MFCC 's {pause} means that people could {pause} choose to pour their ener their effort into trying to do a really good VAD&#10;Speaker: PhD C&#10;Content: I don't s But they seem like two {pause} separate issues .&#10;Speaker: Professor B&#10;Content: or tryi They 're sort of separate .&#10;Speaker: PhD C&#10;Content: Right ? I mean {disfmarker}&#10;Speaker: Professor B&#10;Content: Unfortunately there 's coupling between them , which is part of what I think Stephane is getting to , is that {vocalsound} you can choose your features in such a way as to improve the VAD .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: And you also can choose your features in such a way as to prove {disfmarker} improve recognition . They may" />
    <node id=" and as well as being a lot of work to do a good job on the feature {vocalsound} design ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: so&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: if we can {pause} cut down on that maybe we can make some progress .&#10;Speaker: PhD A&#10;Content: M Yeah .&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: But I guess perhaps {disfmarker} I don't know w {vocalsound} Yeah . Uh , yeah . Per - e s s someone told that perhaps it 's not fair to do that because the , um {disfmarker} to make a good VAD {pause} you don't have enough to {disfmarker} with the {disfmarker} the features that are {disfmarker} the baseline features . So {disfmarker} mmm , you need more features . So you really need to put more {disfmarker} more in the {disfmarker} in {disfmarker} in the" />
    <node id="Content: Good . Work to do .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: So whose VAD is {disfmarker} Is {disfmarker} is this a {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Uh , they didn't decide yet . I guess i this was one point of the conference call also , but {disfmarker} mmm , so I don't know . Um , but {disfmarker} Yeah .&#10;Speaker: Grad E&#10;Content: Oh .&#10;Speaker: Professor B&#10;Content: Oh , I {disfmarker} I think th that would be {vocalsound} good . I mean , it 's not that the design of the VAD isn't important , but it 's just that it {disfmarker} it {disfmarker} it does seem to be i uh , a lot of {pause} work to do a good job on {disfmarker} on that and as well as being a lot of work to do a good job on the feature {vocalsound} design ,&#10;Speaker: PhD A&#10;Content" />
    <node id="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments." />
    <node id="The speakers, who appear to be discussing the evaluation and improvement of a signal processing system, are expressing uncertainty about the final application of the system and whether it will need to handle more difficult cases with high accuracy. Professor B mentions that if the predominate use of the system is in challenging scenarios, current methods may not be sufficient. However, in better cases, the system could potentially work well with some error recovery.&#10;&#10;Later in the discussion, the speakers consider a variance calculation done on a frequency range of 200 Hz to 1500 Hz, which they believe is more logical for their analysis. They also discuss different methods for weighting error rates obtained from various languages and conditions, with some arguing that it would be better to combine error rates before computing improvements.&#10;&#10;In summary, the speakers are discussing the uncertainty of the system's primary use, the potential for reasonable error recovery in better cases, and different approaches to weighing error rates for improved performance in various scenarios." />
    <node id="marker} So , bu&#10;Speaker: PhD C&#10;Content: It sounds like they don't really have a good idea about what the final application is gonna be .&#10;Speaker: PhD A&#10;Content: l de fff ! Mmm .&#10;Speaker: Professor B&#10;Content: Well , you know , the {disfmarker} the thing is {vocalsound} that if you look at the numbers on the {disfmarker} on the more difficult cases , {vocalsound} um , if you really believe that was gonna be the predominant use , {vocalsound} none of this would be good enough .&#10;Speaker: PhD A&#10;Content: Yeah . Mmm . Yeah .&#10;Speaker: Professor B&#10;Content: Nothing anybody 's {disfmarker}&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: whereas {vocalsound} you sort of with some reasonable error recovery could imagine in the better cases that these {disfmarker} these systems working . So , um , I think the hope would be that it would {disfmarker} {vocalsound} uh , it would work well {" />
    <node id=" than the previous standard .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , um , so they said &quot; how much is significantly better ? what do you {disfmarker} ? &quot; And {disfmarker} and so they said &quot; well , {vocalsound} you know , you should have half the errors , &quot; or something , &quot; that you had before &quot; .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Hmm .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So it 's , uh , But it does seem like&#10;Speaker: PhD C&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: i i it does seem like it 's more logical to combine them first and then do the {disfmarker}&#10;Speaker: PhD A&#10;Content: Combine error rates and then {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Yeah . Well {disfmarker}&#10;Speaker:" />
    <node id="Professionor B expresses that making any policy change in meetings is tricky because everyone has different perspectives and opinions. They imply that reaching a consensus or agreement can be difficult due to these varied viewpoints." />
    <node id=": Professor B&#10;Content: Just say &quot; OK , in the {disfmarker} in the highly - matched case this is what happens , in the {disfmarker} {vocalsound} m the , uh {disfmarker} this other m medium if this happens , in the highly - mismatched {pause} that happens &quot; .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh , you should see , uh , a gentle degradation {pause} through that .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Um . But {disfmarker} I don't know .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I think that {disfmarker} that {disfmarker} I {disfmarker} I {disfmarker} I gather that in these meetings it 's {disfmarker} it 's really tricky to make anything {vocalsound} ac {vocalsound} make any {comment} policy change because {vocalsound} {vocalsound} everybody has" />
    <node id=": Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . OK .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: What else ?&#10;Speaker: PhD A&#10;Content: Uh . {vocalsound} Yeah , that 's all . So we 'll perhaps {vocalsound} {vocalsound} {vocalsound} try to convince OGI people to use the new {disfmarker} {vocalsound} the new filters and {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: OK . Uh , has {disfmarker} has anything happened yet on this business of having some sort of standard , uh , source ,&#10;Speaker: PhD A&#10;Content: Uh , not yet&#10;Speaker: Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: but I wi I will {vocalsound} call them and {disfmarker}&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker" />
    <node id="Based on the transcript, PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. She mentions that her speech might be problematic for the unvoiced part of the recognizer and that she has a stronger accent or uses different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause." />
    <node id="1. The process for drawing a system's parameters across time involves analyzing the characteristics of speech signals in both clean and noisy sentences, comparing attributes such as frequency content, amplitude, and duration of sounds within the sentences to understand how noise affects speech perception and develop mitigation methods. Features are extracted from the speech signals to create a statistical model that differentiates between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;2. The team is currently focused on finding suitable features for this task, with one suggestion being the maximum of the auto-correlation function to characterize voiced phonemes more effectively. They have not yet decided whether they will use a trained system or threshold-based system to implement these features in their statistical model.&#10;&#10;3. A noisy sentence is created by adding noise to a clean sentence, as demonstrated when Professor B mentioned that drawing the parameters across time for a clean sentence and then adding noise results in a similar (noisy) sentence. However, the specific method or techniques used to create the noisy sentence are not discussed in the provided transcripts." />
    <node id="1. Length and number of periods: A short frame contains only two periods of the signal, while the original frame contains more depending on the signal's characteristics. This difference is due to the time-frequency trade-off, which requires shorter frames for better frequency resolution but longer frames for better time resolution.&#10;&#10;2. FFT size (number of points in the Fast Fourier Transform): The speakers also discuss differences in FFT size between the short frame and the original frame. A larger FFT size results in better frequency resolution but coarser time resolution, whereas a smaller FFT size has the opposite effect.&#10;&#10;3. Robustness to noise: The speakers mention that using a shorter frame can help improve robustness to noise in signal processing applications like speech analysis. When comparing the same frame from noisy and clean utterances, the difference can be used to better understand and handle noise." />
    <node id="mm .&#10;Speaker: PhD C&#10;Content: Oh . Oh , so this i is this the difference here , for that ?&#10;Speaker: PhD F&#10;Content: No . This is the signal . This is the signal .&#10;Speaker: PhD A&#10;Content: I see that . Oh , yeah .&#10;Speaker: PhD F&#10;Content: The frame .&#10;Speaker: PhD C&#10;Content: Oh , that 's the f the original .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: This is the fra the original frame .&#10;Speaker: PhD A&#10;Content: So with a short frame basically you have only two periods&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and it 's not {disfmarker} not enough to {disfmarker} to have this kind of neat things .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: But {disfmarker}&#10;Speaker: PhD F&#10;" />
    <node id=" see this periodic structure ,&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: because of the first lobe of {disfmarker} of each {disfmarker} each of the harmonics .&#10;Speaker: PhD C&#10;Content: So this one inclu is a longer {disfmarker} Ah .&#10;Speaker: PhD A&#10;Content: So , this is like {disfmarker} yeah , fifty milliseconds or something like that .&#10;Speaker: PhD F&#10;Content: Fifty millis Yeah .&#10;Speaker: PhD A&#10;Content: Yeah , but it 's the same frame and {disfmarker}&#10;Speaker: PhD C&#10;Content: Oh , it 's that time - frequency trade - off thing .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Right ? I see . Yeah .&#10;Speaker: PhD A&#10;Content: So , yeah .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: Oh . Oh , so this i is this the difference here , for that ?&#10;Speaker: PhD" />
    <node id="Speaker: PhD C&#10;Content: Oh , OK&#10;Speaker: PhD F&#10;Content: But they are a difference .&#10;Speaker: PhD A&#10;Content: Yeah , that 's {disfmarker}&#10;Speaker: PhD F&#10;Content: Because here the FFT is only with {vocalsound} two hundred fifty - six point&#10;Speaker: PhD C&#10;Content: Oh .&#10;Speaker: PhD F&#10;Content: and this is with five hundred {pause} twelve .&#10;Speaker: PhD A&#10;Content: Yeah . This is kind of inter interesting also&#10;Speaker: PhD C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: because if we use the standard , {vocalsound} uh , frame length of {disfmarker} of , like , twenty - five milliseconds , {vocalsound} um , {vocalsound} what happens is that for low - pitched voiced , because of the frame length , y you don't really have {disfmarker} {vocalsound} you don't clearly see this periodic structure ,&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: because of the" />
    <node id="1. When Professor B suggests that imperfect distinctions between voiced and unvoiced sounds might not be a significant issue, they are implying that the primary value in this analysis comes from obtaining a general characterization of the speech signal based on our intuition about these categories. This rough characterization can provide insights into the raw data, even if it does not offer a clear-cut distinction between voiced and unvoiced sounds due to overlaps and complexities in speech.&#10;2. The professor emphasizes that relying on this basic intuition might be more valuable than using overly refined or smoothed versions of the signal since real-world data, such as noise, can impact the accuracy of any speech recognition system. Consequently, understanding how noise affects speech perception and developing methods to mitigate its effects is essential for improving speech recognition systems' performance in various environments.&#10;3. The team is currently focused on selecting appropriate features for their task, with one possibility being the maximum of the auto-correlation function to better characterize voiced phonemes. However, they have not yet decided whether to use a trained system or threshold-based system to implement these features in their statistical model.&#10;4. Professor B implies that even if the &quot;world's best voiced-unvoiced classifier&quot; does not perfectly distinguish between these categories, trying different statistical characterizations of the difference based on raw data may still yield valuable information that a speech recognition system can utilize." />
    <node id="1. It is potentially unfair or insufficient to base a good vocal sound design on just the baseline features because the chosen baseline might not be robust or representative of the current state-of-the-art, leading to underestimating the true potential of novel approaches. Additionally, limiting comparative evaluations by reducing efforts in feature design can result in fewer comparisons with alternative features or methods, which are crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;2. The benefits of incorporating additional features into the vocal sound design include:&#10;  a) Improved robustness and fairness: Adding more features to the design ensures that the evaluation is based on a comprehensive comparison, reducing potential biases towards simpler techniques that may not fully address the problem at hand.&#10;  b) Enhanced understanding of strengths and weaknesses: Comparing the new method against various alternatives provides insights into its performance relative to different types of features and methods, aiding in identifying the most suitable approach for specific scenarios or environments.&#10;  c) Better generalization: Including diverse features can help average out fine details and noise in the data, reducing overfitting and improving the model's performance on unseen data.&#10;  d) Capturing all aspects required for a good VAD system: Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments, ensuring that the system can effectively handle various challenges associated with voice activity detection." />
    <node id="Based on the given transcript, when transitioning from a well-matched case to a mismatched case in terms of speaker features, there is a noticeable difference in performance. Specifically, the system performs better in the well-matched case but seems to degrade slightly for mismatched and highly-mismatched conditions when a neural network is implemented. This implies that the current weighting setup may favor the well-matched case, which might be an overestimation of its true performance in real-world scenarios where data is often more diverse and less controlled.&#10;&#10;As for the impact on vocal sound design and feature selection, it is essential to consider various techniques and features beyond simple baselines or MFCC (Mel Frequency Cepstral Coefficients) due to their limited pitch-related information. Including additional features can improve robustness, fairness, understanding of strengths and weaknesses, better generalization, and overall performance in complex scenarios or noisy environments.&#10;&#10;To address the issue of varying performance across different cases, one should investigate various pitch detection algorithms suitable for the specific application and signal type. After integrating the chosen algorithm into the existing front-end processing system, evaluate its performance by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates. Fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements. Additionally, when selecting features for vocal sound design, consider a comprehensive comparison involving various techniques and baselines to ensure robustness and fairness in performance evaluation." />
    <node id=" Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Because we 're still testing . So we have the result for , {vocalsound} uh , just the features&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: and we are currently testing with putting the neural network in the KLT . Um , it seems to improve on the well - matched case , um , {vocalsound} but it 's a little bit worse on the mismatch and highly - mismatched {disfmarker} I mean when we put the neural network . And with the current weighting I think it 's sh it will be better because the well - matched case is better . Mmm .&#10;Speaker: Professor B&#10;Content: But how much worse {disfmarker} since the weighting might change {disfmarker} how {disfmarker} how much worse is it on the other conditions , when you say it 's a little worse ?&#10;Speaker: PhD A&#10;Content: It 's like , uh ," />
    <node id=" , are you gonna have w uh , uh , examples with the windows open , half open , full open ? Going seventy , sixty , fifty , forty miles an hour ? On what kind of roads ?&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: With what passing you ? With {disfmarker} uh , I mean ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: I {disfmarker} I {disfmarker} I think that you could make the opposite argument that the well - matched case is a fantasy .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: You know , so ,&#10;Speaker: Grad E&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: I think the thing is is that if you look at the well - matched case versus the po you know , the {disfmarker} the medium and the {disfmarker} and the fo and then the mismatched case , {vocalsound} um , we 're seeing really , really big differences in performance" />
    <node id="Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Yeah . Well {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: But there is this {disfmarker} this {disfmarker} is this still this problem of weights . When {disfmarker} when you combine error rate it tends to {pause} give more importance to the difficult cases , and some people think that {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , yeah ?&#10;Speaker: PhD A&#10;Content: well , they have different , {vocalsound} um , opinions about this . Some people think that {vocalsound} it 's more important to look at {disfmarker} {vocalsound} to have ten percent imp relative improvement on {pause} well - matched case than to have fifty percent on the m mismatched , and other people think that it 's more important to improve a lot on the mismatch and {disfmarker} So , bu&#10;Speaker: PhD C&#10;Content: It sounds like they don't really have a good idea about what the final application is" />
    <node id="The results of implementing VAD, noise compensation (specifically spectral subtraction), and normalization (on-line normalization) techniques on the baseline experiments from OGI since last week showed improved performance when using these additional features. The comparison was made between two groups of proposals, one with Voice Activity Detection (VAD) and the other without. It was found that the group with VAD performed better than the group without VAD. This demonstrates that incorporating these techniques can lead to improved robustness, fairness, understanding of strengths and weaknesses, better generalization, and more effective handling of complex scenarios or noisy environments in VAD systems. However, it was noted that online normalization did not provide further improvements when spectral subtraction was already in use. The relationship between these findings and a previous issue related to musical tones was unclear due to differences in the spectral subtraction approaches used." />
    <node id="isfmarker} and then when the {disfmarker} the {disfmarker} the proposals actually came in and half of them had V A Ds and half of them didn't , and the half that did did well and the {vocalsound} half that didn't did poorly .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Mm - hmm . Um .&#10;Speaker: Professor B&#10;Content: Uh .&#10;Speaker: PhD A&#10;Content: Yeah . So we 'll see what happen with this . And {disfmarker} Yeah . So what happened since , um , {vocalsound} last week is {disfmarker} well , from OGI , these experiments on {pause} putting VAD on the baseline . And these experiments also are using , uh , some kind of noise compensation , so spectral subtraction , and putting on - line normalization , um , just after this . So I think spectral subtraction , LDA filtering , and on - line normalization , so which is similar to {vocalsound" />
    <node id="Based on the transcript, there is no explicit information given about whether Hynek or Sunil from OGI (Oregon Graduate Institute) were part of the Aurora conference call. Speaker PhD A mentioned not knowing who was involved from OGI, and the discussion moved on to other topics. Therefore, it cannot be determined from this transcript if Hynek or Sunil were present in the Aurora conference call." />
    <node id=" supposed to discuss is still , {vocalsound} uh , things like {vocalsound} the weights , uh {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , this is a conference call for , uh , uh , Aurora participant sort of thing .&#10;Speaker: Grad E&#10;Content: For {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Professor B&#10;Content: I see .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Do you know who was {disfmarker} who was {disfmarker} since we weren't in on it , uh , do you know who was in from OGI ? Was {disfmarker} {vocalsound} was {disfmarker} was Hynek involved or was it Sunil&#10;Speaker: PhD A&#10;Content: I have no idea .&#10;Speaker: Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Mmm , I just {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , you don't know . OK" />
    <node id=" .&#10;Speaker: PhD A&#10;Content: What 's wrong with {disfmarker} ?&#10;Speaker: Professor B&#10;Content: OK . It 's April fifth . Actually , Hynek should be getting back in town shortly if he isn't already .&#10;Speaker: PhD C&#10;Content: Is he gonna come here ?&#10;Speaker: Professor B&#10;Content: Uh . Well , we 'll drag him here . I know where he is .&#10;Speaker: PhD C&#10;Content: So when you said &quot; in town &quot; , you mean {pause} Oregon .&#10;Speaker: Professor B&#10;Content: U u u u uh , I meant , you know , this end of the world , yeah , {vocalsound} is really what I meant ,&#10;Speaker: PhD C&#10;Content: Oh .&#10;Speaker: Grad E&#10;Content: Doo , doo - doo .&#10;Speaker: Professor B&#10;Content: uh , cuz he 's been in Europe .&#10;Speaker: Grad E&#10;Content: Doo - doo .&#10;Speaker: Professor B&#10;Content: So .&#10;Speaker: PhD C&#10;Content: I have something just fairly brief to report on" />
    <node id="Based on the transcript provided, the exact date in June for the next Aurora deadline was not specified during the conversation. Speaker PhD A knew that it was in June but did not have the exact date. Speaker Professor B asked about early, late, or middle June, but no specific date was given. Additionally, none of the participants provided information about who could provide this information." />
    <node id=" I wi I will {vocalsound} call them and {disfmarker}&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: now they are {disfmarker} I think they have more time because they have this {disfmarker} well , Eurospeech deadline is {vocalsound} over&#10;Speaker: PhD C&#10;Content: When is the next , um , Aurora {pause} deadline ?&#10;Speaker: PhD A&#10;Content: and {disfmarker} It 's , um , in June . Yeah .&#10;Speaker: PhD C&#10;Content: June .&#10;Speaker: Professor B&#10;Content: Early June , late June , middle June ?&#10;Speaker: PhD A&#10;Content: I don't know w&#10;Speaker: Professor B&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: OK . Um , and {pause} he 's been doing all the talking but {disfmarker} but {vocalsound} these {disfmarker} {vocalsound} he 's {disfmarker} he 's , uh {disf" />
    <node id="PHP A considers the new system (proposal two) to be better than the previous one (proposal one) mainly due to two reasons: first, it has a 64 hertz cut-off with clean downsampling, and second, it includes a good Voice Activity Detector (VAD). In terms of latency, PHP A mentions that the latencies are shorter in the new system.&#10;&#10;When comparing proposal two to proposal one, PhD A considers the new system to be &quot;a little worse&quot; only when a neural network is added, but it is still better overall due to its shorter latencies and other improvements made during the transition from proposal one to proposal two." />
    <node id=" new system is {disfmarker} is {disfmarker} is better ,&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: because it has {vocalsound} um , this sixty - four hertz cut - off , uh , clean {vocalsound} downsampling , and , um {disfmarker} what else ? Uh , yeah , a good VAD . We put the good VAD . So . Yeah , I don't know . I {disfmarker} I {disfmarker} j uh , uh {disfmarker} pr&#10;Speaker: Professor B&#10;Content: But the latencies {disfmarker} but you 've got the latency shorter now .&#10;Speaker: PhD A&#10;Content: Latency is short {disfmarker} is {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Isn't it&#10;Speaker: PhD A&#10;Content: And so&#10;Speaker: Professor B&#10;Content: So it 's better than the system that we had before .&#10;Speaker: PhD A&#10;Content" />
    <node id=" And so&#10;Speaker: Professor B&#10;Content: So it 's better than the system that we had before .&#10;Speaker: PhD A&#10;Content: Yeah . Mainly because {pause} {vocalsound} of {pause} the sixty - four hertz and the good VAD .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: And then I took this system and , {vocalsound} mmm , w uh , I p we put the old filters also . So we have this good system , with good VAD , with the short filter and with the long filter , and , um , with the short filter it 's not worse . So {disfmarker} well , is it {disfmarker}&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: it 's in {disfmarker}&#10;Speaker: Professor B&#10;Content: So that 's {disfmarker} that 's all fine .&#10;Speaker: PhD A&#10;Content: Yes . Uh {disfmarker}&#10;Speaker: Professor B&#10;Content: But what you 're saying is that when you do" />
    <node id="1. The medium-mismatch condition refers to a situation where the model is trained on data collected in low noise conditions and at low speeds (e.g., stopped car), but during testing, the data is gathered using a distant microphone in high-speed conditions (e.g., on a highway). This creates a mismatch between the training and testing conditions, making it more challenging for the model to maintain good performance.&#10;&#10;Based on the discussion provided, there are two main aspects being considered:&#10;&#10;- The use of robust features that perform well even with gentle degradation in testing conditions different from training.&#10;- A comparison of original signals to smoothed versions, focusing on low frequencies and exploring statistical measures for improving performance in noisy environments.&#10;&#10;The medium-mismatch condition is introduced as an example of a realistic scenario that requires the model to be robust against changes in car conditions, specifically microphone placement (near vs. far) and speed (low vs. high). This is relevant when discussing potential improvements for front-end processing, such as adding a pitch detector that better handles these differences between training and testing conditions.&#10;&#10;In summary, the medium-mismatch condition involves training on low noisy conditions with one microphone placement but testing with a far microphone in higher speed conditions to assess how well the model can handle real-world variability." />
    <node id="ound} if what you 're {disfmarker} {vocalsound} if the goal of this is to come up with robust features , it does mean {disfmarker} So you could argue , in fact , that the well - matched is something you shouldn't be looking at at all , that {disfmarker} that the goal is to come up with features {vocalsound} that will still give you reasonable performance , you know , with again gentle degregra degradation , um , even though the {disfmarker} the testing condition is not the same as the training .&#10;Speaker: PhD C&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: So , you know , I {disfmarker} I could argue strongly that something like the medium mismatch , which is you know not compl pathological but {disfmarker} I mean , what was the {disfmarker} the medium - mismatch condition again ?&#10;Speaker: PhD A&#10;Content: Um , {vocalsound} it 's {disfmarker} Yeah . Medium mismatch is everything with the far {pause} microphone , but trained on , like , low noisy condition , like low speed and {" />
    <node id="marker} Yeah . Medium mismatch is everything with the far {pause} microphone , but trained on , like , low noisy condition , like low speed and {disfmarker} or {pause} stopped car and tested on {pause} high - speed conditions , I think , like on a highway and {disfmarker}&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: So {disfmarker}&#10;Speaker: Professor B&#10;Content: So it 's still the same {disfmarker} same microphone in both cases ,&#10;Speaker: PhD A&#10;Content: Same microphone but {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: but , uh , it 's {disfmarker} there 's a mismatch between the car conditions . And that 's {disfmarker} uh , you could argue that 's a pretty realistic situation&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: and , uh , I 'd almost argue for weighting that highest . But the way they have it now , {" />
    <edge source="In the transcript provided, PhD A mentions that the new system (proposal two) is &quot;a little worse&quot; when compared to proposal one. This statement is made in the context of comparing the relative improvements between the two proposals. Specifically, they mention that for some cases things are &quot;somewhat better&quot; in proposal two, but for other cases they are &quot;somewhat worse.&quot;&#10;&#10;Later in the transcript, PhD A clarifies what they mean by &quot;worse.&quot; When comparing proposal two to proposal one, the new system is not as good as proposal one when a neural network is added. However, even though it is worse than proposal one in this specific aspect, PhD A still considers the new system to be better overall because of its shorter latencies and other improvements made in the transition from proposal one to proposal two.&#10;&#10;In summary, PhD A considers proposal two to be &quot;a little worse&quot; than proposal one in the specific context of comparing relative improvements and when a neural network is added to the new system. However, they still consider the new system to be better overall due to its shorter latencies and other improvements." target="&#10;Content: Yes . Uh {disfmarker}&#10;Speaker: Professor B&#10;Content: But what you 're saying is that when you do these {disfmarker} So let me try to understand . When {disfmarker} when you do these same improvements {vocalsound} to proposal - one ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: that , uh , on the {disfmarker} i things are somewhat better , uh , in proposal - two for the well - matched case and somewhat worse for the other two cases .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So does , uh {disfmarker} when you say , uh {disfmarker} So {disfmarker} The th now that these other things are in there , is it the case maybe that the additions of proposal - two over proposal - one are {pause} less im important ?&#10;Speaker: PhD A&#10;Content: Yeah . Probably , yeah .&#10;Speaker: Professor B&#10;Content: I get it .&#10;Speaker: PhD A&#10;Content: Um {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="In the transcript provided, PhD A mentions that the new system (proposal two) is &quot;a little worse&quot; when compared to proposal one. This statement is made in the context of comparing the relative improvements between the two proposals. Specifically, they mention that for some cases things are &quot;somewhat better&quot; in proposal two, but for other cases they are &quot;somewhat worse.&quot;&#10;&#10;Later in the transcript, PhD A clarifies what they mean by &quot;worse.&quot; When comparing proposal two to proposal one, the new system is not as good as proposal one when a neural network is added. However, even though it is worse than proposal one in this specific aspect, PhD A still considers the new system to be better overall because of its shorter latencies and other improvements made in the transition from proposal one to proposal two.&#10;&#10;In summary, PhD A considers proposal two to be &quot;a little worse&quot; than proposal one in the specific context of comparing relative improvements and when a neural network is added to the new system. However, they still consider the new system to be better overall due to its shorter latencies and other improvements." target=" it on the other conditions , when you say it 's a little worse ?&#10;Speaker: PhD A&#10;Content: It 's like , uh , fff , fff {comment} {vocalsound} {pause} um , {comment} {vocalsound} {vocalsound} {pause} ten percent relative . Yeah .&#10;Speaker: Professor B&#10;Content: OK . Um .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: But it has the , uh {disfmarker} the latencies are much shorter . That 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Uh - y w when I say it 's worse , it 's not {disfmarker} it 's when I {disfmarker} I {disfmarker} uh , compare proposal - two to proposal - one , so , r uh , y putting neural network {vocalsound} compared to n not having any neural network . I mean , this new system is {disfmarker} is {disfmarker} is better ,&#10;Speaker: Professor B&#10;Content: Uh - huh .">
      <data key="d0">1</data>
    </edge>
    <edge source="In the transcript provided, PhD A mentions that the new system (proposal two) is &quot;a little worse&quot; when compared to proposal one. This statement is made in the context of comparing the relative improvements between the two proposals. Specifically, they mention that for some cases things are &quot;somewhat better&quot; in proposal two, but for other cases they are &quot;somewhat worse.&quot;&#10;&#10;Later in the transcript, PhD A clarifies what they mean by &quot;worse.&quot; When comparing proposal two to proposal one, the new system is not as good as proposal one when a neural network is added. However, even though it is worse than proposal one in this specific aspect, PhD A still considers the new system to be better overall because of its shorter latencies and other improvements made in the transition from proposal one to proposal two.&#10;&#10;In summary, PhD A considers proposal two to be &quot;a little worse&quot; than proposal one in the specific context of comparing relative improvements and when a neural network is added to the new system. However, they still consider the new system to be better overall due to its shorter latencies and other improvements." target=" proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much better . So , yeah . I asked {vocalsound} Sunil for more information about that , but , uh , I don't know yet . Um . And what 's happened here is that we {disfmarker} so we have this kind of new , um , reference system which {vocalsound} use a nice {disfmarker} a {disfmarker} a clean downsampling - upsampling , which use a new filter {vocalsound} that 's much shorter and which also cuts the frequency below sixty - four hertz ,&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: which was not done on our first proposal .&#10;Speaker: Professor B&#10;Content: When you say &quot; we have that &quot; , does Sunil have it now , too ,&#10;Speaker: PhD A&#10;Content: I No .&#10;Speaker: Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Professor B&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="In the transcript provided, PhD A mentions that the new system (proposal two) is &quot;a little worse&quot; when compared to proposal one. This statement is made in the context of comparing the relative improvements between the two proposals. Specifically, they mention that for some cases things are &quot;somewhat better&quot; in proposal two, but for other cases they are &quot;somewhat worse.&quot;&#10;&#10;Later in the transcript, PhD A clarifies what they mean by &quot;worse.&quot; When comparing proposal two to proposal one, the new system is not as good as proposal one when a neural network is added. However, even though it is worse than proposal one in this specific aspect, PhD A still considers the new system to be better overall because of its shorter latencies and other improvements made in the transition from proposal one to proposal two.&#10;&#10;In summary, PhD A considers proposal two to be &quot;a little worse&quot; than proposal one in the specific context of comparing relative improvements and when a neural network is added to the new system. However, they still consider the new system to be better overall due to its shorter latencies and other improvements." target=" {disfmarker} it 's like if you say what 's the {disfmarker} what 's the best way to do an average , an arithmetic average or a geometric average ?&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: It depends what you wanna show .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Each {disfmarker} each one is gonna have a different characteristic .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So {disfmarker}&#10;Speaker: PhD C&#10;Content: Well , it seems like they should do , like , the percentage improvement or something , rather than the {pause} absolute improvement .&#10;Speaker: PhD A&#10;Content: Tha - that 's what they do .&#10;Speaker: Professor B&#10;Content: Well , they are doing that .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: No , that is relative . But the question is , do you average the relative improvements {pause} or do you average the error">
      <data key="d0">1</data>
    </edge>
    <edge source="In the transcript provided, PhD A mentions that the new system (proposal two) is &quot;a little worse&quot; when compared to proposal one. This statement is made in the context of comparing the relative improvements between the two proposals. Specifically, they mention that for some cases things are &quot;somewhat better&quot; in proposal two, but for other cases they are &quot;somewhat worse.&quot;&#10;&#10;Later in the transcript, PhD A clarifies what they mean by &quot;worse.&quot; When comparing proposal two to proposal one, the new system is not as good as proposal one when a neural network is added. However, even though it is worse than proposal one in this specific aspect, PhD A still considers the new system to be better overall because of its shorter latencies and other improvements made in the transition from proposal one to proposal two.&#10;&#10;In summary, PhD A considers proposal two to be &quot;a little worse&quot; than proposal one in the specific context of comparing relative improvements and when a neural network is added to the new system. However, they still consider the new system to be better overall due to its shorter latencies and other improvements." target=" use the whole recognizer to do it {vocalsound} but {disfmarker} rather than {vocalsound} a separate thing , but {disfmarker} {vocalsound} but they 'll have it on some level . So , um .&#10;Speaker: PhD C&#10;Content: It seems like whatever they choose they shouldn't , {vocalsound} you know , purposefully brain - damage a part of the system to {pause} make a worse baseline , or {disfmarker}&#10;Speaker: Professor B&#10;Content: Well , I think people just had&#10;Speaker: PhD C&#10;Content: You know ?&#10;Speaker: Professor B&#10;Content: it wasn't that they purposely brain - damaged it . I think people hadn't really thought through {vocalsound} about the , uh {disfmarker} the VAD issue .&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And {disfmarker} and then when the {disfmarker} the {disfmarker} the proposals actually came in and half of them had V">
      <data key="d0">1</data>
    </edge>
    <edge source="In the transcript provided, PhD A mentions that the new system (proposal two) is &quot;a little worse&quot; when compared to proposal one. This statement is made in the context of comparing the relative improvements between the two proposals. Specifically, they mention that for some cases things are &quot;somewhat better&quot; in proposal two, but for other cases they are &quot;somewhat worse.&quot;&#10;&#10;Later in the transcript, PhD A clarifies what they mean by &quot;worse.&quot; When comparing proposal two to proposal one, the new system is not as good as proposal one when a neural network is added. However, even though it is worse than proposal one in this specific aspect, PhD A still considers the new system to be better overall because of its shorter latencies and other improvements made in the transition from proposal one to proposal two.&#10;&#10;In summary, PhD A considers proposal two to be &quot;a little worse&quot; than proposal one in the specific context of comparing relative improvements and when a neural network is added to the new system. However, they still consider the new system to be better overall due to its shorter latencies and other improvements." target="aker: PhD C&#10;Content: And when you have your final thing , we go back to this .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: So , um , and it 's a real simple change to make . I mean , it 's like one little text file you edit and change those numbers , and you don't do anything else .&#10;Speaker: PhD F&#10;Content: Oh , this is a {disfmarker}&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: And then you just run .&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: PhD C&#10;Content: So it 's a very simple change to make and it doesn't seem to hurt all that much .&#10;Speaker: PhD A&#10;Content: So you {disfmarker} you run with three , two , two , five ? That 's a&#10;Speaker: PhD C&#10;Content: So I {disfmarker} Uh , I {disfmarker} I have to look to see what the exact numbers were .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="In the transcript provided, PhD A mentions that the new system (proposal two) is &quot;a little worse&quot; when compared to proposal one. This statement is made in the context of comparing the relative improvements between the two proposals. Specifically, they mention that for some cases things are &quot;somewhat better&quot; in proposal two, but for other cases they are &quot;somewhat worse.&quot;&#10;&#10;Later in the transcript, PhD A clarifies what they mean by &quot;worse.&quot; When comparing proposal two to proposal one, the new system is not as good as proposal one when a neural network is added. However, even though it is worse than proposal one in this specific aspect, PhD A still considers the new system to be better overall because of its shorter latencies and other improvements made in the transition from proposal one to proposal two.&#10;&#10;In summary, PhD A considers proposal two to be &quot;a little worse&quot; than proposal one in the specific context of comparing relative improvements and when a neural network is added to the new system. However, they still consider the new system to be better overall due to its shorter latencies and other improvements." target="PHP A considers the new system (proposal two) to be better than the previous one (proposal one) mainly due to two reasons: first, it has a 64 hertz cut-off with clean downsampling, and second, it includes a good Voice Activity Detector (VAD). In terms of latency, PHP A mentions that the latencies are shorter in the new system.&#10;&#10;When comparing proposal two to proposal one, PhD A considers the new system to be &quot;a little worse&quot; only when a neural network is added, but it is still better overall due to its shorter latencies and other improvements made during the transition from proposal one to proposal two.">
      <data key="d0">1</data>
    </edge>
    <edge source=" it on the other conditions , when you say it 's a little worse ?&#10;Speaker: PhD A&#10;Content: It 's like , uh , fff , fff {comment} {vocalsound} {pause} um , {comment} {vocalsound} {vocalsound} {pause} ten percent relative . Yeah .&#10;Speaker: Professor B&#10;Content: OK . Um .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: But it has the , uh {disfmarker} the latencies are much shorter . That 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Uh - y w when I say it 's worse , it 's not {disfmarker} it 's when I {disfmarker} I {disfmarker} uh , compare proposal - two to proposal - one , so , r uh , y putting neural network {vocalsound} compared to n not having any neural network . I mean , this new system is {disfmarker} is {disfmarker} is better ,&#10;Speaker: Professor B&#10;Content: Uh - huh ." target="PHP A considers the new system (proposal two) to be better than the previous one (proposal one) mainly due to two reasons: first, it has a 64 hertz cut-off with clean downsampling, and second, it includes a good Voice Activity Detector (VAD). In terms of latency, PHP A mentions that the latencies are shorter in the new system.&#10;&#10;When comparing proposal two to proposal one, PhD A considers the new system to be &quot;a little worse&quot; only when a neural network is added, but it is still better overall due to its shorter latencies and other improvements made during the transition from proposal one to proposal two.">
      <data key="d0">1</data>
    </edge>
    <edge source=" proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much better . So , yeah . I asked {vocalsound} Sunil for more information about that , but , uh , I don't know yet . Um . And what 's happened here is that we {disfmarker} so we have this kind of new , um , reference system which {vocalsound} use a nice {disfmarker} a {disfmarker} a clean downsampling - upsampling , which use a new filter {vocalsound} that 's much shorter and which also cuts the frequency below sixty - four hertz ,&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: which was not done on our first proposal .&#10;Speaker: Professor B&#10;Content: When you say &quot; we have that &quot; , does Sunil have it now , too ,&#10;Speaker: PhD A&#10;Content: I No .&#10;Speaker: Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Professor B&#10;Content" target="Based on the transcript, there is no explicit information given about whether Hynek or Sunil from OGI (Oregon Graduate Institute) were part of the Aurora conference call. Speaker PhD A mentioned not knowing who was involved from OGI, and the discussion moved on to other topics. Therefore, it cannot be determined from this transcript if Hynek or Sunil were present in the Aurora conference call.">
      <data key="d0">1</data>
    </edge>
    <edge source=" proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much better . So , yeah . I asked {vocalsound} Sunil for more information about that , but , uh , I don't know yet . Um . And what 's happened here is that we {disfmarker} so we have this kind of new , um , reference system which {vocalsound} use a nice {disfmarker} a {disfmarker} a clean downsampling - upsampling , which use a new filter {vocalsound} that 's much shorter and which also cuts the frequency below sixty - four hertz ,&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: which was not done on our first proposal .&#10;Speaker: Professor B&#10;Content: When you say &quot; we have that &quot; , does Sunil have it now , too ,&#10;Speaker: PhD A&#10;Content: I No .&#10;Speaker: Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Professor B&#10;Content" target="Based on the transcript provided, the exact date in June for the next Aurora deadline was not specified during the conversation. Speaker PhD A knew that it was in June but did not have the exact date. Speaker Professor B asked about early, late, or middle June, but no specific date was given. Additionally, none of the participants provided information about who could provide this information.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker} it 's like if you say what 's the {disfmarker} what 's the best way to do an average , an arithmetic average or a geometric average ?&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: It depends what you wanna show .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Each {disfmarker} each one is gonna have a different characteristic .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So {disfmarker}&#10;Speaker: PhD C&#10;Content: Well , it seems like they should do , like , the percentage improvement or something , rather than the {pause} absolute improvement .&#10;Speaker: PhD A&#10;Content: Tha - that 's what they do .&#10;Speaker: Professor B&#10;Content: Well , they are doing that .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: No , that is relative . But the question is , do you average the relative improvements {pause} or do you average the error" target="1. Using Multiple Numbers: Professor B suggests using two or three numbers instead of relying on a single number to provide a more comprehensive understanding of the situation (content: &quot;I think one thing to do is to just not rely on a single number to maybe have two or three numbers&quot;).&#10;2. Weighted Average: Both professors discuss the idea of using a weighted average, which takes into account different factors and their significance in the given situation (content: &quot;And the thing is it's not just a pure average because there are these weightings. It's a weighted average.&quot;).&#10;3. Relative Improvement or Error Rates: Professor B raises the question of whether to average relative improvements or error rates, suggesting that this decision might be influenced by the specific context and requirements (content: &quot;But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that?&quot;).&#10;4. Analyzing Scores Instead of Improvements: PhD C proposes analyzing scores directly instead of focusing on improvements, which might help in finding a more suitable way to combine different factors (content: &quot;Why don't they not look at improvements but just look at your av your scores? You know, figure out how to combine the scores&quot;).">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker} it 's like if you say what 's the {disfmarker} what 's the best way to do an average , an arithmetic average or a geometric average ?&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: It depends what you wanna show .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Each {disfmarker} each one is gonna have a different characteristic .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So {disfmarker}&#10;Speaker: PhD C&#10;Content: Well , it seems like they should do , like , the percentage improvement or something , rather than the {pause} absolute improvement .&#10;Speaker: PhD A&#10;Content: Tha - that 's what they do .&#10;Speaker: Professor B&#10;Content: Well , they are doing that .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: No , that is relative . But the question is , do you average the relative improvements {pause} or do you average the error" target="PHP A considers the new system (proposal two) to be better than the previous one (proposal one) mainly due to two reasons: first, it has a 64 hertz cut-off with clean downsampling, and second, it includes a good Voice Activity Detector (VAD). In terms of latency, PHP A mentions that the latencies are shorter in the new system.&#10;&#10;When comparing proposal two to proposal one, PhD A considers the new system to be &quot;a little worse&quot; only when a neural network is added, but it is still better overall due to its shorter latencies and other improvements made during the transition from proposal one to proposal two.">
      <data key="d0">1</data>
    </edge>
    <edge source=" use the whole recognizer to do it {vocalsound} but {disfmarker} rather than {vocalsound} a separate thing , but {disfmarker} {vocalsound} but they 'll have it on some level . So , um .&#10;Speaker: PhD C&#10;Content: It seems like whatever they choose they shouldn't , {vocalsound} you know , purposefully brain - damage a part of the system to {pause} make a worse baseline , or {disfmarker}&#10;Speaker: Professor B&#10;Content: Well , I think people just had&#10;Speaker: PhD C&#10;Content: You know ?&#10;Speaker: Professor B&#10;Content: it wasn't that they purposely brain - damaged it . I think people hadn't really thought through {vocalsound} about the , uh {disfmarker} the VAD issue .&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And {disfmarker} and then when the {disfmarker} the {disfmarker} the proposals actually came in and half of them had V" target="Based on the transcript, Speaker PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. Specifically, she mentions that her speech might be problematic for the unvoiced part of the recognizer. Additionally, she seems to have a stronger accent or use different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause.">
      <data key="d0">1</data>
    </edge>
    <edge source=" use the whole recognizer to do it {vocalsound} but {disfmarker} rather than {vocalsound} a separate thing , but {disfmarker} {vocalsound} but they 'll have it on some level . So , um .&#10;Speaker: PhD C&#10;Content: It seems like whatever they choose they shouldn't , {vocalsound} you know , purposefully brain - damage a part of the system to {pause} make a worse baseline , or {disfmarker}&#10;Speaker: Professor B&#10;Content: Well , I think people just had&#10;Speaker: PhD C&#10;Content: You know ?&#10;Speaker: Professor B&#10;Content: it wasn't that they purposely brain - damaged it . I think people hadn't really thought through {vocalsound} about the , uh {disfmarker} the VAD issue .&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And {disfmarker} and then when the {disfmarker} the {disfmarker} the proposals actually came in and half of them had V" target="Based on the transcript, PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. She mentions that her speech might be problematic for the unvoiced part of the recognizer and that she has a stronger accent or uses different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Speaker PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. Specifically, she mentions that her speech might be problematic for the unvoiced part of the recognizer. Additionally, she seems to have a stronger accent or use different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause." target=": Professor B&#10;Content: Uh , first order for speech recognition , you say &quot; I don't care about the source &quot; .&#10;Speaker: PhD F&#10;Content: For {disfmarker} Yeah .&#10;Speaker: PhD A&#10;Content: Well , I mean for the {disfmarker} the energy .&#10;Speaker: PhD F&#10;Content: I have the mean .&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: And so you just want to find out what the filters are .&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: The filters {vocalsound} roughly act like a , um {disfmarker} {vocalsound} a , uh {disfmarker} {vocalsound} a an overall resonant {disfmarker} you know , f some resonances and so forth that th that 's processing excitation .&#10;Speaker: PhD F&#10;Content: Here .&#10;Speaker: PhD A&#10;Content: They should be more close .&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Speaker PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. Specifically, she mentions that her speech might be problematic for the unvoiced part of the recognizer. Additionally, she seems to have a stronger accent or use different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause." target="alsound} these {disfmarker} {vocalsound} he 's {disfmarker} he 's , uh {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: This is {disfmarker} this by the way a bad thing . We 're trying to get , um , m more female voices in this record as well . So . Make sur make sure Carmen {vocalsound} talks as well . Uh , but has he pretty much been talking about what you 're doing also , and {disfmarker} ?&#10;Speaker: PhD F&#10;Content: Oh , I {disfmarker} I am doing this .&#10;Speaker: Professor B&#10;Content: Yes .&#10;Speaker: PhD F&#10;Content: Yeah , yeah . I don't know . I 'm sorry , but I think that for the recognizer for the meeting recorder that it 's better that I don't speak .&#10;Speaker: Professor B&#10;Content: Yeah , well .&#10;Speaker: PhD F&#10;Content: Because {disfmarker}&#10;Speaker: Professor B&#10;Content: You know , uh , we '">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Speaker PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. Specifically, she mentions that her speech might be problematic for the unvoiced part of the recognizer. Additionally, she seems to have a stronger accent or use different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause." target="Content: Oh .&#10;Speaker: PhD F&#10;Content: for unvoiced .&#10;Speaker: Professor B&#10;Content: Yeah . So , you know , all {disfmarker}&#10;Speaker: PhD F&#10;Content: I 'm sorry .&#10;Speaker: PhD A&#10;Content: But {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Yeah . This is the {disfmarker} between {disfmarker}&#10;Speaker: PhD A&#10;Content: This is another voiced example . Yeah .&#10;Speaker: PhD F&#10;Content: No . But it 's this ,&#10;Speaker: PhD A&#10;Content: Oh , yeah . This is {disfmarker}&#10;Speaker: PhD F&#10;Content: but between the frequency that we are considered for the excitation {disfmarker}&#10;Speaker: PhD A&#10;Content: Right . Mm - hmm .&#10;Speaker: PhD F&#10;Content: for the difference and this is the difference .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: This is the difference . OK .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Speaker PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. Specifically, she mentions that her speech might be problematic for the unvoiced part of the recognizer. Additionally, she seems to have a stronger accent or use different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause." target=" no , no . But th the kind of robustness to noise {disfmarker}&#10;Speaker: PhD F&#10;Content: Oh !&#10;Speaker: PhD A&#10;Content: So if {disfmarker} if you take this frame , {vocalsound} uh , from the noisy utterance and the same frame from the clean utterance {disfmarker}&#10;Speaker: PhD F&#10;Content: Hmm .&#10;Speaker: PhD C&#10;Content: You end up with a similar difference&#10;Speaker: PhD A&#10;Content: Y y y yeah . We end up with {disfmarker}&#10;Speaker: PhD C&#10;Content: over here ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: OK . Cool !&#10;Speaker: PhD F&#10;Content: I have here the same frame for the {pause} clean speech {disfmarker}&#10;Speaker: PhD C&#10;Content: Oh , that 's clean .&#10;Speaker: PhD F&#10;Content: the same cle&#10;Speaker: PhD C&#10;Content: Oh , OK&#10;Speaker: PhD F&#10;Content: But they are a difference .&#10;Speaker: PhD A">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Speaker PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. Specifically, she mentions that her speech might be problematic for the unvoiced part of the recognizer. Additionally, she seems to have a stronger accent or use different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause." target=" .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: I can't see you {comment} now .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: I don't have .&#10;Speaker: PhD C&#10;Content: And so you said this is pretty {disfmarker} doing this kind of thing is pretty robust to noise ?&#10;Speaker: PhD A&#10;Content: It seems , yeah . Um ,&#10;Speaker: PhD C&#10;Content: Huh .&#10;Speaker: PhD F&#10;Content: Pfft . Oops . The mean is different {vocalsound} with it , because the {disfmarker} {vocalsound} the histogram for the {disfmarker} {vocalsound} the classifica&#10;Speaker: PhD A&#10;Content: No , no , no . But th the kind of robustness to noise {disfmarker}&#10;Speaker: PhD F&#10;Content: Oh !&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Speaker PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. Specifically, she mentions that her speech might be problematic for the unvoiced part of the recognizer. Additionally, she seems to have a stronger accent or use different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause." target="PHP A and their team obtained the voiced and unvoiced truth data used in their analysis by utilizing TIMIT, a speech recognition database, and applying canonical mappings between the phones. This information was revealed when PhD C asked how they got their voiced and unvoiced truth data, to which PhD A responded by explaining their use of TIMIT and the application of canonical mappings between the phones.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Speaker PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. Specifically, she mentions that her speech might be problematic for the unvoiced part of the recognizer. Additionally, she seems to have a stronger accent or use different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause." target="Based on the transcript, PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. She mentions that her speech might be problematic for the unvoiced part of the recognizer and that she has a stronger accent or uses different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Speaker PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. Specifically, she mentions that her speech might be problematic for the unvoiced part of the recognizer. Additionally, she seems to have a stronger accent or use different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause." target="Based on the transcript, there is no explicit information given about whether Hynek or Sunil from OGI (Oregon Graduate Institute) were part of the Aurora conference call. Speaker PhD A mentioned not knowing who was involved from OGI, and the discussion moved on to other topics. Therefore, it cannot be determined from this transcript if Hynek or Sunil were present in the Aurora conference call.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Professor B&#10;Content: Uh , first order for speech recognition , you say &quot; I don't care about the source &quot; .&#10;Speaker: PhD F&#10;Content: For {disfmarker} Yeah .&#10;Speaker: PhD A&#10;Content: Well , I mean for the {disfmarker} the energy .&#10;Speaker: PhD F&#10;Content: I have the mean .&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: And so you just want to find out what the filters are .&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: The filters {vocalsound} roughly act like a , um {disfmarker} {vocalsound} a , uh {disfmarker} {vocalsound} a an overall resonant {disfmarker} you know , f some resonances and so forth that th that 's processing excitation .&#10;Speaker: PhD F&#10;Content: Here .&#10;Speaker: PhD A&#10;Content: They should be more close .&#10;Speaker: PhD" target="1. Professor B's argument about voiced phonemes giving us intuition: During the discussion, Professor B suggests that the concept of voiced phonemes helps shape our intuition regarding speech recognition (e.g., &quot;what I'm arguing is that that gives you your intuition&quot;). This implies that understanding the difference between voiced and unvoiced sounds contributes to forming a basic framework for analyzing and comprehending speech signals.&#10;2. Complexities in speech and their acceptance: Professor B argues that in reality, there are overlaps and complexities in speech that make it difficult to perfectly separate voiced from unvoiced sounds (&quot;But in reality, it's all of this overlap and so forth&quot;). Despite these challenges, the professor suggests that this might be acceptable for a few reasons. First, what we gain from the analysis is not necessarily a clear-cut distinction between voiced and unvoiced sounds but rather a characterization driven by our intuition. Second, imperfect data may affect the accuracy of any speech recognition system. Consequently, relying on this basic intuition might still provide valuable insights in understanding &quot;almost raw data&quot; instead of relying solely on refined and smoothed versions of the signal.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Professor B&#10;Content: Uh , first order for speech recognition , you say &quot; I don't care about the source &quot; .&#10;Speaker: PhD F&#10;Content: For {disfmarker} Yeah .&#10;Speaker: PhD A&#10;Content: Well , I mean for the {disfmarker} the energy .&#10;Speaker: PhD F&#10;Content: I have the mean .&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: And so you just want to find out what the filters are .&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: The filters {vocalsound} roughly act like a , um {disfmarker} {vocalsound} a , uh {disfmarker} {vocalsound} a an overall resonant {disfmarker} you know , f some resonances and so forth that th that 's processing excitation .&#10;Speaker: PhD F&#10;Content: Here .&#10;Speaker: PhD A&#10;Content: They should be more close .&#10;Speaker: PhD" target="Based on the transcript, PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. She mentions that her speech might be problematic for the unvoiced part of the recognizer and that she has a stronger accent or uses different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause.">
      <data key="d0">1</data>
    </edge>
    <edge source="alsound} these {disfmarker} {vocalsound} he 's {disfmarker} he 's , uh {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: This is {disfmarker} this by the way a bad thing . We 're trying to get , um , m more female voices in this record as well . So . Make sur make sure Carmen {vocalsound} talks as well . Uh , but has he pretty much been talking about what you 're doing also , and {disfmarker} ?&#10;Speaker: PhD F&#10;Content: Oh , I {disfmarker} I am doing this .&#10;Speaker: Professor B&#10;Content: Yes .&#10;Speaker: PhD F&#10;Content: Yeah , yeah . I don't know . I 'm sorry , but I think that for the recognizer for the meeting recorder that it 's better that I don't speak .&#10;Speaker: Professor B&#10;Content: Yeah , well .&#10;Speaker: PhD F&#10;Content: Because {disfmarker}&#10;Speaker: Professor B&#10;Content: You know , uh , we '" target="Based on the transcript, PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. She mentions that her speech might be problematic for the unvoiced part of the recognizer and that she has a stronger accent or uses different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Oh .&#10;Speaker: PhD F&#10;Content: for unvoiced .&#10;Speaker: Professor B&#10;Content: Yeah . So , you know , all {disfmarker}&#10;Speaker: PhD F&#10;Content: I 'm sorry .&#10;Speaker: PhD A&#10;Content: But {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Yeah . This is the {disfmarker} between {disfmarker}&#10;Speaker: PhD A&#10;Content: This is another voiced example . Yeah .&#10;Speaker: PhD F&#10;Content: No . But it 's this ,&#10;Speaker: PhD A&#10;Content: Oh , yeah . This is {disfmarker}&#10;Speaker: PhD F&#10;Content: but between the frequency that we are considered for the excitation {disfmarker}&#10;Speaker: PhD A&#10;Content: Right . Mm - hmm .&#10;Speaker: PhD F&#10;Content: for the difference and this is the difference .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: This is the difference . OK .&#10;" target="PHP A and their team obtained the voiced and unvoiced truth data used in their analysis by utilizing TIMIT, a speech recognition database, and applying canonical mappings between the phones. This information was revealed when PhD C asked how they got their voiced and unvoiced truth data, to which PhD A responded by explaining their use of TIMIT and the application of canonical mappings between the phones.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Oh .&#10;Speaker: PhD F&#10;Content: for unvoiced .&#10;Speaker: Professor B&#10;Content: Yeah . So , you know , all {disfmarker}&#10;Speaker: PhD F&#10;Content: I 'm sorry .&#10;Speaker: PhD A&#10;Content: But {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Yeah . This is the {disfmarker} between {disfmarker}&#10;Speaker: PhD A&#10;Content: This is another voiced example . Yeah .&#10;Speaker: PhD F&#10;Content: No . But it 's this ,&#10;Speaker: PhD A&#10;Content: Oh , yeah . This is {disfmarker}&#10;Speaker: PhD F&#10;Content: but between the frequency that we are considered for the excitation {disfmarker}&#10;Speaker: PhD A&#10;Content: Right . Mm - hmm .&#10;Speaker: PhD F&#10;Content: for the difference and this is the difference .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: This is the difference . OK .&#10;" target="Based on the transcript, PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. She mentions that her speech might be problematic for the unvoiced part of the recognizer and that she has a stronger accent or uses different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause.">
      <data key="d0">1</data>
    </edge>
    <edge source=" no , no . But th the kind of robustness to noise {disfmarker}&#10;Speaker: PhD F&#10;Content: Oh !&#10;Speaker: PhD A&#10;Content: So if {disfmarker} if you take this frame , {vocalsound} uh , from the noisy utterance and the same frame from the clean utterance {disfmarker}&#10;Speaker: PhD F&#10;Content: Hmm .&#10;Speaker: PhD C&#10;Content: You end up with a similar difference&#10;Speaker: PhD A&#10;Content: Y y y yeah . We end up with {disfmarker}&#10;Speaker: PhD C&#10;Content: over here ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: OK . Cool !&#10;Speaker: PhD F&#10;Content: I have here the same frame for the {pause} clean speech {disfmarker}&#10;Speaker: PhD C&#10;Content: Oh , that 's clean .&#10;Speaker: PhD F&#10;Content: the same cle&#10;Speaker: PhD C&#10;Content: Oh , OK&#10;Speaker: PhD F&#10;Content: But they are a difference .&#10;Speaker: PhD A" target="1. Length and number of periods: A short frame contains only two periods of the signal, while the original frame contains more depending on the signal's characteristics. This difference is due to the time-frequency trade-off, which requires shorter frames for better frequency resolution but longer frames for better time resolution.&#10;&#10;2. FFT size (number of points in the Fast Fourier Transform): The speakers also discuss differences in FFT size between the short frame and the original frame. A larger FFT size results in better frequency resolution but coarser time resolution, whereas a smaller FFT size has the opposite effect.&#10;&#10;3. Robustness to noise: The speakers mention that using a shorter frame can help improve robustness to noise in signal processing applications like speech analysis. When comparing the same frame from noisy and clean utterances, the difference can be used to better understand and handle noise.">
      <data key="d0">1</data>
    </edge>
    <edge source=" .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: I can't see you {comment} now .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: I don't have .&#10;Speaker: PhD C&#10;Content: And so you said this is pretty {disfmarker} doing this kind of thing is pretty robust to noise ?&#10;Speaker: PhD A&#10;Content: It seems , yeah . Um ,&#10;Speaker: PhD C&#10;Content: Huh .&#10;Speaker: PhD F&#10;Content: Pfft . Oops . The mean is different {vocalsound} with it , because the {disfmarker} {vocalsound} the histogram for the {disfmarker} {vocalsound} the classifica&#10;Speaker: PhD A&#10;Content: No , no , no . But th the kind of robustness to noise {disfmarker}&#10;Speaker: PhD F&#10;Content: Oh !&#10;Spe" target="Based on the transcript, PhD F believes that it's better for her not to speak for the meeting recorder's recognizer to function optimally because she is concerned about the impact of her voice on the speech recognition system. She mentions that her speech might be problematic for the unvoiced part of the recognizer and that she has a stronger accent or uses different frequencies in her speech, which could negatively affect the robustness of the noise in the recognizer's performance. Therefore, she decides to remain silent to prevent any potential issues that her speech might cause.">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A and their team obtained the voiced and unvoiced truth data used in their analysis by utilizing TIMIT, a speech recognition database, and applying canonical mappings between the phones. This information was revealed when PhD C asked how they got their voiced and unvoiced truth data, to which PhD A responded by explaining their use of TIMIT and the application of canonical mappings between the phones." target="aker: PhD A&#10;Content: Yeah , that 's right .&#10;Speaker: Professor B&#10;Content: Yeah , yeah .&#10;Speaker: PhD A&#10;Content: Um {disfmarker} Yeah .&#10;Speaker: PhD F&#10;Content: We have here some histogram ,&#10;Speaker: PhD A&#10;Content: E yeah ,&#10;Speaker: PhD F&#10;Content: but they have a lot of overlap .&#10;Speaker: PhD A&#10;Content: but it 's {disfmarker} it 's still {disfmarker} Yeah . So , well , for unvoiced portion we have something tha {vocalsound} that has a mean around O point three , and for voiced portion the mean is O point fifty - nine . But the variance seem quite {vocalsound} high .&#10;Speaker: PhD C&#10;Content: How do you know {disfmarker} ?&#10;Speaker: PhD A&#10;Content: So {disfmarker} Mmm .&#10;Speaker: PhD C&#10;Content: How did you get your {pause} voiced and unvoiced truth data ?&#10;Speaker: PhD A&#10;Content: We used , uh , TIMIT and we used">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A and their team obtained the voiced and unvoiced truth data used in their analysis by utilizing TIMIT, a speech recognition database, and applying canonical mappings between the phones. This information was revealed when PhD C asked how they got their voiced and unvoiced truth data, to which PhD A responded by explaining their use of TIMIT and the application of canonical mappings between the phones." target=" get your {pause} voiced and unvoiced truth data ?&#10;Speaker: PhD A&#10;Content: We used , uh , TIMIT and we used canonical mappings between the phones&#10;Speaker: PhD F&#10;Content: Yeah . We , uh , use {pause} TIMIT on this ,&#10;Speaker: PhD A&#10;Content: and&#10;Speaker: PhD F&#10;Content: for {disfmarker}&#10;Speaker: PhD A&#10;Content: th Yeah .&#10;Speaker: PhD F&#10;Content: But if we look at it in one sentence , it {disfmarker} apparently it 's good , I think .&#10;Speaker: PhD A&#10;Content: Yeah , but {disfmarker} Yeah . Uh , so it 's noisy TIMIT . That 's right . Yeah .&#10;Speaker: Grad E&#10;Content: It 's noisy TIMIT .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: It seems quite robust to noise , so when we take {disfmarker} we draw its parameters across time for a clean sentence and then nois the same noisy sentence , it 's very close .&#10;Speaker: Professor B">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A and their team obtained the voiced and unvoiced truth data used in their analysis by utilizing TIMIT, a speech recognition database, and applying canonical mappings between the phones. This information was revealed when PhD C asked how they got their voiced and unvoiced truth data, to which PhD A responded by explaining their use of TIMIT and the application of canonical mappings between the phones." target=": OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Oh ! OK . Yeah .&#10;Speaker: PhD A&#10;Content: and that 's {disfmarker} that should be flat for {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: I see . So do you have a picture that sh ?&#10;Speaker: PhD A&#10;Content: So - It 's {disfmarker} Y&#10;Speaker: PhD C&#10;Content: Is this for a voiced segment ,&#10;Speaker: PhD A&#10;Content: yeah .&#10;Speaker: PhD C&#10;Content: this picture ? What does it look like for unvoiced ?&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: You have several {disfmarker} some unvoiced ?&#10;Speaker: PhD F&#10;Content: The dif No . Unvoiced , I don't have&#10;Speaker: PhD A&#10;Content: Oh .&#10;Speaker: PhD F&#10;Content: for unvoiced .&#10;Speaker: Professor B&#10;Content: Yeah . So , you">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A and their team obtained the voiced and unvoiced truth data used in their analysis by utilizing TIMIT, a speech recognition database, and applying canonical mappings between the phones. This information was revealed when PhD C asked how they got their voiced and unvoiced truth data, to which PhD A responded by explaining their use of TIMIT and the application of canonical mappings between the phones." target=" , um {disfmarker} well , to {disfmarker} to obtain a statistical model on these features and to {disfmarker} or just to use a neural network and hopefully these features w would help {disfmarker}&#10;Speaker: PhD C&#10;Content: Because it seems like what you said about the mean of the {disfmarker} the voiced and the unvoiced {disfmarker} {comment} {vocalsound} that seemed pretty encouraging .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Well , yeah , except the variance was big .&#10;Speaker: PhD C&#10;Content: Right ?&#10;Speaker: PhD A&#10;Content: Yeah . Except the variance is quite high .&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD C&#10;Content: Well , y&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Well , y I {disfmarker} I don't know that I would trust that so much because you 're doing these canonical mappings from TIMIT labellings .&#10;Speaker: PhD A&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A and their team obtained the voiced and unvoiced truth data used in their analysis by utilizing TIMIT, a speech recognition database, and applying canonical mappings between the phones. This information was revealed when PhD C asked how they got their voiced and unvoiced truth data, to which PhD A responded by explaining their use of TIMIT and the application of canonical mappings between the phones." target="1. When Professor B suggests that imperfect distinctions between voiced and unvoiced sounds might not be a significant issue, they are implying that the primary value in this analysis comes from obtaining a general characterization of the speech signal based on our intuition about these categories. This rough characterization can provide insights into the raw data, even if it does not offer a clear-cut distinction between voiced and unvoiced sounds due to overlaps and complexities in speech.&#10;2. The professor emphasizes that relying on this basic intuition might be more valuable than using overly refined or smoothed versions of the signal since real-world data, such as noise, can impact the accuracy of any speech recognition system. Consequently, understanding how noise affects speech perception and developing methods to mitigate its effects is essential for improving speech recognition systems' performance in various environments.&#10;3. The team is currently focused on selecting appropriate features for their task, with one possibility being the maximum of the auto-correlation function to better characterize voiced phonemes. However, they have not yet decided whether to use a trained system or threshold-based system to implement these features in their statistical model.&#10;4. Professor B implies that even if the &quot;world's best voiced-unvoiced classifier&quot; does not perfectly distinguish between these categories, trying different statistical characterizations of the difference based on raw data may still yield valuable information that a speech recognition system can utilize.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , um {disfmarker} well , to {disfmarker} to obtain a statistical model on these features and to {disfmarker} or just to use a neural network and hopefully these features w would help {disfmarker}&#10;Speaker: PhD C&#10;Content: Because it seems like what you said about the mean of the {disfmarker} the voiced and the unvoiced {disfmarker} {comment} {vocalsound} that seemed pretty encouraging .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Well , yeah , except the variance was big .&#10;Speaker: PhD C&#10;Content: Right ?&#10;Speaker: PhD A&#10;Content: Yeah . Except the variance is quite high .&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD C&#10;Content: Well , y&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Well , y I {disfmarker} I don't know that I would trust that so much because you 're doing these canonical mappings from TIMIT labellings .&#10;Speaker: PhD A&#10;" target="The frequency range used for computing the variance in the example being discussed by the speakers is from 200 Hz to 1500 Hz. One of the speakers later corrects this range to 250 thousand Hz, but it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , um {disfmarker} well , to {disfmarker} to obtain a statistical model on these features and to {disfmarker} or just to use a neural network and hopefully these features w would help {disfmarker}&#10;Speaker: PhD C&#10;Content: Because it seems like what you said about the mean of the {disfmarker} the voiced and the unvoiced {disfmarker} {comment} {vocalsound} that seemed pretty encouraging .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Well , yeah , except the variance was big .&#10;Speaker: PhD C&#10;Content: Right ?&#10;Speaker: PhD A&#10;Content: Yeah . Except the variance is quite high .&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD C&#10;Content: Well , y&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Well , y I {disfmarker} I don't know that I would trust that so much because you 're doing these canonical mappings from TIMIT labellings .&#10;Speaker: PhD A&#10;" target="The frequency range used for computing the variance in the example discussed by the speakers is from 200 Hz to 1500 Hz. This is stated explicitly by speaker A, and although professor F mistakenly gives a higher frequency range, it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's argument about voiced phonemes giving us intuition: During the discussion, Professor B suggests that the concept of voiced phonemes helps shape our intuition regarding speech recognition (e.g., &quot;what I'm arguing is that that gives you your intuition&quot;). This implies that understanding the difference between voiced and unvoiced sounds contributes to forming a basic framework for analyzing and comprehending speech signals.&#10;2. Complexities in speech and their acceptance: Professor B argues that in reality, there are overlaps and complexities in speech that make it difficult to perfectly separate voiced from unvoiced sounds (&quot;But in reality, it's all of this overlap and so forth&quot;). Despite these challenges, the professor suggests that this might be acceptable for a few reasons. First, what we gain from the analysis is not necessarily a clear-cut distinction between voiced and unvoiced sounds but rather a characterization driven by our intuition. Second, imperfect data may affect the accuracy of any speech recognition system. Consequently, relying on this basic intuition might still provide valuable insights in understanding &quot;almost raw data&quot; instead of relying solely on refined and smoothed versions of the signal." target=" it gives you just information about if it 's voiced or not voiced , ma mainly , I mean . But {disfmarker} So ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: this is why we {disfmarker} we started to look {pause} by having sort of voiced phonemes&#10;Speaker: Professor B&#10;Content: Well , that 's the rea w w what I 'm arguing is that 's Yeah . I mean , uh , what I 'm arguing is that that {disfmarker} that 's givi you {disfmarker} gives you your intuition .&#10;Speaker: PhD A&#10;Content: and {disfmarker} Mm - hmm .&#10;Speaker: Professor B&#10;Content: But in {disfmarker} in reality , it 's {disfmarker} you know , there 's all of this {disfmarker} this overlap and so forth ,&#10;Speaker: Grad E&#10;Content: Oh , sorry .&#10;Speaker: Professor B&#10;Content: and {disfmarker} But what I 'm saying is that may be OK , because">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's argument about voiced phonemes giving us intuition: During the discussion, Professor B suggests that the concept of voiced phonemes helps shape our intuition regarding speech recognition (e.g., &quot;what I'm arguing is that that gives you your intuition&quot;). This implies that understanding the difference between voiced and unvoiced sounds contributes to forming a basic framework for analyzing and comprehending speech signals.&#10;2. Complexities in speech and their acceptance: Professor B argues that in reality, there are overlaps and complexities in speech that make it difficult to perfectly separate voiced from unvoiced sounds (&quot;But in reality, it's all of this overlap and so forth&quot;). Despite these challenges, the professor suggests that this might be acceptable for a few reasons. First, what we gain from the analysis is not necessarily a clear-cut distinction between voiced and unvoiced sounds but rather a characterization driven by our intuition. Second, imperfect data may affect the accuracy of any speech recognition system. Consequently, relying on this basic intuition might still provide valuable insights in understanding &quot;almost raw data&quot; instead of relying solely on refined and smoothed versions of the signal." target=" .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Two hundred and fifty thousand .&#10;Speaker: PhD A&#10;Content: Fifteen hundred . Because {disfmarker} Yeah .&#10;Speaker: PhD F&#10;Content: Yeah . Two thousand and fifteen hundred .&#10;Speaker: PhD A&#10;Content: Above , um {disfmarker} {vocalsound} it seems that {disfmarker} Well , some voiced sound can have also , {vocalsound} like , a noisy {pause} part on high frequencies , and {disfmarker} But {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Well , it 's just {disfmarker}&#10;Speaker: Professor B&#10;Content: No , it 's {disfmarker} makes sense to look at {pause} low frequencies .&#10;Speaker: PhD C&#10;Content: So this is {disfmarker} uh , basically this is comparing {vocalsound} an original version of the signal to a smoothed version of the same signal ?&#10;Speaker: PhD F&#10;Content: Yeah">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's argument about voiced phonemes giving us intuition: During the discussion, Professor B suggests that the concept of voiced phonemes helps shape our intuition regarding speech recognition (e.g., &quot;what I'm arguing is that that gives you your intuition&quot;). This implies that understanding the difference between voiced and unvoiced sounds contributes to forming a basic framework for analyzing and comprehending speech signals.&#10;2. Complexities in speech and their acceptance: Professor B argues that in reality, there are overlaps and complexities in speech that make it difficult to perfectly separate voiced from unvoiced sounds (&quot;But in reality, it's all of this overlap and so forth&quot;). Despite these challenges, the professor suggests that this might be acceptable for a few reasons. First, what we gain from the analysis is not necessarily a clear-cut distinction between voiced and unvoiced sounds but rather a characterization driven by our intuition. Second, imperfect data may affect the accuracy of any speech recognition system. Consequently, relying on this basic intuition might still provide valuable insights in understanding &quot;almost raw data&quot; instead of relying solely on refined and smoothed versions of the signal." target=" , sorry .&#10;Speaker: Professor B&#10;Content: and {disfmarker} But what I 'm saying is that may be OK , because what you 're really getting is not actually voiced versus unvoiced , both for the fac the reason of the overlap and {disfmarker} and then , uh , th you know , structural reasons , uh , uh , like the one that Chuck said , that {disfmarker} that in fact , well , the data itself is {disfmarker} {vocalsound} that you 're working with is not perfect .&#10;Speaker: PhD A&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor B&#10;Content: So , what I 'm saying is maybe that 's not a killer because you 're just getting some characterization , one that 's driven by your intuition about voiced - unvoiced certainly ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: but it 's just some characterization {vocalsound} of something back in the {disfmarker} in the {disfmarker} in the almost raw data , rather than the smooth version">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's argument about voiced phonemes giving us intuition: During the discussion, Professor B suggests that the concept of voiced phonemes helps shape our intuition regarding speech recognition (e.g., &quot;what I'm arguing is that that gives you your intuition&quot;). This implies that understanding the difference between voiced and unvoiced sounds contributes to forming a basic framework for analyzing and comprehending speech signals.&#10;2. Complexities in speech and their acceptance: Professor B argues that in reality, there are overlaps and complexities in speech that make it difficult to perfectly separate voiced from unvoiced sounds (&quot;But in reality, it's all of this overlap and so forth&quot;). Despite these challenges, the professor suggests that this might be acceptable for a few reasons. First, what we gain from the analysis is not necessarily a clear-cut distinction between voiced and unvoiced sounds but rather a characterization driven by our intuition. Second, imperfect data may affect the accuracy of any speech recognition system. Consequently, relying on this basic intuition might still provide valuable insights in understanding &quot;almost raw data&quot; instead of relying solely on refined and smoothed versions of the signal." target=" the world 's best voiced - unvoiced , uh , uh , classifier ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: but it 's more that , {vocalsound} you know , uh , uh , try some different statistical characterizations of that difference back to the raw data&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: and {disfmarker} and m maybe there 's something there that {pause} the system can use .&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: Yeah . Yeah , but ther more obvious is that {disfmarker} Yeah . The {disfmarker} the more obvious is that {disfmarker} that {disfmarker} well , using the {disfmarker} th the FFT , um , {vocalsound} you just {disfmarker} it gives you just information about if it 's voiced or not voiced , ma mainly , I mean . But {disfmarker} So ,&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's argument about voiced phonemes giving us intuition: During the discussion, Professor B suggests that the concept of voiced phonemes helps shape our intuition regarding speech recognition (e.g., &quot;what I'm arguing is that that gives you your intuition&quot;). This implies that understanding the difference between voiced and unvoiced sounds contributes to forming a basic framework for analyzing and comprehending speech signals.&#10;2. Complexities in speech and their acceptance: Professor B argues that in reality, there are overlaps and complexities in speech that make it difficult to perfectly separate voiced from unvoiced sounds (&quot;But in reality, it's all of this overlap and so forth&quot;). Despite these challenges, the professor suggests that this might be acceptable for a few reasons. First, what we gain from the analysis is not necessarily a clear-cut distinction between voiced and unvoiced sounds but rather a characterization driven by our intuition. Second, imperfect data may affect the accuracy of any speech recognition system. Consequently, relying on this basic intuition might still provide valuable insights in understanding &quot;almost raw data&quot; instead of relying solely on refined and smoothed versions of the signal." target=" Professor B&#10;Content: And {disfmarker} and , uh , the {disfmarker} the MSG features were sort of built up {vocalsound} with this notion {disfmarker}&#10;Speaker: Grad E&#10;Content: Yeah . Right .&#10;Speaker: Professor B&#10;Content: But , I guess , I thought you had brought this up in the context of , um , targets somehow .&#10;Speaker: Grad E&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: But i m&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: Professor B&#10;Content: i it 's not {disfmarker} I mean , they 're sort of not in the same kind of category as , say , a phonetic target or a syllabic target&#10;Speaker: Grad E&#10;Content: Mmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: or a {disfmarker}&#10;Speaker: Grad E&#10;Content: Um , I was thinking more like using them as {disfmarker} as the inputs to {disfmarker} to the detectors .&#10;Speaker: Professor">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's argument about voiced phonemes giving us intuition: During the discussion, Professor B suggests that the concept of voiced phonemes helps shape our intuition regarding speech recognition (e.g., &quot;what I'm arguing is that that gives you your intuition&quot;). This implies that understanding the difference between voiced and unvoiced sounds contributes to forming a basic framework for analyzing and comprehending speech signals.&#10;2. Complexities in speech and their acceptance: Professor B argues that in reality, there are overlaps and complexities in speech that make it difficult to perfectly separate voiced from unvoiced sounds (&quot;But in reality, it's all of this overlap and so forth&quot;). Despite these challenges, the professor suggests that this might be acceptable for a few reasons. First, what we gain from the analysis is not necessarily a clear-cut distinction between voiced and unvoiced sounds but rather a characterization driven by our intuition. Second, imperfect data may affect the accuracy of any speech recognition system. Consequently, relying on this basic intuition might still provide valuable insights in understanding &quot;almost raw data&quot; instead of relying solely on refined and smoothed versions of the signal." target="1. Using Multiple Numbers: Professor B suggests using two or three numbers instead of relying on a single number to provide a more comprehensive understanding of the situation (content: &quot;I think one thing to do is to just not rely on a single number to maybe have two or three numbers&quot;).&#10;2. Weighted Average: Both professors discuss the idea of using a weighted average, which takes into account different factors and their significance in the given situation (content: &quot;And the thing is it's not just a pure average because there are these weightings. It's a weighted average.&quot;).&#10;3. Relative Improvement or Error Rates: Professor B raises the question of whether to average relative improvements or error rates, suggesting that this decision might be influenced by the specific context and requirements (content: &quot;But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that?&quot;).&#10;4. Analyzing Scores Instead of Improvements: PhD C proposes analyzing scores directly instead of focusing on improvements, which might help in finding a more suitable way to combine different factors (content: &quot;Why don't they not look at improvements but just look at your av your scores? You know, figure out how to combine the scores&quot;).">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's argument about voiced phonemes giving us intuition: During the discussion, Professor B suggests that the concept of voiced phonemes helps shape our intuition regarding speech recognition (e.g., &quot;what I'm arguing is that that gives you your intuition&quot;). This implies that understanding the difference between voiced and unvoiced sounds contributes to forming a basic framework for analyzing and comprehending speech signals.&#10;2. Complexities in speech and their acceptance: Professor B argues that in reality, there are overlaps and complexities in speech that make it difficult to perfectly separate voiced from unvoiced sounds (&quot;But in reality, it's all of this overlap and so forth&quot;). Despite these challenges, the professor suggests that this might be acceptable for a few reasons. First, what we gain from the analysis is not necessarily a clear-cut distinction between voiced and unvoiced sounds but rather a characterization driven by our intuition. Second, imperfect data may affect the accuracy of any speech recognition system. Consequently, relying on this basic intuition might still provide valuable insights in understanding &quot;almost raw data&quot; instead of relying solely on refined and smoothed versions of the signal." target="1. Benefits of smooth representation for generalization: A smooth representation generally leads to better generalization because it reduces overfitting by averaging out fine details and noise in the data. This can help improve the model's performance on unseen data, as the model is less likely to learn spurious patterns that only exist in the training set.&#10;&#10;2. Potential drawbacks of losing details during smoothing: Smoothing inevitably leads to loss of information, which might include valuable details about the original signal. This could potentially make the model miss important features or characteristics of the data, leading to reduced accuracy or suboptimal performance in certain cases. In speech recognition, for example, excessive smoothing might cause the loss of crucial information related to phoneme distinction or speaker identity.&#10;&#10;In summary, while smoothing offers benefits such as improved generalization and reduced overfitting, it can also result in the loss of important details that may be necessary for accurate analysis or modeling. Balancing these trade-offs is essential for developing effective speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's argument about voiced phonemes giving us intuition: During the discussion, Professor B suggests that the concept of voiced phonemes helps shape our intuition regarding speech recognition (e.g., &quot;what I'm arguing is that that gives you your intuition&quot;). This implies that understanding the difference between voiced and unvoiced sounds contributes to forming a basic framework for analyzing and comprehending speech signals.&#10;2. Complexities in speech and their acceptance: Professor B argues that in reality, there are overlaps and complexities in speech that make it difficult to perfectly separate voiced from unvoiced sounds (&quot;But in reality, it's all of this overlap and so forth&quot;). Despite these challenges, the professor suggests that this might be acceptable for a few reasons. First, what we gain from the analysis is not necessarily a clear-cut distinction between voiced and unvoiced sounds but rather a characterization driven by our intuition. Second, imperfect data may affect the accuracy of any speech recognition system. Consequently, relying on this basic intuition might still provide valuable insights in understanding &quot;almost raw data&quot; instead of relying solely on refined and smoothed versions of the signal." target="1. The process for drawing a system's parameters across time for a clean sentence and a noisy sentence involves analyzing the characteristics of speech signals in both scenarios. This likely includes examining various attributes such as frequency content, amplitude, and duration of sounds within the sentences. By comparing these parameters between clean and noisy sentences, researchers can gain insights into how noise affects speech perception and develop methods to mitigate its impact.&#10;&#10;2. Features are being used in an attempt to create a statistical model for this process by extracting relevant characteristics from the speech signals that highlight differences between clean and noisy sentences. In this context, features might include measures related to voiced vs. unvoiced sounds, pitch detection, or other spectral properties. The goal is to build a statistical model using these features that can accurately differentiate between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;The discussion revolves around finding suitable features for this task, such as the maximum of the auto-correlation function, which could help characterize voiced phonemes more effectively. The team aims to use these features to create a statistical model that captures the differences between clean and noisy sentences, ultimately leading to better speech recognition performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's argument about voiced phonemes giving us intuition: During the discussion, Professor B suggests that the concept of voiced phonemes helps shape our intuition regarding speech recognition (e.g., &quot;what I'm arguing is that that gives you your intuition&quot;). This implies that understanding the difference between voiced and unvoiced sounds contributes to forming a basic framework for analyzing and comprehending speech signals.&#10;2. Complexities in speech and their acceptance: Professor B argues that in reality, there are overlaps and complexities in speech that make it difficult to perfectly separate voiced from unvoiced sounds (&quot;But in reality, it's all of this overlap and so forth&quot;). Despite these challenges, the professor suggests that this might be acceptable for a few reasons. First, what we gain from the analysis is not necessarily a clear-cut distinction between voiced and unvoiced sounds but rather a characterization driven by our intuition. Second, imperfect data may affect the accuracy of any speech recognition system. Consequently, relying on this basic intuition might still provide valuable insights in understanding &quot;almost raw data&quot; instead of relying solely on refined and smoothed versions of the signal." target="1. When Professor B suggests that imperfect distinctions between voiced and unvoiced sounds might not be a significant issue, they are implying that the primary value in this analysis comes from obtaining a general characterization of the speech signal based on our intuition about these categories. This rough characterization can provide insights into the raw data, even if it does not offer a clear-cut distinction between voiced and unvoiced sounds due to overlaps and complexities in speech.&#10;2. The professor emphasizes that relying on this basic intuition might be more valuable than using overly refined or smoothed versions of the signal since real-world data, such as noise, can impact the accuracy of any speech recognition system. Consequently, understanding how noise affects speech perception and developing methods to mitigate its effects is essential for improving speech recognition systems' performance in various environments.&#10;3. The team is currently focused on selecting appropriate features for their task, with one possibility being the maximum of the auto-correlation function to better characterize voiced phonemes. However, they have not yet decided whether to use a trained system or threshold-based system to implement these features in their statistical model.&#10;4. Professor B implies that even if the &quot;world's best voiced-unvoiced classifier&quot; does not perfectly distinguish between these categories, trying different statistical characterizations of the difference based on raw data may still yield valuable information that a speech recognition system can utilize.">
      <data key="d0">1</data>
    </edge>
    <edge source=" .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Two hundred and fifty thousand .&#10;Speaker: PhD A&#10;Content: Fifteen hundred . Because {disfmarker} Yeah .&#10;Speaker: PhD F&#10;Content: Yeah . Two thousand and fifteen hundred .&#10;Speaker: PhD A&#10;Content: Above , um {disfmarker} {vocalsound} it seems that {disfmarker} Well , some voiced sound can have also , {vocalsound} like , a noisy {pause} part on high frequencies , and {disfmarker} But {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Well , it 's just {disfmarker}&#10;Speaker: Professor B&#10;Content: No , it 's {disfmarker} makes sense to look at {pause} low frequencies .&#10;Speaker: PhD C&#10;Content: So this is {disfmarker} uh , basically this is comparing {vocalsound} an original version of the signal to a smoothed version of the same signal ?&#10;Speaker: PhD F&#10;Content: Yeah" target="The frequency range used for computing the variance in the example being discussed by the speakers is from 200 Hz to 1500 Hz. One of the speakers later corrects this range to 250 thousand Hz, but it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended.">
      <data key="d0">1</data>
    </edge>
    <edge source=" .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Two hundred and fifty thousand .&#10;Speaker: PhD A&#10;Content: Fifteen hundred . Because {disfmarker} Yeah .&#10;Speaker: PhD F&#10;Content: Yeah . Two thousand and fifteen hundred .&#10;Speaker: PhD A&#10;Content: Above , um {disfmarker} {vocalsound} it seems that {disfmarker} Well , some voiced sound can have also , {vocalsound} like , a noisy {pause} part on high frequencies , and {disfmarker} But {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Well , it 's just {disfmarker}&#10;Speaker: Professor B&#10;Content: No , it 's {disfmarker} makes sense to look at {pause} low frequencies .&#10;Speaker: PhD C&#10;Content: So this is {disfmarker} uh , basically this is comparing {vocalsound} an original version of the signal to a smoothed version of the same signal ?&#10;Speaker: PhD F&#10;Content: Yeah" target="The frequency range used for computing the variance in the example discussed by the speakers is from 200 Hz to 1500 Hz. This is stated explicitly by speaker A, and although professor F mistakenly gives a higher frequency range, it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended.">
      <data key="d0">1</data>
    </edge>
    <edge source=" .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Two hundred and fifty thousand .&#10;Speaker: PhD A&#10;Content: Fifteen hundred . Because {disfmarker} Yeah .&#10;Speaker: PhD F&#10;Content: Yeah . Two thousand and fifteen hundred .&#10;Speaker: PhD A&#10;Content: Above , um {disfmarker} {vocalsound} it seems that {disfmarker} Well , some voiced sound can have also , {vocalsound} like , a noisy {pause} part on high frequencies , and {disfmarker} But {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Well , it 's just {disfmarker}&#10;Speaker: Professor B&#10;Content: No , it 's {disfmarker} makes sense to look at {pause} low frequencies .&#10;Speaker: PhD C&#10;Content: So this is {disfmarker} uh , basically this is comparing {vocalsound} an original version of the signal to a smoothed version of the same signal ?&#10;Speaker: PhD F&#10;Content: Yeah" target="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , sorry .&#10;Speaker: Professor B&#10;Content: and {disfmarker} But what I 'm saying is that may be OK , because what you 're really getting is not actually voiced versus unvoiced , both for the fac the reason of the overlap and {disfmarker} and then , uh , th you know , structural reasons , uh , uh , like the one that Chuck said , that {disfmarker} that in fact , well , the data itself is {disfmarker} {vocalsound} that you 're working with is not perfect .&#10;Speaker: PhD A&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor B&#10;Content: So , what I 'm saying is maybe that 's not a killer because you 're just getting some characterization , one that 's driven by your intuition about voiced - unvoiced certainly ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: but it 's just some characterization {vocalsound} of something back in the {disfmarker} in the {disfmarker} in the almost raw data , rather than the smooth version" target="1. When Professor B suggests that imperfect distinctions between voiced and unvoiced sounds might not be a significant issue, they are implying that the primary value in this analysis comes from obtaining a general characterization of the speech signal based on our intuition about these categories. This rough characterization can provide insights into the raw data, even if it does not offer a clear-cut distinction between voiced and unvoiced sounds due to overlaps and complexities in speech.&#10;2. The professor emphasizes that relying on this basic intuition might be more valuable than using overly refined or smoothed versions of the signal since real-world data, such as noise, can impact the accuracy of any speech recognition system. Consequently, understanding how noise affects speech perception and developing methods to mitigate its effects is essential for improving speech recognition systems' performance in various environments.&#10;3. The team is currently focused on selecting appropriate features for their task, with one possibility being the maximum of the auto-correlation function to better characterize voiced phonemes. However, they have not yet decided whether to use a trained system or threshold-based system to implement these features in their statistical model.&#10;4. Professor B implies that even if the &quot;world's best voiced-unvoiced classifier&quot; does not perfectly distinguish between these categories, trying different statistical characterizations of the difference based on raw data may still yield valuable information that a speech recognition system can utilize.">
      <data key="d0">1</data>
    </edge>
    <edge source=" the world 's best voiced - unvoiced , uh , uh , classifier ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: but it 's more that , {vocalsound} you know , uh , uh , try some different statistical characterizations of that difference back to the raw data&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: and {disfmarker} and m maybe there 's something there that {pause} the system can use .&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: Yeah . Yeah , but ther more obvious is that {disfmarker} Yeah . The {disfmarker} the more obvious is that {disfmarker} that {disfmarker} well , using the {disfmarker} th the FFT , um , {vocalsound} you just {disfmarker} it gives you just information about if it 's voiced or not voiced , ma mainly , I mean . But {disfmarker} So ,&#10;Spe" target="1. When Professor B suggests that imperfect distinctions between voiced and unvoiced sounds might not be a significant issue, they are implying that the primary value in this analysis comes from obtaining a general characterization of the speech signal based on our intuition about these categories. This rough characterization can provide insights into the raw data, even if it does not offer a clear-cut distinction between voiced and unvoiced sounds due to overlaps and complexities in speech.&#10;2. The professor emphasizes that relying on this basic intuition might be more valuable than using overly refined or smoothed versions of the signal since real-world data, such as noise, can impact the accuracy of any speech recognition system. Consequently, understanding how noise affects speech perception and developing methods to mitigate its effects is essential for improving speech recognition systems' performance in various environments.&#10;3. The team is currently focused on selecting appropriate features for their task, with one possibility being the maximum of the auto-correlation function to better characterize voiced phonemes. However, they have not yet decided whether to use a trained system or threshold-based system to implement these features in their statistical model.&#10;4. Professor B implies that even if the &quot;world's best voiced-unvoiced classifier&quot; does not perfectly distinguish between these categories, trying different statistical characterizations of the difference based on raw data may still yield valuable information that a speech recognition system can utilize.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Multiple Numbers: Professor B suggests using two or three numbers instead of relying on a single number to provide a more comprehensive understanding of the situation (content: &quot;I think one thing to do is to just not rely on a single number to maybe have two or three numbers&quot;).&#10;2. Weighted Average: Both professors discuss the idea of using a weighted average, which takes into account different factors and their significance in the given situation (content: &quot;And the thing is it's not just a pure average because there are these weightings. It's a weighted average.&quot;).&#10;3. Relative Improvement or Error Rates: Professor B raises the question of whether to average relative improvements or error rates, suggesting that this decision might be influenced by the specific context and requirements (content: &quot;But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that?&quot;).&#10;4. Analyzing Scores Instead of Improvements: PhD C proposes analyzing scores directly instead of focusing on improvements, which might help in finding a more suitable way to combine different factors (content: &quot;Why don't they not look at improvements but just look at your av your scores? You know, figure out how to combine the scores&quot;)." target="Speaker: Professor B&#10;Content: Yeah . Yeah .&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So maybe , {vocalsound} le&#10;Speaker: PhD C&#10;Content: Should we do digits ?&#10;Speaker: Professor B&#10;Content: let 's do digits . Let you {disfmarker} you start .&#10;Speaker: Grad D&#10;Content: Oh , OK .&#10;Speaker: Grad E&#10;Content: L fifty .&#10;Speaker: PhD A&#10;Content: Right .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Multiple Numbers: Professor B suggests using two or three numbers instead of relying on a single number to provide a more comprehensive understanding of the situation (content: &quot;I think one thing to do is to just not rely on a single number to maybe have two or three numbers&quot;).&#10;2. Weighted Average: Both professors discuss the idea of using a weighted average, which takes into account different factors and their significance in the given situation (content: &quot;And the thing is it's not just a pure average because there are these weightings. It's a weighted average.&quot;).&#10;3. Relative Improvement or Error Rates: Professor B raises the question of whether to average relative improvements or error rates, suggesting that this decision might be influenced by the specific context and requirements (content: &quot;But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that?&quot;).&#10;4. Analyzing Scores Instead of Improvements: PhD C proposes analyzing scores directly instead of focusing on improvements, which might help in finding a more suitable way to combine different factors (content: &quot;Why don't they not look at improvements but just look at your av your scores? You know, figure out how to combine the scores&quot;)." target="disfmarker}&#10;Speaker: Professor B&#10;Content: Well , no {disfmarker} well , no . I mean , {vocalsound} it isn't the operating theater . I mean , they don they {disfmarker} they don't {disfmarker} they don't really {pause} know , I think .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean , I th&#10;Speaker: PhD C&#10;Content: So if {disfmarker} if they don't know , doesn't that suggest the way for them to go ? Uh , you assume everything 's equal . I mean , y y I mean , you {disfmarker}&#10;Speaker: Professor B&#10;Content: Well , I mean , I {disfmarker} I think one thing to do is to just not rely on a single number {disfmarker} to maybe have two or three numbers ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: you know ,&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: and {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Multiple Numbers: Professor B suggests using two or three numbers instead of relying on a single number to provide a more comprehensive understanding of the situation (content: &quot;I think one thing to do is to just not rely on a single number to maybe have two or three numbers&quot;).&#10;2. Weighted Average: Both professors discuss the idea of using a weighted average, which takes into account different factors and their significance in the given situation (content: &quot;And the thing is it's not just a pure average because there are these weightings. It's a weighted average.&quot;).&#10;3. Relative Improvement or Error Rates: Professor B raises the question of whether to average relative improvements or error rates, suggesting that this decision might be influenced by the specific context and requirements (content: &quot;But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that?&quot;).&#10;4. Analyzing Scores Instead of Improvements: PhD C proposes analyzing scores directly instead of focusing on improvements, which might help in finding a more suitable way to combine different factors (content: &quot;Why don't they not look at improvements but just look at your av your scores? You know, figure out how to combine the scores&quot;)." target=": Professor B&#10;Content: No , that is relative . But the question is , do you average the relative improvements {pause} or do you average the error rates and take the relative improvement maybe of that ?&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Professor B&#10;Content: And the thing is it 's not just a pure average because there are these weightings .&#10;Speaker: PhD C&#10;Content: Oh .&#10;Speaker: Professor B&#10;Content: It 's a weighted average . Um .&#10;Speaker: PhD A&#10;Content: Yeah . And so when you average the {disfmarker} the relative improvement it tends to {disfmarker} {vocalsound} to give a lot of {disfmarker} of , um , {vocalsound} importance to the well - matched case because {pause} the baseline is already very good and , um , i it 's {disfmarker}&#10;Speaker: PhD C&#10;Content: Why don't they not look at improvements but just look at your av your scores ? You know , figure out how to combine the scores&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Multiple Numbers: Professor B suggests using two or three numbers instead of relying on a single number to provide a more comprehensive understanding of the situation (content: &quot;I think one thing to do is to just not rely on a single number to maybe have two or three numbers&quot;).&#10;2. Weighted Average: Both professors discuss the idea of using a weighted average, which takes into account different factors and their significance in the given situation (content: &quot;And the thing is it's not just a pure average because there are these weightings. It's a weighted average.&quot;).&#10;3. Relative Improvement or Error Rates: Professor B raises the question of whether to average relative improvements or error rates, suggesting that this decision might be influenced by the specific context and requirements (content: &quot;But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that?&quot;).&#10;4. Analyzing Scores Instead of Improvements: PhD C proposes analyzing scores directly instead of focusing on improvements, which might help in finding a more suitable way to combine different factors (content: &quot;Why don't they not look at improvements but just look at your av your scores? You know, figure out how to combine the scores&quot;)." target=" A&#10;Content: Mmm , I just {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , you don't know . OK .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Alright .&#10;Speaker: PhD A&#10;Content: Um , yeah . So the points were the {disfmarker} the weights {disfmarker} how to weight the different error rates {vocalsound} that are obtained from different language and {disfmarker} and conditions . Um , it 's not clear that they will keep the same kind of weighting . Right now it 's a weighting on {disfmarker} on improvement .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Some people are arguing that it would be better to have weights on uh {disfmarker} well , to {disfmarker} to combine error rates {pause} before computing improvement . Uh , and the fact is that for {disfmarker} right now for {pause} the English , they have weights {disfmarker} they {disfmarker} they combine">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Multiple Numbers: Professor B suggests using two or three numbers instead of relying on a single number to provide a more comprehensive understanding of the situation (content: &quot;I think one thing to do is to just not rely on a single number to maybe have two or three numbers&quot;).&#10;2. Weighted Average: Both professors discuss the idea of using a weighted average, which takes into account different factors and their significance in the given situation (content: &quot;And the thing is it's not just a pure average because there are these weightings. It's a weighted average.&quot;).&#10;3. Relative Improvement or Error Rates: Professor B raises the question of whether to average relative improvements or error rates, suggesting that this decision might be influenced by the specific context and requirements (content: &quot;But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that?&quot;).&#10;4. Analyzing Scores Instead of Improvements: PhD C proposes analyzing scores directly instead of focusing on improvements, which might help in finding a more suitable way to combine different factors (content: &quot;Why don't they not look at improvements but just look at your av your scores? You know, figure out how to combine the scores&quot;)." target="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Multiple Numbers: Professor B suggests using two or three numbers instead of relying on a single number to provide a more comprehensive understanding of the situation (content: &quot;I think one thing to do is to just not rely on a single number to maybe have two or three numbers&quot;).&#10;2. Weighted Average: Both professors discuss the idea of using a weighted average, which takes into account different factors and their significance in the given situation (content: &quot;And the thing is it's not just a pure average because there are these weightings. It's a weighted average.&quot;).&#10;3. Relative Improvement or Error Rates: Professor B raises the question of whether to average relative improvements or error rates, suggesting that this decision might be influenced by the specific context and requirements (content: &quot;But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that?&quot;).&#10;4. Analyzing Scores Instead of Improvements: PhD C proposes analyzing scores directly instead of focusing on improvements, which might help in finding a more suitable way to combine different factors (content: &quot;Why don't they not look at improvements but just look at your av your scores? You know, figure out how to combine the scores&quot;)." target="In the meeting transcript, PhD C reported on their experiments related to the HTK (Hidden Markov Model Toolkit) back-end models. They focused on the number of training iterations in different rounds of the model training process. Specifically, there are four rounds of training, and the number of iterations for each round is as follows:&#10;&#10;1. First round: 3 iterations&#10;2. Second to fourth rounds: 7 iterations for each round&#10;&#10;These iterations refer to the number of times the HMM (Hidden Markov Model) re-estimation is run during each training round. The higher number of iterations in later rounds indicates that more computational resources are devoted to refining the models in those stages.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Multiple Numbers: Professor B suggests using two or three numbers instead of relying on a single number to provide a more comprehensive understanding of the situation (content: &quot;I think one thing to do is to just not rely on a single number to maybe have two or three numbers&quot;).&#10;2. Weighted Average: Both professors discuss the idea of using a weighted average, which takes into account different factors and their significance in the given situation (content: &quot;And the thing is it's not just a pure average because there are these weightings. It's a weighted average.&quot;).&#10;3. Relative Improvement or Error Rates: Professor B raises the question of whether to average relative improvements or error rates, suggesting that this decision might be influenced by the specific context and requirements (content: &quot;But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that?&quot;).&#10;4. Analyzing Scores Instead of Improvements: PhD C proposes analyzing scores directly instead of focusing on improvements, which might help in finding a more suitable way to combine different factors (content: &quot;Why don't they not look at improvements but just look at your av your scores? You know, figure out how to combine the scores&quot;)." target="Professionor B expresses that making any policy change in meetings is tricky because everyone has different perspectives and opinions. They imply that reaching a consensus or agreement can be difficult due to these varied viewpoints.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Multiple Numbers: Professor B suggests using two or three numbers instead of relying on a single number to provide a more comprehensive understanding of the situation (content: &quot;I think one thing to do is to just not rely on a single number to maybe have two or three numbers&quot;).&#10;2. Weighted Average: Both professors discuss the idea of using a weighted average, which takes into account different factors and their significance in the given situation (content: &quot;And the thing is it's not just a pure average because there are these weightings. It's a weighted average.&quot;).&#10;3. Relative Improvement or Error Rates: Professor B raises the question of whether to average relative improvements or error rates, suggesting that this decision might be influenced by the specific context and requirements (content: &quot;But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that?&quot;).&#10;4. Analyzing Scores Instead of Improvements: PhD C proposes analyzing scores directly instead of focusing on improvements, which might help in finding a more suitable way to combine different factors (content: &quot;Why don't they not look at improvements but just look at your av your scores? You know, figure out how to combine the scores&quot;)." target="Based on the transcript provided, the exact date in June for the next Aurora deadline was not specified during the conversation. Speaker PhD A knew that it was in June but did not have the exact date. Speaker Professor B asked about early, late, or middle June, but no specific date was given. Additionally, none of the participants provided information about who could provide this information.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Professor B&#10;Content: No , that is relative . But the question is , do you average the relative improvements {pause} or do you average the error rates and take the relative improvement maybe of that ?&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Professor B&#10;Content: And the thing is it 's not just a pure average because there are these weightings .&#10;Speaker: PhD C&#10;Content: Oh .&#10;Speaker: Professor B&#10;Content: It 's a weighted average . Um .&#10;Speaker: PhD A&#10;Content: Yeah . And so when you average the {disfmarker} the relative improvement it tends to {disfmarker} {vocalsound} to give a lot of {disfmarker} of , um , {vocalsound} importance to the well - matched case because {pause} the baseline is already very good and , um , i it 's {disfmarker}&#10;Speaker: PhD C&#10;Content: Why don't they not look at improvements but just look at your av your scores ? You know , figure out how to combine the scores&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Spe" target="Professionor B expresses that making any policy change in meetings is tricky because everyone has different perspectives and opinions. They imply that reaching a consensus or agreement can be difficult due to these varied viewpoints.">
      <data key="d0">1</data>
    </edge>
    <edge source=" A&#10;Content: Mmm , I just {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , you don't know . OK .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Alright .&#10;Speaker: PhD A&#10;Content: Um , yeah . So the points were the {disfmarker} the weights {disfmarker} how to weight the different error rates {vocalsound} that are obtained from different language and {disfmarker} and conditions . Um , it 's not clear that they will keep the same kind of weighting . Right now it 's a weighting on {disfmarker} on improvement .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Some people are arguing that it would be better to have weights on uh {disfmarker} well , to {disfmarker} to combine error rates {pause} before computing improvement . Uh , and the fact is that for {disfmarker} right now for {pause} the English , they have weights {disfmarker} they {disfmarker} they combine" target="The speakers, who appear to be discussing the evaluation and improvement of a signal processing system, are expressing uncertainty about the final application of the system and whether it will need to handle more difficult cases with high accuracy. Professor B mentions that if the predominate use of the system is in challenging scenarios, current methods may not be sufficient. However, in better cases, the system could potentially work well with some error recovery.&#10;&#10;Later in the discussion, the speakers consider a variance calculation done on a frequency range of 200 Hz to 1500 Hz, which they believe is more logical for their analysis. They also discuss different methods for weighting error rates obtained from various languages and conditions, with some arguing that it would be better to combine error rates before computing improvements.&#10;&#10;In summary, the speakers are discussing the uncertainty of the system's primary use, the potential for reasonable error recovery in better cases, and different approaches to weighing error rates for improved performance in various scenarios.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. First, the signal's FFT (Fast Fourier Transform) spectrum is computed to obtain a frequency-domain representation of the signal.&#10;2. Then, the mel filter bank is applied to the signal. A mel filter bank is a set of triangular filters spaced on the mel scale, which approximately corresponds to the human auditory system's frequency perception. This process results in a new spectrum that emphasizes the important features for human hearing.&#10;3. The mel filter bank spectrum is then subtracted from the original FFT spectrum. This difference represents the excitation signal, which carries information about the speech production mechanism (vocal cords for voiced sounds and noise for unvoiced sounds).&#10;4. The resulting excitation signal may be used as a feature for various speech processing tasks, such as voice activity detection, pitch estimation, or speaker identification. It's essential to note that this excitation isn't the actual physiological excitation but an approximation that conveys information about it." target=" the excitation , as I recall , is you 're subtracting the {disfmarker} the , um {disfmarker} the mel {disfmarker} mel {disfmarker} {vocalsound} mel filter , uh , spectrum from the FFT spectrum .&#10;Speaker: PhD A&#10;Content: e That 's right . Yeah . So {disfmarker}&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So we have the mel f filter bank , we have the FFT , so we {pause} just {disfmarker}&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker} it 's not really an excitation ,&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Professor B&#10;Content: but it 's something that hopefully tells you something about the excitation .&#10;Speaker: PhD A&#10;Content: Yeah , that 's right .&#10;Speaker: Professor B&#10;Content: Yeah , yeah .&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="1. First, the signal's FFT (Fast Fourier Transform) spectrum is computed to obtain a frequency-domain representation of the signal.&#10;2. Then, the mel filter bank is applied to the signal. A mel filter bank is a set of triangular filters spaced on the mel scale, which approximately corresponds to the human auditory system's frequency perception. This process results in a new spectrum that emphasizes the important features for human hearing.&#10;3. The mel filter bank spectrum is then subtracted from the original FFT spectrum. This difference represents the excitation signal, which carries information about the speech production mechanism (vocal cords for voiced sounds and noise for unvoiced sounds).&#10;4. The resulting excitation signal may be used as a feature for various speech processing tasks, such as voice activity detection, pitch estimation, or speaker identification. It's essential to note that this excitation isn't the actual physiological excitation but an approximation that conveys information about it." target=" an FFT , it may be {disfmarker} it may be pushing things .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: And {disfmarker} and , uh {disfmarker}&#10;Speaker: PhD C&#10;Content: Would you {disfmarker} would you wanna do this kind of , uh , difference thing {vocalsound} after you do spectral subtraction ?&#10;Speaker: PhD A&#10;Content: Uh , {vocalsound} maybe .&#10;Speaker: PhD F&#10;Content: No . Maybe we can do that .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Hmm . The spectral subtraction is being done at what level ? Is it being done at the level of FFT bins or at the level of , uh , mel spectrum or something ?&#10;Speaker: PhD A&#10;Content: Um , I guess it depends .&#10;Speaker: Professor B&#10;Content: I mean , how are they doing it ?&#10;Speaker: PhD A&#10;Content: How they 're doing it ? Yeah . Um , I guess Ericsson is on the , um , filter bank ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. First, the signal's FFT (Fast Fourier Transform) spectrum is computed to obtain a frequency-domain representation of the signal.&#10;2. Then, the mel filter bank is applied to the signal. A mel filter bank is a set of triangular filters spaced on the mel scale, which approximately corresponds to the human auditory system's frequency perception. This process results in a new spectrum that emphasizes the important features for human hearing.&#10;3. The mel filter bank spectrum is then subtracted from the original FFT spectrum. This difference represents the excitation signal, which carries information about the speech production mechanism (vocal cords for voiced sounds and noise for unvoiced sounds).&#10;4. The resulting excitation signal may be used as a feature for various speech processing tasks, such as voice activity detection, pitch estimation, or speaker identification. It's essential to note that this excitation isn't the actual physiological excitation but an approximation that conveys information about it." target="isfmarker}&#10;Speaker: PhD C&#10;Content: what sorts of features are you looking at ?&#10;Speaker: PhD F&#10;Content: We have some {disfmarker}&#10;Speaker: PhD A&#10;Content: So we would be looking at , um , the {pause} variance of the spectrum of the excitation ,&#10;Speaker: PhD F&#10;Content: uh , um , this , this , and this .&#10;Speaker: PhD A&#10;Content: something like this , which is {disfmarker} should be high for voiced sounds . Uh , we {disfmarker}&#10;Speaker: PhD C&#10;Content: Wait a minute . I {disfmarker} what does that mean ? The variance of the spectrum of excitation .&#10;Speaker: PhD A&#10;Content: Yeah . So the {disfmarker} So basically the spectrum of the excitation {vocalsound} for a purely periodic sig signal shou sh&#10;Speaker: Professor B&#10;Content: OK . Yeah , w what yo what you 're calling the excitation , as I recall , is you 're subtracting the {disfmarker} the , um {disfmarker} the mel">
      <data key="d0">1</data>
    </edge>
    <edge source="1. First, the signal's FFT (Fast Fourier Transform) spectrum is computed to obtain a frequency-domain representation of the signal.&#10;2. Then, the mel filter bank is applied to the signal. A mel filter bank is a set of triangular filters spaced on the mel scale, which approximately corresponds to the human auditory system's frequency perception. This process results in a new spectrum that emphasizes the important features for human hearing.&#10;3. The mel filter bank spectrum is then subtracted from the original FFT spectrum. This difference represents the excitation signal, which carries information about the speech production mechanism (vocal cords for voiced sounds and noise for unvoiced sounds).&#10;4. The resulting excitation signal may be used as a feature for various speech processing tasks, such as voice activity detection, pitch estimation, or speaker identification. It's essential to note that this excitation isn't the actual physiological excitation but an approximation that conveys information about it." target="&#10;Speaker: PhD A&#10;Content: And the way to do this {vocalsound} is that {disfmarker} well , we have the {disfmarker} we have the FFT because it 's computed in {disfmarker} in the {disfmarker} in the system , and we have {vocalsound} the mel {vocalsound} filter banks ,&#10;Speaker: PhD C&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD A&#10;Content: and so if we {disfmarker} if we , like , remove the mel filter bank from the FFT , {vocalsound} we have something that 's {pause} close to the {pause} excitation signal .&#10;Speaker: Grad E&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: It 's something that 's like {vocalsound} a {disfmarker} a a train of p a pulse train for voiced sound&#10;Speaker: PhD C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Oh ! OK . Yeah .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. First, the signal's FFT (Fast Fourier Transform) spectrum is computed to obtain a frequency-domain representation of the signal.&#10;2. Then, the mel filter bank is applied to the signal. A mel filter bank is a set of triangular filters spaced on the mel scale, which approximately corresponds to the human auditory system's frequency perception. This process results in a new spectrum that emphasizes the important features for human hearing.&#10;3. The mel filter bank spectrum is then subtracted from the original FFT spectrum. This difference represents the excitation signal, which carries information about the speech production mechanism (vocal cords for voiced sounds and noise for unvoiced sounds).&#10;4. The resulting excitation signal may be used as a feature for various speech processing tasks, such as voice activity detection, pitch estimation, or speaker identification. It's essential to note that this excitation isn't the actual physiological excitation but an approximation that conveys information about it." target=" a smooth spectral envelope , so there must be something else that you get {pause} in return for that {disfmarker}&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: that , uh {disfmarker} uh {disfmarker} So .&#10;Speaker: PhD C&#10;Content: So how does {disfmarker} uh , maybe I 'm going in too much detail , but {vocalsound} how exactly do you make the difference between the FFT and the smoothed {pause} spectral envelope ? Wha - wh i i uh , how is that , uh {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Um , we just {disfmarker} How did we do it up again ?&#10;Speaker: PhD F&#10;Content: Uh , we distend the {disfmarker} we have the twenty - three coefficient af after the mel f {vocalsound} filter ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: and we extend these coefficient between the {disfmarker} all the frequency range .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. First, the signal's FFT (Fast Fourier Transform) spectrum is computed to obtain a frequency-domain representation of the signal.&#10;2. Then, the mel filter bank is applied to the signal. A mel filter bank is a set of triangular filters spaced on the mel scale, which approximately corresponds to the human auditory system's frequency perception. This process results in a new spectrum that emphasizes the important features for human hearing.&#10;3. The mel filter bank spectrum is then subtracted from the original FFT spectrum. This difference represents the excitation signal, which carries information about the speech production mechanism (vocal cords for voiced sounds and noise for unvoiced sounds).&#10;4. The resulting excitation signal may be used as a feature for various speech processing tasks, such as voice activity detection, pitch estimation, or speaker identification. It's essential to note that this excitation isn't the actual physiological excitation but an approximation that conveys information about it." target=" um , the mel cepstru mel {pause} spectrum , mel cepstrum , {vocalsound} any of these variants , um , give you the smooth spectrum . It 's the spectral envelope . By going back to the FFT , {vocalsound} you 're getting something that is {pause} more like the raw data . So the question is , what characterization {disfmarker} and you 're playing around with this {disfmarker} another way of looking at it is what characterization {vocalsound} of the difference between {pause} the raw data {pause} and this smooth version {pause} is something that you 're missing that could help ? So , I mean , looking at different statistical measures of that difference , coming up with some things and just trying them out and seeing if you add them onto the feature vector does that make things better or worse in noise , where you 're really just i i the way I 'm looking at it is not so much you 're trying to f find the best {disfmarker} the world 's best voiced - unvoiced , uh , uh , classifier ,&#10;Speaker: PhD C&#10;Content: Mm - hmm">
      <data key="d0">1</data>
    </edge>
    <edge source="1. First, the signal's FFT (Fast Fourier Transform) spectrum is computed to obtain a frequency-domain representation of the signal.&#10;2. Then, the mel filter bank is applied to the signal. A mel filter bank is a set of triangular filters spaced on the mel scale, which approximately corresponds to the human auditory system's frequency perception. This process results in a new spectrum that emphasizes the important features for human hearing.&#10;3. The mel filter bank spectrum is then subtracted from the original FFT spectrum. This difference represents the excitation signal, which carries information about the speech production mechanism (vocal cords for voiced sounds and noise for unvoiced sounds).&#10;4. The resulting excitation signal may be used as a feature for various speech processing tasks, such as voice activity detection, pitch estimation, or speaker identification. It's essential to note that this excitation isn't the actual physiological excitation but an approximation that conveys information about it." target="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. First, the signal's FFT (Fast Fourier Transform) spectrum is computed to obtain a frequency-domain representation of the signal.&#10;2. Then, the mel filter bank is applied to the signal. A mel filter bank is a set of triangular filters spaced on the mel scale, which approximately corresponds to the human auditory system's frequency perception. This process results in a new spectrum that emphasizes the important features for human hearing.&#10;3. The mel filter bank spectrum is then subtracted from the original FFT spectrum. This difference represents the excitation signal, which carries information about the speech production mechanism (vocal cords for voiced sounds and noise for unvoiced sounds).&#10;4. The resulting excitation signal may be used as a feature for various speech processing tasks, such as voice activity detection, pitch estimation, or speaker identification. It's essential to note that this excitation isn't the actual physiological excitation but an approximation that conveys information about it." target="1. Benefits of smooth representation for generalization: A smooth representation generally leads to better generalization because it reduces overfitting by averaging out fine details and noise in the data. This can help improve the model's performance on unseen data, as the model is less likely to learn spurious patterns that only exist in the training set.&#10;&#10;2. Potential drawbacks of losing details during smoothing: Smoothing inevitably leads to loss of information, which might include valuable details about the original signal. This could potentially make the model miss important features or characteristics of the data, leading to reduced accuracy or suboptimal performance in certain cases. In speech recognition, for example, excessive smoothing might cause the loss of crucial information related to phoneme distinction or speaker identity.&#10;&#10;In summary, while smoothing offers benefits such as improved generalization and reduced overfitting, it can also result in the loss of important details that may be necessary for accurate analysis or modeling. Balancing these trade-offs is essential for developing effective speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. First, the signal's FFT (Fast Fourier Transform) spectrum is computed to obtain a frequency-domain representation of the signal.&#10;2. Then, the mel filter bank is applied to the signal. A mel filter bank is a set of triangular filters spaced on the mel scale, which approximately corresponds to the human auditory system's frequency perception. This process results in a new spectrum that emphasizes the important features for human hearing.&#10;3. The mel filter bank spectrum is then subtracted from the original FFT spectrum. This difference represents the excitation signal, which carries information about the speech production mechanism (vocal cords for voiced sounds and noise for unvoiced sounds).&#10;4. The resulting excitation signal may be used as a feature for various speech processing tasks, such as voice activity detection, pitch estimation, or speaker identification. It's essential to note that this excitation isn't the actual physiological excitation but an approximation that conveys information about it." target="1. The voiced-unvoiced (VU) detector's purpose is to determine if a given segment of speech is voiced or unvoiced. Voiced sounds are produced when the vocal cords vibrate, such as vowel sounds, whereas unvoiced sounds result from airflow through the mouth without vocal cord vibration, like consonant sounds (e.g., 'f' or 's').&#10;2. In this conversation, the VU detector is intended to be used as a feature extractor for speech processing tasks, not part of a typical voiced-unvoiced detection system. The idea is that it could be incorporated into either a separate neural network designed specifically for VU detection or combined with a larger neural network responsible for phoneme classification.&#10;3. Additionally, there is discussion about using the VU detector as part of an improved front-end processing system to enhance pitch detection, which is crucial for better speech processing performance since Mel Frequency Cepstral Coefficients (MFCCs) do not include any pitch-related information. By implementing a good pitch detection algorithm, they aim to improve voice activity detection and other speech processing tasks' accuracy and robustness.&#10;&#10;In summary, the voiced-unvoiced detector is intended for use as a feature extractor in speech processing tasks and potentially improving pitch detection algorithms within front-end processing systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker}&#10;Speaker: PhD C&#10;Content: what sorts of features are you looking at ?&#10;Speaker: PhD F&#10;Content: We have some {disfmarker}&#10;Speaker: PhD A&#10;Content: So we would be looking at , um , the {pause} variance of the spectrum of the excitation ,&#10;Speaker: PhD F&#10;Content: uh , um , this , this , and this .&#10;Speaker: PhD A&#10;Content: something like this , which is {disfmarker} should be high for voiced sounds . Uh , we {disfmarker}&#10;Speaker: PhD C&#10;Content: Wait a minute . I {disfmarker} what does that mean ? The variance of the spectrum of excitation .&#10;Speaker: PhD A&#10;Content: Yeah . So the {disfmarker} So basically the spectrum of the excitation {vocalsound} for a purely periodic sig signal shou sh&#10;Speaker: Professor B&#10;Content: OK . Yeah , w what yo what you 're calling the excitation , as I recall , is you 're subtracting the {disfmarker} the , um {disfmarker} the mel" target="The frequency range used for computing the variance in the example discussed by the speakers is from 200 Hz to 1500 Hz. This is stated explicitly by speaker A, and although professor F mistakenly gives a higher frequency range, it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended.">
      <data key="d0">1</data>
    </edge>
    <edge source=" um , the mel cepstru mel {pause} spectrum , mel cepstrum , {vocalsound} any of these variants , um , give you the smooth spectrum . It 's the spectral envelope . By going back to the FFT , {vocalsound} you 're getting something that is {pause} more like the raw data . So the question is , what characterization {disfmarker} and you 're playing around with this {disfmarker} another way of looking at it is what characterization {vocalsound} of the difference between {pause} the raw data {pause} and this smooth version {pause} is something that you 're missing that could help ? So , I mean , looking at different statistical measures of that difference , coming up with some things and just trying them out and seeing if you add them onto the feature vector does that make things better or worse in noise , where you 're really just i i the way I 'm looking at it is not so much you 're trying to f find the best {disfmarker} the world 's best voiced - unvoiced , uh , uh , classifier ,&#10;Speaker: PhD C&#10;Content: Mm - hmm" target="1. Advanced processing techniques: If re-synthesis turns out to be useful in the long term, more advanced or customized processing techniques can be applied to the signal. (Potential long-term benefits of using re-synthesis)&#10;2. Robustness: Even with the sacrifice of a smooth spectral envelope, combining probabilities of individual features within each mixture might lead to more robust results, as suggested by Professor B and PhD A when discussing the comparison between raw data and smooth versions of the spectrum.&#10;3. Exploration of statistical measures: By analyzing the difference between raw data and the smooth version, new statistical measures can be explored and added to feature vectors to improve performance in noisy environments. This approach is mentioned by PhD as a way of finding something that might have been missed in the smoothed spectrum but could contribute positively to the final result.&#10;4. Potential improvements through spectral subtraction and LDA filtering: As suggested by Professor B, combining probabilities within each mixture along with spectral subtraction and LDA filtering might yield better results than simpler approaches, as mentioned in a previous meeting. However, this depends on the specific implementation of spectral subtraction and its compatibility with online normalization.&#10;5. Addressing musical tones issue: Although not explicitly stated, combining probabilities within each mixture might help address issues related to musical tones raised by PhD A in a previous meeting. However, further investigation is needed to confirm this potential advantage.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." target=" the code .&#10;Speaker: Professor B&#10;Content: Oh , OK . Yeah . I mean , it 's {disfmarker} um , certainly in a short {disfmarker} short - term this just sounds easier .&#10;Speaker: Grad D&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Yeah . I mean , longer - term if it 's {disfmarker} {vocalsound} if it turns out to be useful , one {disfmarker} one might want to do something else ,&#10;Speaker: Grad D&#10;Content: Right . That 's true .&#10;Speaker: Professor B&#10;Content: but {disfmarker} Uh , uh , I mean , in {disfmarker} in other words , you {disfmarker} you may be putting other kinds of errors in {pause} from the re - synthesis process .&#10;Speaker: Grad D&#10;Content: But {disfmarker} e u From the re - synthesis ? Um ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad D&#10;Content: O - OK . I don't know anything about re - synthesis . Uh ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." target=" Professor B&#10;Content: Yeah .&#10;Speaker: Grad D&#10;Content: O - OK . I don't know anything about re - synthesis . Uh , how likely do you think that is ?&#10;Speaker: Professor B&#10;Content: Uh , it depends what you {disfmarker} what you do . I mean , it 's {disfmarker} it 's {disfmarker} it 's , uh , um {disfmarker} Don't know . But anyway it sounds like a reasonable way to go for a {disfmarker} for an initial thing , and we can look at {disfmarker} {vocalsound} at exactly what you end up doing and {disfmarker} and then figure out if there 's some {disfmarker} {vocalsound} something that could be {disfmarker} be hurt by the end part of the process .&#10;Speaker: Grad D&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: OK . So that 's {disfmarker} That was it , huh ?&#10;Speaker: Grad D&#10;Content: That {disfmarker} Yeah , e That 's">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." target=" seven iterations of re - estimation in each of those three . And so , you know , that 's part of what takes so long to train the {disfmarker} the {disfmarker} the back - end for this .&#10;Speaker: Professor B&#10;Content: I 'm sorry , I didn't quite get that . There 's {disfmarker} there 's four and there 's seven and {disfmarker} I {disfmarker} I 'm sorry .&#10;Speaker: PhD C&#10;Content: Yeah . Uh , maybe I should write it on the board . So , {vocalsound} there 's four rounds of training . Um , I g I g I guess you could say iterations . The first one is three , then seven , seven , and seven . And what these numbers refer to is the number of times that the , uh , HMM re - estimation is run . It 's this program called H E&#10;Speaker: Professor B&#10;Content: But in HTK , what 's the difference between , uh , a {disfmarker} an inner loop and an outer loop in these iterations ?&#10;Speaker: PhD C&#10;Content: OK .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." target="er} I realized to use it I 'd need to have these short analysis frames get plugged directly into the feature computation somehow&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Grad D&#10;Content: and right now I think our feature computation is set to up to , um , {vocalsound} take , um , audio as input , in general . So I decided that I {disfmarker} I 'll do the reverberation removal on the long analysis windows and then just re - synthesize audio and then send that .&#10;Speaker: Professor B&#10;Content: This is in order to use the SRI system or something . Right ?&#10;Speaker: Grad D&#10;Content: Um , or {disfmarker} or even if I 'm using our system , I was thinking it might be easier to just re - synthesize the audio ,&#10;Speaker: Professor B&#10;Content: Yeah ?&#10;Speaker: Grad D&#10;Content: because then I could just feacalc as is and I wouldn't have to change the code .&#10;Speaker: Professor B&#10;Content: Oh , OK . Yeah . I mean , it 's {disfmarker} um ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." target="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." target="1. Benefits of smooth representation for generalization: A smooth representation generally leads to better generalization because it reduces overfitting by averaging out fine details and noise in the data. This can help improve the model's performance on unseen data, as the model is less likely to learn spurious patterns that only exist in the training set.&#10;&#10;2. Potential drawbacks of losing details during smoothing: Smoothing inevitably leads to loss of information, which might include valuable details about the original signal. This could potentially make the model miss important features or characteristics of the data, leading to reduced accuracy or suboptimal performance in certain cases. In speech recognition, for example, excessive smoothing might cause the loss of crucial information related to phoneme distinction or speaker identity.&#10;&#10;In summary, while smoothing offers benefits such as improved generalization and reduced overfitting, it can also result in the loss of important details that may be necessary for accurate analysis or modeling. Balancing these trade-offs is essential for developing effective speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." target="1. The voiced-unvoiced (VU) detector's purpose is to determine if a given segment of speech is voiced or unvoiced. Voiced sounds are produced when the vocal cords vibrate, such as vowel sounds, whereas unvoiced sounds result from airflow through the mouth without vocal cord vibration, like consonant sounds (e.g., 'f' or 's').&#10;2. In this conversation, the VU detector is intended to be used as a feature extractor for speech processing tasks, not part of a typical voiced-unvoiced detection system. The idea is that it could be incorporated into either a separate neural network designed specifically for VU detection or combined with a larger neural network responsible for phoneme classification.&#10;3. Additionally, there is discussion about using the VU detector as part of an improved front-end processing system to enhance pitch detection, which is crucial for better speech processing performance since Mel Frequency Cepstral Coefficients (MFCCs) do not include any pitch-related information. By implementing a good pitch detection algorithm, they aim to improve voice activity detection and other speech processing tasks' accuracy and robustness.&#10;&#10;In summary, the voiced-unvoiced detector is intended for use as a feature extractor in speech processing tasks and potentially improving pitch detection algorithms within front-end processing systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." target="1. Advanced processing techniques: If re-synthesis turns out to be useful in the long term, more advanced or customized processing techniques can be applied to the signal. (Potential long-term benefits of using re-synthesis)&#10;2. Robustness: Even with the sacrifice of a smooth spectral envelope, combining probabilities of individual features within each mixture might lead to more robust results, as suggested by Professor B and PhD A when discussing the comparison between raw data and smooth versions of the spectrum.&#10;3. Exploration of statistical measures: By analyzing the difference between raw data and the smooth version, new statistical measures can be explored and added to feature vectors to improve performance in noisy environments. This approach is mentioned by PhD as a way of finding something that might have been missed in the smoothed spectrum but could contribute positively to the final result.&#10;4. Potential improvements through spectral subtraction and LDA filtering: As suggested by Professor B, combining probabilities within each mixture along with spectral subtraction and LDA filtering might yield better results than simpler approaches, as mentioned in a previous meeting. However, this depends on the specific implementation of spectral subtraction and its compatibility with online normalization.&#10;5. Addressing musical tones issue: Although not explicitly stated, combining probabilities within each mixture might help address issues related to musical tones raised by PhD A in a previous meeting. However, further investigation is needed to confirm this potential advantage.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." target="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." target="1. Length and number of periods: A short frame contains only two periods of the signal, while the original frame contains more depending on the signal's characteristics. This difference is due to the time-frequency trade-off, which requires shorter frames for better frequency resolution but longer frames for better time resolution.&#10;&#10;2. FFT size (number of points in the Fast Fourier Transform): The speakers also discuss differences in FFT size between the short frame and the original frame. A larger FFT size results in better frequency resolution but coarser time resolution, whereas a smaller FFT size has the opposite effect.&#10;&#10;3. Robustness to noise: The speakers mention that using a shorter frame can help improve robustness to noise in signal processing applications like speech analysis. When comparing the same frame from noisy and clean utterances, the difference can be used to better understand and handle noise.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." target="The results of implementing VAD, noise compensation (specifically spectral subtraction), and normalization (on-line normalization) techniques on the baseline experiments from OGI since last week showed improved performance when using these additional features. The comparison was made between two groups of proposals, one with Voice Activity Detection (VAD) and the other without. It was found that the group with VAD performed better than the group without VAD. This demonstrates that incorporating these techniques can lead to improved robustness, fairness, understanding of strengths and weaknesses, better generalization, and more effective handling of complex scenarios or noisy environments in VAD systems. However, it was noted that online normalization did not provide further improvements when spectral subtraction was already in use. The relationship between these findings and a previous issue related to musical tones was unclear due to differences in the spectral subtraction approaches used.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential long-term benefits of using re-synthesis:&#10;   - If re-synthesis turns out to be useful in the long term, it might allow for more advanced or customized processing techniques to be applied to the signal." target="1. The medium-mismatch condition refers to a situation where the model is trained on data collected in low noise conditions and at low speeds (e.g., stopped car), but during testing, the data is gathered using a distant microphone in high-speed conditions (e.g., on a highway). This creates a mismatch between the training and testing conditions, making it more challenging for the model to maintain good performance.&#10;&#10;Based on the discussion provided, there are two main aspects being considered:&#10;&#10;- The use of robust features that perform well even with gentle degradation in testing conditions different from training.&#10;- A comparison of original signals to smoothed versions, focusing on low frequencies and exploring statistical measures for improving performance in noisy environments.&#10;&#10;The medium-mismatch condition is introduced as an example of a realistic scenario that requires the model to be robust against changes in car conditions, specifically microphone placement (near vs. far) and speed (low vs. high). This is relevant when discussing potential improvements for front-end processing, such as adding a pitch detector that better handles these differences between training and testing conditions.&#10;&#10;In summary, the medium-mismatch condition involves training on low noisy conditions with one microphone placement but testing with a far microphone in higher speed conditions to assess how well the model can handle real-world variability.">
      <data key="d0">1</data>
    </edge>
    <edge source=" seven iterations of re - estimation in each of those three . And so , you know , that 's part of what takes so long to train the {disfmarker} the {disfmarker} the back - end for this .&#10;Speaker: Professor B&#10;Content: I 'm sorry , I didn't quite get that . There 's {disfmarker} there 's four and there 's seven and {disfmarker} I {disfmarker} I 'm sorry .&#10;Speaker: PhD C&#10;Content: Yeah . Uh , maybe I should write it on the board . So , {vocalsound} there 's four rounds of training . Um , I g I g I guess you could say iterations . The first one is three , then seven , seven , and seven . And what these numbers refer to is the number of times that the , uh , HMM re - estimation is run . It 's this program called H E&#10;Speaker: Professor B&#10;Content: But in HTK , what 's the difference between , uh , a {disfmarker} an inner loop and an outer loop in these iterations ?&#10;Speaker: PhD C&#10;Content: OK ." target="In the meeting transcript, PhD C reported on their experiments related to the HTK (Hidden Markov Model Toolkit) back-end models. They focused on the number of training iterations in different rounds of the model training process. Specifically, there are four rounds of training, and the number of iterations for each round is as follows:&#10;&#10;1. First round: 3 iterations&#10;2. Second to fourth rounds: 7 iterations for each round&#10;&#10;These iterations refer to the number of times the HMM (Hidden Markov Model) re-estimation is run during each training round. The higher number of iterations in later rounds indicates that more computational resources are devoted to refining the models in those stages.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements." target=" what they were doing wasn't so bad at all .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: But , um .&#10;Speaker: PhD C&#10;Content: Yeah . It seems like you should try to make your baseline as good as possible . And if it turns out that {pause} you can't improve on that , well , I mean , then , you know , nobody wins and you just use MFCC . Right ?&#10;Speaker: Professor B&#10;Content: Yeah . I mean , it seems like , uh , it should include sort of the current state of the art {vocalsound} that you want {disfmarker} are trying to improve , and MFCC 's , you know , or PLP or something {disfmarker} it seems like {vocalsound} reasonable baseline for the features , and anybody doing this task , {vocalsound} uh , is gonna have some sort of voice activity detection at some level , in some way . They might use the whole recognizer to do it {vocalsound} but {disfmarker} rather than {vocalsound} a separate thing , but">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements." target=" suppose you 've {disfmarker} that {vocalsound} what you really wanna do is put a good pitch detector on there and if it gets an unambiguous {disfmarker}&#10;Speaker: PhD C&#10;Content: Oh , oh . I see .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: if it gets an unambiguous result then you 're definitely in a {disfmarker} in a {disfmarker} in a voice in a , uh , s region with speech . Uh .&#10;Speaker: PhD C&#10;Content: So there 's this assumption that the v the voice activity detector can only use the MFCC ?&#10;Speaker: PhD A&#10;Content: That 's not clear , but this {disfmarker} {vocalsound} e&#10;Speaker: Professor B&#10;Content: Well , for the baseline .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So {disfmarker} so if you use other features then y But it 's just a question of what is your baseline . Right ? What is it that you 're supposed">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements." target=" , just after this . So I think spectral subtraction , LDA filtering , and on - line normalization , so which is similar to {vocalsound} the pro proposal - one , but with {pause} spectral subtraction in addition , and it seems that on - line normalization doesn't help further when you have spectral subtraction .&#10;Speaker: PhD C&#10;Content: Is this related to the issue that you brought up a couple of meetings ago with the {disfmarker} the {vocalsound} musical tones&#10;Speaker: PhD A&#10;Content: I {disfmarker}&#10;Speaker: PhD C&#10;Content: and {disfmarker} ?&#10;Speaker: PhD A&#10;Content: I have no idea , because the issue I brought up was with a very simple spectral subtraction approach ,&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: and the one that {vocalsound} they use at OGI is one from {disfmarker} from {vocalsound} the proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements." target=" . So you really need to put more {disfmarker} more in the {disfmarker} in {disfmarker} in the front - end .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So i&#10;Speaker: Professor B&#10;Content: Um ,&#10;Speaker: PhD A&#10;Content: S&#10;Speaker: Professor B&#10;Content: sure . But i bu&#10;Speaker: PhD C&#10;Content: Wait a minute . I {disfmarker} I 'm confused .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Wha - what do you mean ?&#10;Speaker: PhD A&#10;Content: Yeah , if i&#10;Speaker: Professor B&#10;Content: So y so you m s Yeah , but {disfmarker} Well , let 's say for ins see , MFCC for instance doesn't have anything in it , uh , related to the pitch . So just {disfmarker} just for example . So suppose you 've {disfmarker} that {vocalsound} what you really wanna do is put a good pitch detector on there and if it">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements." target=" anything {vocalsound} ac {vocalsound} make any {comment} policy change because {vocalsound} {vocalsound} everybody has {disfmarker} has , uh , their own opinion&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: and {disfmarker} I don't know .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh , so {disfmarker} Yeah . Yeah , but there is probably a {disfmarker} a big change that will {vocalsound} be made is that the {disfmarker} the baseline {disfmarker} th they want to have a new baseline , perhaps , which is , um , MFCC but with {vocalsound} a voice activity detector . And apparently , {vocalsound} uh , some people are pushing to still keep this fifty percent number . So they want {vocalsound} to have at least fifty percent improvement on the baseline , but w which would be a much better baseline .&#10;Speaker: Professor B&#10;Content: M">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements." target="1. Benefits of smooth representation for generalization: A smooth representation generally leads to better generalization because it reduces overfitting by averaging out fine details and noise in the data. This can help improve the model's performance on unseen data, as the model is less likely to learn spurious patterns that only exist in the training set.&#10;&#10;2. Potential drawbacks of losing details during smoothing: Smoothing inevitably leads to loss of information, which might include valuable details about the original signal. This could potentially make the model miss important features or characteristics of the data, leading to reduced accuracy or suboptimal performance in certain cases. In speech recognition, for example, excessive smoothing might cause the loss of crucial information related to phoneme distinction or speaker identity.&#10;&#10;In summary, while smoothing offers benefits such as improved generalization and reduced overfitting, it can also result in the loss of important details that may be necessary for accurate analysis or modeling. Balancing these trade-offs is essential for developing effective speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements." target="1. The voiced-unvoiced (VU) detector's purpose is to determine if a given segment of speech is voiced or unvoiced. Voiced sounds are produced when the vocal cords vibrate, such as vowel sounds, whereas unvoiced sounds result from airflow through the mouth without vocal cord vibration, like consonant sounds (e.g., 'f' or 's').&#10;2. In this conversation, the VU detector is intended to be used as a feature extractor for speech processing tasks, not part of a typical voiced-unvoiced detection system. The idea is that it could be incorporated into either a separate neural network designed specifically for VU detection or combined with a larger neural network responsible for phoneme classification.&#10;3. Additionally, there is discussion about using the VU detector as part of an improved front-end processing system to enhance pitch detection, which is crucial for better speech processing performance since Mel Frequency Cepstral Coefficients (MFCCs) do not include any pitch-related information. By implementing a good pitch detection algorithm, they aim to improve voice activity detection and other speech processing tasks' accuracy and robustness.&#10;&#10;In summary, the voiced-unvoiced detector is intended for use as a feature extractor in speech processing tasks and potentially improving pitch detection algorithms within front-end processing systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements." target="1. Choosing a weak baseline: If the baseline is not robust or representative of the current state-of-the-art, comparing improvements against it might not provide accurate insights into the performance of new methods. This could lead to underestimating the true potential of novel approaches.&#10;&#10;2. Limited comparative evaluation: Reducing efforts in feature design may result in fewer comparisons with alternative features or methods, which could be crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;3. Incomplete VAD design: A simpler feature design might not capture all aspects required for a good Voice Activity Detection (VAD) system. Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments.&#10;&#10;4. Relying solely on the recognizer for VAD: While it is possible to use the whole recognizer to perform VAD, this approach may not always provide optimal results compared to dedicated VAD systems tailored to specific applications and signal types.&#10;&#10;5. Fairness concerns: It might be considered unfair to compare a simplified feature design to more complex approaches if additional baseline features are required to create a good VAD system. This could lead to an inaccurate assessment of the performance of novel methods, potentially favoring simpler techniques that do not fully address the problem at hand.&#10;&#10;To ensure fairness and accurate comparisons, it is essential to:&#10;- Choose a robust and representative baseline.&#10;- Perform thorough evaluations against various alternatives.&#10;- Consider all aspects required for a good VAD system, including complexity and real-time requirements.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements." target="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements." target="Based on the given transcript, when transitioning from a well-matched case to a mismatched case in terms of speaker features, there is a noticeable difference in performance. Specifically, the system performs better in the well-matched case but seems to degrade slightly for mismatched and highly-mismatched conditions when a neural network is implemented. This implies that the current weighting setup may favor the well-matched case, which might be an overestimation of its true performance in real-world scenarios where data is often more diverse and less controlled.&#10;&#10;As for the impact on vocal sound design and feature selection, it is essential to consider various techniques and features beyond simple baselines or MFCC (Mel Frequency Cepstral Coefficients) due to their limited pitch-related information. Including additional features can improve robustness, fairness, understanding of strengths and weaknesses, better generalization, and overall performance in complex scenarios or noisy environments.&#10;&#10;To address the issue of varying performance across different cases, one should investigate various pitch detection algorithms suitable for the specific application and signal type. After integrating the chosen algorithm into the existing front-end processing system, evaluate its performance by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates. Fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements. Additionally, when selecting features for vocal sound design, consider a comprehensive comparison involving various techniques and baselines to ensure robustness and fairness in performance evaluation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To improve the front-end processing and add a good pitch detector, since MFCC does not include any pitch-related information, consider implementing the following steps:&#10;2. Investigate different pitch detection algorithms that suit the specific application and signal type. Some popular pitch detection methods include YIN, SWIPE, and MCW.&#10;3. Integrate the chosen pitch detection algorithm into the existing front-end processing system.&#10;4. Evaluate the performance of the updated system by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates.&#10;5. Ensure that the addition of a pitch detector does not significantly increase computational complexity or latency, as this may impact real-time applications.&#10;6. Continuously fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements." target="1. The medium-mismatch condition refers to a situation where the model is trained on data collected in low noise conditions and at low speeds (e.g., stopped car), but during testing, the data is gathered using a distant microphone in high-speed conditions (e.g., on a highway). This creates a mismatch between the training and testing conditions, making it more challenging for the model to maintain good performance.&#10;&#10;Based on the discussion provided, there are two main aspects being considered:&#10;&#10;- The use of robust features that perform well even with gentle degradation in testing conditions different from training.&#10;- A comparison of original signals to smoothed versions, focusing on low frequencies and exploring statistical measures for improving performance in noisy environments.&#10;&#10;The medium-mismatch condition is introduced as an example of a realistic scenario that requires the model to be robust against changes in car conditions, specifically microphone placement (near vs. far) and speed (low vs. high). This is relevant when discussing potential improvements for front-end processing, such as adding a pitch detector that better handles these differences between training and testing conditions.&#10;&#10;In summary, the medium-mismatch condition involves training on low noisy conditions with one microphone placement but testing with a far microphone in higher speed conditions to assess how well the model can handle real-world variability.">
      <data key="d0">1</data>
    </edge>
    <edge source=" what they were doing wasn't so bad at all .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: But , um .&#10;Speaker: PhD C&#10;Content: Yeah . It seems like you should try to make your baseline as good as possible . And if it turns out that {pause} you can't improve on that , well , I mean , then , you know , nobody wins and you just use MFCC . Right ?&#10;Speaker: Professor B&#10;Content: Yeah . I mean , it seems like , uh , it should include sort of the current state of the art {vocalsound} that you want {disfmarker} are trying to improve , and MFCC 's , you know , or PLP or something {disfmarker} it seems like {vocalsound} reasonable baseline for the features , and anybody doing this task , {vocalsound} uh , is gonna have some sort of voice activity detection at some level , in some way . They might use the whole recognizer to do it {vocalsound} but {disfmarker} rather than {vocalsound} a separate thing , but" target="1. Choosing a weak baseline: If the baseline is not robust or representative of the current state-of-the-art, comparing improvements against it might not provide accurate insights into the performance of new methods. This could lead to underestimating the true potential of novel approaches.&#10;&#10;2. Limited comparative evaluation: Reducing efforts in feature design may result in fewer comparisons with alternative features or methods, which could be crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;3. Incomplete VAD design: A simpler feature design might not capture all aspects required for a good Voice Activity Detection (VAD) system. Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments.&#10;&#10;4. Relying solely on the recognizer for VAD: While it is possible to use the whole recognizer to perform VAD, this approach may not always provide optimal results compared to dedicated VAD systems tailored to specific applications and signal types.&#10;&#10;5. Fairness concerns: It might be considered unfair to compare a simplified feature design to more complex approaches if additional baseline features are required to create a good VAD system. This could lead to an inaccurate assessment of the performance of novel methods, potentially favoring simpler techniques that do not fully address the problem at hand.&#10;&#10;To ensure fairness and accurate comparisons, it is essential to:&#10;- Choose a robust and representative baseline.&#10;- Perform thorough evaluations against various alternatives.&#10;- Consider all aspects required for a good VAD system, including complexity and real-time requirements.">
      <data key="d0">1</data>
    </edge>
    <edge source=" suppose you 've {disfmarker} that {vocalsound} what you really wanna do is put a good pitch detector on there and if it gets an unambiguous {disfmarker}&#10;Speaker: PhD C&#10;Content: Oh , oh . I see .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: if it gets an unambiguous result then you 're definitely in a {disfmarker} in a {disfmarker} in a voice in a , uh , s region with speech . Uh .&#10;Speaker: PhD C&#10;Content: So there 's this assumption that the v the voice activity detector can only use the MFCC ?&#10;Speaker: PhD A&#10;Content: That 's not clear , but this {disfmarker} {vocalsound} e&#10;Speaker: Professor B&#10;Content: Well , for the baseline .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So {disfmarker} so if you use other features then y But it 's just a question of what is your baseline . Right ? What is it that you 're supposed" target="1. The voiced-unvoiced (VU) detector's purpose is to determine if a given segment of speech is voiced or unvoiced. Voiced sounds are produced when the vocal cords vibrate, such as vowel sounds, whereas unvoiced sounds result from airflow through the mouth without vocal cord vibration, like consonant sounds (e.g., 'f' or 's').&#10;2. In this conversation, the VU detector is intended to be used as a feature extractor for speech processing tasks, not part of a typical voiced-unvoiced detection system. The idea is that it could be incorporated into either a separate neural network designed specifically for VU detection or combined with a larger neural network responsible for phoneme classification.&#10;3. Additionally, there is discussion about using the VU detector as part of an improved front-end processing system to enhance pitch detection, which is crucial for better speech processing performance since Mel Frequency Cepstral Coefficients (MFCCs) do not include any pitch-related information. By implementing a good pitch detection algorithm, they aim to improve voice activity detection and other speech processing tasks' accuracy and robustness.&#10;&#10;In summary, the voiced-unvoiced detector is intended for use as a feature extractor in speech processing tasks and potentially improving pitch detection algorithms within front-end processing systems.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , just after this . So I think spectral subtraction , LDA filtering , and on - line normalization , so which is similar to {vocalsound} the pro proposal - one , but with {pause} spectral subtraction in addition , and it seems that on - line normalization doesn't help further when you have spectral subtraction .&#10;Speaker: PhD C&#10;Content: Is this related to the issue that you brought up a couple of meetings ago with the {disfmarker} the {vocalsound} musical tones&#10;Speaker: PhD A&#10;Content: I {disfmarker}&#10;Speaker: PhD C&#10;Content: and {disfmarker} ?&#10;Speaker: PhD A&#10;Content: I have no idea , because the issue I brought up was with a very simple spectral subtraction approach ,&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: and the one that {vocalsound} they use at OGI is one from {disfmarker} from {vocalsound} the proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much" target="The frequency range used for computing the variance in the example being discussed by the speakers is from 200 Hz to 1500 Hz. One of the speakers later corrects this range to 250 thousand Hz, but it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , just after this . So I think spectral subtraction , LDA filtering , and on - line normalization , so which is similar to {vocalsound} the pro proposal - one , but with {pause} spectral subtraction in addition , and it seems that on - line normalization doesn't help further when you have spectral subtraction .&#10;Speaker: PhD C&#10;Content: Is this related to the issue that you brought up a couple of meetings ago with the {disfmarker} the {vocalsound} musical tones&#10;Speaker: PhD A&#10;Content: I {disfmarker}&#10;Speaker: PhD C&#10;Content: and {disfmarker} ?&#10;Speaker: PhD A&#10;Content: I have no idea , because the issue I brought up was with a very simple spectral subtraction approach ,&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: and the one that {vocalsound} they use at OGI is one from {disfmarker} from {vocalsound} the proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much" target="1. Advanced processing techniques: If re-synthesis turns out to be useful in the long term, more advanced or customized processing techniques can be applied to the signal. (Potential long-term benefits of using re-synthesis)&#10;2. Robustness: Even with the sacrifice of a smooth spectral envelope, combining probabilities of individual features within each mixture might lead to more robust results, as suggested by Professor B and PhD A when discussing the comparison between raw data and smooth versions of the spectrum.&#10;3. Exploration of statistical measures: By analyzing the difference between raw data and the smooth version, new statistical measures can be explored and added to feature vectors to improve performance in noisy environments. This approach is mentioned by PhD as a way of finding something that might have been missed in the smoothed spectrum but could contribute positively to the final result.&#10;4. Potential improvements through spectral subtraction and LDA filtering: As suggested by Professor B, combining probabilities within each mixture along with spectral subtraction and LDA filtering might yield better results than simpler approaches, as mentioned in a previous meeting. However, this depends on the specific implementation of spectral subtraction and its compatibility with online normalization.&#10;5. Addressing musical tones issue: Although not explicitly stated, combining probabilities within each mixture might help address issues related to musical tones raised by PhD A in a previous meeting. However, further investigation is needed to confirm this potential advantage.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , just after this . So I think spectral subtraction , LDA filtering , and on - line normalization , so which is similar to {vocalsound} the pro proposal - one , but with {pause} spectral subtraction in addition , and it seems that on - line normalization doesn't help further when you have spectral subtraction .&#10;Speaker: PhD C&#10;Content: Is this related to the issue that you brought up a couple of meetings ago with the {disfmarker} the {vocalsound} musical tones&#10;Speaker: PhD A&#10;Content: I {disfmarker}&#10;Speaker: PhD C&#10;Content: and {disfmarker} ?&#10;Speaker: PhD A&#10;Content: I have no idea , because the issue I brought up was with a very simple spectral subtraction approach ,&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: and the one that {vocalsound} they use at OGI is one from {disfmarker} from {vocalsound} the proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much" target="The results of implementing VAD, noise compensation (specifically spectral subtraction), and normalization (on-line normalization) techniques on the baseline experiments from OGI since last week showed improved performance when using these additional features. The comparison was made between two groups of proposals, one with Voice Activity Detection (VAD) and the other without. It was found that the group with VAD performed better than the group without VAD. This demonstrates that incorporating these techniques can lead to improved robustness, fairness, understanding of strengths and weaknesses, better generalization, and more effective handling of complex scenarios or noisy environments in VAD systems. However, it was noted that online normalization did not provide further improvements when spectral subtraction was already in use. The relationship between these findings and a previous issue related to musical tones was unclear due to differences in the spectral subtraction approaches used.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , just after this . So I think spectral subtraction , LDA filtering , and on - line normalization , so which is similar to {vocalsound} the pro proposal - one , but with {pause} spectral subtraction in addition , and it seems that on - line normalization doesn't help further when you have spectral subtraction .&#10;Speaker: PhD C&#10;Content: Is this related to the issue that you brought up a couple of meetings ago with the {disfmarker} the {vocalsound} musical tones&#10;Speaker: PhD A&#10;Content: I {disfmarker}&#10;Speaker: PhD C&#10;Content: and {disfmarker} ?&#10;Speaker: PhD A&#10;Content: I have no idea , because the issue I brought up was with a very simple spectral subtraction approach ,&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: and the one that {vocalsound} they use at OGI is one from {disfmarker} from {vocalsound} the proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much" target="Based on the transcript, there is no explicit information given about whether Hynek or Sunil from OGI (Oregon Graduate Institute) were part of the Aurora conference call. Speaker PhD A mentioned not knowing who was involved from OGI, and the discussion moved on to other topics. Therefore, it cannot be determined from this transcript if Hynek or Sunil were present in the Aurora conference call.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , just after this . So I think spectral subtraction , LDA filtering , and on - line normalization , so which is similar to {vocalsound} the pro proposal - one , but with {pause} spectral subtraction in addition , and it seems that on - line normalization doesn't help further when you have spectral subtraction .&#10;Speaker: PhD C&#10;Content: Is this related to the issue that you brought up a couple of meetings ago with the {disfmarker} the {vocalsound} musical tones&#10;Speaker: PhD A&#10;Content: I {disfmarker}&#10;Speaker: PhD C&#10;Content: and {disfmarker} ?&#10;Speaker: PhD A&#10;Content: I have no idea , because the issue I brought up was with a very simple spectral subtraction approach ,&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: and the one that {vocalsound} they use at OGI is one from {disfmarker} from {vocalsound} the proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much" target="Based on the transcript provided, the exact date in June for the next Aurora deadline was not specified during the conversation. Speaker PhD A knew that it was in June but did not have the exact date. Speaker Professor B asked about early, late, or middle June, but no specific date was given. Additionally, none of the participants provided information about who could provide this information.">
      <data key="d0">1</data>
    </edge>
    <edge source="The frequency range used for computing the variance in the example being discussed by the speakers is from 200 Hz to 1500 Hz. One of the speakers later corrects this range to 250 thousand Hz, but it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." target=" . I have here one example if you {disfmarker} if you want see something like that .&#10;Speaker: PhD A&#10;Content: Then we compute the difference .&#10;Speaker: PhD C&#10;Content: and ,&#10;Speaker: PhD A&#10;Content: Yeah . Uh - huh .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD C&#10;Content: uh , sum the differences ?&#10;Speaker: PhD A&#10;Content: So . And I think the variance is computed only from , like , two hundred hertz to {pause} one {disfmarker} to fifteen hundred .&#10;Speaker: PhD C&#10;Content: Oh ! OK .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Two thou two {disfmarker} {comment} fifteen hundred ?&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Because {disfmarker}&#10;Speaker: PhD F&#10;Content: No .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Two hundred and fifty thousand .&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="The frequency range used for computing the variance in the example being discussed by the speakers is from 200 Hz to 1500 Hz. One of the speakers later corrects this range to 250 thousand Hz, but it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." target="} we draw its parameters across time for a clean sentence and then nois the same noisy sentence , it 's very close .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Yeah . So there are {disfmarker} there is this . There could be also the , um {disfmarker} {vocalsound} something like the maximum of the auto - correlation function or {disfmarker} which {disfmarker}&#10;Speaker: PhD C&#10;Content: Is this a {disfmarker} a s a trained system ? Or is it a system where you just pick some thresholds ? Ho - how does it work ?&#10;Speaker: PhD A&#10;Content: Right now we just are trying to find some features . And ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: uh {disfmarker} Yeah . Hopefully , I think what we want to have is to put these features in s some kind of , um {disfmarker} well , to {disfmarker} to obtain a statistical model on these features and to {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="The frequency range used for computing the variance in the example being discussed by the speakers is from 200 Hz to 1500 Hz. One of the speakers later corrects this range to 250 thousand Hz, but it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." target=" - hmm .&#10;Speaker: PhD F&#10;Content: and we extend these coefficient between the {disfmarker} all the frequency range .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: And i the interpolation i between the point {vocalsound} is {disfmarker} give for the triang triangular filter , the value of the triangular filter and of this way we obtained this mode this model speech .&#10;Speaker: PhD A&#10;Content: S&#10;Speaker: Professor B&#10;Content: So you essentially take the values that {disfmarker} th that you get from the triangular filter and extend them to sor sort of like a rectangle , that 's at that m value .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Yeah . I think we have linear interpolation .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So we have {disfmarker} we have one point for {disfmarker} one energy for each filter bank ,&#10;Speaker: PhD F&#10;Content: mmm Yeah , it">
      <data key="d0">1</data>
    </edge>
    <edge source="The frequency range used for computing the variance in the example being discussed by the speakers is from 200 Hz to 1500 Hz. One of the speakers later corrects this range to 250 thousand Hz, but it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." target="The frequency range used for computing the variance in the example discussed by the speakers is from 200 Hz to 1500 Hz. This is stated explicitly by speaker A, and although professor F mistakenly gives a higher frequency range, it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended.">
      <data key="d0">1</data>
    </edge>
    <edge source="The frequency range used for computing the variance in the example being discussed by the speakers is from 200 Hz to 1500 Hz. One of the speakers later corrects this range to 250 thousand Hz, but it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." target="1. The process for drawing a system's parameters across time for a clean sentence and a noisy sentence involves analyzing the characteristics of speech signals in both scenarios. This likely includes examining various attributes such as frequency content, amplitude, and duration of sounds within the sentences. By comparing these parameters between clean and noisy sentences, researchers can gain insights into how noise affects speech perception and develop methods to mitigate its impact.&#10;&#10;2. Features are being used in an attempt to create a statistical model for this process by extracting relevant characteristics from the speech signals that highlight differences between clean and noisy sentences. In this context, features might include measures related to voiced vs. unvoiced sounds, pitch detection, or other spectral properties. The goal is to build a statistical model using these features that can accurately differentiate between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;The discussion revolves around finding suitable features for this task, such as the maximum of the auto-correlation function, which could help characterize voiced phonemes more effectively. The team aims to use these features to create a statistical model that captures the differences between clean and noisy sentences, ultimately leading to better speech recognition performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="The frequency range used for computing the variance in the example being discussed by the speakers is from 200 Hz to 1500 Hz. One of the speakers later corrects this range to 250 thousand Hz, but it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." target="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments.">
      <data key="d0">1</data>
    </edge>
    <edge source="The frequency range used for computing the variance in the example being discussed by the speakers is from 200 Hz to 1500 Hz. One of the speakers later corrects this range to 250 thousand Hz, but it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." target="The speakers, who appear to be discussing the evaluation and improvement of a signal processing system, are expressing uncertainty about the final application of the system and whether it will need to handle more difficult cases with high accuracy. Professor B mentions that if the predominate use of the system is in challenging scenarios, current methods may not be sufficient. However, in better cases, the system could potentially work well with some error recovery.&#10;&#10;Later in the discussion, the speakers consider a variance calculation done on a frequency range of 200 Hz to 1500 Hz, which they believe is more logical for their analysis. They also discuss different methods for weighting error rates obtained from various languages and conditions, with some arguing that it would be better to combine error rates before computing improvements.&#10;&#10;In summary, the speakers are discussing the uncertainty of the system's primary use, the potential for reasonable error recovery in better cases, and different approaches to weighing error rates for improved performance in various scenarios.">
      <data key="d0">1</data>
    </edge>
    <edge source=" . I have here one example if you {disfmarker} if you want see something like that .&#10;Speaker: PhD A&#10;Content: Then we compute the difference .&#10;Speaker: PhD C&#10;Content: and ,&#10;Speaker: PhD A&#10;Content: Yeah . Uh - huh .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD C&#10;Content: uh , sum the differences ?&#10;Speaker: PhD A&#10;Content: So . And I think the variance is computed only from , like , two hundred hertz to {pause} one {disfmarker} to fifteen hundred .&#10;Speaker: PhD C&#10;Content: Oh ! OK .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Two thou two {disfmarker} {comment} fifteen hundred ?&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Because {disfmarker}&#10;Speaker: PhD F&#10;Content: No .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Two hundred and fifty thousand .&#10;Speaker: PhD" target="The frequency range used for computing the variance in the example discussed by the speakers is from 200 Hz to 1500 Hz. This is stated explicitly by speaker A, and although professor F mistakenly gives a higher frequency range, it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended.">
      <data key="d0">1</data>
    </edge>
    <edge source="} we draw its parameters across time for a clean sentence and then nois the same noisy sentence , it 's very close .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Yeah . So there are {disfmarker} there is this . There could be also the , um {disfmarker} {vocalsound} something like the maximum of the auto - correlation function or {disfmarker} which {disfmarker}&#10;Speaker: PhD C&#10;Content: Is this a {disfmarker} a s a trained system ? Or is it a system where you just pick some thresholds ? Ho - how does it work ?&#10;Speaker: PhD A&#10;Content: Right now we just are trying to find some features . And ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: uh {disfmarker} Yeah . Hopefully , I think what we want to have is to put these features in s some kind of , um {disfmarker} well , to {disfmarker} to obtain a statistical model on these features and to {disfmarker" target="1. Benefits of smooth representation for generalization: A smooth representation generally leads to better generalization because it reduces overfitting by averaging out fine details and noise in the data. This can help improve the model's performance on unseen data, as the model is less likely to learn spurious patterns that only exist in the training set.&#10;&#10;2. Potential drawbacks of losing details during smoothing: Smoothing inevitably leads to loss of information, which might include valuable details about the original signal. This could potentially make the model miss important features or characteristics of the data, leading to reduced accuracy or suboptimal performance in certain cases. In speech recognition, for example, excessive smoothing might cause the loss of crucial information related to phoneme distinction or speaker identity.&#10;&#10;In summary, while smoothing offers benefits such as improved generalization and reduced overfitting, it can also result in the loss of important details that may be necessary for accurate analysis or modeling. Balancing these trade-offs is essential for developing effective speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="} we draw its parameters across time for a clean sentence and then nois the same noisy sentence , it 's very close .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Yeah . So there are {disfmarker} there is this . There could be also the , um {disfmarker} {vocalsound} something like the maximum of the auto - correlation function or {disfmarker} which {disfmarker}&#10;Speaker: PhD C&#10;Content: Is this a {disfmarker} a s a trained system ? Or is it a system where you just pick some thresholds ? Ho - how does it work ?&#10;Speaker: PhD A&#10;Content: Right now we just are trying to find some features . And ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: uh {disfmarker} Yeah . Hopefully , I think what we want to have is to put these features in s some kind of , um {disfmarker} well , to {disfmarker} to obtain a statistical model on these features and to {disfmarker" target="The frequency range used for computing the variance in the example discussed by the speakers is from 200 Hz to 1500 Hz. This is stated explicitly by speaker A, and although professor F mistakenly gives a higher frequency range, it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended.">
      <data key="d0">1</data>
    </edge>
    <edge source="} we draw its parameters across time for a clean sentence and then nois the same noisy sentence , it 's very close .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Yeah . So there are {disfmarker} there is this . There could be also the , um {disfmarker} {vocalsound} something like the maximum of the auto - correlation function or {disfmarker} which {disfmarker}&#10;Speaker: PhD C&#10;Content: Is this a {disfmarker} a s a trained system ? Or is it a system where you just pick some thresholds ? Ho - how does it work ?&#10;Speaker: PhD A&#10;Content: Right now we just are trying to find some features . And ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: uh {disfmarker} Yeah . Hopefully , I think what we want to have is to put these features in s some kind of , um {disfmarker} well , to {disfmarker} to obtain a statistical model on these features and to {disfmarker" target="1. The process for drawing a system's parameters across time for a clean sentence and a noisy sentence involves analyzing the characteristics of speech signals in both scenarios. This likely includes examining various attributes such as frequency content, amplitude, and duration of sounds within the sentences. By comparing these parameters between clean and noisy sentences, researchers can gain insights into how noise affects speech perception and develop methods to mitigate its impact.&#10;&#10;2. Features are being used in an attempt to create a statistical model for this process by extracting relevant characteristics from the speech signals that highlight differences between clean and noisy sentences. In this context, features might include measures related to voiced vs. unvoiced sounds, pitch detection, or other spectral properties. The goal is to build a statistical model using these features that can accurately differentiate between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;The discussion revolves around finding suitable features for this task, such as the maximum of the auto-correlation function, which could help characterize voiced phonemes more effectively. The team aims to use these features to create a statistical model that captures the differences between clean and noisy sentences, ultimately leading to better speech recognition performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="} we draw its parameters across time for a clean sentence and then nois the same noisy sentence , it 's very close .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Yeah . So there are {disfmarker} there is this . There could be also the , um {disfmarker} {vocalsound} something like the maximum of the auto - correlation function or {disfmarker} which {disfmarker}&#10;Speaker: PhD C&#10;Content: Is this a {disfmarker} a s a trained system ? Or is it a system where you just pick some thresholds ? Ho - how does it work ?&#10;Speaker: PhD A&#10;Content: Right now we just are trying to find some features . And ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: uh {disfmarker} Yeah . Hopefully , I think what we want to have is to put these features in s some kind of , um {disfmarker} well , to {disfmarker} to obtain a statistical model on these features and to {disfmarker" target="1. The process for drawing a system's parameters across time involves analyzing the characteristics of speech signals in both clean and noisy sentences, comparing attributes such as frequency content, amplitude, and duration of sounds within the sentences to understand how noise affects speech perception and develop mitigation methods. Features are extracted from the speech signals to create a statistical model that differentiates between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;2. The team is currently focused on finding suitable features for this task, with one suggestion being the maximum of the auto-correlation function to characterize voiced phonemes more effectively. They have not yet decided whether they will use a trained system or threshold-based system to implement these features in their statistical model.&#10;&#10;3. A noisy sentence is created by adding noise to a clean sentence, as demonstrated when Professor B mentioned that drawing the parameters across time for a clean sentence and then adding noise results in a similar (noisy) sentence. However, the specific method or techniques used to create the noisy sentence are not discussed in the provided transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Benefits of smooth representation for generalization: A smooth representation generally leads to better generalization because it reduces overfitting by averaging out fine details and noise in the data. This can help improve the model's performance on unseen data, as the model is less likely to learn spurious patterns that only exist in the training set.&#10;&#10;2. Potential drawbacks of losing details during smoothing: Smoothing inevitably leads to loss of information, which might include valuable details about the original signal. This could potentially make the model miss important features or characteristics of the data, leading to reduced accuracy or suboptimal performance in certain cases. In speech recognition, for example, excessive smoothing might cause the loss of crucial information related to phoneme distinction or speaker identity.&#10;&#10;In summary, while smoothing offers benefits such as improved generalization and reduced overfitting, it can also result in the loss of important details that may be necessary for accurate analysis or modeling. Balancing these trade-offs is essential for developing effective speech recognition systems." target="er} so , stand standing back from that , you sort of say there 's this very detailed representation .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: You go to a smooth representation .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: You go to a smooth representation cuz this typically generalizes better .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um , but whenever you smooth you lose something , so the question is have you lost something you can you use ?&#10;Speaker: PhD C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: Um , probably you wouldn't want to go to the extreme of just ta saying &quot; OK , our feature set will be the FFT &quot; , cuz we really think we do gain something in robustness from going to something smoother , but maybe there 's something that we missed .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So what is it ?&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Benefits of smooth representation for generalization: A smooth representation generally leads to better generalization because it reduces overfitting by averaging out fine details and noise in the data. This can help improve the model's performance on unseen data, as the model is less likely to learn spurious patterns that only exist in the training set.&#10;&#10;2. Potential drawbacks of losing details during smoothing: Smoothing inevitably leads to loss of information, which might include valuable details about the original signal. This could potentially make the model miss important features or characteristics of the data, leading to reduced accuracy or suboptimal performance in certain cases. In speech recognition, for example, excessive smoothing might cause the loss of crucial information related to phoneme distinction or speaker identity.&#10;&#10;In summary, while smoothing offers benefits such as improved generalization and reduced overfitting, it can also result in the loss of important details that may be necessary for accurate analysis or modeling. Balancing these trade-offs is essential for developing effective speech recognition systems." target="1. The process for drawing a system's parameters across time for a clean sentence and a noisy sentence involves analyzing the characteristics of speech signals in both scenarios. This likely includes examining various attributes such as frequency content, amplitude, and duration of sounds within the sentences. By comparing these parameters between clean and noisy sentences, researchers can gain insights into how noise affects speech perception and develop methods to mitigate its impact.&#10;&#10;2. Features are being used in an attempt to create a statistical model for this process by extracting relevant characteristics from the speech signals that highlight differences between clean and noisy sentences. In this context, features might include measures related to voiced vs. unvoiced sounds, pitch detection, or other spectral properties. The goal is to build a statistical model using these features that can accurately differentiate between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;The discussion revolves around finding suitable features for this task, such as the maximum of the auto-correlation function, which could help characterize voiced phonemes more effectively. The team aims to use these features to create a statistical model that captures the differences between clean and noisy sentences, ultimately leading to better speech recognition performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Benefits of smooth representation for generalization: A smooth representation generally leads to better generalization because it reduces overfitting by averaging out fine details and noise in the data. This can help improve the model's performance on unseen data, as the model is less likely to learn spurious patterns that only exist in the training set.&#10;&#10;2. Potential drawbacks of losing details during smoothing: Smoothing inevitably leads to loss of information, which might include valuable details about the original signal. This could potentially make the model miss important features or characteristics of the data, leading to reduced accuracy or suboptimal performance in certain cases. In speech recognition, for example, excessive smoothing might cause the loss of crucial information related to phoneme distinction or speaker identity.&#10;&#10;In summary, while smoothing offers benefits such as improved generalization and reduced overfitting, it can also result in the loss of important details that may be necessary for accurate analysis or modeling. Balancing these trade-offs is essential for developing effective speech recognition systems." target="1. The process for drawing a system's parameters across time involves analyzing the characteristics of speech signals in both clean and noisy sentences, comparing attributes such as frequency content, amplitude, and duration of sounds within the sentences to understand how noise affects speech perception and develop mitigation methods. Features are extracted from the speech signals to create a statistical model that differentiates between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;2. The team is currently focused on finding suitable features for this task, with one suggestion being the maximum of the auto-correlation function to characterize voiced phonemes more effectively. They have not yet decided whether they will use a trained system or threshold-based system to implement these features in their statistical model.&#10;&#10;3. A noisy sentence is created by adding noise to a clean sentence, as demonstrated when Professor B mentioned that drawing the parameters across time for a clean sentence and then adding noise results in a similar (noisy) sentence. However, the specific method or techniques used to create the noisy sentence are not discussed in the provided transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Benefits of smooth representation for generalization: A smooth representation generally leads to better generalization because it reduces overfitting by averaging out fine details and noise in the data. This can help improve the model's performance on unseen data, as the model is less likely to learn spurious patterns that only exist in the training set.&#10;&#10;2. Potential drawbacks of losing details during smoothing: Smoothing inevitably leads to loss of information, which might include valuable details about the original signal. This could potentially make the model miss important features or characteristics of the data, leading to reduced accuracy or suboptimal performance in certain cases. In speech recognition, for example, excessive smoothing might cause the loss of crucial information related to phoneme distinction or speaker identity.&#10;&#10;In summary, while smoothing offers benefits such as improved generalization and reduced overfitting, it can also result in the loss of important details that may be necessary for accurate analysis or modeling. Balancing these trade-offs is essential for developing effective speech recognition systems." target="1. It is potentially unfair or insufficient to base a good vocal sound design on just the baseline features because the chosen baseline might not be robust or representative of the current state-of-the-art, leading to underestimating the true potential of novel approaches. Additionally, limiting comparative evaluations by reducing efforts in feature design can result in fewer comparisons with alternative features or methods, which are crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;2. The benefits of incorporating additional features into the vocal sound design include:&#10;  a) Improved robustness and fairness: Adding more features to the design ensures that the evaluation is based on a comprehensive comparison, reducing potential biases towards simpler techniques that may not fully address the problem at hand.&#10;  b) Enhanced understanding of strengths and weaknesses: Comparing the new method against various alternatives provides insights into its performance relative to different types of features and methods, aiding in identifying the most suitable approach for specific scenarios or environments.&#10;  c) Better generalization: Including diverse features can help average out fine details and noise in the data, reducing overfitting and improving the model's performance on unseen data.&#10;  d) Capturing all aspects required for a good VAD system: Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments, ensuring that the system can effectively handle various challenges associated with voice activity detection.">
      <data key="d0">1</data>
    </edge>
    <edge source="In the meeting transcript, PhD C reported on their experiments related to the HTK (Hidden Markov Model Toolkit) back-end models. They focused on the number of training iterations in different rounds of the model training process. Specifically, there are four rounds of training, and the number of iterations for each round is as follows:&#10;&#10;1. First round: 3 iterations&#10;2. Second to fourth rounds: 7 iterations for each round&#10;&#10;These iterations refer to the number of times the HMM (Hidden Markov Model) re-estimation is run during each training round. The higher number of iterations in later rounds indicates that more computational resources are devoted to refining the models in those stages." target=" doo .&#10;Speaker: Professor B&#10;Content: So .&#10;Speaker: PhD C&#10;Content: I have something just fairly brief to report on .&#10;Speaker: Professor B&#10;Content: Mmm .&#10;Speaker: PhD C&#10;Content: Um , I did some {pause} experim uh , uh , just a few more experiments before I had to , {vocalsound} uh , go away for the w well , that week .&#10;Speaker: Professor B&#10;Content: Great !&#10;Speaker: PhD C&#10;Content: Was it last week or whenever ? Um , so what I was started playing with was the {disfmarker} th again , this is the HTK back - end . And , um , I was curious because the way that they train up the models , {vocalsound} they go through about four sort of rounds of {disfmarker} of training . And in the first round they do {disfmarker} uh , I think it 's three iterations , and for the last three rounds e e they do seven iterations of re - estimation in each of those three . And so , you know , that 's part of what takes so long to train the {d">
      <data key="d0">1</data>
    </edge>
    <edge source="In the meeting transcript, PhD C reported on their experiments related to the HTK (Hidden Markov Model Toolkit) back-end models. They focused on the number of training iterations in different rounds of the model training process. Specifically, there are four rounds of training, and the number of iterations for each round is as follows:&#10;&#10;1. First round: 3 iterations&#10;2. Second to fourth rounds: 7 iterations for each round&#10;&#10;These iterations refer to the number of times the HMM (Hidden Markov Model) re-estimation is run during each training round. The higher number of iterations in later rounds indicates that more computational resources are devoted to refining the models in those stages." target=" Grad E&#10;Content: Hmm .&#10;Speaker: PhD C&#10;Content: So we could do a lot more experiments and throw a lot more stuff in there .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: That 's great .&#10;Speaker: PhD C&#10;Content: Um . Oh , the other thing that I did was , um , {vocalsound} I compiled {pause} the HTK stuff for the Linux boxes . So we have this big thing that we got from IBM , which is a five - processor machine . Really fast , but it 's running Linux . So , you can now run your experiments on that machine and you can run five at a time and it runs , {vocalsound} uh , as fast as , you know , uh , five different machines .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , um , I 've forgotten now what the name of that machine is but I can {disfmarker} I can send email around about it .&#10;Speaker: PhD A&#10;Content: Yeah .">
      <data key="d0">1</data>
    </edge>
    <edge source="In the meeting transcript, PhD C reported on their experiments related to the HTK (Hidden Markov Model Toolkit) back-end models. They focused on the number of training iterations in different rounds of the model training process. Specifically, there are four rounds of training, and the number of iterations for each round is as follows:&#10;&#10;1. First round: 3 iterations&#10;2. Second to fourth rounds: 7 iterations for each round&#10;&#10;These iterations refer to the number of times the HMM (Hidden Markov Model) re-estimation is run during each training round. The higher number of iterations in later rounds indicates that more computational resources are devoted to refining the models in those stages." target="aker: Professor B&#10;Content: And you also can choose your features in such a way as to prove {disfmarker} improve recognition . They may not be the same thing .&#10;Speaker: PhD C&#10;Content: But it seems like you should do both .&#10;Speaker: Professor B&#10;Content: You should do both&#10;Speaker: PhD C&#10;Content: Right ?&#10;Speaker: Professor B&#10;Content: and {disfmarker} and I {disfmarker} I think that this still makes {disfmarker} I still think this makes sense as a baseline . It 's just saying , as a baseline , we know {disfmarker}&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: you know , we had the MFCC 's before , lots of people have done voice activity detectors ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: you might as well pick some voice activity detector and make that the baseline , just like you picked some version of HTK and made that the baseline .&#10;Speaker: PhD A&#10;Content: Yeah . Right .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="In the meeting transcript, PhD C reported on their experiments related to the HTK (Hidden Markov Model Toolkit) back-end models. They focused on the number of training iterations in different rounds of the model training process. Specifically, there are four rounds of training, and the number of iterations for each round is as follows:&#10;&#10;1. First round: 3 iterations&#10;2. Second to fourth rounds: 7 iterations for each round&#10;&#10;These iterations refer to the number of times the HMM (Hidden Markov Model) re-estimation is run during each training round. The higher number of iterations in later rounds indicates that more computational resources are devoted to refining the models in those stages." target=" , just like you picked some version of HTK and made that the baseline .&#10;Speaker: PhD A&#10;Content: Yeah . Right .&#10;Speaker: Professor B&#10;Content: And then {pause} let 's try and make everything better . Um , and if one of the ways you make it better is by having your features {pause} be better features for the VAD then that 's {disfmarker} so be it .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: But , uh , uh , uh , at least you have a starting point that 's {disfmarker} um , cuz i i some of {disfmarker} the some of the people didn't have a VAD at all , I guess . Right ? And {disfmarker} and&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: then they {disfmarker} they looked pretty bad and {disfmarker} and in fact what they were doing wasn't so bad at all .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mm - hmm .">
      <data key="d0">1</data>
    </edge>
    <edge source="In the meeting transcript, PhD C reported on their experiments related to the HTK (Hidden Markov Model Toolkit) back-end models. They focused on the number of training iterations in different rounds of the model training process. Specifically, there are four rounds of training, and the number of iterations for each round is as follows:&#10;&#10;1. First round: 3 iterations&#10;2. Second to fourth rounds: 7 iterations for each round&#10;&#10;These iterations refer to the number of times the HMM (Hidden Markov Model) re-estimation is run during each training round. The higher number of iterations in later rounds indicates that more computational resources are devoted to refining the models in those stages." target="Based on the transcript, there is no explicit information given about whether Hynek or Sunil from OGI (Oregon Graduate Institute) were part of the Aurora conference call. Speaker PhD A mentioned not knowing who was involved from OGI, and the discussion moved on to other topics. Therefore, it cannot be determined from this transcript if Hynek or Sunil were present in the Aurora conference call.">
      <data key="d0">1</data>
    </edge>
    <edge source="The frequency range used for computing the variance in the example discussed by the speakers is from 200 Hz to 1500 Hz. This is stated explicitly by speaker A, and although professor F mistakenly gives a higher frequency range, it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." target="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments.">
      <data key="d0">1</data>
    </edge>
    <edge source="The frequency range used for computing the variance in the example discussed by the speakers is from 200 Hz to 1500 Hz. This is stated explicitly by speaker A, and although professor F mistakenly gives a higher frequency range, it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." target="The speakers, who appear to be discussing the evaluation and improvement of a signal processing system, are expressing uncertainty about the final application of the system and whether it will need to handle more difficult cases with high accuracy. Professor B mentions that if the predominate use of the system is in challenging scenarios, current methods may not be sufficient. However, in better cases, the system could potentially work well with some error recovery.&#10;&#10;Later in the discussion, the speakers consider a variance calculation done on a frequency range of 200 Hz to 1500 Hz, which they believe is more logical for their analysis. They also discuss different methods for weighting error rates obtained from various languages and conditions, with some arguing that it would be better to combine error rates before computing improvements.&#10;&#10;In summary, the speakers are discussing the uncertainty of the system's primary use, the potential for reasonable error recovery in better cases, and different approaches to weighing error rates for improved performance in various scenarios.">
      <data key="d0">1</data>
    </edge>
    <edge source="The frequency range used for computing the variance in the example discussed by the speakers is from 200 Hz to 1500 Hz. This is stated explicitly by speaker A, and although professor F mistakenly gives a higher frequency range, it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." target="Professionor B expresses that making any policy change in meetings is tricky because everyone has different perspectives and opinions. They imply that reaching a consensus or agreement can be difficult due to these varied viewpoints.">
      <data key="d0">1</data>
    </edge>
    <edge source="The frequency range used for computing the variance in the example discussed by the speakers is from 200 Hz to 1500 Hz. This is stated explicitly by speaker A, and although professor F mistakenly gives a higher frequency range, it is clear from the context that this is not correct and the original range of 200 Hz to 1500 Hz was intended." target="1. When Professor B suggests that imperfect distinctions between voiced and unvoiced sounds might not be a significant issue, they are implying that the primary value in this analysis comes from obtaining a general characterization of the speech signal based on our intuition about these categories. This rough characterization can provide insights into the raw data, even if it does not offer a clear-cut distinction between voiced and unvoiced sounds due to overlaps and complexities in speech.&#10;2. The professor emphasizes that relying on this basic intuition might be more valuable than using overly refined or smoothed versions of the signal since real-world data, such as noise, can impact the accuracy of any speech recognition system. Consequently, understanding how noise affects speech perception and developing methods to mitigate its effects is essential for improving speech recognition systems' performance in various environments.&#10;3. The team is currently focused on selecting appropriate features for their task, with one possibility being the maximum of the auto-correlation function to better characterize voiced phonemes. However, they have not yet decided whether to use a trained system or threshold-based system to implement these features in their statistical model.&#10;4. Professor B implies that even if the &quot;world's best voiced-unvoiced classifier&quot; does not perfectly distinguish between these categories, trying different statistical characterizations of the difference based on raw data may still yield valuable information that a speech recognition system can utilize.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The voiced-unvoiced (VU) detector's purpose is to determine if a given segment of speech is voiced or unvoiced. Voiced sounds are produced when the vocal cords vibrate, such as vowel sounds, whereas unvoiced sounds result from airflow through the mouth without vocal cord vibration, like consonant sounds (e.g., 'f' or 's').&#10;2. In this conversation, the VU detector is intended to be used as a feature extractor for speech processing tasks, not part of a typical voiced-unvoiced detection system. The idea is that it could be incorporated into either a separate neural network designed specifically for VU detection or combined with a larger neural network responsible for phoneme classification.&#10;3. Additionally, there is discussion about using the VU detector as part of an improved front-end processing system to enhance pitch detection, which is crucial for better speech processing performance since Mel Frequency Cepstral Coefficients (MFCCs) do not include any pitch-related information. By implementing a good pitch detection algorithm, they aim to improve voice activity detection and other speech processing tasks' accuracy and robustness.&#10;&#10;In summary, the voiced-unvoiced detector is intended for use as a feature extractor in speech processing tasks and potentially improving pitch detection algorithms within front-end processing systems." target=" at a few different {vocalsound} ways of look of characterizing that difference and , uh , you could have one of them but {disfmarker} and {disfmarker} and see , you know , which of them helps .&#10;Speaker: PhD A&#10;Content: Mm - hmm . OK .&#10;Speaker: PhD C&#10;Content: So i is the idea that you 're going to take {pause} whatever features you develop and {disfmarker} and just add them onto the future vector ? Or , what 's the use of the {disfmarker} the voiced - unvoiced detector ?&#10;Speaker: PhD A&#10;Content: Uh , I guess we don't know exactly yet . But , {vocalsound} um {disfmarker} Yeah . Th&#10;Speaker: PhD C&#10;Content: It 's not part of a VAD system that you 're doing ?&#10;Speaker: PhD F&#10;Content: No .&#10;Speaker: PhD A&#10;Content: Uh , no . No .&#10;Speaker: PhD C&#10;Content: Oh , OK .&#10;Speaker: PhD A&#10;Content: No , the idea was , I guess , to">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The voiced-unvoiced (VU) detector's purpose is to determine if a given segment of speech is voiced or unvoiced. Voiced sounds are produced when the vocal cords vibrate, such as vowel sounds, whereas unvoiced sounds result from airflow through the mouth without vocal cord vibration, like consonant sounds (e.g., 'f' or 's').&#10;2. In this conversation, the VU detector is intended to be used as a feature extractor for speech processing tasks, not part of a typical voiced-unvoiced detection system. The idea is that it could be incorporated into either a separate neural network designed specifically for VU detection or combined with a larger neural network responsible for phoneme classification.&#10;3. Additionally, there is discussion about using the VU detector as part of an improved front-end processing system to enhance pitch detection, which is crucial for better speech processing performance since Mel Frequency Cepstral Coefficients (MFCCs) do not include any pitch-related information. By implementing a good pitch detection algorithm, they aim to improve voice activity detection and other speech processing tasks' accuracy and robustness.&#10;&#10;In summary, the voiced-unvoiced detector is intended for use as a feature extractor in speech processing tasks and potentially improving pitch detection algorithms within front-end processing systems." target="&#10;Speaker: PhD C&#10;Content: Oh , OK .&#10;Speaker: PhD A&#10;Content: No , the idea was , I guess , to {disfmarker} to use them as {disfmarker} as features .&#10;Speaker: PhD C&#10;Content: Features . I see .&#10;Speaker: PhD A&#10;Content: Uh {disfmarker} Yeah , it could be , uh {disfmarker} it could be {vocalsound} a neural network that does voiced and unvoiced detection ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: but it could be in the {disfmarker} also the big neural network that does phoneme classification .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mmm . Yeah .&#10;Speaker: Professor B&#10;Content: But each one of the mixture components {disfmarker} I mean , you have , uh , uh , variance only , so it 's kind of like you 're just multiplying together these , um , probabilities from the individual features {pause} within each mixture . So it">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The voiced-unvoiced (VU) detector's purpose is to determine if a given segment of speech is voiced or unvoiced. Voiced sounds are produced when the vocal cords vibrate, such as vowel sounds, whereas unvoiced sounds result from airflow through the mouth without vocal cord vibration, like consonant sounds (e.g., 'f' or 's').&#10;2. In this conversation, the VU detector is intended to be used as a feature extractor for speech processing tasks, not part of a typical voiced-unvoiced detection system. The idea is that it could be incorporated into either a separate neural network designed specifically for VU detection or combined with a larger neural network responsible for phoneme classification.&#10;3. Additionally, there is discussion about using the VU detector as part of an improved front-end processing system to enhance pitch detection, which is crucial for better speech processing performance since Mel Frequency Cepstral Coefficients (MFCCs) do not include any pitch-related information. By implementing a good pitch detection algorithm, they aim to improve voice activity detection and other speech processing tasks' accuracy and robustness.&#10;&#10;In summary, the voiced-unvoiced detector is intended for use as a feature extractor in speech processing tasks and potentially improving pitch detection algorithms within front-end processing systems." target="1. The process for drawing a system's parameters across time for a clean sentence and a noisy sentence involves analyzing the characteristics of speech signals in both scenarios. This likely includes examining various attributes such as frequency content, amplitude, and duration of sounds within the sentences. By comparing these parameters between clean and noisy sentences, researchers can gain insights into how noise affects speech perception and develop methods to mitigate its impact.&#10;&#10;2. Features are being used in an attempt to create a statistical model for this process by extracting relevant characteristics from the speech signals that highlight differences between clean and noisy sentences. In this context, features might include measures related to voiced vs. unvoiced sounds, pitch detection, or other spectral properties. The goal is to build a statistical model using these features that can accurately differentiate between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;The discussion revolves around finding suitable features for this task, such as the maximum of the auto-correlation function, which could help characterize voiced phonemes more effectively. The team aims to use these features to create a statistical model that captures the differences between clean and noisy sentences, ultimately leading to better speech recognition performance.">
      <data key="d0">1</data>
    </edge>
    <edge source=" at a few different {vocalsound} ways of look of characterizing that difference and , uh , you could have one of them but {disfmarker} and {disfmarker} and see , you know , which of them helps .&#10;Speaker: PhD A&#10;Content: Mm - hmm . OK .&#10;Speaker: PhD C&#10;Content: So i is the idea that you 're going to take {pause} whatever features you develop and {disfmarker} and just add them onto the future vector ? Or , what 's the use of the {disfmarker} the voiced - unvoiced detector ?&#10;Speaker: PhD A&#10;Content: Uh , I guess we don't know exactly yet . But , {vocalsound} um {disfmarker} Yeah . Th&#10;Speaker: PhD C&#10;Content: It 's not part of a VAD system that you 're doing ?&#10;Speaker: PhD F&#10;Content: No .&#10;Speaker: PhD A&#10;Content: Uh , no . No .&#10;Speaker: PhD C&#10;Content: Oh , OK .&#10;Speaker: PhD A&#10;Content: No , the idea was , I guess , to" target="1. Choosing a weak baseline: If the baseline is not robust or representative of the current state-of-the-art, comparing improvements against it might not provide accurate insights into the performance of new methods. This could lead to underestimating the true potential of novel approaches.&#10;&#10;2. Limited comparative evaluation: Reducing efforts in feature design may result in fewer comparisons with alternative features or methods, which could be crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;3. Incomplete VAD design: A simpler feature design might not capture all aspects required for a good Voice Activity Detection (VAD) system. Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments.&#10;&#10;4. Relying solely on the recognizer for VAD: While it is possible to use the whole recognizer to perform VAD, this approach may not always provide optimal results compared to dedicated VAD systems tailored to specific applications and signal types.&#10;&#10;5. Fairness concerns: It might be considered unfair to compare a simplified feature design to more complex approaches if additional baseline features are required to create a good VAD system. This could lead to an inaccurate assessment of the performance of novel methods, potentially favoring simpler techniques that do not fully address the problem at hand.&#10;&#10;To ensure fairness and accurate comparisons, it is essential to:&#10;- Choose a robust and representative baseline.&#10;- Perform thorough evaluations against various alternatives.&#10;- Consider all aspects required for a good VAD system, including complexity and real-time requirements.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: PhD C&#10;Content: Oh , OK .&#10;Speaker: PhD A&#10;Content: No , the idea was , I guess , to {disfmarker} to use them as {disfmarker} as features .&#10;Speaker: PhD C&#10;Content: Features . I see .&#10;Speaker: PhD A&#10;Content: Uh {disfmarker} Yeah , it could be , uh {disfmarker} it could be {vocalsound} a neural network that does voiced and unvoiced detection ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: but it could be in the {disfmarker} also the big neural network that does phoneme classification .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mmm . Yeah .&#10;Speaker: Professor B&#10;Content: But each one of the mixture components {disfmarker} I mean , you have , uh , uh , variance only , so it 's kind of like you 're just multiplying together these , um , probabilities from the individual features {pause} within each mixture . So it" target="1. Advanced processing techniques: If re-synthesis turns out to be useful in the long term, more advanced or customized processing techniques can be applied to the signal. (Potential long-term benefits of using re-synthesis)&#10;2. Robustness: Even with the sacrifice of a smooth spectral envelope, combining probabilities of individual features within each mixture might lead to more robust results, as suggested by Professor B and PhD A when discussing the comparison between raw data and smooth versions of the spectrum.&#10;3. Exploration of statistical measures: By analyzing the difference between raw data and the smooth version, new statistical measures can be explored and added to feature vectors to improve performance in noisy environments. This approach is mentioned by PhD as a way of finding something that might have been missed in the smoothed spectrum but could contribute positively to the final result.&#10;4. Potential improvements through spectral subtraction and LDA filtering: As suggested by Professor B, combining probabilities within each mixture along with spectral subtraction and LDA filtering might yield better results than simpler approaches, as mentioned in a previous meeting. However, this depends on the specific implementation of spectral subtraction and its compatibility with online normalization.&#10;5. Addressing musical tones issue: Although not explicitly stated, combining probabilities within each mixture might help address issues related to musical tones raised by PhD A in a previous meeting. However, further investigation is needed to confirm this potential advantage.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Advanced processing techniques: If re-synthesis turns out to be useful in the long term, more advanced or customized processing techniques can be applied to the signal. (Potential long-term benefits of using re-synthesis)&#10;2. Robustness: Even with the sacrifice of a smooth spectral envelope, combining probabilities of individual features within each mixture might lead to more robust results, as suggested by Professor B and PhD A when discussing the comparison between raw data and smooth versions of the spectrum.&#10;3. Exploration of statistical measures: By analyzing the difference between raw data and the smooth version, new statistical measures can be explored and added to feature vectors to improve performance in noisy environments. This approach is mentioned by PhD as a way of finding something that might have been missed in the smoothed spectrum but could contribute positively to the final result.&#10;4. Potential improvements through spectral subtraction and LDA filtering: As suggested by Professor B, combining probabilities within each mixture along with spectral subtraction and LDA filtering might yield better results than simpler approaches, as mentioned in a previous meeting. However, this depends on the specific implementation of spectral subtraction and its compatibility with online normalization.&#10;5. Addressing musical tones issue: Although not explicitly stated, combining probabilities within each mixture might help address issues related to musical tones raised by PhD A in a previous meeting. However, further investigation is needed to confirm this potential advantage." target=" , so it 's kind of like you 're just multiplying together these , um , probabilities from the individual features {pause} within each mixture . So it 's {disfmarker} so , uh , it seems l you know {disfmarker}&#10;Speaker: PhD C&#10;Content: I think it 's a neat thing . Uh , it seems like a good idea .&#10;Speaker: Professor B&#10;Content: Yeah . Um . Yeah . I mean , {vocalsound} I know that , um , people doing some robustness things a ways back were {disfmarker} were just doing {disfmarker} just being gross and just throwing in the FFT and actually it wasn't {disfmarker} wasn't {disfmarker} wasn't so bad . Uh , so it would s and {disfmarker} and you know that i it 's gotta hurt you a little bit to not have a {disfmarker} {vocalsound} a spectral , uh {disfmarker} a s a smooth spectral envelope , so there must be something else that you get {pause} in return for that {disfmarker}&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Advanced processing techniques: If re-synthesis turns out to be useful in the long term, more advanced or customized processing techniques can be applied to the signal. (Potential long-term benefits of using re-synthesis)&#10;2. Robustness: Even with the sacrifice of a smooth spectral envelope, combining probabilities of individual features within each mixture might lead to more robust results, as suggested by Professor B and PhD A when discussing the comparison between raw data and smooth versions of the spectrum.&#10;3. Exploration of statistical measures: By analyzing the difference between raw data and the smooth version, new statistical measures can be explored and added to feature vectors to improve performance in noisy environments. This approach is mentioned by PhD as a way of finding something that might have been missed in the smoothed spectrum but could contribute positively to the final result.&#10;4. Potential improvements through spectral subtraction and LDA filtering: As suggested by Professor B, combining probabilities within each mixture along with spectral subtraction and LDA filtering might yield better results than simpler approaches, as mentioned in a previous meeting. However, this depends on the specific implementation of spectral subtraction and its compatibility with online normalization.&#10;5. Addressing musical tones issue: Although not explicitly stated, combining probabilities within each mixture might help address issues related to musical tones raised by PhD A in a previous meeting. However, further investigation is needed to confirm this potential advantage." target=" .&#10;Speaker: PhD F&#10;Content: Here .&#10;Speaker: PhD A&#10;Content: They should be more close .&#10;Speaker: PhD F&#10;Content: Ah , no . This is this ? More close . Is this ? And this .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So they are {disfmarker} this is {disfmarker} there is less difference .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So if you look at the spectral envelope , just the very smooth properties of it , {vocalsound} you get something closer to that .&#10;Speaker: PhD A&#10;Content: This is less {disfmarker} it 's less robust .&#10;Speaker: PhD F&#10;Content: Less robust . Yeah .&#10;Speaker: PhD A&#10;Content: Oh , yeah .&#10;Speaker: Professor B&#10;Content: And the notion is if you have the full spectrum , with all the little nitty -">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Advanced processing techniques: If re-synthesis turns out to be useful in the long term, more advanced or customized processing techniques can be applied to the signal. (Potential long-term benefits of using re-synthesis)&#10;2. Robustness: Even with the sacrifice of a smooth spectral envelope, combining probabilities of individual features within each mixture might lead to more robust results, as suggested by Professor B and PhD A when discussing the comparison between raw data and smooth versions of the spectrum.&#10;3. Exploration of statistical measures: By analyzing the difference between raw data and the smooth version, new statistical measures can be explored and added to feature vectors to improve performance in noisy environments. This approach is mentioned by PhD as a way of finding something that might have been missed in the smoothed spectrum but could contribute positively to the final result.&#10;4. Potential improvements through spectral subtraction and LDA filtering: As suggested by Professor B, combining probabilities within each mixture along with spectral subtraction and LDA filtering might yield better results than simpler approaches, as mentioned in a previous meeting. However, this depends on the specific implementation of spectral subtraction and its compatibility with online normalization.&#10;5. Addressing musical tones issue: Although not explicitly stated, combining probabilities within each mixture might help address issues related to musical tones raised by PhD A in a previous meeting. However, further investigation is needed to confirm this potential advantage." target="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process for drawing a system's parameters across time for a clean sentence and a noisy sentence involves analyzing the characteristics of speech signals in both scenarios. This likely includes examining various attributes such as frequency content, amplitude, and duration of sounds within the sentences. By comparing these parameters between clean and noisy sentences, researchers can gain insights into how noise affects speech perception and develop methods to mitigate its impact.&#10;&#10;2. Features are being used in an attempt to create a statistical model for this process by extracting relevant characteristics from the speech signals that highlight differences between clean and noisy sentences. In this context, features might include measures related to voiced vs. unvoiced sounds, pitch detection, or other spectral properties. The goal is to build a statistical model using these features that can accurately differentiate between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;The discussion revolves around finding suitable features for this task, such as the maximum of the auto-correlation function, which could help characterize voiced phonemes more effectively. The team aims to use these features to create a statistical model that captures the differences between clean and noisy sentences, ultimately leading to better speech recognition performance." target=" details and all the ways which is {disfmarker} that these are complete lies . Uh , the {disfmarker} the {disfmarker} you know , what you 're doing in feature extraction for speech recognition is you have , {vocalsound} uh , in your head a {disfmarker} a {disfmarker} a {disfmarker} a simplified production model for speech ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: in which you have a periodic or aperiodic source that 's driving some filters .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Yeah . This is the {disfmarker} the auto - correlation {disfmarker} the R - zero energy .&#10;Speaker: PhD A&#10;Content: Do you have the mean {disfmarker} do you have the mean for the auto - correlation {disfmarker} ?&#10;Speaker: Professor B&#10;Content: Uh , first order for speech recognition , you say &quot; I don't care about the source &quot; .&#10;Speaker: PhD F">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process for drawing a system's parameters across time for a clean sentence and a noisy sentence involves analyzing the characteristics of speech signals in both scenarios. This likely includes examining various attributes such as frequency content, amplitude, and duration of sounds within the sentences. By comparing these parameters between clean and noisy sentences, researchers can gain insights into how noise affects speech perception and develop methods to mitigate its impact.&#10;&#10;2. Features are being used in an attempt to create a statistical model for this process by extracting relevant characteristics from the speech signals that highlight differences between clean and noisy sentences. In this context, features might include measures related to voiced vs. unvoiced sounds, pitch detection, or other spectral properties. The goal is to build a statistical model using these features that can accurately differentiate between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;The discussion revolves around finding suitable features for this task, such as the maximum of the auto-correlation function, which could help characterize voiced phonemes more effectively. The team aims to use these features to create a statistical model that captures the differences between clean and noisy sentences, ultimately leading to better speech recognition performance." target="1. The process for drawing a system's parameters across time involves analyzing the characteristics of speech signals in both clean and noisy sentences, comparing attributes such as frequency content, amplitude, and duration of sounds within the sentences to understand how noise affects speech perception and develop mitigation methods. Features are extracted from the speech signals to create a statistical model that differentiates between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;2. The team is currently focused on finding suitable features for this task, with one suggestion being the maximum of the auto-correlation function to characterize voiced phonemes more effectively. They have not yet decided whether they will use a trained system or threshold-based system to implement these features in their statistical model.&#10;&#10;3. A noisy sentence is created by adding noise to a clean sentence, as demonstrated when Professor B mentioned that drawing the parameters across time for a clean sentence and then adding noise results in a similar (noisy) sentence. However, the specific method or techniques used to create the noisy sentence are not discussed in the provided transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process for drawing a system's parameters across time for a clean sentence and a noisy sentence involves analyzing the characteristics of speech signals in both scenarios. This likely includes examining various attributes such as frequency content, amplitude, and duration of sounds within the sentences. By comparing these parameters between clean and noisy sentences, researchers can gain insights into how noise affects speech perception and develop methods to mitigate its impact.&#10;&#10;2. Features are being used in an attempt to create a statistical model for this process by extracting relevant characteristics from the speech signals that highlight differences between clean and noisy sentences. In this context, features might include measures related to voiced vs. unvoiced sounds, pitch detection, or other spectral properties. The goal is to build a statistical model using these features that can accurately differentiate between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;The discussion revolves around finding suitable features for this task, such as the maximum of the auto-correlation function, which could help characterize voiced phonemes more effectively. The team aims to use these features to create a statistical model that captures the differences between clean and noisy sentences, ultimately leading to better speech recognition performance." target="1. It is potentially unfair or insufficient to base a good vocal sound design on just the baseline features because the chosen baseline might not be robust or representative of the current state-of-the-art, leading to underestimating the true potential of novel approaches. Additionally, limiting comparative evaluations by reducing efforts in feature design can result in fewer comparisons with alternative features or methods, which are crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;2. The benefits of incorporating additional features into the vocal sound design include:&#10;  a) Improved robustness and fairness: Adding more features to the design ensures that the evaluation is based on a comprehensive comparison, reducing potential biases towards simpler techniques that may not fully address the problem at hand.&#10;  b) Enhanced understanding of strengths and weaknesses: Comparing the new method against various alternatives provides insights into its performance relative to different types of features and methods, aiding in identifying the most suitable approach for specific scenarios or environments.&#10;  c) Better generalization: Including diverse features can help average out fine details and noise in the data, reducing overfitting and improving the model's performance on unseen data.&#10;  d) Capturing all aspects required for a good VAD system: Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments, ensuring that the system can effectively handle various challenges associated with voice activity detection.">
      <data key="d0">1</data>
    </edge>
    <edge source=" details and all the ways which is {disfmarker} that these are complete lies . Uh , the {disfmarker} the {disfmarker} you know , what you 're doing in feature extraction for speech recognition is you have , {vocalsound} uh , in your head a {disfmarker} a {disfmarker} a {disfmarker} a simplified production model for speech ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: in which you have a periodic or aperiodic source that 's driving some filters .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Yeah . This is the {disfmarker} the auto - correlation {disfmarker} the R - zero energy .&#10;Speaker: PhD A&#10;Content: Do you have the mean {disfmarker} do you have the mean for the auto - correlation {disfmarker} ?&#10;Speaker: Professor B&#10;Content: Uh , first order for speech recognition , you say &quot; I don't care about the source &quot; .&#10;Speaker: PhD F" target="1. The process for drawing a system's parameters across time involves analyzing the characteristics of speech signals in both clean and noisy sentences, comparing attributes such as frequency content, amplitude, and duration of sounds within the sentences to understand how noise affects speech perception and develop mitigation methods. Features are extracted from the speech signals to create a statistical model that differentiates between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;2. The team is currently focused on finding suitable features for this task, with one suggestion being the maximum of the auto-correlation function to characterize voiced phonemes more effectively. They have not yet decided whether they will use a trained system or threshold-based system to implement these features in their statistical model.&#10;&#10;3. A noisy sentence is created by adding noise to a clean sentence, as demonstrated when Professor B mentioned that drawing the parameters across time for a clean sentence and then adding noise results in a similar (noisy) sentence. However, the specific method or techniques used to create the noisy sentence are not discussed in the provided transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Choosing a weak baseline: If the baseline is not robust or representative of the current state-of-the-art, comparing improvements against it might not provide accurate insights into the performance of new methods. This could lead to underestimating the true potential of novel approaches.&#10;&#10;2. Limited comparative evaluation: Reducing efforts in feature design may result in fewer comparisons with alternative features or methods, which could be crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;3. Incomplete VAD design: A simpler feature design might not capture all aspects required for a good Voice Activity Detection (VAD) system. Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments.&#10;&#10;4. Relying solely on the recognizer for VAD: While it is possible to use the whole recognizer to perform VAD, this approach may not always provide optimal results compared to dedicated VAD systems tailored to specific applications and signal types.&#10;&#10;5. Fairness concerns: It might be considered unfair to compare a simplified feature design to more complex approaches if additional baseline features are required to create a good VAD system. This could lead to an inaccurate assessment of the performance of novel methods, potentially favoring simpler techniques that do not fully address the problem at hand.&#10;&#10;To ensure fairness and accurate comparisons, it is essential to:&#10;- Choose a robust and representative baseline.&#10;- Perform thorough evaluations against various alternatives.&#10;- Consider all aspects required for a good VAD system, including complexity and real-time requirements." target="} so if you use other features then y But it 's just a question of what is your baseline . Right ? What is it that you 're supposed to do better than ?&#10;Speaker: PhD C&#10;Content: I g Yeah .&#10;Speaker: Professor B&#10;Content: And so having the baseline be the MFCC 's {pause} means that people could {pause} choose to pour their ener their effort into trying to do a really good VAD&#10;Speaker: PhD C&#10;Content: I don't s But they seem like two {pause} separate issues .&#10;Speaker: Professor B&#10;Content: or tryi They 're sort of separate .&#10;Speaker: PhD C&#10;Content: Right ? I mean {disfmarker}&#10;Speaker: Professor B&#10;Content: Unfortunately there 's coupling between them , which is part of what I think Stephane is getting to , is that {vocalsound} you can choose your features in such a way as to improve the VAD .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: And you also can choose your features in such a way as to prove {disfmarker} improve recognition . They may">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Choosing a weak baseline: If the baseline is not robust or representative of the current state-of-the-art, comparing improvements against it might not provide accurate insights into the performance of new methods. This could lead to underestimating the true potential of novel approaches.&#10;&#10;2. Limited comparative evaluation: Reducing efforts in feature design may result in fewer comparisons with alternative features or methods, which could be crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;3. Incomplete VAD design: A simpler feature design might not capture all aspects required for a good Voice Activity Detection (VAD) system. Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments.&#10;&#10;4. Relying solely on the recognizer for VAD: While it is possible to use the whole recognizer to perform VAD, this approach may not always provide optimal results compared to dedicated VAD systems tailored to specific applications and signal types.&#10;&#10;5. Fairness concerns: It might be considered unfair to compare a simplified feature design to more complex approaches if additional baseline features are required to create a good VAD system. This could lead to an inaccurate assessment of the performance of novel methods, potentially favoring simpler techniques that do not fully address the problem at hand.&#10;&#10;To ensure fairness and accurate comparisons, it is essential to:&#10;- Choose a robust and representative baseline.&#10;- Perform thorough evaluations against various alternatives.&#10;- Consider all aspects required for a good VAD system, including complexity and real-time requirements." target=" and as well as being a lot of work to do a good job on the feature {vocalsound} design ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: so&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: if we can {pause} cut down on that maybe we can make some progress .&#10;Speaker: PhD A&#10;Content: M Yeah .&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: But I guess perhaps {disfmarker} I don't know w {vocalsound} Yeah . Uh , yeah . Per - e s s someone told that perhaps it 's not fair to do that because the , um {disfmarker} to make a good VAD {pause} you don't have enough to {disfmarker} with the {disfmarker} the features that are {disfmarker} the baseline features . So {disfmarker} mmm , you need more features . So you really need to put more {disfmarker} more in the {disfmarker} in {disfmarker} in the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Choosing a weak baseline: If the baseline is not robust or representative of the current state-of-the-art, comparing improvements against it might not provide accurate insights into the performance of new methods. This could lead to underestimating the true potential of novel approaches.&#10;&#10;2. Limited comparative evaluation: Reducing efforts in feature design may result in fewer comparisons with alternative features or methods, which could be crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;3. Incomplete VAD design: A simpler feature design might not capture all aspects required for a good Voice Activity Detection (VAD) system. Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments.&#10;&#10;4. Relying solely on the recognizer for VAD: While it is possible to use the whole recognizer to perform VAD, this approach may not always provide optimal results compared to dedicated VAD systems tailored to specific applications and signal types.&#10;&#10;5. Fairness concerns: It might be considered unfair to compare a simplified feature design to more complex approaches if additional baseline features are required to create a good VAD system. This could lead to an inaccurate assessment of the performance of novel methods, potentially favoring simpler techniques that do not fully address the problem at hand.&#10;&#10;To ensure fairness and accurate comparisons, it is essential to:&#10;- Choose a robust and representative baseline.&#10;- Perform thorough evaluations against various alternatives.&#10;- Consider all aspects required for a good VAD system, including complexity and real-time requirements." target="Content: Good . Work to do .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: So whose VAD is {disfmarker} Is {disfmarker} is this a {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Uh , they didn't decide yet . I guess i this was one point of the conference call also , but {disfmarker} mmm , so I don't know . Um , but {disfmarker} Yeah .&#10;Speaker: Grad E&#10;Content: Oh .&#10;Speaker: Professor B&#10;Content: Oh , I {disfmarker} I think th that would be {vocalsound} good . I mean , it 's not that the design of the VAD isn't important , but it 's just that it {disfmarker} it {disfmarker} it does seem to be i uh , a lot of {pause} work to do a good job on {disfmarker} on that and as well as being a lot of work to do a good job on the feature {vocalsound} design ,&#10;Speaker: PhD A&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Choosing a weak baseline: If the baseline is not robust or representative of the current state-of-the-art, comparing improvements against it might not provide accurate insights into the performance of new methods. This could lead to underestimating the true potential of novel approaches.&#10;&#10;2. Limited comparative evaluation: Reducing efforts in feature design may result in fewer comparisons with alternative features or methods, which could be crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;3. Incomplete VAD design: A simpler feature design might not capture all aspects required for a good Voice Activity Detection (VAD) system. Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments.&#10;&#10;4. Relying solely on the recognizer for VAD: While it is possible to use the whole recognizer to perform VAD, this approach may not always provide optimal results compared to dedicated VAD systems tailored to specific applications and signal types.&#10;&#10;5. Fairness concerns: It might be considered unfair to compare a simplified feature design to more complex approaches if additional baseline features are required to create a good VAD system. This could lead to an inaccurate assessment of the performance of novel methods, potentially favoring simpler techniques that do not fully address the problem at hand.&#10;&#10;To ensure fairness and accurate comparisons, it is essential to:&#10;- Choose a robust and representative baseline.&#10;- Perform thorough evaluations against various alternatives.&#10;- Consider all aspects required for a good VAD system, including complexity and real-time requirements." target="1. It is potentially unfair or insufficient to base a good vocal sound design on just the baseline features because the chosen baseline might not be robust or representative of the current state-of-the-art, leading to underestimating the true potential of novel approaches. Additionally, limiting comparative evaluations by reducing efforts in feature design can result in fewer comparisons with alternative features or methods, which are crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;2. The benefits of incorporating additional features into the vocal sound design include:&#10;  a) Improved robustness and fairness: Adding more features to the design ensures that the evaluation is based on a comprehensive comparison, reducing potential biases towards simpler techniques that may not fully address the problem at hand.&#10;  b) Enhanced understanding of strengths and weaknesses: Comparing the new method against various alternatives provides insights into its performance relative to different types of features and methods, aiding in identifying the most suitable approach for specific scenarios or environments.&#10;  c) Better generalization: Including diverse features can help average out fine details and noise in the data, reducing overfitting and improving the model's performance on unseen data.&#10;  d) Capturing all aspects required for a good VAD system: Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments, ensuring that the system can effectively handle various challenges associated with voice activity detection.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Choosing a weak baseline: If the baseline is not robust or representative of the current state-of-the-art, comparing improvements against it might not provide accurate insights into the performance of new methods. This could lead to underestimating the true potential of novel approaches.&#10;&#10;2. Limited comparative evaluation: Reducing efforts in feature design may result in fewer comparisons with alternative features or methods, which could be crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;3. Incomplete VAD design: A simpler feature design might not capture all aspects required for a good Voice Activity Detection (VAD) system. Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments.&#10;&#10;4. Relying solely on the recognizer for VAD: While it is possible to use the whole recognizer to perform VAD, this approach may not always provide optimal results compared to dedicated VAD systems tailored to specific applications and signal types.&#10;&#10;5. Fairness concerns: It might be considered unfair to compare a simplified feature design to more complex approaches if additional baseline features are required to create a good VAD system. This could lead to an inaccurate assessment of the performance of novel methods, potentially favoring simpler techniques that do not fully address the problem at hand.&#10;&#10;To ensure fairness and accurate comparisons, it is essential to:&#10;- Choose a robust and representative baseline.&#10;- Perform thorough evaluations against various alternatives.&#10;- Consider all aspects required for a good VAD system, including complexity and real-time requirements." target="The results of implementing VAD, noise compensation (specifically spectral subtraction), and normalization (on-line normalization) techniques on the baseline experiments from OGI since last week showed improved performance when using these additional features. The comparison was made between two groups of proposals, one with Voice Activity Detection (VAD) and the other without. It was found that the group with VAD performed better than the group without VAD. This demonstrates that incorporating these techniques can lead to improved robustness, fairness, understanding of strengths and weaknesses, better generalization, and more effective handling of complex scenarios or noisy environments in VAD systems. However, it was noted that online normalization did not provide further improvements when spectral subtraction was already in use. The relationship between these findings and a previous issue related to musical tones was unclear due to differences in the spectral subtraction approaches used.">
      <data key="d0">1</data>
    </edge>
    <edge source="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments." target="The speakers, who appear to be discussing the evaluation and improvement of a signal processing system, are expressing uncertainty about the final application of the system and whether it will need to handle more difficult cases with high accuracy. Professor B mentions that if the predominate use of the system is in challenging scenarios, current methods may not be sufficient. However, in better cases, the system could potentially work well with some error recovery.&#10;&#10;Later in the discussion, the speakers consider a variance calculation done on a frequency range of 200 Hz to 1500 Hz, which they believe is more logical for their analysis. They also discuss different methods for weighting error rates obtained from various languages and conditions, with some arguing that it would be better to combine error rates before computing improvements.&#10;&#10;In summary, the speakers are discussing the uncertainty of the system's primary use, the potential for reasonable error recovery in better cases, and different approaches to weighing error rates for improved performance in various scenarios.">
      <data key="d0">1</data>
    </edge>
    <edge source="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments." target="1. The process for drawing a system's parameters across time involves analyzing the characteristics of speech signals in both clean and noisy sentences, comparing attributes such as frequency content, amplitude, and duration of sounds within the sentences to understand how noise affects speech perception and develop mitigation methods. Features are extracted from the speech signals to create a statistical model that differentiates between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;2. The team is currently focused on finding suitable features for this task, with one suggestion being the maximum of the auto-correlation function to characterize voiced phonemes more effectively. They have not yet decided whether they will use a trained system or threshold-based system to implement these features in their statistical model.&#10;&#10;3. A noisy sentence is created by adding noise to a clean sentence, as demonstrated when Professor B mentioned that drawing the parameters across time for a clean sentence and then adding noise results in a similar (noisy) sentence. However, the specific method or techniques used to create the noisy sentence are not discussed in the provided transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments." target="1. Length and number of periods: A short frame contains only two periods of the signal, while the original frame contains more depending on the signal's characteristics. This difference is due to the time-frequency trade-off, which requires shorter frames for better frequency resolution but longer frames for better time resolution.&#10;&#10;2. FFT size (number of points in the Fast Fourier Transform): The speakers also discuss differences in FFT size between the short frame and the original frame. A larger FFT size results in better frequency resolution but coarser time resolution, whereas a smaller FFT size has the opposite effect.&#10;&#10;3. Robustness to noise: The speakers mention that using a shorter frame can help improve robustness to noise in signal processing applications like speech analysis. When comparing the same frame from noisy and clean utterances, the difference can be used to better understand and handle noise.">
      <data key="d0">1</data>
    </edge>
    <edge source="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments." target="1. It is potentially unfair or insufficient to base a good vocal sound design on just the baseline features because the chosen baseline might not be robust or representative of the current state-of-the-art, leading to underestimating the true potential of novel approaches. Additionally, limiting comparative evaluations by reducing efforts in feature design can result in fewer comparisons with alternative features or methods, which are crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;2. The benefits of incorporating additional features into the vocal sound design include:&#10;  a) Improved robustness and fairness: Adding more features to the design ensures that the evaluation is based on a comprehensive comparison, reducing potential biases towards simpler techniques that may not fully address the problem at hand.&#10;  b) Enhanced understanding of strengths and weaknesses: Comparing the new method against various alternatives provides insights into its performance relative to different types of features and methods, aiding in identifying the most suitable approach for specific scenarios or environments.&#10;  c) Better generalization: Including diverse features can help average out fine details and noise in the data, reducing overfitting and improving the model's performance on unseen data.&#10;  d) Capturing all aspects required for a good VAD system: Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments, ensuring that the system can effectively handle various challenges associated with voice activity detection.">
      <data key="d0">1</data>
    </edge>
    <edge source="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments." target="Based on the given transcript, when transitioning from a well-matched case to a mismatched case in terms of speaker features, there is a noticeable difference in performance. Specifically, the system performs better in the well-matched case but seems to degrade slightly for mismatched and highly-mismatched conditions when a neural network is implemented. This implies that the current weighting setup may favor the well-matched case, which might be an overestimation of its true performance in real-world scenarios where data is often more diverse and less controlled.&#10;&#10;As for the impact on vocal sound design and feature selection, it is essential to consider various techniques and features beyond simple baselines or MFCC (Mel Frequency Cepstral Coefficients) due to their limited pitch-related information. Including additional features can improve robustness, fairness, understanding of strengths and weaknesses, better generalization, and overall performance in complex scenarios or noisy environments.&#10;&#10;To address the issue of varying performance across different cases, one should investigate various pitch detection algorithms suitable for the specific application and signal type. After integrating the chosen algorithm into the existing front-end processing system, evaluate its performance by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates. Fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements. Additionally, when selecting features for vocal sound design, consider a comprehensive comparison involving various techniques and baselines to ensure robustness and fairness in performance evaluation.">
      <data key="d0">1</data>
    </edge>
    <edge source="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments." target="The results of implementing VAD, noise compensation (specifically spectral subtraction), and normalization (on-line normalization) techniques on the baseline experiments from OGI since last week showed improved performance when using these additional features. The comparison was made between two groups of proposals, one with Voice Activity Detection (VAD) and the other without. It was found that the group with VAD performed better than the group without VAD. This demonstrates that incorporating these techniques can lead to improved robustness, fairness, understanding of strengths and weaknesses, better generalization, and more effective handling of complex scenarios or noisy environments in VAD systems. However, it was noted that online normalization did not provide further improvements when spectral subtraction was already in use. The relationship between these findings and a previous issue related to musical tones was unclear due to differences in the spectral subtraction approaches used.">
      <data key="d0">1</data>
    </edge>
    <edge source="indeed, the discussion revolves around comparing an original version of a signal to a smoothed version of the same signal, specifically focusing on the analysis of low frequencies. This comparison is made to explore statistical measures and improve robustness in feature vectors for better performance in noisy environments." target="1. The medium-mismatch condition refers to a situation where the model is trained on data collected in low noise conditions and at low speeds (e.g., stopped car), but during testing, the data is gathered using a distant microphone in high-speed conditions (e.g., on a highway). This creates a mismatch between the training and testing conditions, making it more challenging for the model to maintain good performance.&#10;&#10;Based on the discussion provided, there are two main aspects being considered:&#10;&#10;- The use of robust features that perform well even with gentle degradation in testing conditions different from training.&#10;- A comparison of original signals to smoothed versions, focusing on low frequencies and exploring statistical measures for improving performance in noisy environments.&#10;&#10;The medium-mismatch condition is introduced as an example of a realistic scenario that requires the model to be robust against changes in car conditions, specifically microphone placement (near vs. far) and speed (low vs. high). This is relevant when discussing potential improvements for front-end processing, such as adding a pitch detector that better handles these differences between training and testing conditions.&#10;&#10;In summary, the medium-mismatch condition involves training on low noisy conditions with one microphone placement but testing with a far microphone in higher speed conditions to assess how well the model can handle real-world variability.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, who appear to be discussing the evaluation and improvement of a signal processing system, are expressing uncertainty about the final application of the system and whether it will need to handle more difficult cases with high accuracy. Professor B mentions that if the predominate use of the system is in challenging scenarios, current methods may not be sufficient. However, in better cases, the system could potentially work well with some error recovery.&#10;&#10;Later in the discussion, the speakers consider a variance calculation done on a frequency range of 200 Hz to 1500 Hz, which they believe is more logical for their analysis. They also discuss different methods for weighting error rates obtained from various languages and conditions, with some arguing that it would be better to combine error rates before computing improvements.&#10;&#10;In summary, the speakers are discussing the uncertainty of the system's primary use, the potential for reasonable error recovery in better cases, and different approaches to weighing error rates for improved performance in various scenarios." target="marker} So , bu&#10;Speaker: PhD C&#10;Content: It sounds like they don't really have a good idea about what the final application is gonna be .&#10;Speaker: PhD A&#10;Content: l de fff ! Mmm .&#10;Speaker: Professor B&#10;Content: Well , you know , the {disfmarker} the thing is {vocalsound} that if you look at the numbers on the {disfmarker} on the more difficult cases , {vocalsound} um , if you really believe that was gonna be the predominant use , {vocalsound} none of this would be good enough .&#10;Speaker: PhD A&#10;Content: Yeah . Mmm . Yeah .&#10;Speaker: Professor B&#10;Content: Nothing anybody 's {disfmarker}&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: whereas {vocalsound} you sort of with some reasonable error recovery could imagine in the better cases that these {disfmarker} these systems working . So , um , I think the hope would be that it would {disfmarker} {vocalsound} uh , it would work well {">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, who appear to be discussing the evaluation and improvement of a signal processing system, are expressing uncertainty about the final application of the system and whether it will need to handle more difficult cases with high accuracy. Professor B mentions that if the predominate use of the system is in challenging scenarios, current methods may not be sufficient. However, in better cases, the system could potentially work well with some error recovery.&#10;&#10;Later in the discussion, the speakers consider a variance calculation done on a frequency range of 200 Hz to 1500 Hz, which they believe is more logical for their analysis. They also discuss different methods for weighting error rates obtained from various languages and conditions, with some arguing that it would be better to combine error rates before computing improvements.&#10;&#10;In summary, the speakers are discussing the uncertainty of the system's primary use, the potential for reasonable error recovery in better cases, and different approaches to weighing error rates for improved performance in various scenarios." target=" than the previous standard .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , um , so they said &quot; how much is significantly better ? what do you {disfmarker} ? &quot; And {disfmarker} and so they said &quot; well , {vocalsound} you know , you should have half the errors , &quot; or something , &quot; that you had before &quot; .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Hmm .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So it 's , uh , But it does seem like&#10;Speaker: PhD C&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: i i it does seem like it 's more logical to combine them first and then do the {disfmarker}&#10;Speaker: PhD A&#10;Content: Combine error rates and then {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Yeah . Well {disfmarker}&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, who appear to be discussing the evaluation and improvement of a signal processing system, are expressing uncertainty about the final application of the system and whether it will need to handle more difficult cases with high accuracy. Professor B mentions that if the predominate use of the system is in challenging scenarios, current methods may not be sufficient. However, in better cases, the system could potentially work well with some error recovery.&#10;&#10;Later in the discussion, the speakers consider a variance calculation done on a frequency range of 200 Hz to 1500 Hz, which they believe is more logical for their analysis. They also discuss different methods for weighting error rates obtained from various languages and conditions, with some arguing that it would be better to combine error rates before computing improvements.&#10;&#10;In summary, the speakers are discussing the uncertainty of the system's primary use, the potential for reasonable error recovery in better cases, and different approaches to weighing error rates for improved performance in various scenarios." target="Professionor B expresses that making any policy change in meetings is tricky because everyone has different perspectives and opinions. They imply that reaching a consensus or agreement can be difficult due to these varied viewpoints.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, who appear to be discussing the evaluation and improvement of a signal processing system, are expressing uncertainty about the final application of the system and whether it will need to handle more difficult cases with high accuracy. Professor B mentions that if the predominate use of the system is in challenging scenarios, current methods may not be sufficient. However, in better cases, the system could potentially work well with some error recovery.&#10;&#10;Later in the discussion, the speakers consider a variance calculation done on a frequency range of 200 Hz to 1500 Hz, which they believe is more logical for their analysis. They also discuss different methods for weighting error rates obtained from various languages and conditions, with some arguing that it would be better to combine error rates before computing improvements.&#10;&#10;In summary, the speakers are discussing the uncertainty of the system's primary use, the potential for reasonable error recovery in better cases, and different approaches to weighing error rates for improved performance in various scenarios." target="1. The process for drawing a system's parameters across time involves analyzing the characteristics of speech signals in both clean and noisy sentences, comparing attributes such as frequency content, amplitude, and duration of sounds within the sentences to understand how noise affects speech perception and develop mitigation methods. Features are extracted from the speech signals to create a statistical model that differentiates between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;2. The team is currently focused on finding suitable features for this task, with one suggestion being the maximum of the auto-correlation function to characterize voiced phonemes more effectively. They have not yet decided whether they will use a trained system or threshold-based system to implement these features in their statistical model.&#10;&#10;3. A noisy sentence is created by adding noise to a clean sentence, as demonstrated when Professor B mentioned that drawing the parameters across time for a clean sentence and then adding noise results in a similar (noisy) sentence. However, the specific method or techniques used to create the noisy sentence are not discussed in the provided transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, who appear to be discussing the evaluation and improvement of a signal processing system, are expressing uncertainty about the final application of the system and whether it will need to handle more difficult cases with high accuracy. Professor B mentions that if the predominate use of the system is in challenging scenarios, current methods may not be sufficient. However, in better cases, the system could potentially work well with some error recovery.&#10;&#10;Later in the discussion, the speakers consider a variance calculation done on a frequency range of 200 Hz to 1500 Hz, which they believe is more logical for their analysis. They also discuss different methods for weighting error rates obtained from various languages and conditions, with some arguing that it would be better to combine error rates before computing improvements.&#10;&#10;In summary, the speakers are discussing the uncertainty of the system's primary use, the potential for reasonable error recovery in better cases, and different approaches to weighing error rates for improved performance in various scenarios." target="PHP A considers the new system (proposal two) to be better than the previous one (proposal one) mainly due to two reasons: first, it has a 64 hertz cut-off with clean downsampling, and second, it includes a good Voice Activity Detector (VAD). In terms of latency, PHP A mentions that the latencies are shorter in the new system.&#10;&#10;When comparing proposal two to proposal one, PhD A considers the new system to be &quot;a little worse&quot; only when a neural network is added, but it is still better overall due to its shorter latencies and other improvements made during the transition from proposal one to proposal two.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professionor B expresses that making any policy change in meetings is tricky because everyone has different perspectives and opinions. They imply that reaching a consensus or agreement can be difficult due to these varied viewpoints." target=": Professor B&#10;Content: Just say &quot; OK , in the {disfmarker} in the highly - matched case this is what happens , in the {disfmarker} {vocalsound} m the , uh {disfmarker} this other m medium if this happens , in the highly - mismatched {pause} that happens &quot; .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh , you should see , uh , a gentle degradation {pause} through that .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Um . But {disfmarker} I don't know .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I think that {disfmarker} that {disfmarker} I {disfmarker} I {disfmarker} I gather that in these meetings it 's {disfmarker} it 's really tricky to make anything {vocalsound} ac {vocalsound} make any {comment} policy change because {vocalsound} {vocalsound} everybody has">
      <data key="d0">1</data>
    </edge>
    <edge source="Professionor B expresses that making any policy change in meetings is tricky because everyone has different perspectives and opinions. They imply that reaching a consensus or agreement can be difficult due to these varied viewpoints." target=": Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . OK .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: What else ?&#10;Speaker: PhD A&#10;Content: Uh . {vocalsound} Yeah , that 's all . So we 'll perhaps {vocalsound} {vocalsound} {vocalsound} try to convince OGI people to use the new {disfmarker} {vocalsound} the new filters and {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: OK . Uh , has {disfmarker} has anything happened yet on this business of having some sort of standard , uh , source ,&#10;Speaker: PhD A&#10;Content: Uh , not yet&#10;Speaker: Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: but I wi I will {vocalsound} call them and {disfmarker}&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process for drawing a system's parameters across time involves analyzing the characteristics of speech signals in both clean and noisy sentences, comparing attributes such as frequency content, amplitude, and duration of sounds within the sentences to understand how noise affects speech perception and develop mitigation methods. Features are extracted from the speech signals to create a statistical model that differentiates between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;2. The team is currently focused on finding suitable features for this task, with one suggestion being the maximum of the auto-correlation function to characterize voiced phonemes more effectively. They have not yet decided whether they will use a trained system or threshold-based system to implement these features in their statistical model.&#10;&#10;3. A noisy sentence is created by adding noise to a clean sentence, as demonstrated when Professor B mentioned that drawing the parameters across time for a clean sentence and then adding noise results in a similar (noisy) sentence. However, the specific method or techniques used to create the noisy sentence are not discussed in the provided transcripts." target="1. When Professor B suggests that imperfect distinctions between voiced and unvoiced sounds might not be a significant issue, they are implying that the primary value in this analysis comes from obtaining a general characterization of the speech signal based on our intuition about these categories. This rough characterization can provide insights into the raw data, even if it does not offer a clear-cut distinction between voiced and unvoiced sounds due to overlaps and complexities in speech.&#10;2. The professor emphasizes that relying on this basic intuition might be more valuable than using overly refined or smoothed versions of the signal since real-world data, such as noise, can impact the accuracy of any speech recognition system. Consequently, understanding how noise affects speech perception and developing methods to mitigate its effects is essential for improving speech recognition systems' performance in various environments.&#10;3. The team is currently focused on selecting appropriate features for their task, with one possibility being the maximum of the auto-correlation function to better characterize voiced phonemes. However, they have not yet decided whether to use a trained system or threshold-based system to implement these features in their statistical model.&#10;4. Professor B implies that even if the &quot;world's best voiced-unvoiced classifier&quot; does not perfectly distinguish between these categories, trying different statistical characterizations of the difference based on raw data may still yield valuable information that a speech recognition system can utilize.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process for drawing a system's parameters across time involves analyzing the characteristics of speech signals in both clean and noisy sentences, comparing attributes such as frequency content, amplitude, and duration of sounds within the sentences to understand how noise affects speech perception and develop mitigation methods. Features are extracted from the speech signals to create a statistical model that differentiates between clean and noisy sentences, potentially improving speech recognition systems' performance in various environments.&#10;&#10;2. The team is currently focused on finding suitable features for this task, with one suggestion being the maximum of the auto-correlation function to characterize voiced phonemes more effectively. They have not yet decided whether they will use a trained system or threshold-based system to implement these features in their statistical model.&#10;&#10;3. A noisy sentence is created by adding noise to a clean sentence, as demonstrated when Professor B mentioned that drawing the parameters across time for a clean sentence and then adding noise results in a similar (noisy) sentence. However, the specific method or techniques used to create the noisy sentence are not discussed in the provided transcripts." target="1. It is potentially unfair or insufficient to base a good vocal sound design on just the baseline features because the chosen baseline might not be robust or representative of the current state-of-the-art, leading to underestimating the true potential of novel approaches. Additionally, limiting comparative evaluations by reducing efforts in feature design can result in fewer comparisons with alternative features or methods, which are crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;2. The benefits of incorporating additional features into the vocal sound design include:&#10;  a) Improved robustness and fairness: Adding more features to the design ensures that the evaluation is based on a comprehensive comparison, reducing potential biases towards simpler techniques that may not fully address the problem at hand.&#10;  b) Enhanced understanding of strengths and weaknesses: Comparing the new method against various alternatives provides insights into its performance relative to different types of features and methods, aiding in identifying the most suitable approach for specific scenarios or environments.&#10;  c) Better generalization: Including diverse features can help average out fine details and noise in the data, reducing overfitting and improving the model's performance on unseen data.&#10;  d) Capturing all aspects required for a good VAD system: Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments, ensuring that the system can effectively handle various challenges associated with voice activity detection.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Length and number of periods: A short frame contains only two periods of the signal, while the original frame contains more depending on the signal's characteristics. This difference is due to the time-frequency trade-off, which requires shorter frames for better frequency resolution but longer frames for better time resolution.&#10;&#10;2. FFT size (number of points in the Fast Fourier Transform): The speakers also discuss differences in FFT size between the short frame and the original frame. A larger FFT size results in better frequency resolution but coarser time resolution, whereas a smaller FFT size has the opposite effect.&#10;&#10;3. Robustness to noise: The speakers mention that using a shorter frame can help improve robustness to noise in signal processing applications like speech analysis. When comparing the same frame from noisy and clean utterances, the difference can be used to better understand and handle noise." target="mm .&#10;Speaker: PhD C&#10;Content: Oh . Oh , so this i is this the difference here , for that ?&#10;Speaker: PhD F&#10;Content: No . This is the signal . This is the signal .&#10;Speaker: PhD A&#10;Content: I see that . Oh , yeah .&#10;Speaker: PhD F&#10;Content: The frame .&#10;Speaker: PhD C&#10;Content: Oh , that 's the f the original .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: This is the fra the original frame .&#10;Speaker: PhD A&#10;Content: So with a short frame basically you have only two periods&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and it 's not {disfmarker} not enough to {disfmarker} to have this kind of neat things .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: But {disfmarker}&#10;Speaker: PhD F&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Length and number of periods: A short frame contains only two periods of the signal, while the original frame contains more depending on the signal's characteristics. This difference is due to the time-frequency trade-off, which requires shorter frames for better frequency resolution but longer frames for better time resolution.&#10;&#10;2. FFT size (number of points in the Fast Fourier Transform): The speakers also discuss differences in FFT size between the short frame and the original frame. A larger FFT size results in better frequency resolution but coarser time resolution, whereas a smaller FFT size has the opposite effect.&#10;&#10;3. Robustness to noise: The speakers mention that using a shorter frame can help improve robustness to noise in signal processing applications like speech analysis. When comparing the same frame from noisy and clean utterances, the difference can be used to better understand and handle noise." target=" see this periodic structure ,&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: because of the first lobe of {disfmarker} of each {disfmarker} each of the harmonics .&#10;Speaker: PhD C&#10;Content: So this one inclu is a longer {disfmarker} Ah .&#10;Speaker: PhD A&#10;Content: So , this is like {disfmarker} yeah , fifty milliseconds or something like that .&#10;Speaker: PhD F&#10;Content: Fifty millis Yeah .&#10;Speaker: PhD A&#10;Content: Yeah , but it 's the same frame and {disfmarker}&#10;Speaker: PhD C&#10;Content: Oh , it 's that time - frequency trade - off thing .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Right ? I see . Yeah .&#10;Speaker: PhD A&#10;Content: So , yeah .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: Oh . Oh , so this i is this the difference here , for that ?&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Length and number of periods: A short frame contains only two periods of the signal, while the original frame contains more depending on the signal's characteristics. This difference is due to the time-frequency trade-off, which requires shorter frames for better frequency resolution but longer frames for better time resolution.&#10;&#10;2. FFT size (number of points in the Fast Fourier Transform): The speakers also discuss differences in FFT size between the short frame and the original frame. A larger FFT size results in better frequency resolution but coarser time resolution, whereas a smaller FFT size has the opposite effect.&#10;&#10;3. Robustness to noise: The speakers mention that using a shorter frame can help improve robustness to noise in signal processing applications like speech analysis. When comparing the same frame from noisy and clean utterances, the difference can be used to better understand and handle noise." target="Speaker: PhD C&#10;Content: Oh , OK&#10;Speaker: PhD F&#10;Content: But they are a difference .&#10;Speaker: PhD A&#10;Content: Yeah , that 's {disfmarker}&#10;Speaker: PhD F&#10;Content: Because here the FFT is only with {vocalsound} two hundred fifty - six point&#10;Speaker: PhD C&#10;Content: Oh .&#10;Speaker: PhD F&#10;Content: and this is with five hundred {pause} twelve .&#10;Speaker: PhD A&#10;Content: Yeah . This is kind of inter interesting also&#10;Speaker: PhD C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: because if we use the standard , {vocalsound} uh , frame length of {disfmarker} of , like , twenty - five milliseconds , {vocalsound} um , {vocalsound} what happens is that for low - pitched voiced , because of the frame length , y you don't really have {disfmarker} {vocalsound} you don't clearly see this periodic structure ,&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: because of the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When Professor B suggests that imperfect distinctions between voiced and unvoiced sounds might not be a significant issue, they are implying that the primary value in this analysis comes from obtaining a general characterization of the speech signal based on our intuition about these categories. This rough characterization can provide insights into the raw data, even if it does not offer a clear-cut distinction between voiced and unvoiced sounds due to overlaps and complexities in speech.&#10;2. The professor emphasizes that relying on this basic intuition might be more valuable than using overly refined or smoothed versions of the signal since real-world data, such as noise, can impact the accuracy of any speech recognition system. Consequently, understanding how noise affects speech perception and developing methods to mitigate its effects is essential for improving speech recognition systems' performance in various environments.&#10;3. The team is currently focused on selecting appropriate features for their task, with one possibility being the maximum of the auto-correlation function to better characterize voiced phonemes. However, they have not yet decided whether to use a trained system or threshold-based system to implement these features in their statistical model.&#10;4. Professor B implies that even if the &quot;world's best voiced-unvoiced classifier&quot; does not perfectly distinguish between these categories, trying different statistical characterizations of the difference based on raw data may still yield valuable information that a speech recognition system can utilize." target="1. It is potentially unfair or insufficient to base a good vocal sound design on just the baseline features because the chosen baseline might not be robust or representative of the current state-of-the-art, leading to underestimating the true potential of novel approaches. Additionally, limiting comparative evaluations by reducing efforts in feature design can result in fewer comparisons with alternative features or methods, which are crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;2. The benefits of incorporating additional features into the vocal sound design include:&#10;  a) Improved robustness and fairness: Adding more features to the design ensures that the evaluation is based on a comprehensive comparison, reducing potential biases towards simpler techniques that may not fully address the problem at hand.&#10;  b) Enhanced understanding of strengths and weaknesses: Comparing the new method against various alternatives provides insights into its performance relative to different types of features and methods, aiding in identifying the most suitable approach for specific scenarios or environments.&#10;  c) Better generalization: Including diverse features can help average out fine details and noise in the data, reducing overfitting and improving the model's performance on unseen data.&#10;  d) Capturing all aspects required for a good VAD system: Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments, ensuring that the system can effectively handle various challenges associated with voice activity detection.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. It is potentially unfair or insufficient to base a good vocal sound design on just the baseline features because the chosen baseline might not be robust or representative of the current state-of-the-art, leading to underestimating the true potential of novel approaches. Additionally, limiting comparative evaluations by reducing efforts in feature design can result in fewer comparisons with alternative features or methods, which are crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;2. The benefits of incorporating additional features into the vocal sound design include:&#10;  a) Improved robustness and fairness: Adding more features to the design ensures that the evaluation is based on a comprehensive comparison, reducing potential biases towards simpler techniques that may not fully address the problem at hand.&#10;  b) Enhanced understanding of strengths and weaknesses: Comparing the new method against various alternatives provides insights into its performance relative to different types of features and methods, aiding in identifying the most suitable approach for specific scenarios or environments.&#10;  c) Better generalization: Including diverse features can help average out fine details and noise in the data, reducing overfitting and improving the model's performance on unseen data.&#10;  d) Capturing all aspects required for a good VAD system: Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments, ensuring that the system can effectively handle various challenges associated with voice activity detection." target="Based on the given transcript, when transitioning from a well-matched case to a mismatched case in terms of speaker features, there is a noticeable difference in performance. Specifically, the system performs better in the well-matched case but seems to degrade slightly for mismatched and highly-mismatched conditions when a neural network is implemented. This implies that the current weighting setup may favor the well-matched case, which might be an overestimation of its true performance in real-world scenarios where data is often more diverse and less controlled.&#10;&#10;As for the impact on vocal sound design and feature selection, it is essential to consider various techniques and features beyond simple baselines or MFCC (Mel Frequency Cepstral Coefficients) due to their limited pitch-related information. Including additional features can improve robustness, fairness, understanding of strengths and weaknesses, better generalization, and overall performance in complex scenarios or noisy environments.&#10;&#10;To address the issue of varying performance across different cases, one should investigate various pitch detection algorithms suitable for the specific application and signal type. After integrating the chosen algorithm into the existing front-end processing system, evaluate its performance by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates. Fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements. Additionally, when selecting features for vocal sound design, consider a comprehensive comparison involving various techniques and baselines to ensure robustness and fairness in performance evaluation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. It is potentially unfair or insufficient to base a good vocal sound design on just the baseline features because the chosen baseline might not be robust or representative of the current state-of-the-art, leading to underestimating the true potential of novel approaches. Additionally, limiting comparative evaluations by reducing efforts in feature design can result in fewer comparisons with alternative features or methods, which are crucial for understanding the strengths and weaknesses of different techniques.&#10;&#10;2. The benefits of incorporating additional features into the vocal sound design include:&#10;  a) Improved robustness and fairness: Adding more features to the design ensures that the evaluation is based on a comprehensive comparison, reducing potential biases towards simpler techniques that may not fully address the problem at hand.&#10;  b) Enhanced understanding of strengths and weaknesses: Comparing the new method against various alternatives provides insights into its performance relative to different types of features and methods, aiding in identifying the most suitable approach for specific scenarios or environments.&#10;  c) Better generalization: Including diverse features can help average out fine details and noise in the data, reducing overfitting and improving the model's performance on unseen data.&#10;  d) Capturing all aspects required for a good VAD system: Additional baseline features might be necessary to achieve satisfactory performance in complex scenarios or noisy environments, ensuring that the system can effectively handle various challenges associated with voice activity detection." target="The results of implementing VAD, noise compensation (specifically spectral subtraction), and normalization (on-line normalization) techniques on the baseline experiments from OGI since last week showed improved performance when using these additional features. The comparison was made between two groups of proposals, one with Voice Activity Detection (VAD) and the other without. It was found that the group with VAD performed better than the group without VAD. This demonstrates that incorporating these techniques can lead to improved robustness, fairness, understanding of strengths and weaknesses, better generalization, and more effective handling of complex scenarios or noisy environments in VAD systems. However, it was noted that online normalization did not provide further improvements when spectral subtraction was already in use. The relationship between these findings and a previous issue related to musical tones was unclear due to differences in the spectral subtraction approaches used.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the given transcript, when transitioning from a well-matched case to a mismatched case in terms of speaker features, there is a noticeable difference in performance. Specifically, the system performs better in the well-matched case but seems to degrade slightly for mismatched and highly-mismatched conditions when a neural network is implemented. This implies that the current weighting setup may favor the well-matched case, which might be an overestimation of its true performance in real-world scenarios where data is often more diverse and less controlled.&#10;&#10;As for the impact on vocal sound design and feature selection, it is essential to consider various techniques and features beyond simple baselines or MFCC (Mel Frequency Cepstral Coefficients) due to their limited pitch-related information. Including additional features can improve robustness, fairness, understanding of strengths and weaknesses, better generalization, and overall performance in complex scenarios or noisy environments.&#10;&#10;To address the issue of varying performance across different cases, one should investigate various pitch detection algorithms suitable for the specific application and signal type. After integrating the chosen algorithm into the existing front-end processing system, evaluate its performance by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates. Fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements. Additionally, when selecting features for vocal sound design, consider a comprehensive comparison involving various techniques and baselines to ensure robustness and fairness in performance evaluation." target=" Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Because we 're still testing . So we have the result for , {vocalsound} uh , just the features&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: and we are currently testing with putting the neural network in the KLT . Um , it seems to improve on the well - matched case , um , {vocalsound} but it 's a little bit worse on the mismatch and highly - mismatched {disfmarker} I mean when we put the neural network . And with the current weighting I think it 's sh it will be better because the well - matched case is better . Mmm .&#10;Speaker: Professor B&#10;Content: But how much worse {disfmarker} since the weighting might change {disfmarker} how {disfmarker} how much worse is it on the other conditions , when you say it 's a little worse ?&#10;Speaker: PhD A&#10;Content: It 's like , uh ,">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the given transcript, when transitioning from a well-matched case to a mismatched case in terms of speaker features, there is a noticeable difference in performance. Specifically, the system performs better in the well-matched case but seems to degrade slightly for mismatched and highly-mismatched conditions when a neural network is implemented. This implies that the current weighting setup may favor the well-matched case, which might be an overestimation of its true performance in real-world scenarios where data is often more diverse and less controlled.&#10;&#10;As for the impact on vocal sound design and feature selection, it is essential to consider various techniques and features beyond simple baselines or MFCC (Mel Frequency Cepstral Coefficients) due to their limited pitch-related information. Including additional features can improve robustness, fairness, understanding of strengths and weaknesses, better generalization, and overall performance in complex scenarios or noisy environments.&#10;&#10;To address the issue of varying performance across different cases, one should investigate various pitch detection algorithms suitable for the specific application and signal type. After integrating the chosen algorithm into the existing front-end processing system, evaluate its performance by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates. Fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements. Additionally, when selecting features for vocal sound design, consider a comprehensive comparison involving various techniques and baselines to ensure robustness and fairness in performance evaluation." target=" , are you gonna have w uh , uh , examples with the windows open , half open , full open ? Going seventy , sixty , fifty , forty miles an hour ? On what kind of roads ?&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: With what passing you ? With {disfmarker} uh , I mean ,&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: I {disfmarker} I {disfmarker} I think that you could make the opposite argument that the well - matched case is a fantasy .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: You know , so ,&#10;Speaker: Grad E&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: I think the thing is is that if you look at the well - matched case versus the po you know , the {disfmarker} the medium and the {disfmarker} and the fo and then the mismatched case , {vocalsound} um , we 're seeing really , really big differences in performance">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the given transcript, when transitioning from a well-matched case to a mismatched case in terms of speaker features, there is a noticeable difference in performance. Specifically, the system performs better in the well-matched case but seems to degrade slightly for mismatched and highly-mismatched conditions when a neural network is implemented. This implies that the current weighting setup may favor the well-matched case, which might be an overestimation of its true performance in real-world scenarios where data is often more diverse and less controlled.&#10;&#10;As for the impact on vocal sound design and feature selection, it is essential to consider various techniques and features beyond simple baselines or MFCC (Mel Frequency Cepstral Coefficients) due to their limited pitch-related information. Including additional features can improve robustness, fairness, understanding of strengths and weaknesses, better generalization, and overall performance in complex scenarios or noisy environments.&#10;&#10;To address the issue of varying performance across different cases, one should investigate various pitch detection algorithms suitable for the specific application and signal type. After integrating the chosen algorithm into the existing front-end processing system, evaluate its performance by comparing it to the previous MFCC-based system using appropriate metrics like accuracy or error rates. Fine-tune and update the pitch detection algorithm to accommodate different signal characteristics or changing requirements. Additionally, when selecting features for vocal sound design, consider a comprehensive comparison involving various techniques and baselines to ensure robustness and fairness in performance evaluation." target="Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Yeah . Well {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: But there is this {disfmarker} this {disfmarker} is this still this problem of weights . When {disfmarker} when you combine error rate it tends to {pause} give more importance to the difficult cases , and some people think that {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , yeah ?&#10;Speaker: PhD A&#10;Content: well , they have different , {vocalsound} um , opinions about this . Some people think that {vocalsound} it 's more important to look at {disfmarker} {vocalsound} to have ten percent imp relative improvement on {pause} well - matched case than to have fifty percent on the m mismatched , and other people think that it 's more important to improve a lot on the mismatch and {disfmarker} So , bu&#10;Speaker: PhD C&#10;Content: It sounds like they don't really have a good idea about what the final application is">
      <data key="d0">1</data>
    </edge>
    <edge source=" Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Because we 're still testing . So we have the result for , {vocalsound} uh , just the features&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: and we are currently testing with putting the neural network in the KLT . Um , it seems to improve on the well - matched case , um , {vocalsound} but it 's a little bit worse on the mismatch and highly - mismatched {disfmarker} I mean when we put the neural network . And with the current weighting I think it 's sh it will be better because the well - matched case is better . Mmm .&#10;Speaker: Professor B&#10;Content: But how much worse {disfmarker} since the weighting might change {disfmarker} how {disfmarker} how much worse is it on the other conditions , when you say it 's a little worse ?&#10;Speaker: PhD A&#10;Content: It 's like , uh ," target="1. The medium-mismatch condition refers to a situation where the model is trained on data collected in low noise conditions and at low speeds (e.g., stopped car), but during testing, the data is gathered using a distant microphone in high-speed conditions (e.g., on a highway). This creates a mismatch between the training and testing conditions, making it more challenging for the model to maintain good performance.&#10;&#10;Based on the discussion provided, there are two main aspects being considered:&#10;&#10;- The use of robust features that perform well even with gentle degradation in testing conditions different from training.&#10;- A comparison of original signals to smoothed versions, focusing on low frequencies and exploring statistical measures for improving performance in noisy environments.&#10;&#10;The medium-mismatch condition is introduced as an example of a realistic scenario that requires the model to be robust against changes in car conditions, specifically microphone placement (near vs. far) and speed (low vs. high). This is relevant when discussing potential improvements for front-end processing, such as adding a pitch detector that better handles these differences between training and testing conditions.&#10;&#10;In summary, the medium-mismatch condition involves training on low noisy conditions with one microphone placement but testing with a far microphone in higher speed conditions to assess how well the model can handle real-world variability.">
      <data key="d0">1</data>
    </edge>
    <edge source="The results of implementing VAD, noise compensation (specifically spectral subtraction), and normalization (on-line normalization) techniques on the baseline experiments from OGI since last week showed improved performance when using these additional features. The comparison was made between two groups of proposals, one with Voice Activity Detection (VAD) and the other without. It was found that the group with VAD performed better than the group without VAD. This demonstrates that incorporating these techniques can lead to improved robustness, fairness, understanding of strengths and weaknesses, better generalization, and more effective handling of complex scenarios or noisy environments in VAD systems. However, it was noted that online normalization did not provide further improvements when spectral subtraction was already in use. The relationship between these findings and a previous issue related to musical tones was unclear due to differences in the spectral subtraction approaches used." target="isfmarker} and then when the {disfmarker} the {disfmarker} the proposals actually came in and half of them had V A Ds and half of them didn't , and the half that did did well and the {vocalsound} half that didn't did poorly .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Mm - hmm . Um .&#10;Speaker: Professor B&#10;Content: Uh .&#10;Speaker: PhD A&#10;Content: Yeah . So we 'll see what happen with this . And {disfmarker} Yeah . So what happened since , um , {vocalsound} last week is {disfmarker} well , from OGI , these experiments on {pause} putting VAD on the baseline . And these experiments also are using , uh , some kind of noise compensation , so spectral subtraction , and putting on - line normalization , um , just after this . So I think spectral subtraction , LDA filtering , and on - line normalization , so which is similar to {vocalsound">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there is no explicit information given about whether Hynek or Sunil from OGI (Oregon Graduate Institute) were part of the Aurora conference call. Speaker PhD A mentioned not knowing who was involved from OGI, and the discussion moved on to other topics. Therefore, it cannot be determined from this transcript if Hynek or Sunil were present in the Aurora conference call." target=" supposed to discuss is still , {vocalsound} uh , things like {vocalsound} the weights , uh {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , this is a conference call for , uh , uh , Aurora participant sort of thing .&#10;Speaker: Grad E&#10;Content: For {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Professor B&#10;Content: I see .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Do you know who was {disfmarker} who was {disfmarker} since we weren't in on it , uh , do you know who was in from OGI ? Was {disfmarker} {vocalsound} was {disfmarker} was Hynek involved or was it Sunil&#10;Speaker: PhD A&#10;Content: I have no idea .&#10;Speaker: Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Mmm , I just {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , you don't know . OK">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there is no explicit information given about whether Hynek or Sunil from OGI (Oregon Graduate Institute) were part of the Aurora conference call. Speaker PhD A mentioned not knowing who was involved from OGI, and the discussion moved on to other topics. Therefore, it cannot be determined from this transcript if Hynek or Sunil were present in the Aurora conference call." target=" .&#10;Speaker: PhD A&#10;Content: What 's wrong with {disfmarker} ?&#10;Speaker: Professor B&#10;Content: OK . It 's April fifth . Actually , Hynek should be getting back in town shortly if he isn't already .&#10;Speaker: PhD C&#10;Content: Is he gonna come here ?&#10;Speaker: Professor B&#10;Content: Uh . Well , we 'll drag him here . I know where he is .&#10;Speaker: PhD C&#10;Content: So when you said &quot; in town &quot; , you mean {pause} Oregon .&#10;Speaker: Professor B&#10;Content: U u u u uh , I meant , you know , this end of the world , yeah , {vocalsound} is really what I meant ,&#10;Speaker: PhD C&#10;Content: Oh .&#10;Speaker: Grad E&#10;Content: Doo , doo - doo .&#10;Speaker: Professor B&#10;Content: uh , cuz he 's been in Europe .&#10;Speaker: Grad E&#10;Content: Doo - doo .&#10;Speaker: Professor B&#10;Content: So .&#10;Speaker: PhD C&#10;Content: I have something just fairly brief to report on">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there is no explicit information given about whether Hynek or Sunil from OGI (Oregon Graduate Institute) were part of the Aurora conference call. Speaker PhD A mentioned not knowing who was involved from OGI, and the discussion moved on to other topics. Therefore, it cannot be determined from this transcript if Hynek or Sunil were present in the Aurora conference call." target="Based on the transcript provided, the exact date in June for the next Aurora deadline was not specified during the conversation. Speaker PhD A knew that it was in June but did not have the exact date. Speaker Professor B asked about early, late, or middle June, but no specific date was given. Additionally, none of the participants provided information about who could provide this information.">
      <data key="d0">1</data>
    </edge>
    <edge source=" supposed to discuss is still , {vocalsound} uh , things like {vocalsound} the weights , uh {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , this is a conference call for , uh , uh , Aurora participant sort of thing .&#10;Speaker: Grad E&#10;Content: For {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Professor B&#10;Content: I see .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Do you know who was {disfmarker} who was {disfmarker} since we weren't in on it , uh , do you know who was in from OGI ? Was {disfmarker} {vocalsound} was {disfmarker} was Hynek involved or was it Sunil&#10;Speaker: PhD A&#10;Content: I have no idea .&#10;Speaker: Professor B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Mmm , I just {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , you don't know . OK" target="Based on the transcript provided, the exact date in June for the next Aurora deadline was not specified during the conversation. Speaker PhD A knew that it was in June but did not have the exact date. Speaker Professor B asked about early, late, or middle June, but no specific date was given. Additionally, none of the participants provided information about who could provide this information.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, the exact date in June for the next Aurora deadline was not specified during the conversation. Speaker PhD A knew that it was in June but did not have the exact date. Speaker Professor B asked about early, late, or middle June, but no specific date was given. Additionally, none of the participants provided information about who could provide this information." target=" I wi I will {vocalsound} call them and {disfmarker}&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: now they are {disfmarker} I think they have more time because they have this {disfmarker} well , Eurospeech deadline is {vocalsound} over&#10;Speaker: PhD C&#10;Content: When is the next , um , Aurora {pause} deadline ?&#10;Speaker: PhD A&#10;Content: and {disfmarker} It 's , um , in June . Yeah .&#10;Speaker: PhD C&#10;Content: June .&#10;Speaker: Professor B&#10;Content: Early June , late June , middle June ?&#10;Speaker: PhD A&#10;Content: I don't know w&#10;Speaker: Professor B&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: OK . Um , and {pause} he 's been doing all the talking but {disfmarker} but {vocalsound} these {disfmarker} {vocalsound} he 's {disfmarker} he 's , uh {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A considers the new system (proposal two) to be better than the previous one (proposal one) mainly due to two reasons: first, it has a 64 hertz cut-off with clean downsampling, and second, it includes a good Voice Activity Detector (VAD). In terms of latency, PHP A mentions that the latencies are shorter in the new system.&#10;&#10;When comparing proposal two to proposal one, PhD A considers the new system to be &quot;a little worse&quot; only when a neural network is added, but it is still better overall due to its shorter latencies and other improvements made during the transition from proposal one to proposal two." target=" new system is {disfmarker} is {disfmarker} is better ,&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: because it has {vocalsound} um , this sixty - four hertz cut - off , uh , clean {vocalsound} downsampling , and , um {disfmarker} what else ? Uh , yeah , a good VAD . We put the good VAD . So . Yeah , I don't know . I {disfmarker} I {disfmarker} j uh , uh {disfmarker} pr&#10;Speaker: Professor B&#10;Content: But the latencies {disfmarker} but you 've got the latency shorter now .&#10;Speaker: PhD A&#10;Content: Latency is short {disfmarker} is {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Isn't it&#10;Speaker: PhD A&#10;Content: And so&#10;Speaker: Professor B&#10;Content: So it 's better than the system that we had before .&#10;Speaker: PhD A&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A considers the new system (proposal two) to be better than the previous one (proposal one) mainly due to two reasons: first, it has a 64 hertz cut-off with clean downsampling, and second, it includes a good Voice Activity Detector (VAD). In terms of latency, PHP A mentions that the latencies are shorter in the new system.&#10;&#10;When comparing proposal two to proposal one, PhD A considers the new system to be &quot;a little worse&quot; only when a neural network is added, but it is still better overall due to its shorter latencies and other improvements made during the transition from proposal one to proposal two." target=" And so&#10;Speaker: Professor B&#10;Content: So it 's better than the system that we had before .&#10;Speaker: PhD A&#10;Content: Yeah . Mainly because {pause} {vocalsound} of {pause} the sixty - four hertz and the good VAD .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: And then I took this system and , {vocalsound} mmm , w uh , I p we put the old filters also . So we have this good system , with good VAD , with the short filter and with the long filter , and , um , with the short filter it 's not worse . So {disfmarker} well , is it {disfmarker}&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: it 's in {disfmarker}&#10;Speaker: Professor B&#10;Content: So that 's {disfmarker} that 's all fine .&#10;Speaker: PhD A&#10;Content: Yes . Uh {disfmarker}&#10;Speaker: Professor B&#10;Content: But what you 're saying is that when you do">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The medium-mismatch condition refers to a situation where the model is trained on data collected in low noise conditions and at low speeds (e.g., stopped car), but during testing, the data is gathered using a distant microphone in high-speed conditions (e.g., on a highway). This creates a mismatch between the training and testing conditions, making it more challenging for the model to maintain good performance.&#10;&#10;Based on the discussion provided, there are two main aspects being considered:&#10;&#10;- The use of robust features that perform well even with gentle degradation in testing conditions different from training.&#10;- A comparison of original signals to smoothed versions, focusing on low frequencies and exploring statistical measures for improving performance in noisy environments.&#10;&#10;The medium-mismatch condition is introduced as an example of a realistic scenario that requires the model to be robust against changes in car conditions, specifically microphone placement (near vs. far) and speed (low vs. high). This is relevant when discussing potential improvements for front-end processing, such as adding a pitch detector that better handles these differences between training and testing conditions.&#10;&#10;In summary, the medium-mismatch condition involves training on low noisy conditions with one microphone placement but testing with a far microphone in higher speed conditions to assess how well the model can handle real-world variability." target="ound} if what you 're {disfmarker} {vocalsound} if the goal of this is to come up with robust features , it does mean {disfmarker} So you could argue , in fact , that the well - matched is something you shouldn't be looking at at all , that {disfmarker} that the goal is to come up with features {vocalsound} that will still give you reasonable performance , you know , with again gentle degregra degradation , um , even though the {disfmarker} the testing condition is not the same as the training .&#10;Speaker: PhD C&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: So , you know , I {disfmarker} I could argue strongly that something like the medium mismatch , which is you know not compl pathological but {disfmarker} I mean , what was the {disfmarker} the medium - mismatch condition again ?&#10;Speaker: PhD A&#10;Content: Um , {vocalsound} it 's {disfmarker} Yeah . Medium mismatch is everything with the far {pause} microphone , but trained on , like , low noisy condition , like low speed and {">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The medium-mismatch condition refers to a situation where the model is trained on data collected in low noise conditions and at low speeds (e.g., stopped car), but during testing, the data is gathered using a distant microphone in high-speed conditions (e.g., on a highway). This creates a mismatch between the training and testing conditions, making it more challenging for the model to maintain good performance.&#10;&#10;Based on the discussion provided, there are two main aspects being considered:&#10;&#10;- The use of robust features that perform well even with gentle degradation in testing conditions different from training.&#10;- A comparison of original signals to smoothed versions, focusing on low frequencies and exploring statistical measures for improving performance in noisy environments.&#10;&#10;The medium-mismatch condition is introduced as an example of a realistic scenario that requires the model to be robust against changes in car conditions, specifically microphone placement (near vs. far) and speed (low vs. high). This is relevant when discussing potential improvements for front-end processing, such as adding a pitch detector that better handles these differences between training and testing conditions.&#10;&#10;In summary, the medium-mismatch condition involves training on low noisy conditions with one microphone placement but testing with a far microphone in higher speed conditions to assess how well the model can handle real-world variability." target="marker} Yeah . Medium mismatch is everything with the far {pause} microphone , but trained on , like , low noisy condition , like low speed and {disfmarker} or {pause} stopped car and tested on {pause} high - speed conditions , I think , like on a highway and {disfmarker}&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: So {disfmarker}&#10;Speaker: Professor B&#10;Content: So it 's still the same {disfmarker} same microphone in both cases ,&#10;Speaker: PhD A&#10;Content: Same microphone but {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: but , uh , it 's {disfmarker} there 's a mismatch between the car conditions . And that 's {disfmarker} uh , you could argue that 's a pretty realistic situation&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: and , uh , I 'd almost argue for weighting that highest . But the way they have it now , {">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
