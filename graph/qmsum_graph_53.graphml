<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="1. Identify independent information sources: When combining probabilities from independent information sources, it is crucial to ensure that these sources are statistically independent, meaning that the outcome of one source does not affect the other. In this transcript, Professor A emphasizes the importance of independent errors from different information sources.&#10;2. Calculate probabilities for cases and finer categories: First, calculate the probability of each case in your set. Then, determine the probability of finer categories on the other side (as mentioned by Professor A). For example, if you have two categories, A and B, calculate the probability of observing evidence E1, E2, ... En for both A and B.&#10;3. Multiply probabilities where appropriate: If your information sources are independent, you can multiply their corresponding probabilities to obtain the combined probability. This is based on the multiplication rule of probabilities for independent events. In the transcript, PhD F acknowledges this step by saying &quot;I see, yeah.&quot;&#10;4. Normalize means and variances: To remove biases due to variability in data, normalize the means and variances of your information sources. This helps ensure that any differences between categories are related to the linguistic component you're interested in and not confounding factors. PhD B brings up this point when they mention &quot;got overall variance&quot; and &quot;normalizing the means.&quot;&#10;5. Consider higher-order effects: While normalizing means will have a first-order effect on your results, be aware that variances can also impact your ability to distinguish between categories. Addressing these second-order effects can further improve your analysis.&#10;6. Monitor error rates: When evaluating your combined probabilities, focus on word error rates rather than accuracies, as Professor A suggests. This will give you a more nuanced understanding of how well your model is performing.&#10;&#10;By following these steps, you can effectively combine probabilities from independent information sources to distinguish between finer categories in a given set of cases." />
    <node id="aker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: uh so then it you {disfmarker} you {disfmarker} you can pretty much guarantee it 's stuff that you 're not looking at very well with the other one , and uh then you only use for this one distinction .&#10;Speaker: PhD F&#10;Content: Alright .&#10;Speaker: Professor A&#10;Content: And {disfmarker} and so now you 've got a probability of the cases , and you 've got uh the probability of the finer uh categories on the other side . You multiply them where appropriate and uh {vocalsound} um&#10;Speaker: PhD F&#10;Content: I see , yeah . Mm - hmm .&#10;Speaker: Professor A&#10;Content: if they really are from independent {pause} information sources then {vocalsound} they should have different kinds of errors&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: and roughly independent errors , and {vocalsound} it 's a good choice for {disfmarker}&#10;Speaker: PhD F&#10;Content: M" />
    <node id=" PhD B&#10;Content: No , I understand that ,&#10;Speaker: PhD F&#10;Content: You {disfmarker} you get an estimate of the standard deviation .&#10;Speaker: PhD B&#10;Content: but I mean {disfmarker}&#10;Speaker: PhD F&#10;Content: That 's&#10;Speaker: PhD B&#10;Content: No .&#10;Speaker: PhD F&#10;Content: um {disfmarker}&#10;Speaker: PhD B&#10;Content: No , I understand what it is , but I mean , what does it {disfmarker} what 's {disfmarker} what is&#10;Speaker: PhD F&#10;Content: Yeah but .&#10;Speaker: PhD B&#10;Content: uh {disfmarker}&#10;Speaker: Professor A&#10;Content: What 's the rationale ?&#10;Speaker: PhD B&#10;Content: We Yeah . Yeah . Why {disfmarker} why do it ?&#10;Speaker: PhD F&#10;Content: Uh .&#10;Speaker: Professor A&#10;Content: Well , I mean , because {vocalsound} everything uh {disfmarker} If you have a system based on Gaussians , everything is based on" />
    <node id=" trying to distinguish between E and B&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: if it just so happens that the E 's {vocalsound} were a more {disfmarker} you know , were recorded when {disfmarker} when the energy was {disfmarker} was {disfmarker} was larger or something ,&#10;Speaker: PhD B&#10;Content: Mm - hmm . Mm - hmm . Mm - hmm . &#10;Speaker: Professor A&#10;Content: or the variation in it was larger , {vocalsound} uh than with the B 's , then this will be {disfmarker} give you some {disfmarker} some bias .&#10;Speaker: PhD B&#10;Content: &#10;Speaker: Professor A&#10;Content: So the {disfmarker} {vocalsound} it 's removing these sources of variability in the data {vocalsound} that have nothing to do with the linguistic component .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: Got" />
    <node id=" overall variance .&#10;Speaker: PhD B&#10;Content: Oh , OK . Uh - huh .&#10;Speaker: Professor A&#10;Content: And so , in principle , you {disfmarker} if you remove that source , then , you know , you can {disfmarker}&#10;Speaker: PhD B&#10;Content: I see . OK . So would {disfmarker} the major effect is {disfmarker} that you 're gonna get is by normalizing the means ,&#10;Speaker: Professor A&#10;Content: That 's the first order but {disfmarker} thing ,&#10;Speaker: PhD B&#10;Content: but it may help {disfmarker} First - order effects .&#10;Speaker: Professor A&#10;Content: but then the second order is {disfmarker} is the variances&#10;Speaker: PhD B&#10;Content: And it may help to do the variance . OK .&#10;Speaker: Professor A&#10;Content: because , again , if you {disfmarker} if you 're trying to distinguish between E and B&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: if it just so happens" />
    <node id="aker: PhD B&#10;Content: and I think you mentioned this in your email too {disfmarker} it 's just very um {disfmarker}&#10;Speaker: PhD F&#10;Content: Mmm , yeah .&#10;Speaker: PhD B&#10;Content: you know get stuck in some local minimum and this thing throws you out of it I guess .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Well , what 's {disfmarker} what are {disfmarker} according to the rules what {disfmarker} what are we supposed to do about the transition probabilities ? Are they supposed to be point five or point six ?&#10;Speaker: PhD B&#10;Content: I think you 're not allowed to {disfmarker} Yeah . That 's supposed to be point six , for the self - loop .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Point {disfmarker} It 's supposed to be point six .&#10;Speaker: PhD B&#10;Content: Yeah . But changing it to point five I think is {disfmarker} which" />
    <node id="Content: not {disfmarker}&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: So when we look at this error rate&#10;Speaker: Professor A&#10;Content: No . That 's why I 've been saying we should be looking at word error rate uh and {disfmarker} and not {disfmarker} not at {vocalsound} at accuracies .&#10;Speaker: PhD F&#10;Content: uh {disfmarker} Mmm , yeah . Mmm , yeah .&#10;Speaker: Professor A&#10;Content: It 's {disfmarker}&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: I mean uh we probably should have standardized on that all the way through . It 's just {disfmarker}&#10;Speaker: PhD B&#10;Content: Well .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: I mean , it 's not {disfmarker} it 's not that different , right ? I mean , just subtract the accuracy .&#10;Speaker: Professor A" />
    <node id="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." />
    <node id=" . Uh yeah . B We should let him finish what he w he was gonna say ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: and {disfmarker}&#10;Speaker: PhD B&#10;Content: So go ahead .&#10;Speaker: PhD F&#10;Content: Um yeah , so yeah , I think if we try to develop a second stream well , there would be one stream that is the envelope and the second , it could be interesting to have that 's {disfmarker} something that 's more related to the fine structure of the spectrum . And . Yeah , so I don't know . We were thinking about like using ideas from {disfmarker} from Larry Saul , have a good voice detector , have a good , well , voiced - speech detector , that 's working on {disfmarker} on the FFT and {vocalsound} uh&#10;Speaker: Professor A&#10;Content: U&#10;Speaker: PhD F&#10;Content: Larry Saul could be an idea . We were are thinking about just {vocalsound} kind of uh taking the spectrum and computing the variance of {disf" />
    <node id=" Larry Saul could be an idea . We were are thinking about just {vocalsound} kind of uh taking the spectrum and computing the variance of {disfmarker} of the high resolution spectrum {vocalsound} and things like this .&#10;Speaker: Professor A&#10;Content: So u s u OK . So {disfmarker} So many {vocalsound} tell you something about that . Uh we had a guy here some years ago who did some work on {vocalsound} um {vocalsound} making use of voicing information uh to {vocalsound} help in reducing the noise .&#10;Speaker: PhD F&#10;Content: Yeah ?&#10;Speaker: Professor A&#10;Content: So what he was doing is basically y you {disfmarker} {vocalsound} you do estimate the pitch .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: And um you {disfmarker} from that you {disfmarker} you estimate {disfmarker} or you estimate fine harmonic structure , whichev ei either way , it 's more or less the same . But {vocalsound} uh the" />
    <node id=" lots of ideas for doing voice activity , or speech - nonspeech rather , {comment} um by looking at {vocalsound} um , you know , uh {vocalsound} I guess harmonics or looking across time {disfmarker}&#10;Speaker: Professor A&#10;Content: Well , I think he was talking about the voiced - unvoiced , though ,&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: right ? So , not the speech - nonspeech .&#10;Speaker: PhD B&#10;Content: Yeah . Well even with e&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: uh w ah you know , uh even with the voiced - non {pause} voiced - unvoiced&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: um {disfmarker} I thought that you or {pause} somebody was talking about {disfmarker}&#10;Speaker: Professor A&#10;Content: Well . Uh yeah . B We should let him finish what he w he was gonna say ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker" />
    <node id="Content: Um . It seems that after spectral subtraction , speech is more emerging now uh {vocalsound} than {disfmarker} than before .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Speech is more what ?&#10;Speaker: PhD F&#10;Content: Well , the difference between the energy of the speech and the energy of the n spectral subtrac subtracted noise portion is {disfmarker} is larger .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Well , if you compare the first figure to this one {disfmarker} Actually the scale is not the same , but if you look at the {disfmarker} the numbers um {vocalsound} you clearly see that the difference between the C - zero of the speech and C - zero of the noise portion is larger . Uh but what happens is that after spectral subtraction , {vocalsound} you also increase the variance of this {disfmarker} of C - zero .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content" />
    <node id=" trying to {vocalsound} integrate this other {disfmarker} other uh spectral subtraction , {vocalsound} why are we worrying about it ? &quot;&#10;Speaker: PhD F&#10;Content: Mm - hmm . About ? Spectral subtraction ?&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: It 's just uh {disfmarker} Well it 's another {disfmarker} They are trying to u to use the um {disfmarker} {vocalsound} the Ericsson and we 're trying to use something {disfmarker} something else . And . Yeah , and also to understand what happens because&#10;Speaker: Professor A&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: uh fff Well . When we do spectral subtraction , actually , I think {vocalsound} that this is the {disfmarker} the two last figures .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Um . It seems that after spectral subtraction , speech is more emerging now uh {vocalsound} than {disfmarker} than before" />
    <node id=" we 've already observed . But uh , yeah , voice activity detection is not {vocalsound} {vocalsound} an easy thing neither .&#10;Speaker: PhD B&#10;Content: But after you do this , after you do the variance normalization {disfmarker} I mean .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: I don't know , it seems like this would be a lot easier than this signal to work with .&#10;Speaker: PhD F&#10;Content: Yeah . So . What I notice is that , while I prefer to look at the second figure than at the third one , well , because you clearly see where speech is .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: But the problem is that on the speech portion , channel zero and channel one are more different than when you use variance normalization where channel zero and channel one become closer .&#10;Speaker: Professor A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: But for the purposes of finding the speech {disfmarker}&#10;Speaker: PhD F" />
    <node id="The reason they cannot use Aurora for the suggested voicing bit in frame classification is that there is no phoneme level labeling available for Aurora, only word level labeling. In contrast, they have access to phoneme level labeling for Italian, which makes it suitable for their current task." />
    <node id=" huh .&#10;Speaker: PhD F&#10;Content: and see if giving the d uh , this voicing bit would help in {disfmarker} in terms of uh frame classification .&#10;Speaker: Professor A&#10;Content: Why don't you {disfmarker} why don't you just do it with Aurora ?&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: Just any i in {disfmarker} in each {disfmarker} in each frame&#10;Speaker: PhD F&#10;Content: Yeah , but {disfmarker} but {disfmarker} B but we cannot do the cheating , this cheating thing .&#10;Speaker: Grad E&#10;Content: We 're {disfmarker}&#10;Speaker: Professor A&#10;Content: uh {disfmarker}&#10;Speaker: Grad E&#10;Content: We need labels .&#10;Speaker: Professor A&#10;Content: Why not ?&#10;Speaker: PhD F&#10;Content: Well . Cuz we don't have {disfmarker} Well , for Italian perhaps we have , but we don't have this labeling for Aurora . We just have a labeling with word models&#10;Spe" />
    <node id="isfmarker} Well , for Italian perhaps we have , but we don't have this labeling for Aurora . We just have a labeling with word models&#10;Speaker: Professor A&#10;Content: I see .&#10;Speaker: PhD F&#10;Content: but not for phonemes .&#10;Speaker: PhD D&#10;Content: Not for foreigners .&#10;Speaker: Grad E&#10;Content: we don't have frame {disfmarker} frame level transcriptions .&#10;Speaker: Professor A&#10;Content: Um .&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Um . {vocalsound} Yeah .&#10;Speaker: Professor A&#10;Content: But you could {disfmarker} I mean you can {disfmarker} you can align so that {disfmarker} It 's not perfect , but if you {disfmarker} if you know what was said and {disfmarker}&#10;Speaker: PhD B&#10;Content: But the problem is that their models are all word level models . So there 's no phone models {pause} that you get alignments for .&#10;Speaker: PhD F&#10;Content: Mm - hmm" />
    <node id=" . So there 's no phone models {pause} that you get alignments for .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: You {disfmarker} So you could find out where the word boundaries are but that 's about it .&#10;Speaker: Professor A&#10;Content: Yeah . I see .&#10;Speaker: Grad E&#10;Content: S But we could use uh the {disfmarker} the noisy version that TIMIT , which {vocalsound} you know , is similar to the {disfmarker} the noises found in the TI - digits {vocalsound} um portion of Aurora .&#10;Speaker: PhD F&#10;Content: Yeah . noise , yeah . Yeah , that 's right , yep . Mmm .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Well , I guess {disfmarker} I guess we can {disfmarker} we can say that it will help , but I don't know . If this voicing bit doesn't help , uh , I think we don't have to" />
    <node id=" {disfmarker} {vocalsound} We are trying to {disfmarker} to do something with the Meeting Recorder digits ,&#10;Speaker: Professor A&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: and {disfmarker} But yeah . Yeah . And the good thing is that {pause} there is this first deadline ,&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: and , well , some people from OGI are working on a paper for this , but there is also the um {vocalsound} special session about th Aurora which is {disfmarker} {vocalsound} uh which has an extended deadline . So . The deadline is in May .&#10;Speaker: Professor A&#10;Content: For uh {disfmarker} {vocalsound} Oh , for Eurospeech ?&#10;Speaker: PhD F&#10;Content: For th Yeah .&#10;Speaker: Professor A&#10;Content: Oh !&#10;Speaker: PhD F&#10;Content: So f only for the experiments on Aurora . So it {disfmarker} it 's good ,&#10;Speaker: Professor A&#10;Content: Oh , a special dispens" />
    <node id=" you {disfmarker} it goes &quot; digit &quot;&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: and then that can be {disfmarker} either go to silence or go to another digit , which {disfmarker} That model would allow for the production of {vocalsound} infinitely long sequences of digits , right ?&#10;Speaker: Professor A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: So . I thought &quot; well I 'm gonna just look at the {disfmarker} what actual digit strings do occur in the training data . &quot;&#10;Speaker: Professor A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: And the interesting thing was it turns out that there are no sequences of two - long or three - long digit strings {pause} in any of the Aurora training data . So it 's either one , four , five , six , uh up to eleven , and then it skips and then there 's some at sixteen .&#10;Speaker: Professor A&#10;Content: But what about the testing data ?&#10;Speaker: PhD B&#10;Content: Um . I don't know . I" />
    <node id="aker: PhD F&#10;Content: because I guess voice activity detection on this should {disfmarker} could be worse .&#10;Speaker: PhD B&#10;Content: Yeah . Is it applied all the way back here ?&#10;Speaker: PhD F&#10;Content: It 's applied the um on , yeah , something like this ,&#10;Speaker: PhD B&#10;Content: Maybe that 's why it doesn't work for channel one .&#10;Speaker: PhD F&#10;Content: yeah . Perhaps , yeah .&#10;Speaker: Professor A&#10;Content: Can I {disfmarker}&#10;Speaker: PhD F&#10;Content: So we could perhaps do just mean normalization before VAD .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Mm - hmm . Can I ask a , I mean {disfmarker} a sort of top - level question , which is {vocalsound} um &quot; if {disfmarker} if most of what the OGI folk are working with is trying to {vocalsound} integrate this other {disfmarker} other uh spectral subtraction , {vocalsound} why are we worrying about" />
    <node id="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." />
    <node id=": Right .&#10;Speaker: PhD B&#10;Content: But for the purposes of finding the speech {disfmarker}&#10;Speaker: PhD F&#10;Content: And {disfmarker} Yeah , but here {disfmarker}&#10;Speaker: PhD B&#10;Content: You 're more interested in the difference between the speech and the nonspeech ,&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: right ?&#10;Speaker: PhD F&#10;Content: Yeah . So I think , yeah . For I th I think that it {disfmarker} perhaps it shows that {vocalsound} uh the parameters that the voice activity detector should use {disfmarker} uh have to use should be different than the parameter that have to be used for speech recognition .&#10;Speaker: Professor A&#10;Content: Yeah . So basically you want to reduce this effect .&#10;Speaker: PhD F&#10;Content: Well , y&#10;Speaker: Professor A&#10;Content: So you can do that by doing the voi voice activity detection . You also could do it by spect uh spectral subtraction before the {vocalsound} variance normalization , right ?&#10;Speaker" />
    <node id="i voice activity detection . You also could do it by spect uh spectral subtraction before the {vocalsound} variance normalization , right ?&#10;Speaker: PhD F&#10;Content: Yeah , but it 's not clear , yeah .&#10;Speaker: Professor A&#10;Content: So uh {disfmarker}&#10;Speaker: PhD F&#10;Content: We So . Well . It 's just to&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: the {disfmarker} the number that at that are here are recognition experiments on Italian HM and MM {vocalsound} with these two kinds of parameters . And , {pause} well , it 's better with variance normalization .&#10;Speaker: Professor A&#10;Content: Yeah . Yeah . So it does get better even though it looks ugly .&#10;Speaker: PhD F&#10;Content: Uh {disfmarker}&#10;Speaker: Professor A&#10;Content: OK . but does this have the voice activity detection in it ?&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Um .&#10;Speaker: Professor A" />
    <node id="Content: Yeah .&#10;Speaker: Professor A&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Um .&#10;Speaker: Professor A&#10;Content: So .&#10;Speaker: Grad E&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Where 's th&#10;Speaker: PhD F&#10;Content: But the fact is that the voice activity detector doesn't work on channel one . So . Yeah .&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: Where {disfmarker} at what stage is the voice activity detector applied ? Is it applied here or a after the variance normalization ?&#10;Speaker: PhD F&#10;Content: Hmm ?&#10;Speaker: Professor A&#10;Content: Spectral subtraction , I guess .&#10;Speaker: PhD B&#10;Content: or {disfmarker}&#10;Speaker: PhD F&#10;Content: It 's applied before variance normalization . So it 's a good thing ,&#10;Speaker: PhD B&#10;Content: Oh .&#10;Speaker: PhD F&#10;Content: because I guess voice activity detection on this should {disfmarker} could be worse .&#10;Speaker: PhD B" />
    <node id=" yeah . When you have noise there is no um {disfmarker} {vocalsound} if {disfmarker} if you have a low frequency noise it could be taken for {disfmarker} for voiced speech and .&#10;Speaker: Professor A&#10;Content: Yeah , you can make these mistakes ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: Professor A&#10;Content: but {disfmarker} but {disfmarker}&#10;Speaker: PhD B&#10;Content: Isn't there some other&#10;Speaker: PhD F&#10;Content: S&#10;Speaker: PhD B&#10;Content: uh d&#10;Speaker: PhD F&#10;Content: So I think that it {disfmarker} it would be good {disfmarker} Yeah , yeah , well , go {disfmarker} go on .&#10;Speaker: PhD B&#10;Content: Uh , I was just gonna say isn't there {disfmarker} {vocalsound} aren't {disfmarker} aren't there lots of ideas for doing voice activity , or speech - nonspeech rather , {comment} um by looking at {vocalsound} um , you know" />
    <node id="The person they were discussing with suggested using a digital filter to preprocess the data before analyzing it. This would be an alternative to changing the capacitor on the input box." />
    <node id="ound} change a capacitor on the input box for that or whether we should&#10;Speaker: PhD B&#10;Content: Yeah , he suggested a smaller capacitor , right ?&#10;Speaker: Professor A&#10;Content: Right . But then I had some other uh thing discussions with him&#10;Speaker: PhD B&#10;Content: For the P D&#10;Speaker: Professor A&#10;Content: and the feeling was {vocalsound} once we start monk monkeying with that , uh , many other problems could ha happen . And additionally we {disfmarker} we already have a lot of data that 's been collected with that , so .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: A simple thing to do is he {disfmarker} he {disfmarker} he has a {disfmarker} I forget if it {disfmarker} this was in that mail or in the following mail , but he has a {disfmarker} a simple filter , a digital filter that he suggested . We just run over the data before we deal with it .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A" />
    <node id="} I have to be more careful about using that as a {disfmarker} as a {disfmarker} {vocalsound} as a good illustration , uh , in fact it 's not , of uh {disfmarker} {vocalsound} of the effects of room reverberation . It is isn't a bad illustration of the effects of uh room noise . {vocalsound} on {disfmarker} on uh some mikes uh but So . And then we had this other discussion about um {vocalsound} whether this affects the dynamic range , cuz I know , although we start off with thirty two bits , you end up with uh sixteen bits and {vocalsound} you know , are we getting hurt there ? But uh Dan is pretty confident that we 're not , that {disfmarker} that quantization error is not {disfmarker} is still not a significant {vocalsound} factor there . So . So there was a question of whether we should change things here , whether we should {vocalsound} change a capacitor on the input box for that or whether we should&#10;Speaker: PhD B&#10;Content: Yeah , he suggested a smaller capacitor ," />
    <node id="} that 's a question for this uh you know extending the feature vector versus having different streams .&#10;Speaker: PhD F&#10;Content: Oh . Was it nois noisy condition ? the example that you {disfmarker} you just&#10;Speaker: Professor A&#10;Content: And {disfmarker} and it may not have been noisy conditions .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Yeah . I {disfmarker} I don't remember the example but it was {disfmarker} {vocalsound} it was on some DARPA data and some years ago and so it probably wasn't , actually&#10;Speaker: PhD F&#10;Content: Mm - hmm . Mm - hmm . Yeah . But we were thinking , we discussed with Barry about this , and {vocalsound} perhaps {vocalsound} thinking {disfmarker} we were thinking about some kind of sheet cheating experiment where we would use TIMIT&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: and see if giving the d uh , this voicing bit would help in {disfmarker} in" />
    <node id=" filtering before we {vocalsound} process it . And then again if it 's uh depending on the option that the {disfmarker} our {disfmarker} our software is being run with , it 's {disfmarker} it 's quite possible that 's already being taken care of . uh But I also have to pick a different picture to show the effects of reverberation . uh&#10;Speaker: PhD B&#10;Content: Did somebody notice it during your talk ?&#10;Speaker: Professor A&#10;Content: uh No .&#10;Speaker: PhD B&#10;Content: Huh .&#10;Speaker: Professor A&#10;Content: Well . uh Well . If they made output they were {disfmarker} they were , you know {disfmarker} they were nice .&#10;Speaker: PhD B&#10;Content: Didn't say anything ?&#10;Speaker: Professor A&#10;Content: But . {vocalsound} I mean the thing is it was since I was talking about reverberation and showing this thing that was noise , it wasn't a good match , but it certainly was still uh an indication of the fact that you get noise with distant mikes . uh It 's just not a great example" />
    <node id=" {vocalsound} are moving quite a bit . &quot; And then you look in the other one and they look practically flat .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: So I mean you could {disfmarker} that 's why I was thinking , in a section like that , you could take a look {disfmarker} look at just that part of the spectrogram and you could say &quot; Oh yeah . This {disfmarker} this really distorted it quite a bit . &quot;&#10;Speaker: PhD B&#10;Content: Yeah . The main thing that struck me in looking at those two spectrograms was the difference in the high frequencies . It looked like {vocalsound} for the one that was farther away , you know , it really {disfmarker} everything was attenuated&#10;Speaker: Professor A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: and {disfmarker} I mean that was the main visual thing that I noticed .&#10;Speaker: Professor A&#10;Content: Right . But it 's {disfmarker} it 's uh {disfmarker} So . Yeah" />
    <node id="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation." />
    <node id="&#10;Content: R Right . So you have a second neural net .&#10;Speaker: PhD F&#10;Content: and then use a tandem system&#10;Speaker: Professor A&#10;Content: It could be pretty small . Yeah . If you have a tandem system and then you have some kind of {disfmarker} it can be pretty small {disfmarker} net {disfmarker}&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: we used {disfmarker} we d did some of this stuff . Uh I {disfmarker} I did , some years ago ,&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: and the {disfmarker} and {disfmarker} and you use {disfmarker} {vocalsound} the thing is to use information primarily that 's different as you say , it 's more fine - structure - based than {disfmarker} than envelope - based&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: uh so then it you {disfmarker" />
    <node id=" as the&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: uh {disfmarker} take the log of that or {vocalsound} uh pre pre uh {disfmarker} pre - nonlinearity ,&#10;Speaker: PhD F&#10;Content: Yeah . i if {disfmarker}&#10;Speaker: Professor A&#10;Content: uh and do the KLT on the {disfmarker} on {disfmarker} on that ,&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: then that would {disfmarker} that would I guess be uh a reasonable use of independent information . So maybe that 's what you meant . And then that would be {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah , well , I was not thinking this {disfmarker} yeah , this could be an yeah So you mean have some kind of probability for the v the voicing&#10;Speaker: Professor A&#10;Content: R Right . So you have a second neural net .&#10;Speaker: PhD F&#10;Content: and then use a tandem system&#10;Speaker" />
    <node id=" many parameters and how many frames .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: And there are uh almost one point eight million frames of training data and less than forty thousand parameters in the baseline system .&#10;Speaker: Professor A&#10;Content: Hmm .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: So it 's very , very few parameters compared to how much training data .&#10;Speaker: Professor A&#10;Content: Well . Yes .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: So . And that {disfmarker} that says that we could have lots more parameters actually .&#10;Speaker: PhD B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: I did one quick experiment just to make sure I had everything worked out and I just {disfmarker} {vocalsound} uh f for most of the um {disfmarker} For {d" />
    <node id="&#10;Speaker: PhD F&#10;Content: Worse , yep .&#10;Speaker: Professor A&#10;Content: Out of what ? I mean . s&#10;Speaker: PhD F&#10;Content: Uh well we start from ninety - four point sixty - four , and we go to ninety - four point O four .&#10;Speaker: Professor A&#10;Content: Uh - huh . So that 's six {disfmarker} six point th&#10;Speaker: PhD F&#10;Content: Uh .&#10;Speaker: PhD B&#10;Content: Ninety - three point six four , right ? is the baseline .&#10;Speaker: PhD F&#10;Content: Oh , no , I 've ninety - four . Oh , the baseline , you mean .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Well I don't {disfmarker} I 'm not talking about the baseline here .&#10;Speaker: PhD B&#10;Content: Oh . Oh . I 'm sorry .&#10;Speaker: PhD F&#10;Content: I uh {disfmarker} My baseline is the submitted system .&#10;Speaker: PhD B&#10;Content: Ah ! OK . Ah , ah .&#10;Speaker:" />
    <node id="1. During the meeting, Professor A acknowledged that the example they used to demonstrate reverberation effects during their talk was not an ideal match for the topic. The example involved noise from distant microphones, which is not the same as reverberation.&#10;2. However, Professor A also stated that the example was still valuable because it demonstrated the real-world challenge of dealing with background noise in audio recordings. They mentioned that someone later pointed out the discrepancy between the example and the topic, but no one had noticed during the actual talk.&#10;3. In summary, although the example used by Professor A was not perfect for demonstrating reverberation effects, it still served a purpose by highlighting other real-world issues related to audio recordings, such as background noise from distant microphones. The lack of concern during the talk indicates that the audience may have focused more on the main topic rather than the specific example used." />
    <node id=" that it 's lower amplitude and they hear there 's a second voice ,&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: Professor A&#10;Content: um {vocalsound} but uh that {disfmarker} actually that makes for a perfectly good demo because that 's a real obvious thing , that you hear two voices .&#10;Speaker: PhD B&#10;Content: But not of reverberation .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: A boom .&#10;Speaker: Professor A&#10;Content: Well that {disfmarker} that {disfmarker} that 's OK . But for the {disfmarker} the visual , just , you know , I 'd like to have uh {vocalsound} uh , you know , the spectrogram again ,&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: because you 're {disfmarker} you 're {disfmarker} you 're visual {vocalsound} uh abilities as a human being are so good {vocalsound} you can pick out {disfmarker} you know" />
    <node id=" It made a good {disfmarker} good audio demonstration because when we could play that clip the {disfmarker} the {disfmarker} the really {vocalsound} obvious difference is that you can hear two voices and {disfmarker} {vocalsound} {vocalsound} in the second one and only hear {disfmarker}&#10;Speaker: PhD B&#10;Content: Maybe we could just {pause} like , talk into a cup .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Some good reverb .&#10;Speaker: Professor A&#10;Content: No , I mean , it sound {disfmarker} it sounds pretty reverberant , but I mean you can't {disfmarker} when you play it back in a room with a {disfmarker} you know a big room , {vocalsound} nobody can hear that difference really .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: They hear that it 's lower amplitude and they hear there 's a second voice ,&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker" />
    <node id=" good match , but it certainly was still uh an indication of the fact that you get noise with distant mikes . uh It 's just not a great example because not only isn't it reverberation but it 's a noise that we definitely know what to do .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: So , I mean , it doesn't take deep {disfmarker} {vocalsound} a new {disfmarker} bold new methods to get rid of uh five hertz noise , so .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: um {vocalsound} uh But . So it was {disfmarker} it was a bad example in that way , but it 's {disfmarker} it still is {disfmarker} it 's the real thing that we did get out of the microphone at distance , so it wasn't {vocalsound} it w it w wasn't wrong it was inappropriate . So . {vocalsound} So uh , but uh , Yeah , someone noticed it later pointed it out to me , and I went &quot; oh ," />
    <node id="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." />
    <node id=" Well back twenty years ago when I did this voiced - unvoiced stuff , we were getting more like {vocalsound} ninety - seven or ninety - eight percent correct in voicing . But that was {vocalsound} speaker - dependent {vocalsound} actually . We were doing training {vocalsound} on a particular announcer&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: and {disfmarker} and getting a {vocalsound} very good handle on the features .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: And we did this complex feature selection thing where we looked at all the different possible features one could have for voicing and {disfmarker} {vocalsound} and {disfmarker} and uh {disfmarker} and exhaustively searched {vocalsound} all size subsets and {disfmarker} and uh {disfmarker} for {disfmarker} for that particular speaker and you 'd find you know the five or six features which really did well on them .&#10;Speaker: PhD B&#10;Content: Wow" />
    <node id=" that particular speaker and you 'd find you know the five or six features which really did well on them .&#10;Speaker: PhD B&#10;Content: Wow !&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: And then doing {disfmarker} doing all of that we could get down to two or three percent error . But that , again , was speaker - dependent with {vocalsound} lots of feature selection&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: and a very complex sort of thing .&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: So I would {disfmarker} I would believe {vocalsound} that uh it was quite likely that um looking at envelope only , that we 'd be {vocalsound} significantly worse than that .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Uh .&#10;Speaker: PhD F&#10;Content: And the {disfmarker} all the {disfmarker} the SpeechCorders ? what '" />
    <node id="&#10;Speaker: Professor A&#10;Content: what that said is that , sort of , left to its own devices , like without the {disfmarker} a strong language model and so forth , that you would {disfmarker} {vocalsound} you would make significant number of errors {vocalsound} just with your uh probabilistic machinery in deciding&#10;Speaker: PhD B&#10;Content: It also {disfmarker}&#10;Speaker: Professor A&#10;Content: one oh&#10;Speaker: PhD B&#10;Content: Yeah , the {disfmarker} though I think uh there was one problem with that in that , you know , we used canonical mapping so {vocalsound} our truth may not have really been {pause} true to the acoustics .&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: So .&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: Yeah . Well back twenty years ago when I did this voiced - unvoiced stuff , we were getting more like {vocalsound} ninety - seven or ninety -" />
    <node id="1. Professor A and PhD F plan to resume their work on voice activity detection (VAD) and speech processing when Hynek returns to town the week after next. They aim to organize more visits, connections, and start working towards June. This may suggest that they have a deadline or event in June for which they need to make progress on their VAD research.&#10;2. PhD D suggested working on voiced-unvoiced detection as part of Stephane's ideas. This involves analyzing the difference between speech and nonspeech, focusing on the voiced-unvoiced characteristics. They plan to explore this idea further in order to improve voice activity detection." />
    <node id=" {disfmarker} before the VAD . Because {disfmarker} {vocalsound} as {disfmarker} as you 've {pause} mentioned .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: H Hynek will be back in town uh the week after next , back {disfmarker} back in the country . So . And start {disfmarker} start organizing uh {vocalsound} more visits and connections and so forth ,&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: and {disfmarker} uh working towards June .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Also is Stephane was thinking that {vocalsound} maybe it was useful to f to think about uh {vocalsound} voiced - unvoiced {disfmarker}&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: to work uh here in voiced - unvoiced detection .&#10;Spe" />
    <node id="1. The team plans to continue working on reducing latency, although they acknowledge that their current plan may not be ideal.&#10;2. There is also a possibility of adding a second stream to the project, such as a multi-band or TRAP (Generalized TRAP) system, which both parties are willing to work on.&#10;3. They are currently exchanging emails and discussing significant results regarding this second stream.&#10;4. One party is working on integrating spectral subtraction from Ericsson, while the other party is trying their own spectral subtraction method.&#10;5. Additionally, they are considering using Larry Saul's ideas to develop a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. This could help in identifying finer details related to speech." />
    <node id=": Professor A&#10;Content: I don't know . {vocalsound} I mean , we still evidently have a latency reduction plan which {disfmarker} which isn't quite what you 'd like it to be . That {disfmarker} that seems like one prominent thing . And then uh weren't issues of {disfmarker} of having a {disfmarker} a second stream or something ? That was {disfmarker} Was it {disfmarker} There was this business that , you know , we {disfmarker} we could use up the full forty - eight hundred bits , and {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah . But I think they ' I think we want to work on this . They also want to work on this , so . Uh . {vocalsound} yeah . We {disfmarker} we will try MSG , but um , yeah . And they are t I think they want to work on the second stream also , but more with {vocalsound} some kind of multi - band or , well , what they call TRAP or generalized TRAP .&#10;Speaker: Professor A&#10;Content: Mm" />
    <node id=" now , about this and {disfmarker}&#10;Speaker: PhD F&#10;Content: Well , we are exchanging mail as soon as we {disfmarker} {vocalsound} we have significant results .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Um . Yeah . For the moment , they are working on integrating {vocalsound} the um {vocalsound} spectral subtraction apparently from Ericsson .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Um . Yeah . And so . Yeah . We are working on our side on other things like {vocalsound} uh also trying a sup spectral subtraction but of {disfmarker} of our own , I mean , another {vocalsound} spectral substraction .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Um . Yeah . So I think it 's {disfmarker} it 's OK . It 's going {disfmarker}&#10;Speaker: Professor A&#10;Content: Is there any further discussion about this {disfmarker" />
    <node id="aker: PhD B&#10;Content: And so . I 'll let you know what {disfmarker} what happens with that . But if we can {vocalsound} you know , run all of these back - ends f with many fewer iterations and {vocalsound} on Linux boxes we should be able to get a lot more experimenting done .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So . So I wanted to experiment with cutting down the number of iterations before I {vocalsound} increased the number of Gaussians .&#10;Speaker: Professor A&#10;Content: Right . Sorry . So um , how 's it going on the {disfmarker}&#10;Speaker: PhD F&#10;Content: Um .&#10;Speaker: Professor A&#10;Content: So . You {disfmarker} you did some things . They didn't improve things in a way that convinced you you 'd substantially improved anything .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: But they 're not making things worse and we have reduced latency , right ?&#10;Speaker: PhD F&#10;Content: Yeah . But actually" />
    <node id="1. The outcome of the experiment was that the French VAD performed significantly better than some other VADs when using only the baseline set of features, which included mel-cepstra. However, the performance varied at times, with the French VAD being better in some instances and worse in others. According to Professor A, the difference in performance between the French VAD and other VADs was substantial enough that it could account for a significant portion of the discrepancy in their own system's performance. PhD D mentioned that they should ask which VAD was used by Pratibha, who conducted the experiment.&#10;&#10;In summary, the French VAD outperformed some other VADs when using only the baseline set of features like mel-cepstra, and this difference in performance could potentially explain a considerable amount of the discrepancy in their own system's performance." />
    <node id=" of features , just mel cepstra , and you inc incorporate the different V A And it looks like the {disfmarker} the French VAD is actually uh better {disfmarker} significantly better .&#10;Speaker: PhD B&#10;Content: Improves the baseline ?&#10;Speaker: Professor A&#10;Content: Yeah . Yeah .&#10;Speaker: PhD F&#10;Content: Yeah but I don't know which VAD they use . Uh . If the use the small VAD I th I think it 's on {disfmarker} I think it 's easy to do better because it doesn't work at all . So . I {disfmarker} I don't know which {disfmarker} which one . It 's Pratibha that {disfmarker} that did this experiment .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Um . We should ask which VAD she used .&#10;Speaker: PhD D&#10;Content: I don't @ @ . He {disfmarker} Actually , I think that he say with the good VAD of {disfmarker} from OGI and with the Alcatel V" />
    <node id="} some kind of multi - band or , well , what they call TRAP or generalized TRAP .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Um . So .&#10;Speaker: Professor A&#10;Content: OK . Do you remember when the next meeting is supposed to be ? the next uh {disfmarker}&#10;Speaker: PhD F&#10;Content: It 's uh in June .&#10;Speaker: Professor A&#10;Content: In June . OK .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Yeah . Um . Yeah , the other thing is that you saw that {disfmarker} that mail about uh the VAD {disfmarker} V A Ds performing quite differently ? That that uh So um . This {disfmarker} there was this experiment of uh &quot; what if we just take the baseline ? &quot;&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: set uh of features , just mel cepstra , and you inc incorporate the different V A And it looks like the {disfmarker} the French VAD" />
    <node id="isfmarker} Actually , I think that he say with the good VAD of {disfmarker} from OGI and with the Alcatel VAD . And the experiment was sometime better , sometime worse .&#10;Speaker: PhD F&#10;Content: Yeah but I {disfmarker} it 's uh {disfmarker} I think you were talking about the other mail that used VAD on the reference features .&#10;Speaker: Professor A&#10;Content: Yes .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: And on that one , uh the French one is {disfmarker} was better .&#10;Speaker: PhD D&#10;Content: I don't remember .&#10;Speaker: Professor A&#10;Content: It was just better .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: I mean it was enough better that {disfmarker} that it would {vocalsound} uh account for a fair amount of the difference between our performance , actually .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Mm - h" />
    <node id="1. A convenient way to distribute changes and share information over multiple sites, while minimizing interference with regular work and potentially being useful for other tasks as well, would be to use a version control system like CVS (Concurrent Versions System). This was suggested during the discussion and it is a popular method for managing and distributing changes to software projects across multiple sites. It allows users to easily access the latest versions of files, track revisions, and work collaboratively on the project without interfering with each other's regular work. Additionally, such systems can be used for various types of tasks beyond the initial voice activity detector development, making them a versatile tool for different projects and teams." />
    <node id=" but I could {pause} try to look into like this uh CVS over the web . That seems to be a very popular {vocalsound} way of {pause} people distributing changes and {disfmarker} over , you know , multiple sites and things&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: so maybe {vocalsound} if I can figure out how do that easily and then pass the information on to everybody so that it 's {vocalsound} you know , as easy to do as possible and {disfmarker} and people don't {disfmarker} it won't interfere with {comment} their regular work , then maybe that would be good . And I think we could use it for other things around here too . So .&#10;Speaker: Professor A&#10;Content: Good .&#10;Speaker: Grad C&#10;Content: That 's cool . And if you 're interested in using CVS , I 've set it up here ,&#10;Speaker: PhD B&#10;Content: Oh great .&#10;Speaker: Grad C&#10;Content: so .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker:" />
    <node id="1. Analyzing high resolution spectrum: The team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. They plan to compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech and distinguishing it from non-speech sounds.&#10;2. Voiced-unvoiced characteristics: The team aims to analyze the difference between speech and nonspeech by focusing on the voiced-unvoiced characteristics. This is based on a speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved high correctness rates.&#10;3. Reducing background noise: Spectral subtraction before variance normalization is suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance in distinguishing between voiced speech and non-speech sounds.&#10;4. Low frequency noise consideration: It is important to consider that low frequency noise could be taken for voiced speech, leading to mistakes in distinction. Therefore, appropriate filtering or analysis methods should be used to mitigate this issue." />
    <node id="1. The result of adding spectral subtraction before the Wall process, which contains LDA (Linear Discriminant Analysis) and online normalization, was that it hurt the performance significantly.&#10;2. The sentence &quot;Due&quot; spoken in an Italian utterance, recorded by two channels (close mic microphone as channel 0 and distant microphone as channel 1), could not be clearly seen when plotted because of the following reasons:&#10;* The plot was a plot of C-zero parameters for one Italian utterance.&#10;* The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;* Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels." />
    <node id=" start {disfmarker} we started to work on spectral subtraction . And {vocalsound} um {vocalsound} the preliminary results were very bad .&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: So the thing that we did is just to add spectral subtraction before this , the Wall uh process , which contains LDA on - line normalization . And it hurts uh a lot .&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: And so we started to look at {disfmarker} at um things like this , which is , well , it 's {disfmarker} Yeah . So you have the C - zero parameters for one uh Italian utterance .&#10;Speaker: PhD D&#10;Content: You can @ @ .&#10;Speaker: PhD F&#10;Content: And I plotted this for two channels . Channel zero is the close mic microphone , and channel one is the distant microphone . And it 's perfectly synchronized , so . And the sentence contain only one word , which is &quot; Due &quot; And it can't clearly be seen . Where {disfmarker} where is it ?&#10;" />
    <node id=" the sentence contain only one word , which is &quot; Due &quot; And it can't clearly be seen . Where {disfmarker} where is it ?&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: Where is the word ?&#10;Speaker: PhD B&#10;Content: This is {disfmarker} this is ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: oh , a plot of C - zero ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: PhD B&#10;Content: the energy .&#10;Speaker: PhD F&#10;Content: This is a plot of C - zero , uh when we don't use spectral substraction , and when there is no on - line normalization .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: So . There is just some filtering with the LDA and {vocalsound} and some downsampling , upsampling .&#10;Speaker: PhD B&#10;Content: C - zero is the close talking ? {disfmarker}&#10;Speaker: PhD F&#10;Content: So .&#10;Spe" />
    <node id="The speakers observe that when they apply mean normalization to a clean channel (referred to as C-zero), it resembles the second figure, which is a positive outcome. They also note that the noise part is around zero. When both mean and variance normalization are applied, the speech portion of two channels becomes very close in the third figure. This indicates that the visualizations correspond well to their expectations for both clean speech and noisy conditions, suggesting good quality. However, they do not explicitly discuss human vocal sound abilities in this context." />
    <node id=" PhD B&#10;Content: C - zero is the close talking ? {disfmarker}&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: PhD B&#10;Content: uh the close channel ?&#10;Speaker: PhD F&#10;Content: Yeah . Yeah .&#10;Speaker: PhD B&#10;Content: and s channel one is the {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah . So C - zero is very clean , actually .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Uh then when we apply mean normalization it looks like the second figure , though it is not . Which is good . Well , the noise part is around zero&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: and {disfmarker} {vocalsound} {vocalsound} And then the third figure is what happens when we apply mean normalization and variance normalization . So . What we can clearly see is that on the speech portion {vocalsound} the two channel come {disfmarker} becomes very close , but also what happens on the noisy portion is that the variance of" />
    <node id="1. The suggestion to apply mean normalization before Voice Activity Detection (VAD) comes up as the speakers discuss Larry Saul's ideas for developing a voice or voiced-speech detector using the Fast Fourier Transform (FFT) and analyzing high resolution spectrum features. This normalization step might be beneficial for improving the performance of VAD by reducing background noise and making it easier to identify finer details related to speech in the frequency components.&#10;&#10;2. The context of integrating spectral subtraction, as mentioned by &quot;the OGI folk,&quot; implies that they are focusing on methods to minimize background noise and enhance speech recognition. Applying mean normalization before VAD could help achieve these goals by making the voice activity detection process more robust and accurate in various noisy environments.&#10;&#10;3. In summary, applying mean normalization before VAD is significant because it can potentially improve the performance of voice activity detection in adverse acoustic conditions. This is particularly relevant when working on integrating spectral subtraction techniques for speech recognition, where minimizing background noise and accurately identifying speech are crucial." />
    <node id="1. Based on the transcript, the current level of communication with the OGI gang regarding improving the vocalsound test is through exchanging emails. They are discussing Larry Saul's ideas for developing a voice or voiced-speech detector and analyzing high resolution spectrum features.&#10;&#10;2. There has been no explicit mention of success in approximating old numbers while keeping latency down. However, there have been attempts to implement changes such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. These efforts aim to improve the performance of VAD and reduce latency but their success is not explicitly mentioned in the conversation.&#10;&#10;3. In summary, while there has been ongoing communication with the OGI gang about improving the vocalsound test, there is no clear indication of success in approximating old numbers while keeping latency down. They are still working on different approaches to enhance VAD performance and reduce latency." />
    <node id="er} if in all these different cases {vocalsound} it never gets better , and there 's significant number of cases where it gets worse , {vocalsound} then you 're probably {pause} hurting things , {vocalsound} I would say . So um {vocalsound} I mean at the very least that would be a reasonably prediction of what would happen with {disfmarker} with a different test set , that you 're not jiggling things with . So I guess the question is if you can do better than this . If you can {disfmarker} if we can approximate {vocalsound} the old numbers while still keeping the latency down .&#10;Speaker: PhD F&#10;Content: Mmm . Yeah .&#10;Speaker: Professor A&#10;Content: Uh , so . Um . What I was asking , though , is uh {disfmarker} are {disfmarker} what 's {disfmarker} what 's the level of communication with uh {vocalsound} the O G I gang now , about this and {disfmarker}&#10;Speaker: PhD F&#10;Content: Well , we are exchanging mail as soon as we {d" />
    <node id="aker: PhD F&#10;Content: uh .&#10;Speaker: PhD B&#10;Content: if {vocalsound} um you know , if {disfmarker} if {disfmarker} if you work on something else and {disfmarker} and you 're waiting for them to give you {vocalsound} spectral subtraction {disfmarker} I mean it 's hard to know whether {vocalsound} the effects that you get from the other experiments you do will {vocalsound} carry over once you then bring in their spectral subtraction module . So it 's {disfmarker} it 's almost like everything 's held up waiting for this {vocalsound} one thing . I don't know if that 's true or not , but I could see how {disfmarker}&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: I don't know .&#10;Speaker: PhD B&#10;Content: Maybe that 's what you were thinking .&#10;Speaker: Professor A&#10;Content: I don't know . {vocalsound} I mean , we still evidently have a latency reduction plan which {disfmark" />
    <node id=": Uh . I guess cuz we haven't wanted to move it . We {disfmarker} we could {disfmarker} {vocalsound} we could move us ,&#10;Speaker: PhD F&#10;Content: Yeah ?&#10;Speaker: Professor A&#10;Content: and .&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Grad E&#10;Content: That 's right .&#10;Speaker: PhD F&#10;Content: Well , anyway . Um . Yeah , so . Uh . Since the last meeting we 've {disfmarker} we 've tried to put together um {vocalsound} the clean low - pass um downsampling , upsampling , I mean , Uh the new filter that 's replacing the LDA filters , and also {vocalsound} the um delay issue so that {disfmarker} We considered th the {disfmarker} the delay issue on the {disfmarker} for the on - line normalization . Mmm . So we 've put together all this and then we have results that are not um {vocalsound} {vocalsound} very impressive . Well , there is no {vocalsound} real" />
    <node id="The speakers do not directly discuss enhancing harmonic structure to reduce noise during audio processing in the given transcripts. The conversation revolves around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure for this purpose." />
    <node id="1. The researchers were discussing a potential experiment using TIMIT data and a &quot;voicing bit&quot; to possibly enhance performance in challenging audio conditions, such as noisy environments or separate audio streams. This conversation was related to Larry Saul's ideas for developing a voice or voiced-speech detector that utilizes the Fast Fourier Transform (FFT) and analyzes high resolution spectrum features.&#10;2. The voicing bit could refer to the voiced-unvoiced characteristics of speech, which are mentioned in the context of differentiating between speech and non-speech. By incorporating this information into their voice activity detector, they aim to improve its performance by better recognizing speech within various types of noise.&#10;3. The researchers also discussed applying mean normalization before Voice Activity Detection (VAD) as part of their plans for minimizing background noise and enhancing speech recognition. This step might help make the VAD more robust and accurate in different noisy conditions, which is particularly relevant when integrating spectral subtraction techniques for speech processing." />
    <node id="The transcripts do not provide direct information about measures for ensuring good audio quality when using different microphones or the recommended approach for using a pre-emphasis filter. These topics were not discussed in the meetings." />
    <node id="The significance of the OGI's focus on integrating spectral subtraction lies in their goal of minimizing background noise and enhancing speech recognition. This focus area aligns with addressing voice activity detection (VAD) issues, as applying mean normalization before VAD could potentially improve its performance in adverse acoustic conditions. By integrating spectral subtraction techniques, the OGI aims to create a more robust and accurate VAD system that can effectively distinguish speech from background noise in various environments." />
    <node id="1. Spectral Effects: The spectral effects refer to the reduction in high frequencies in the indirect energy. This is likely due to the filtering process mentioned in the transcript, where there is some attenuation of frequencies above a certain cut-off point (approximately twenty hertz). This filtering results in reduced high frequency content in the signal.&#10;&#10;2. Temporal Changes: The temporal changes are alterations in the timing characteristics of the signal. These changes can cause waveforms to appear different, even if the underlying signal is similar. For example, variations in energy levels at different time points or the presence of certain transient elements can result in visible differences between waveforms.&#10;&#10;3. Reduced High Frequencies: The reduction in high frequencies (caused by filtering) affects the overall spectral balance of the signal, resulting in a more pronounced lower frequency content. This change is responsible for some of the observed differences between waveforms and can also contribute to variations in speech perception or recognition performance.&#10;&#10;4. Distinguishing Between E and B: The discussion about distinguishing between E and B refers to trying to separate two sources of variability that may affect the data, potentially introducing bias. One possible explanation for this statement is that if one source (E) was recorded with higher energy levels or more variation compared to another source (B), it could lead to a systematic difference in the perceived signal characteristics. This potential bias can be mitigated by removing sources of variability that are unrelated to the linguistic component being analyzed.&#10;&#10;5. Spectral Subtraction: The application of spectral subtraction increases the difference between speech and noise energy levels, improving the overall signal-to-noise ratio. However, it also increases the variance of C-zero (the residual noise portion after spectral subtraction). This trade-off should be carefully considered during data analysis to balance improvements in signal quality with the increased variability due to the processing technique." />
    <node id="aker: Professor A&#10;Content: Right . But it 's {disfmarker} it 's uh {disfmarker} So . Yeah . So there are {disfmarker} clearly are spectral effects . Since you 're getting all this indirect energy , then a lot of it does have {disfmarker} have uh {vocalsound} reduced high frequencies . But um the other thing is the temporal courses of things really are changed , and {disfmarker} {vocalsound} and uh we want to show that , in some obvious way . The reason I put the wave forms in there was because {vocalsound} uh they {disfmarker} they do look quite different . Uh . And so I thought &quot; Oh , this is good . &quot; but I {disfmarker} {vocalsound} I just uh {disfmarker} After {disfmarker} after uh they were put in there I didn't really look at them anymore , cuz I just {disfmarker} they were different . So {vocalsound} I want something that has a {disfmarker} is a more interesting explanation for why they 're different . Um ." />
    <node id=" filtering and in general we like to do that , because of things like this and {vocalsound} it 's {disfmarker} it 's pretty {disfmarker} it 's not a very severe filter . Doesn't affect speech frequencies , even pretty low speech frequencies , at all , but it 's&#10;Speaker: PhD B&#10;Content: What 's the {pause} cut - off frequency it used ?&#10;Speaker: Professor A&#10;Content: Oh . I don't know I wrote this a while ago&#10;Speaker: PhD B&#10;Content: Is it like twenty ?&#10;Speaker: Professor A&#10;Content: Something like that .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Yeah . I mean I think there 's some effect above twenty but it 's {disfmarker} it 's {disfmarker} it 's {disfmarker} it 's mild . So , I mean it probably {disfmarker} there 's probably some effect up to a hundred hertz or something but it 's {disfmarker} it 's pretty mild . I don't know in the {disf" />
    <node id=" . And so maybe {disfmarker} maybe that would be more obvious .&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Grad C&#10;Content: Spectral slices ?&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: W w what d what do you mean ?&#10;Speaker: Professor A&#10;Content: Well , I mean um all the recognizers look at frames . So they {disfmarker} they look at {disfmarker}&#10;Speaker: PhD B&#10;Content: So like one instant in time .&#10;Speaker: Professor A&#10;Content: Yeah , look at a {disfmarker}&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: So it 's , yeah , at one point in time or uh twenty {disfmarker} over twenty milliseconds or something , {vocalsound} you have a spectrum or a cepstrum .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: That 's what I meant by a slice .&#10;Speaker: Grad C&#10;Content: I see .&#10;Speaker: Professor A" />
    <node id=" I mean , because {vocalsound} everything uh {disfmarker} If you have a system based on Gaussians , everything is based on means and variances .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: So if there 's an overall {vocalsound} reason {disfmarker} You know , it 's like uh if you were doing uh image processing and in some of the pictures you were looking at , uh there was a lot of light uh and {disfmarker} and in some , there was low light ,&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: you know , you would want to adjust for that in order to compare things .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: And the variance is just sort of like the next moment , you know ? So uh {vocalsound} what if um one set of pictures was taken uh so that throughout the course it was {disfmarker} went through daylight and night uh {vocalsound} um um ten times , another time it went thr" />
    <node id="The transcript does not provide a direct answer to this question. However, it includes a comment from PhD F about the Personal Digital Assistant (PDA) being consistently located far away during meetings, with the speakers noting that they have not moved it closer. The reason for this was not discussed in the meeting." />
    <node id=" B&#10;Content: I don't mean a graph . I mean the actual numbers .&#10;Speaker: Professor A&#10;Content: Oh . I see . Oh . That would be lovely , yeah .&#10;Speaker: PhD B&#10;Content: Yeah . &quot; See how different these {vocalsound} sequences of numbers are ? &quot;&#10;Speaker: Professor A&#10;Content: Yeah . Or I could just add them up and get a different total .&#10;Speaker: PhD B&#10;Content: Yeah . It 's not the square .&#10;Speaker: Professor A&#10;Content: OK . Uh . What else {disfmarker} wh what 's {disfmarker} what else is going on ?&#10;Speaker: PhD F&#10;Content: Uh , yeah . Yeah , at first I had a remark why {disfmarker} I am wondering why the PDA is always so far . I mean we are always meeting at the {vocalsound} beginning of the table and {vocalsound} the PDA 's there .&#10;Speaker: Professor A&#10;Content: Uh . I guess cuz we haven't wanted to move it . We {disfmarker} we could {disfmarker} {voc" />
    <node id="The transcripts do not provide direct information about the method used by the person who worked there some years ago to help reduce noise in signals by utilizing voicing information and estimating pitch and fine harmonic structure. The discussions revolve around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure or estimating pitch for this purpose." />
    <node id="1. Spectral subtraction increases the difference between the energy of speech and the energy of the noise portion because it is a technique used to reduce background noise in audio signals. By subtracting an estimate of the noise spectrum from the speech plus noise spectrum, the result highlights the speech components that stand out more than they would against the original noisy background. The difference between the energies becomes larger after spectral subtraction compared to before.&#10;   &#10;2. Spectral subtraction also increases the variance of C-zero (the mean normalized channel). When applying spectral subtraction, it is possible to increase the variability of the signal around its mean value due to a less accurate noise estimation or fluctuations in the speech signal itself. As a result, after spectral subtraction, the variance of C-zero will be larger than before." />
    <node id="According to the transcript, changing the self-loop transition probability by a tenth of a percent causes a ten percent difference in the word error rate. This was stated by PhD B: &quot;and it causes ten percent difference in the word error rate.&quot;" />
    <node id=" results when Stephane did that&#10;Speaker: PhD F&#10;Content: Well . Eh uh {disfmarker}&#10;Speaker: PhD B&#10;Content: and it 's {disfmarker} it 's really wo really happens .&#10;Speaker: PhD F&#10;Content: This really happens .&#10;Speaker: PhD B&#10;Content: I mean th the only difference is you change the self - loop transition probability by a tenth of a percent&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: and it causes ten percent difference in the word error rate .&#10;Speaker: Professor A&#10;Content: A tenth of a per cent .&#10;Speaker: PhD B&#10;Content: Yeah . From point {disfmarker}&#10;Speaker: PhD F&#10;Content: Even tenth of a percent ?&#10;Speaker: PhD B&#10;Content: I {disfmarker} I 'm sorry&#10;Speaker: PhD F&#10;Content: Well , we tried {disfmarker} we tried point one ,&#10;Speaker: PhD B&#10;Content: f for point {disfmarker} from {disfmarker" />
    <node id=" the improvement {vocalsound} uh relative to the {disfmarker} the baseline is small {disfmarker}&#10;Speaker: Professor A&#10;Content: So they do improvement in terms of uh accuracy ? rather than word error rate ?&#10;Speaker: PhD F&#10;Content: Uh . Uh improvement ?&#10;Speaker: Professor A&#10;Content: So {disfmarker}&#10;Speaker: PhD F&#10;Content: No , it 's compared to the word er it 's improvement on the word error rate ,&#10;Speaker: Professor A&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: yeah . Sorry .&#10;Speaker: Professor A&#10;Content: So if you have uh ten percent error and you get five percent absolute uh {vocalsound} improvement then that 's fifty percent .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: OK . So what you 're saying then is that if it 's something that has a small word error rate , {vocalsound} then uh a {disfmarker} even a relatively small improvement on it , in absolute terms , {vocalsound} will show up as quite" />
    <node id="} then uh a {disfmarker} even a relatively small improvement on it , in absolute terms , {vocalsound} will show up as quite {disfmarker} quite large in this .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Is that what you 're saying ?&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Yes .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: OK . But yeah that 's {disfmarker} that 's {disfmarker} it 's the notion of relative improvement . Word error rate .&#10;Speaker: PhD F&#10;Content: Yeah . Sure , but when we think about the weighting , which is point five , point three , point two , {vocalsound} it 's on absolute on {disfmarker} on relative figures ,&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: not {disfmarker}&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: So when we" />
    <node id="&#10;Content: But they 're not making things worse and we have reduced latency , right ?&#10;Speaker: PhD F&#10;Content: Yeah . But actually {disfmarker} um actually it seems to do a little bit worse for the well - matched case and we just noticed that {disfmarker} Yeah , actually the way the final score is computed is quite funny . It 's not a mean of word error rate . It 's not a weighted mean of word error rate , it 's a weighted mean of improvements .&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: So . Which means that {vocalsound} actually the weight on the well - matched is {disfmarker} Well I well what what {disfmarker} What happened is that if you have a small improvement or a small if on the well - matched case {vocalsound} it will have uh huge influence on the improvement compared to the reference because the reference system is {disfmarker} is {disfmarker} is quite good for {disfmarker} for the well - ma well - matched case also .&#10;Speaker: PhD B&#10;Content: So" />
    <node id="The concern about integrating a different type of spectral subtraction stems from the fact that this method can increase the variance of C-zero (the mean normalized channel) after its application. While spectral subtraction does increase the difference between the energy of speech and noise, it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance. Therefore, careful consideration and adjustments are required when integrating a new spectral subtraction technique to maintain optimal speech recognition and VAD performance." />
    <node id="1. Based on the transcript, there is no explicit statement or confirmation that they have managed to substantially improve the system while also reducing latency. The conversation involves discussions about cutting down the number of iterations, increasing the number of Gaussians, and attempting different methods such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. However, there is no clear indication of success in substantially improving the system while reducing latency." />
    <node id="Professionor A mentioned the issue of noise during his talk on reverberation, even though no one seemed to notice it, because he considered it a valuable example of real-world challenges in audio recordings. Specifically, the noise from distant microphones demonstrated background noise that can still be informative and useful for discussion. Although the example was not perfect for illustrating reverberation effects, it served as an opportunity to highlight other relevant issues in audio processing. Furthermore, Professor A used this example to emphasize the importance of filtering and processing techniques to handle such noise." />
    <node id="1. The issue encountered when adding spectral subtraction before the Wall process (which contains LDA and online normalization) was that it significantly hurt the performance. This is because spectral subtraction can increase the difference between the energy of speech and noise, but it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance.&#10;&#10;2. The C-zero parameters for a single Italian utterance in two channels cannot be clearly seen in the plot due to several reasons:&#10;   - The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;   - Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels.&#10;   - The plot being a plot of C-zero parameters for one Italian utterance may not provide enough context or clear differentiation between channels and frequencies to easily distinguish specific words or sounds." />
    <edge source="1. Identify independent information sources: When combining probabilities from independent information sources, it is crucial to ensure that these sources are statistically independent, meaning that the outcome of one source does not affect the other. In this transcript, Professor A emphasizes the importance of independent errors from different information sources.&#10;2. Calculate probabilities for cases and finer categories: First, calculate the probability of each case in your set. Then, determine the probability of finer categories on the other side (as mentioned by Professor A). For example, if you have two categories, A and B, calculate the probability of observing evidence E1, E2, ... En for both A and B.&#10;3. Multiply probabilities where appropriate: If your information sources are independent, you can multiply their corresponding probabilities to obtain the combined probability. This is based on the multiplication rule of probabilities for independent events. In the transcript, PhD F acknowledges this step by saying &quot;I see, yeah.&quot;&#10;4. Normalize means and variances: To remove biases due to variability in data, normalize the means and variances of your information sources. This helps ensure that any differences between categories are related to the linguistic component you're interested in and not confounding factors. PhD B brings up this point when they mention &quot;got overall variance&quot; and &quot;normalizing the means.&quot;&#10;5. Consider higher-order effects: While normalizing means will have a first-order effect on your results, be aware that variances can also impact your ability to distinguish between categories. Addressing these second-order effects can further improve your analysis.&#10;6. Monitor error rates: When evaluating your combined probabilities, focus on word error rates rather than accuracies, as Professor A suggests. This will give you a more nuanced understanding of how well your model is performing.&#10;&#10;By following these steps, you can effectively combine probabilities from independent information sources to distinguish between finer categories in a given set of cases." target="aker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: uh so then it you {disfmarker} you {disfmarker} you can pretty much guarantee it 's stuff that you 're not looking at very well with the other one , and uh then you only use for this one distinction .&#10;Speaker: PhD F&#10;Content: Alright .&#10;Speaker: Professor A&#10;Content: And {disfmarker} and so now you 've got a probability of the cases , and you 've got uh the probability of the finer uh categories on the other side . You multiply them where appropriate and uh {vocalsound} um&#10;Speaker: PhD F&#10;Content: I see , yeah . Mm - hmm .&#10;Speaker: Professor A&#10;Content: if they really are from independent {pause} information sources then {vocalsound} they should have different kinds of errors&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: and roughly independent errors , and {vocalsound} it 's a good choice for {disfmarker}&#10;Speaker: PhD F&#10;Content: M">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Identify independent information sources: When combining probabilities from independent information sources, it is crucial to ensure that these sources are statistically independent, meaning that the outcome of one source does not affect the other. In this transcript, Professor A emphasizes the importance of independent errors from different information sources.&#10;2. Calculate probabilities for cases and finer categories: First, calculate the probability of each case in your set. Then, determine the probability of finer categories on the other side (as mentioned by Professor A). For example, if you have two categories, A and B, calculate the probability of observing evidence E1, E2, ... En for both A and B.&#10;3. Multiply probabilities where appropriate: If your information sources are independent, you can multiply their corresponding probabilities to obtain the combined probability. This is based on the multiplication rule of probabilities for independent events. In the transcript, PhD F acknowledges this step by saying &quot;I see, yeah.&quot;&#10;4. Normalize means and variances: To remove biases due to variability in data, normalize the means and variances of your information sources. This helps ensure that any differences between categories are related to the linguistic component you're interested in and not confounding factors. PhD B brings up this point when they mention &quot;got overall variance&quot; and &quot;normalizing the means.&quot;&#10;5. Consider higher-order effects: While normalizing means will have a first-order effect on your results, be aware that variances can also impact your ability to distinguish between categories. Addressing these second-order effects can further improve your analysis.&#10;6. Monitor error rates: When evaluating your combined probabilities, focus on word error rates rather than accuracies, as Professor A suggests. This will give you a more nuanced understanding of how well your model is performing.&#10;&#10;By following these steps, you can effectively combine probabilities from independent information sources to distinguish between finer categories in a given set of cases." target=" PhD B&#10;Content: No , I understand that ,&#10;Speaker: PhD F&#10;Content: You {disfmarker} you get an estimate of the standard deviation .&#10;Speaker: PhD B&#10;Content: but I mean {disfmarker}&#10;Speaker: PhD F&#10;Content: That 's&#10;Speaker: PhD B&#10;Content: No .&#10;Speaker: PhD F&#10;Content: um {disfmarker}&#10;Speaker: PhD B&#10;Content: No , I understand what it is , but I mean , what does it {disfmarker} what 's {disfmarker} what is&#10;Speaker: PhD F&#10;Content: Yeah but .&#10;Speaker: PhD B&#10;Content: uh {disfmarker}&#10;Speaker: Professor A&#10;Content: What 's the rationale ?&#10;Speaker: PhD B&#10;Content: We Yeah . Yeah . Why {disfmarker} why do it ?&#10;Speaker: PhD F&#10;Content: Uh .&#10;Speaker: Professor A&#10;Content: Well , I mean , because {vocalsound} everything uh {disfmarker} If you have a system based on Gaussians , everything is based on">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Identify independent information sources: When combining probabilities from independent information sources, it is crucial to ensure that these sources are statistically independent, meaning that the outcome of one source does not affect the other. In this transcript, Professor A emphasizes the importance of independent errors from different information sources.&#10;2. Calculate probabilities for cases and finer categories: First, calculate the probability of each case in your set. Then, determine the probability of finer categories on the other side (as mentioned by Professor A). For example, if you have two categories, A and B, calculate the probability of observing evidence E1, E2, ... En for both A and B.&#10;3. Multiply probabilities where appropriate: If your information sources are independent, you can multiply their corresponding probabilities to obtain the combined probability. This is based on the multiplication rule of probabilities for independent events. In the transcript, PhD F acknowledges this step by saying &quot;I see, yeah.&quot;&#10;4. Normalize means and variances: To remove biases due to variability in data, normalize the means and variances of your information sources. This helps ensure that any differences between categories are related to the linguistic component you're interested in and not confounding factors. PhD B brings up this point when they mention &quot;got overall variance&quot; and &quot;normalizing the means.&quot;&#10;5. Consider higher-order effects: While normalizing means will have a first-order effect on your results, be aware that variances can also impact your ability to distinguish between categories. Addressing these second-order effects can further improve your analysis.&#10;6. Monitor error rates: When evaluating your combined probabilities, focus on word error rates rather than accuracies, as Professor A suggests. This will give you a more nuanced understanding of how well your model is performing.&#10;&#10;By following these steps, you can effectively combine probabilities from independent information sources to distinguish between finer categories in a given set of cases." target=" trying to distinguish between E and B&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: if it just so happens that the E 's {vocalsound} were a more {disfmarker} you know , were recorded when {disfmarker} when the energy was {disfmarker} was {disfmarker} was larger or something ,&#10;Speaker: PhD B&#10;Content: Mm - hmm . Mm - hmm . Mm - hmm . &#10;Speaker: Professor A&#10;Content: or the variation in it was larger , {vocalsound} uh than with the B 's , then this will be {disfmarker} give you some {disfmarker} some bias .&#10;Speaker: PhD B&#10;Content: &#10;Speaker: Professor A&#10;Content: So the {disfmarker} {vocalsound} it 's removing these sources of variability in the data {vocalsound} that have nothing to do with the linguistic component .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: Got">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Identify independent information sources: When combining probabilities from independent information sources, it is crucial to ensure that these sources are statistically independent, meaning that the outcome of one source does not affect the other. In this transcript, Professor A emphasizes the importance of independent errors from different information sources.&#10;2. Calculate probabilities for cases and finer categories: First, calculate the probability of each case in your set. Then, determine the probability of finer categories on the other side (as mentioned by Professor A). For example, if you have two categories, A and B, calculate the probability of observing evidence E1, E2, ... En for both A and B.&#10;3. Multiply probabilities where appropriate: If your information sources are independent, you can multiply their corresponding probabilities to obtain the combined probability. This is based on the multiplication rule of probabilities for independent events. In the transcript, PhD F acknowledges this step by saying &quot;I see, yeah.&quot;&#10;4. Normalize means and variances: To remove biases due to variability in data, normalize the means and variances of your information sources. This helps ensure that any differences between categories are related to the linguistic component you're interested in and not confounding factors. PhD B brings up this point when they mention &quot;got overall variance&quot; and &quot;normalizing the means.&quot;&#10;5. Consider higher-order effects: While normalizing means will have a first-order effect on your results, be aware that variances can also impact your ability to distinguish between categories. Addressing these second-order effects can further improve your analysis.&#10;6. Monitor error rates: When evaluating your combined probabilities, focus on word error rates rather than accuracies, as Professor A suggests. This will give you a more nuanced understanding of how well your model is performing.&#10;&#10;By following these steps, you can effectively combine probabilities from independent information sources to distinguish between finer categories in a given set of cases." target=" overall variance .&#10;Speaker: PhD B&#10;Content: Oh , OK . Uh - huh .&#10;Speaker: Professor A&#10;Content: And so , in principle , you {disfmarker} if you remove that source , then , you know , you can {disfmarker}&#10;Speaker: PhD B&#10;Content: I see . OK . So would {disfmarker} the major effect is {disfmarker} that you 're gonna get is by normalizing the means ,&#10;Speaker: Professor A&#10;Content: That 's the first order but {disfmarker} thing ,&#10;Speaker: PhD B&#10;Content: but it may help {disfmarker} First - order effects .&#10;Speaker: Professor A&#10;Content: but then the second order is {disfmarker} is the variances&#10;Speaker: PhD B&#10;Content: And it may help to do the variance . OK .&#10;Speaker: Professor A&#10;Content: because , again , if you {disfmarker} if you 're trying to distinguish between E and B&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: if it just so happens">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Identify independent information sources: When combining probabilities from independent information sources, it is crucial to ensure that these sources are statistically independent, meaning that the outcome of one source does not affect the other. In this transcript, Professor A emphasizes the importance of independent errors from different information sources.&#10;2. Calculate probabilities for cases and finer categories: First, calculate the probability of each case in your set. Then, determine the probability of finer categories on the other side (as mentioned by Professor A). For example, if you have two categories, A and B, calculate the probability of observing evidence E1, E2, ... En for both A and B.&#10;3. Multiply probabilities where appropriate: If your information sources are independent, you can multiply their corresponding probabilities to obtain the combined probability. This is based on the multiplication rule of probabilities for independent events. In the transcript, PhD F acknowledges this step by saying &quot;I see, yeah.&quot;&#10;4. Normalize means and variances: To remove biases due to variability in data, normalize the means and variances of your information sources. This helps ensure that any differences between categories are related to the linguistic component you're interested in and not confounding factors. PhD B brings up this point when they mention &quot;got overall variance&quot; and &quot;normalizing the means.&quot;&#10;5. Consider higher-order effects: While normalizing means will have a first-order effect on your results, be aware that variances can also impact your ability to distinguish between categories. Addressing these second-order effects can further improve your analysis.&#10;6. Monitor error rates: When evaluating your combined probabilities, focus on word error rates rather than accuracies, as Professor A suggests. This will give you a more nuanced understanding of how well your model is performing.&#10;&#10;By following these steps, you can effectively combine probabilities from independent information sources to distinguish between finer categories in a given set of cases." target="aker: PhD B&#10;Content: and I think you mentioned this in your email too {disfmarker} it 's just very um {disfmarker}&#10;Speaker: PhD F&#10;Content: Mmm , yeah .&#10;Speaker: PhD B&#10;Content: you know get stuck in some local minimum and this thing throws you out of it I guess .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Well , what 's {disfmarker} what are {disfmarker} according to the rules what {disfmarker} what are we supposed to do about the transition probabilities ? Are they supposed to be point five or point six ?&#10;Speaker: PhD B&#10;Content: I think you 're not allowed to {disfmarker} Yeah . That 's supposed to be point six , for the self - loop .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Point {disfmarker} It 's supposed to be point six .&#10;Speaker: PhD B&#10;Content: Yeah . But changing it to point five I think is {disfmarker} which">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Identify independent information sources: When combining probabilities from independent information sources, it is crucial to ensure that these sources are statistically independent, meaning that the outcome of one source does not affect the other. In this transcript, Professor A emphasizes the importance of independent errors from different information sources.&#10;2. Calculate probabilities for cases and finer categories: First, calculate the probability of each case in your set. Then, determine the probability of finer categories on the other side (as mentioned by Professor A). For example, if you have two categories, A and B, calculate the probability of observing evidence E1, E2, ... En for both A and B.&#10;3. Multiply probabilities where appropriate: If your information sources are independent, you can multiply their corresponding probabilities to obtain the combined probability. This is based on the multiplication rule of probabilities for independent events. In the transcript, PhD F acknowledges this step by saying &quot;I see, yeah.&quot;&#10;4. Normalize means and variances: To remove biases due to variability in data, normalize the means and variances of your information sources. This helps ensure that any differences between categories are related to the linguistic component you're interested in and not confounding factors. PhD B brings up this point when they mention &quot;got overall variance&quot; and &quot;normalizing the means.&quot;&#10;5. Consider higher-order effects: While normalizing means will have a first-order effect on your results, be aware that variances can also impact your ability to distinguish between categories. Addressing these second-order effects can further improve your analysis.&#10;6. Monitor error rates: When evaluating your combined probabilities, focus on word error rates rather than accuracies, as Professor A suggests. This will give you a more nuanced understanding of how well your model is performing.&#10;&#10;By following these steps, you can effectively combine probabilities from independent information sources to distinguish between finer categories in a given set of cases." target="Content: not {disfmarker}&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: So when we look at this error rate&#10;Speaker: Professor A&#10;Content: No . That 's why I 've been saying we should be looking at word error rate uh and {disfmarker} and not {disfmarker} not at {vocalsound} at accuracies .&#10;Speaker: PhD F&#10;Content: uh {disfmarker} Mmm , yeah . Mmm , yeah .&#10;Speaker: Professor A&#10;Content: It 's {disfmarker}&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: I mean uh we probably should have standardized on that all the way through . It 's just {disfmarker}&#10;Speaker: PhD B&#10;Content: Well .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: I mean , it 's not {disfmarker} it 's not that different , right ? I mean , just subtract the accuracy .&#10;Speaker: Professor A">
      <data key="d0">1</data>
    </edge>
    <edge source=" PhD B&#10;Content: No , I understand that ,&#10;Speaker: PhD F&#10;Content: You {disfmarker} you get an estimate of the standard deviation .&#10;Speaker: PhD B&#10;Content: but I mean {disfmarker}&#10;Speaker: PhD F&#10;Content: That 's&#10;Speaker: PhD B&#10;Content: No .&#10;Speaker: PhD F&#10;Content: um {disfmarker}&#10;Speaker: PhD B&#10;Content: No , I understand what it is , but I mean , what does it {disfmarker} what 's {disfmarker} what is&#10;Speaker: PhD F&#10;Content: Yeah but .&#10;Speaker: PhD B&#10;Content: uh {disfmarker}&#10;Speaker: Professor A&#10;Content: What 's the rationale ?&#10;Speaker: PhD B&#10;Content: We Yeah . Yeah . Why {disfmarker} why do it ?&#10;Speaker: PhD F&#10;Content: Uh .&#10;Speaker: Professor A&#10;Content: Well , I mean , because {vocalsound} everything uh {disfmarker} If you have a system based on Gaussians , everything is based on" target="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source=" PhD B&#10;Content: No , I understand that ,&#10;Speaker: PhD F&#10;Content: You {disfmarker} you get an estimate of the standard deviation .&#10;Speaker: PhD B&#10;Content: but I mean {disfmarker}&#10;Speaker: PhD F&#10;Content: That 's&#10;Speaker: PhD B&#10;Content: No .&#10;Speaker: PhD F&#10;Content: um {disfmarker}&#10;Speaker: PhD B&#10;Content: No , I understand what it is , but I mean , what does it {disfmarker} what 's {disfmarker} what is&#10;Speaker: PhD F&#10;Content: Yeah but .&#10;Speaker: PhD B&#10;Content: uh {disfmarker}&#10;Speaker: Professor A&#10;Content: What 's the rationale ?&#10;Speaker: PhD B&#10;Content: We Yeah . Yeah . Why {disfmarker} why do it ?&#10;Speaker: PhD F&#10;Content: Uh .&#10;Speaker: Professor A&#10;Content: Well , I mean , because {vocalsound} everything uh {disfmarker} If you have a system based on Gaussians , everything is based on" target="1. Based on the transcript, there is no explicit statement or confirmation that they have managed to substantially improve the system while also reducing latency. The conversation involves discussions about cutting down the number of iterations, increasing the number of Gaussians, and attempting different methods such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. However, there is no clear indication of success in substantially improving the system while reducing latency.">
      <data key="d0">1</data>
    </edge>
    <edge source=" trying to distinguish between E and B&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: if it just so happens that the E 's {vocalsound} were a more {disfmarker} you know , were recorded when {disfmarker} when the energy was {disfmarker} was {disfmarker} was larger or something ,&#10;Speaker: PhD B&#10;Content: Mm - hmm . Mm - hmm . Mm - hmm . &#10;Speaker: Professor A&#10;Content: or the variation in it was larger , {vocalsound} uh than with the B 's , then this will be {disfmarker} give you some {disfmarker} some bias .&#10;Speaker: PhD B&#10;Content: &#10;Speaker: Professor A&#10;Content: So the {disfmarker} {vocalsound} it 's removing these sources of variability in the data {vocalsound} that have nothing to do with the linguistic component .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: Got" target="1. During the meeting, Professor A acknowledged that the example they used to demonstrate reverberation effects during their talk was not an ideal match for the topic. The example involved noise from distant microphones, which is not the same as reverberation.&#10;2. However, Professor A also stated that the example was still valuable because it demonstrated the real-world challenge of dealing with background noise in audio recordings. They mentioned that someone later pointed out the discrepancy between the example and the topic, but no one had noticed during the actual talk.&#10;3. In summary, although the example used by Professor A was not perfect for demonstrating reverberation effects, it still served a purpose by highlighting other real-world issues related to audio recordings, such as background noise from distant microphones. The lack of concern during the talk indicates that the audience may have focused more on the main topic rather than the specific example used.">
      <data key="d0">1</data>
    </edge>
    <edge source=" trying to distinguish between E and B&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: if it just so happens that the E 's {vocalsound} were a more {disfmarker} you know , were recorded when {disfmarker} when the energy was {disfmarker} was {disfmarker} was larger or something ,&#10;Speaker: PhD B&#10;Content: Mm - hmm . Mm - hmm . Mm - hmm . &#10;Speaker: Professor A&#10;Content: or the variation in it was larger , {vocalsound} uh than with the B 's , then this will be {disfmarker} give you some {disfmarker} some bias .&#10;Speaker: PhD B&#10;Content: &#10;Speaker: Professor A&#10;Content: So the {disfmarker} {vocalsound} it 's removing these sources of variability in the data {vocalsound} that have nothing to do with the linguistic component .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: Got" target="1. Spectral Effects: The spectral effects refer to the reduction in high frequencies in the indirect energy. This is likely due to the filtering process mentioned in the transcript, where there is some attenuation of frequencies above a certain cut-off point (approximately twenty hertz). This filtering results in reduced high frequency content in the signal.&#10;&#10;2. Temporal Changes: The temporal changes are alterations in the timing characteristics of the signal. These changes can cause waveforms to appear different, even if the underlying signal is similar. For example, variations in energy levels at different time points or the presence of certain transient elements can result in visible differences between waveforms.&#10;&#10;3. Reduced High Frequencies: The reduction in high frequencies (caused by filtering) affects the overall spectral balance of the signal, resulting in a more pronounced lower frequency content. This change is responsible for some of the observed differences between waveforms and can also contribute to variations in speech perception or recognition performance.&#10;&#10;4. Distinguishing Between E and B: The discussion about distinguishing between E and B refers to trying to separate two sources of variability that may affect the data, potentially introducing bias. One possible explanation for this statement is that if one source (E) was recorded with higher energy levels or more variation compared to another source (B), it could lead to a systematic difference in the perceived signal characteristics. This potential bias can be mitigated by removing sources of variability that are unrelated to the linguistic component being analyzed.&#10;&#10;5. Spectral Subtraction: The application of spectral subtraction increases the difference between speech and noise energy levels, improving the overall signal-to-noise ratio. However, it also increases the variance of C-zero (the residual noise portion after spectral subtraction). This trade-off should be carefully considered during data analysis to balance improvements in signal quality with the increased variability due to the processing technique.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD B&#10;Content: and I think you mentioned this in your email too {disfmarker} it 's just very um {disfmarker}&#10;Speaker: PhD F&#10;Content: Mmm , yeah .&#10;Speaker: PhD B&#10;Content: you know get stuck in some local minimum and this thing throws you out of it I guess .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Well , what 's {disfmarker} what are {disfmarker} according to the rules what {disfmarker} what are we supposed to do about the transition probabilities ? Are they supposed to be point five or point six ?&#10;Speaker: PhD B&#10;Content: I think you 're not allowed to {disfmarker} Yeah . That 's supposed to be point six , for the self - loop .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Point {disfmarker} It 's supposed to be point six .&#10;Speaker: PhD B&#10;Content: Yeah . But changing it to point five I think is {disfmarker} which" target="The transcript does not provide a direct answer to this question. However, it includes a comment from PhD F about the Personal Digital Assistant (PDA) being consistently located far away during meetings, with the speakers noting that they have not moved it closer. The reason for this was not discussed in the meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD B&#10;Content: and I think you mentioned this in your email too {disfmarker} it 's just very um {disfmarker}&#10;Speaker: PhD F&#10;Content: Mmm , yeah .&#10;Speaker: PhD B&#10;Content: you know get stuck in some local minimum and this thing throws you out of it I guess .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Well , what 's {disfmarker} what are {disfmarker} according to the rules what {disfmarker} what are we supposed to do about the transition probabilities ? Are they supposed to be point five or point six ?&#10;Speaker: PhD B&#10;Content: I think you 're not allowed to {disfmarker} Yeah . That 's supposed to be point six , for the self - loop .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Point {disfmarker} It 's supposed to be point six .&#10;Speaker: PhD B&#10;Content: Yeah . But changing it to point five I think is {disfmarker} which" target="According to the transcript, changing the self-loop transition probability by a tenth of a percent causes a ten percent difference in the word error rate. This was stated by PhD B: &quot;and it causes ten percent difference in the word error rate.&quot;">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: not {disfmarker}&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: So when we look at this error rate&#10;Speaker: Professor A&#10;Content: No . That 's why I 've been saying we should be looking at word error rate uh and {disfmarker} and not {disfmarker} not at {vocalsound} at accuracies .&#10;Speaker: PhD F&#10;Content: uh {disfmarker} Mmm , yeah . Mmm , yeah .&#10;Speaker: Professor A&#10;Content: It 's {disfmarker}&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: I mean uh we probably should have standardized on that all the way through . It 's just {disfmarker}&#10;Speaker: PhD B&#10;Content: Well .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: I mean , it 's not {disfmarker} it 's not that different , right ? I mean , just subtract the accuracy .&#10;Speaker: Professor A" target="According to the transcript, changing the self-loop transition probability by a tenth of a percent causes a ten percent difference in the word error rate. This was stated by PhD B: &quot;and it causes ten percent difference in the word error rate.&quot;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target=" . Uh yeah . B We should let him finish what he w he was gonna say ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: and {disfmarker}&#10;Speaker: PhD B&#10;Content: So go ahead .&#10;Speaker: PhD F&#10;Content: Um yeah , so yeah , I think if we try to develop a second stream well , there would be one stream that is the envelope and the second , it could be interesting to have that 's {disfmarker} something that 's more related to the fine structure of the spectrum . And . Yeah , so I don't know . We were thinking about like using ideas from {disfmarker} from Larry Saul , have a good voice detector , have a good , well , voiced - speech detector , that 's working on {disfmarker} on the FFT and {vocalsound} uh&#10;Speaker: Professor A&#10;Content: U&#10;Speaker: PhD F&#10;Content: Larry Saul could be an idea . We were are thinking about just {vocalsound} kind of uh taking the spectrum and computing the variance of {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target=" Larry Saul could be an idea . We were are thinking about just {vocalsound} kind of uh taking the spectrum and computing the variance of {disfmarker} of the high resolution spectrum {vocalsound} and things like this .&#10;Speaker: Professor A&#10;Content: So u s u OK . So {disfmarker} So many {vocalsound} tell you something about that . Uh we had a guy here some years ago who did some work on {vocalsound} um {vocalsound} making use of voicing information uh to {vocalsound} help in reducing the noise .&#10;Speaker: PhD F&#10;Content: Yeah ?&#10;Speaker: Professor A&#10;Content: So what he was doing is basically y you {disfmarker} {vocalsound} you do estimate the pitch .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: And um you {disfmarker} from that you {disfmarker} you estimate {disfmarker} or you estimate fine harmonic structure , whichev ei either way , it 's more or less the same . But {vocalsound} uh the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target=" lots of ideas for doing voice activity , or speech - nonspeech rather , {comment} um by looking at {vocalsound} um , you know , uh {vocalsound} I guess harmonics or looking across time {disfmarker}&#10;Speaker: Professor A&#10;Content: Well , I think he was talking about the voiced - unvoiced , though ,&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: right ? So , not the speech - nonspeech .&#10;Speaker: PhD B&#10;Content: Yeah . Well even with e&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: uh w ah you know , uh even with the voiced - non {pause} voiced - unvoiced&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: um {disfmarker} I thought that you or {pause} somebody was talking about {disfmarker}&#10;Speaker: Professor A&#10;Content: Well . Uh yeah . B We should let him finish what he w he was gonna say ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="Content: Um . It seems that after spectral subtraction , speech is more emerging now uh {vocalsound} than {disfmarker} than before .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Speech is more what ?&#10;Speaker: PhD F&#10;Content: Well , the difference between the energy of the speech and the energy of the n spectral subtrac subtracted noise portion is {disfmarker} is larger .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Well , if you compare the first figure to this one {disfmarker} Actually the scale is not the same , but if you look at the {disfmarker} the numbers um {vocalsound} you clearly see that the difference between the C - zero of the speech and C - zero of the noise portion is larger . Uh but what happens is that after spectral subtraction , {vocalsound} you also increase the variance of this {disfmarker} of C - zero .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target=" trying to {vocalsound} integrate this other {disfmarker} other uh spectral subtraction , {vocalsound} why are we worrying about it ? &quot;&#10;Speaker: PhD F&#10;Content: Mm - hmm . About ? Spectral subtraction ?&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: It 's just uh {disfmarker} Well it 's another {disfmarker} They are trying to u to use the um {disfmarker} {vocalsound} the Ericsson and we 're trying to use something {disfmarker} something else . And . Yeah , and also to understand what happens because&#10;Speaker: Professor A&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: uh fff Well . When we do spectral subtraction , actually , I think {vocalsound} that this is the {disfmarker} the two last figures .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Um . It seems that after spectral subtraction , speech is more emerging now uh {vocalsound} than {disfmarker} than before">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target=" we 've already observed . But uh , yeah , voice activity detection is not {vocalsound} {vocalsound} an easy thing neither .&#10;Speaker: PhD B&#10;Content: But after you do this , after you do the variance normalization {disfmarker} I mean .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: I don't know , it seems like this would be a lot easier than this signal to work with .&#10;Speaker: PhD F&#10;Content: Yeah . So . What I notice is that , while I prefer to look at the second figure than at the third one , well , because you clearly see where speech is .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: But the problem is that on the speech portion , channel zero and channel one are more different than when you use variance normalization where channel zero and channel one become closer .&#10;Speaker: Professor A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: But for the purposes of finding the speech {disfmarker}&#10;Speaker: PhD F">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="The person they were discussing with suggested using a digital filter to preprocess the data before analyzing it. This would be an alternative to changing the capacitor on the input box.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="1. Professor A and PhD F plan to resume their work on voice activity detection (VAD) and speech processing when Hynek returns to town the week after next. They aim to organize more visits, connections, and start working towards June. This may suggest that they have a deadline or event in June for which they need to make progress on their VAD research.&#10;2. PhD D suggested working on voiced-unvoiced detection as part of Stephane's ideas. This involves analyzing the difference between speech and nonspeech, focusing on the voiced-unvoiced characteristics. They plan to explore this idea further in order to improve voice activity detection.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="1. The team plans to continue working on reducing latency, although they acknowledge that their current plan may not be ideal.&#10;2. There is also a possibility of adding a second stream to the project, such as a multi-band or TRAP (Generalized TRAP) system, which both parties are willing to work on.&#10;3. They are currently exchanging emails and discussing significant results regarding this second stream.&#10;4. One party is working on integrating spectral subtraction from Ericsson, while the other party is trying their own spectral subtraction method.&#10;5. Additionally, they are considering using Larry Saul's ideas to develop a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. This could help in identifying finer details related to speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="1. A convenient way to distribute changes and share information over multiple sites, while minimizing interference with regular work and potentially being useful for other tasks as well, would be to use a version control system like CVS (Concurrent Versions System). This was suggested during the discussion and it is a popular method for managing and distributing changes to software projects across multiple sites. It allows users to easily access the latest versions of files, track revisions, and work collaboratively on the project without interfering with each other's regular work. Additionally, such systems can be used for various types of tasks beyond the initial voice activity detector development, making them a versatile tool for different projects and teams.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="1. Analyzing high resolution spectrum: The team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. They plan to compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech and distinguishing it from non-speech sounds.&#10;2. Voiced-unvoiced characteristics: The team aims to analyze the difference between speech and nonspeech by focusing on the voiced-unvoiced characteristics. This is based on a speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved high correctness rates.&#10;3. Reducing background noise: Spectral subtraction before variance normalization is suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance in distinguishing between voiced speech and non-speech sounds.&#10;4. Low frequency noise consideration: It is important to consider that low frequency noise could be taken for voiced speech, leading to mistakes in distinction. Therefore, appropriate filtering or analysis methods should be used to mitigate this issue.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="1. The result of adding spectral subtraction before the Wall process, which contains LDA (Linear Discriminant Analysis) and online normalization, was that it hurt the performance significantly.&#10;2. The sentence &quot;Due&quot; spoken in an Italian utterance, recorded by two channels (close mic microphone as channel 0 and distant microphone as channel 1), could not be clearly seen when plotted because of the following reasons:&#10;* The plot was a plot of C-zero parameters for one Italian utterance.&#10;* The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;* Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="The speakers observe that when they apply mean normalization to a clean channel (referred to as C-zero), it resembles the second figure, which is a positive outcome. They also note that the noise part is around zero. When both mean and variance normalization are applied, the speech portion of two channels becomes very close in the third figure. This indicates that the visualizations correspond well to their expectations for both clean speech and noisy conditions, suggesting good quality. However, they do not explicitly discuss human vocal sound abilities in this context.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="1. The suggestion to apply mean normalization before Voice Activity Detection (VAD) comes up as the speakers discuss Larry Saul's ideas for developing a voice or voiced-speech detector using the Fast Fourier Transform (FFT) and analyzing high resolution spectrum features. This normalization step might be beneficial for improving the performance of VAD by reducing background noise and making it easier to identify finer details related to speech in the frequency components.&#10;&#10;2. The context of integrating spectral subtraction, as mentioned by &quot;the OGI folk,&quot; implies that they are focusing on methods to minimize background noise and enhance speech recognition. Applying mean normalization before VAD could help achieve these goals by making the voice activity detection process more robust and accurate in various noisy environments.&#10;&#10;3. In summary, applying mean normalization before VAD is significant because it can potentially improve the performance of voice activity detection in adverse acoustic conditions. This is particularly relevant when working on integrating spectral subtraction techniques for speech recognition, where minimizing background noise and accurately identifying speech are crucial.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="The speakers do not directly discuss enhancing harmonic structure to reduce noise during audio processing in the given transcripts. The conversation revolves around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="1. The researchers were discussing a potential experiment using TIMIT data and a &quot;voicing bit&quot; to possibly enhance performance in challenging audio conditions, such as noisy environments or separate audio streams. This conversation was related to Larry Saul's ideas for developing a voice or voiced-speech detector that utilizes the Fast Fourier Transform (FFT) and analyzes high resolution spectrum features.&#10;2. The voicing bit could refer to the voiced-unvoiced characteristics of speech, which are mentioned in the context of differentiating between speech and non-speech. By incorporating this information into their voice activity detector, they aim to improve its performance by better recognizing speech within various types of noise.&#10;3. The researchers also discussed applying mean normalization before Voice Activity Detection (VAD) as part of their plans for minimizing background noise and enhancing speech recognition. This step might help make the VAD more robust and accurate in different noisy conditions, which is particularly relevant when integrating spectral subtraction techniques for speech processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="The significance of the OGI's focus on integrating spectral subtraction lies in their goal of minimizing background noise and enhancing speech recognition. This focus area aligns with addressing voice activity detection (VAD) issues, as applying mean normalization before VAD could potentially improve its performance in adverse acoustic conditions. By integrating spectral subtraction techniques, the OGI aims to create a more robust and accurate VAD system that can effectively distinguish speech from background noise in various environments.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="The transcripts do not provide direct information about the method used by the person who worked there some years ago to help reduce noise in signals by utilizing voicing information and estimating pitch and fine harmonic structure. The discussions revolve around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure or estimating pitch for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="1. Spectral subtraction increases the difference between the energy of speech and the energy of the noise portion because it is a technique used to reduce background noise in audio signals. By subtracting an estimate of the noise spectrum from the speech plus noise spectrum, the result highlights the speech components that stand out more than they would against the original noisy background. The difference between the energies becomes larger after spectral subtraction compared to before.&#10;   &#10;2. Spectral subtraction also increases the variance of C-zero (the mean normalized channel). When applying spectral subtraction, it is possible to increase the variability of the signal around its mean value due to a less accurate noise estimation or fluctuations in the speech signal itself. As a result, after spectral subtraction, the variance of C-zero will be larger than before.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="The concern about integrating a different type of spectral subtraction stems from the fact that this method can increase the variance of C-zero (the mean normalized channel) after its application. While spectral subtraction does increase the difference between the energy of speech and noise, it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance. Therefore, careful consideration and adjustments are required when integrating a new spectral subtraction technique to maintain optimal speech recognition and VAD performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using Larry Saul's ideas, the team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech." target="1. The issue encountered when adding spectral subtraction before the Wall process (which contains LDA and online normalization) was that it significantly hurt the performance. This is because spectral subtraction can increase the difference between the energy of speech and noise, but it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance.&#10;&#10;2. The C-zero parameters for a single Italian utterance in two channels cannot be clearly seen in the plot due to several reasons:&#10;   - The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;   - Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels.&#10;   - The plot being a plot of C-zero parameters for one Italian utterance may not provide enough context or clear differentiation between channels and frequencies to easily distinguish specific words or sounds.">
      <data key="d0">1</data>
    </edge>
    <edge source=" . Uh yeah . B We should let him finish what he w he was gonna say ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: and {disfmarker}&#10;Speaker: PhD B&#10;Content: So go ahead .&#10;Speaker: PhD F&#10;Content: Um yeah , so yeah , I think if we try to develop a second stream well , there would be one stream that is the envelope and the second , it could be interesting to have that 's {disfmarker} something that 's more related to the fine structure of the spectrum . And . Yeah , so I don't know . We were thinking about like using ideas from {disfmarker} from Larry Saul , have a good voice detector , have a good , well , voiced - speech detector , that 's working on {disfmarker} on the FFT and {vocalsound} uh&#10;Speaker: Professor A&#10;Content: U&#10;Speaker: PhD F&#10;Content: Larry Saul could be an idea . We were are thinking about just {vocalsound} kind of uh taking the spectrum and computing the variance of {disf" target="1. The team plans to continue working on reducing latency, although they acknowledge that their current plan may not be ideal.&#10;2. There is also a possibility of adding a second stream to the project, such as a multi-band or TRAP (Generalized TRAP) system, which both parties are willing to work on.&#10;3. They are currently exchanging emails and discussing significant results regarding this second stream.&#10;4. One party is working on integrating spectral subtraction from Ericsson, while the other party is trying their own spectral subtraction method.&#10;5. Additionally, they are considering using Larry Saul's ideas to develop a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. This could help in identifying finer details related to speech.">
      <data key="d0">1</data>
    </edge>
    <edge source=" lots of ideas for doing voice activity , or speech - nonspeech rather , {comment} um by looking at {vocalsound} um , you know , uh {vocalsound} I guess harmonics or looking across time {disfmarker}&#10;Speaker: Professor A&#10;Content: Well , I think he was talking about the voiced - unvoiced , though ,&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: right ? So , not the speech - nonspeech .&#10;Speaker: PhD B&#10;Content: Yeah . Well even with e&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: uh w ah you know , uh even with the voiced - non {pause} voiced - unvoiced&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: um {disfmarker} I thought that you or {pause} somebody was talking about {disfmarker}&#10;Speaker: Professor A&#10;Content: Well . Uh yeah . B We should let him finish what he w he was gonna say ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker" target="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises.">
      <data key="d0">1</data>
    </edge>
    <edge source=" lots of ideas for doing voice activity , or speech - nonspeech rather , {comment} um by looking at {vocalsound} um , you know , uh {vocalsound} I guess harmonics or looking across time {disfmarker}&#10;Speaker: Professor A&#10;Content: Well , I think he was talking about the voiced - unvoiced , though ,&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: right ? So , not the speech - nonspeech .&#10;Speaker: PhD B&#10;Content: Yeah . Well even with e&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: uh w ah you know , uh even with the voiced - non {pause} voiced - unvoiced&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: um {disfmarker} I thought that you or {pause} somebody was talking about {disfmarker}&#10;Speaker: Professor A&#10;Content: Well . Uh yeah . B We should let him finish what he w he was gonna say ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker" target="1. Professor A and PhD F plan to resume their work on voice activity detection (VAD) and speech processing when Hynek returns to town the week after next. They aim to organize more visits, connections, and start working towards June. This may suggest that they have a deadline or event in June for which they need to make progress on their VAD research.&#10;2. PhD D suggested working on voiced-unvoiced detection as part of Stephane's ideas. This involves analyzing the difference between speech and nonspeech, focusing on the voiced-unvoiced characteristics. They plan to explore this idea further in order to improve voice activity detection.">
      <data key="d0">1</data>
    </edge>
    <edge source=" lots of ideas for doing voice activity , or speech - nonspeech rather , {comment} um by looking at {vocalsound} um , you know , uh {vocalsound} I guess harmonics or looking across time {disfmarker}&#10;Speaker: Professor A&#10;Content: Well , I think he was talking about the voiced - unvoiced , though ,&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: right ? So , not the speech - nonspeech .&#10;Speaker: PhD B&#10;Content: Yeah . Well even with e&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: uh w ah you know , uh even with the voiced - non {pause} voiced - unvoiced&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: um {disfmarker} I thought that you or {pause} somebody was talking about {disfmarker}&#10;Speaker: Professor A&#10;Content: Well . Uh yeah . B We should let him finish what he w he was gonna say ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker" target="1. Analyzing high resolution spectrum: The team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. They plan to compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech and distinguishing it from non-speech sounds.&#10;2. Voiced-unvoiced characteristics: The team aims to analyze the difference between speech and nonspeech by focusing on the voiced-unvoiced characteristics. This is based on a speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved high correctness rates.&#10;3. Reducing background noise: Spectral subtraction before variance normalization is suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance in distinguishing between voiced speech and non-speech sounds.&#10;4. Low frequency noise consideration: It is important to consider that low frequency noise could be taken for voiced speech, leading to mistakes in distinction. Therefore, appropriate filtering or analysis methods should be used to mitigate this issue.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Um . It seems that after spectral subtraction , speech is more emerging now uh {vocalsound} than {disfmarker} than before .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Speech is more what ?&#10;Speaker: PhD F&#10;Content: Well , the difference between the energy of the speech and the energy of the n spectral subtrac subtracted noise portion is {disfmarker} is larger .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Well , if you compare the first figure to this one {disfmarker} Actually the scale is not the same , but if you look at the {disfmarker} the numbers um {vocalsound} you clearly see that the difference between the C - zero of the speech and C - zero of the noise portion is larger . Uh but what happens is that after spectral subtraction , {vocalsound} you also increase the variance of this {disfmarker} of C - zero .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content" target="1. The result of adding spectral subtraction before the Wall process, which contains LDA (Linear Discriminant Analysis) and online normalization, was that it hurt the performance significantly.&#10;2. The sentence &quot;Due&quot; spoken in an Italian utterance, recorded by two channels (close mic microphone as channel 0 and distant microphone as channel 1), could not be clearly seen when plotted because of the following reasons:&#10;* The plot was a plot of C-zero parameters for one Italian utterance.&#10;* The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;* Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Um . It seems that after spectral subtraction , speech is more emerging now uh {vocalsound} than {disfmarker} than before .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Speech is more what ?&#10;Speaker: PhD F&#10;Content: Well , the difference between the energy of the speech and the energy of the n spectral subtrac subtracted noise portion is {disfmarker} is larger .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Well , if you compare the first figure to this one {disfmarker} Actually the scale is not the same , but if you look at the {disfmarker} the numbers um {vocalsound} you clearly see that the difference between the C - zero of the speech and C - zero of the noise portion is larger . Uh but what happens is that after spectral subtraction , {vocalsound} you also increase the variance of this {disfmarker} of C - zero .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content" target="1. Spectral Effects: The spectral effects refer to the reduction in high frequencies in the indirect energy. This is likely due to the filtering process mentioned in the transcript, where there is some attenuation of frequencies above a certain cut-off point (approximately twenty hertz). This filtering results in reduced high frequency content in the signal.&#10;&#10;2. Temporal Changes: The temporal changes are alterations in the timing characteristics of the signal. These changes can cause waveforms to appear different, even if the underlying signal is similar. For example, variations in energy levels at different time points or the presence of certain transient elements can result in visible differences between waveforms.&#10;&#10;3. Reduced High Frequencies: The reduction in high frequencies (caused by filtering) affects the overall spectral balance of the signal, resulting in a more pronounced lower frequency content. This change is responsible for some of the observed differences between waveforms and can also contribute to variations in speech perception or recognition performance.&#10;&#10;4. Distinguishing Between E and B: The discussion about distinguishing between E and B refers to trying to separate two sources of variability that may affect the data, potentially introducing bias. One possible explanation for this statement is that if one source (E) was recorded with higher energy levels or more variation compared to another source (B), it could lead to a systematic difference in the perceived signal characteristics. This potential bias can be mitigated by removing sources of variability that are unrelated to the linguistic component being analyzed.&#10;&#10;5. Spectral Subtraction: The application of spectral subtraction increases the difference between speech and noise energy levels, improving the overall signal-to-noise ratio. However, it also increases the variance of C-zero (the residual noise portion after spectral subtraction). This trade-off should be carefully considered during data analysis to balance improvements in signal quality with the increased variability due to the processing technique.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Um . It seems that after spectral subtraction , speech is more emerging now uh {vocalsound} than {disfmarker} than before .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Speech is more what ?&#10;Speaker: PhD F&#10;Content: Well , the difference between the energy of the speech and the energy of the n spectral subtrac subtracted noise portion is {disfmarker} is larger .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Well , if you compare the first figure to this one {disfmarker} Actually the scale is not the same , but if you look at the {disfmarker} the numbers um {vocalsound} you clearly see that the difference between the C - zero of the speech and C - zero of the noise portion is larger . Uh but what happens is that after spectral subtraction , {vocalsound} you also increase the variance of this {disfmarker} of C - zero .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content" target="1. Spectral subtraction increases the difference between the energy of speech and the energy of the noise portion because it is a technique used to reduce background noise in audio signals. By subtracting an estimate of the noise spectrum from the speech plus noise spectrum, the result highlights the speech components that stand out more than they would against the original noisy background. The difference between the energies becomes larger after spectral subtraction compared to before.&#10;   &#10;2. Spectral subtraction also increases the variance of C-zero (the mean normalized channel). When applying spectral subtraction, it is possible to increase the variability of the signal around its mean value due to a less accurate noise estimation or fluctuations in the speech signal itself. As a result, after spectral subtraction, the variance of C-zero will be larger than before.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Um . It seems that after spectral subtraction , speech is more emerging now uh {vocalsound} than {disfmarker} than before .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Speech is more what ?&#10;Speaker: PhD F&#10;Content: Well , the difference between the energy of the speech and the energy of the n spectral subtrac subtracted noise portion is {disfmarker} is larger .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Well , if you compare the first figure to this one {disfmarker} Actually the scale is not the same , but if you look at the {disfmarker} the numbers um {vocalsound} you clearly see that the difference between the C - zero of the speech and C - zero of the noise portion is larger . Uh but what happens is that after spectral subtraction , {vocalsound} you also increase the variance of this {disfmarker} of C - zero .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content" target="The concern about integrating a different type of spectral subtraction stems from the fact that this method can increase the variance of C-zero (the mean normalized channel) after its application. While spectral subtraction does increase the difference between the energy of speech and noise, it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance. Therefore, careful consideration and adjustments are required when integrating a new spectral subtraction technique to maintain optimal speech recognition and VAD performance.">
      <data key="d0">1</data>
    </edge>
    <edge source=" trying to {vocalsound} integrate this other {disfmarker} other uh spectral subtraction , {vocalsound} why are we worrying about it ? &quot;&#10;Speaker: PhD F&#10;Content: Mm - hmm . About ? Spectral subtraction ?&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: It 's just uh {disfmarker} Well it 's another {disfmarker} They are trying to u to use the um {disfmarker} {vocalsound} the Ericsson and we 're trying to use something {disfmarker} something else . And . Yeah , and also to understand what happens because&#10;Speaker: Professor A&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: uh fff Well . When we do spectral subtraction , actually , I think {vocalsound} that this is the {disfmarker} the two last figures .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Um . It seems that after spectral subtraction , speech is more emerging now uh {vocalsound} than {disfmarker} than before" target="The transcript does not provide a direct answer to this question. However, it includes a comment from PhD F about the Personal Digital Assistant (PDA) being consistently located far away during meetings, with the speakers noting that they have not moved it closer. The reason for this was not discussed in the meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="The reason they cannot use Aurora for the suggested voicing bit in frame classification is that there is no phoneme level labeling available for Aurora, only word level labeling. In contrast, they have access to phoneme level labeling for Italian, which makes it suitable for their current task." target=" huh .&#10;Speaker: PhD F&#10;Content: and see if giving the d uh , this voicing bit would help in {disfmarker} in terms of uh frame classification .&#10;Speaker: Professor A&#10;Content: Why don't you {disfmarker} why don't you just do it with Aurora ?&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: Just any i in {disfmarker} in each {disfmarker} in each frame&#10;Speaker: PhD F&#10;Content: Yeah , but {disfmarker} but {disfmarker} B but we cannot do the cheating , this cheating thing .&#10;Speaker: Grad E&#10;Content: We 're {disfmarker}&#10;Speaker: Professor A&#10;Content: uh {disfmarker}&#10;Speaker: Grad E&#10;Content: We need labels .&#10;Speaker: Professor A&#10;Content: Why not ?&#10;Speaker: PhD F&#10;Content: Well . Cuz we don't have {disfmarker} Well , for Italian perhaps we have , but we don't have this labeling for Aurora . We just have a labeling with word models&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="The reason they cannot use Aurora for the suggested voicing bit in frame classification is that there is no phoneme level labeling available for Aurora, only word level labeling. In contrast, they have access to phoneme level labeling for Italian, which makes it suitable for their current task." target="isfmarker} Well , for Italian perhaps we have , but we don't have this labeling for Aurora . We just have a labeling with word models&#10;Speaker: Professor A&#10;Content: I see .&#10;Speaker: PhD F&#10;Content: but not for phonemes .&#10;Speaker: PhD D&#10;Content: Not for foreigners .&#10;Speaker: Grad E&#10;Content: we don't have frame {disfmarker} frame level transcriptions .&#10;Speaker: Professor A&#10;Content: Um .&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Um . {vocalsound} Yeah .&#10;Speaker: Professor A&#10;Content: But you could {disfmarker} I mean you can {disfmarker} you can align so that {disfmarker} It 's not perfect , but if you {disfmarker} if you know what was said and {disfmarker}&#10;Speaker: PhD B&#10;Content: But the problem is that their models are all word level models . So there 's no phone models {pause} that you get alignments for .&#10;Speaker: PhD F&#10;Content: Mm - hmm">
      <data key="d0">1</data>
    </edge>
    <edge source="The reason they cannot use Aurora for the suggested voicing bit in frame classification is that there is no phoneme level labeling available for Aurora, only word level labeling. In contrast, they have access to phoneme level labeling for Italian, which makes it suitable for their current task." target=" . So there 's no phone models {pause} that you get alignments for .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Oh .&#10;Speaker: PhD B&#10;Content: You {disfmarker} So you could find out where the word boundaries are but that 's about it .&#10;Speaker: Professor A&#10;Content: Yeah . I see .&#10;Speaker: Grad E&#10;Content: S But we could use uh the {disfmarker} the noisy version that TIMIT , which {vocalsound} you know , is similar to the {disfmarker} the noises found in the TI - digits {vocalsound} um portion of Aurora .&#10;Speaker: PhD F&#10;Content: Yeah . noise , yeah . Yeah , that 's right , yep . Mmm .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Well , I guess {disfmarker} I guess we can {disfmarker} we can say that it will help , but I don't know . If this voicing bit doesn't help , uh , I think we don't have to">
      <data key="d0">1</data>
    </edge>
    <edge source="The reason they cannot use Aurora for the suggested voicing bit in frame classification is that there is no phoneme level labeling available for Aurora, only word level labeling. In contrast, they have access to phoneme level labeling for Italian, which makes it suitable for their current task." target=" {disfmarker} {vocalsound} We are trying to {disfmarker} to do something with the Meeting Recorder digits ,&#10;Speaker: Professor A&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: and {disfmarker} But yeah . Yeah . And the good thing is that {pause} there is this first deadline ,&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: and , well , some people from OGI are working on a paper for this , but there is also the um {vocalsound} special session about th Aurora which is {disfmarker} {vocalsound} uh which has an extended deadline . So . The deadline is in May .&#10;Speaker: Professor A&#10;Content: For uh {disfmarker} {vocalsound} Oh , for Eurospeech ?&#10;Speaker: PhD F&#10;Content: For th Yeah .&#10;Speaker: Professor A&#10;Content: Oh !&#10;Speaker: PhD F&#10;Content: So f only for the experiments on Aurora . So it {disfmarker} it 's good ,&#10;Speaker: Professor A&#10;Content: Oh , a special dispens">
      <data key="d0">1</data>
    </edge>
    <edge source="The reason they cannot use Aurora for the suggested voicing bit in frame classification is that there is no phoneme level labeling available for Aurora, only word level labeling. In contrast, they have access to phoneme level labeling for Italian, which makes it suitable for their current task." target=" you {disfmarker} it goes &quot; digit &quot;&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: and then that can be {disfmarker} either go to silence or go to another digit , which {disfmarker} That model would allow for the production of {vocalsound} infinitely long sequences of digits , right ?&#10;Speaker: Professor A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: So . I thought &quot; well I 'm gonna just look at the {disfmarker} what actual digit strings do occur in the training data . &quot;&#10;Speaker: Professor A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: And the interesting thing was it turns out that there are no sequences of two - long or three - long digit strings {pause} in any of the Aurora training data . So it 's either one , four , five , six , uh up to eleven , and then it skips and then there 's some at sixteen .&#10;Speaker: Professor A&#10;Content: But what about the testing data ?&#10;Speaker: PhD B&#10;Content: Um . I don't know . I">
      <data key="d0">1</data>
    </edge>
    <edge source="The reason they cannot use Aurora for the suggested voicing bit in frame classification is that there is no phoneme level labeling available for Aurora, only word level labeling. In contrast, they have access to phoneme level labeling for Italian, which makes it suitable for their current task." target="aker: PhD F&#10;Content: because I guess voice activity detection on this should {disfmarker} could be worse .&#10;Speaker: PhD B&#10;Content: Yeah . Is it applied all the way back here ?&#10;Speaker: PhD F&#10;Content: It 's applied the um on , yeah , something like this ,&#10;Speaker: PhD B&#10;Content: Maybe that 's why it doesn't work for channel one .&#10;Speaker: PhD F&#10;Content: yeah . Perhaps , yeah .&#10;Speaker: Professor A&#10;Content: Can I {disfmarker}&#10;Speaker: PhD F&#10;Content: So we could perhaps do just mean normalization before VAD .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Mm - hmm . Can I ask a , I mean {disfmarker} a sort of top - level question , which is {vocalsound} um &quot; if {disfmarker} if most of what the OGI folk are working with is trying to {vocalsound} integrate this other {disfmarker} other uh spectral subtraction , {vocalsound} why are we worrying about">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD F&#10;Content: because I guess voice activity detection on this should {disfmarker} could be worse .&#10;Speaker: PhD B&#10;Content: Yeah . Is it applied all the way back here ?&#10;Speaker: PhD F&#10;Content: It 's applied the um on , yeah , something like this ,&#10;Speaker: PhD B&#10;Content: Maybe that 's why it doesn't work for channel one .&#10;Speaker: PhD F&#10;Content: yeah . Perhaps , yeah .&#10;Speaker: Professor A&#10;Content: Can I {disfmarker}&#10;Speaker: PhD F&#10;Content: So we could perhaps do just mean normalization before VAD .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Mm - hmm . Can I ask a , I mean {disfmarker} a sort of top - level question , which is {vocalsound} um &quot; if {disfmarker} if most of what the OGI folk are working with is trying to {vocalsound} integrate this other {disfmarker} other uh spectral subtraction , {vocalsound} why are we worrying about" target="1. Professor A and PhD F plan to resume their work on voice activity detection (VAD) and speech processing when Hynek returns to town the week after next. They aim to organize more visits, connections, and start working towards June. This may suggest that they have a deadline or event in June for which they need to make progress on their VAD research.&#10;2. PhD D suggested working on voiced-unvoiced detection as part of Stephane's ideas. This involves analyzing the difference between speech and nonspeech, focusing on the voiced-unvoiced characteristics. They plan to explore this idea further in order to improve voice activity detection.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD F&#10;Content: because I guess voice activity detection on this should {disfmarker} could be worse .&#10;Speaker: PhD B&#10;Content: Yeah . Is it applied all the way back here ?&#10;Speaker: PhD F&#10;Content: It 's applied the um on , yeah , something like this ,&#10;Speaker: PhD B&#10;Content: Maybe that 's why it doesn't work for channel one .&#10;Speaker: PhD F&#10;Content: yeah . Perhaps , yeah .&#10;Speaker: Professor A&#10;Content: Can I {disfmarker}&#10;Speaker: PhD F&#10;Content: So we could perhaps do just mean normalization before VAD .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Mm - hmm . Can I ask a , I mean {disfmarker} a sort of top - level question , which is {vocalsound} um &quot; if {disfmarker} if most of what the OGI folk are working with is trying to {vocalsound} integrate this other {disfmarker} other uh spectral subtraction , {vocalsound} why are we worrying about" target="1. The suggestion to apply mean normalization before Voice Activity Detection (VAD) comes up as the speakers discuss Larry Saul's ideas for developing a voice or voiced-speech detector using the Fast Fourier Transform (FFT) and analyzing high resolution spectrum features. This normalization step might be beneficial for improving the performance of VAD by reducing background noise and making it easier to identify finer details related to speech in the frequency components.&#10;&#10;2. The context of integrating spectral subtraction, as mentioned by &quot;the OGI folk,&quot; implies that they are focusing on methods to minimize background noise and enhance speech recognition. Applying mean normalization before VAD could help achieve these goals by making the voice activity detection process more robust and accurate in various noisy environments.&#10;&#10;3. In summary, applying mean normalization before VAD is significant because it can potentially improve the performance of voice activity detection in adverse acoustic conditions. This is particularly relevant when working on integrating spectral subtraction techniques for speech recognition, where minimizing background noise and accurately identifying speech are crucial.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD F&#10;Content: because I guess voice activity detection on this should {disfmarker} could be worse .&#10;Speaker: PhD B&#10;Content: Yeah . Is it applied all the way back here ?&#10;Speaker: PhD F&#10;Content: It 's applied the um on , yeah , something like this ,&#10;Speaker: PhD B&#10;Content: Maybe that 's why it doesn't work for channel one .&#10;Speaker: PhD F&#10;Content: yeah . Perhaps , yeah .&#10;Speaker: Professor A&#10;Content: Can I {disfmarker}&#10;Speaker: PhD F&#10;Content: So we could perhaps do just mean normalization before VAD .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Mm - hmm . Can I ask a , I mean {disfmarker} a sort of top - level question , which is {vocalsound} um &quot; if {disfmarker} if most of what the OGI folk are working with is trying to {vocalsound} integrate this other {disfmarker} other uh spectral subtraction , {vocalsound} why are we worrying about" target="1. Based on the transcript, the current level of communication with the OGI gang regarding improving the vocalsound test is through exchanging emails. They are discussing Larry Saul's ideas for developing a voice or voiced-speech detector and analyzing high resolution spectrum features.&#10;&#10;2. There has been no explicit mention of success in approximating old numbers while keeping latency down. However, there have been attempts to implement changes such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. These efforts aim to improve the performance of VAD and reduce latency but their success is not explicitly mentioned in the conversation.&#10;&#10;3. In summary, while there has been ongoing communication with the OGI gang about improving the vocalsound test, there is no clear indication of success in approximating old numbers while keeping latency down. They are still working on different approaches to enhance VAD performance and reduce latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target=": Right .&#10;Speaker: PhD B&#10;Content: But for the purposes of finding the speech {disfmarker}&#10;Speaker: PhD F&#10;Content: And {disfmarker} Yeah , but here {disfmarker}&#10;Speaker: PhD B&#10;Content: You 're more interested in the difference between the speech and the nonspeech ,&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: right ?&#10;Speaker: PhD F&#10;Content: Yeah . So I think , yeah . For I th I think that it {disfmarker} perhaps it shows that {vocalsound} uh the parameters that the voice activity detector should use {disfmarker} uh have to use should be different than the parameter that have to be used for speech recognition .&#10;Speaker: Professor A&#10;Content: Yeah . So basically you want to reduce this effect .&#10;Speaker: PhD F&#10;Content: Well , y&#10;Speaker: Professor A&#10;Content: So you can do that by doing the voi voice activity detection . You also could do it by spect uh spectral subtraction before the {vocalsound} variance normalization , right ?&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="i voice activity detection . You also could do it by spect uh spectral subtraction before the {vocalsound} variance normalization , right ?&#10;Speaker: PhD F&#10;Content: Yeah , but it 's not clear , yeah .&#10;Speaker: Professor A&#10;Content: So uh {disfmarker}&#10;Speaker: PhD F&#10;Content: We So . Well . It 's just to&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: the {disfmarker} the number that at that are here are recognition experiments on Italian HM and MM {vocalsound} with these two kinds of parameters . And , {pause} well , it 's better with variance normalization .&#10;Speaker: Professor A&#10;Content: Yeah . Yeah . So it does get better even though it looks ugly .&#10;Speaker: PhD F&#10;Content: Uh {disfmarker}&#10;Speaker: Professor A&#10;Content: OK . but does this have the voice activity detection in it ?&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Um .&#10;Speaker: Professor A">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="Content: Yeah .&#10;Speaker: Professor A&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Um .&#10;Speaker: Professor A&#10;Content: So .&#10;Speaker: Grad E&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Where 's th&#10;Speaker: PhD F&#10;Content: But the fact is that the voice activity detector doesn't work on channel one . So . Yeah .&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: Where {disfmarker} at what stage is the voice activity detector applied ? Is it applied here or a after the variance normalization ?&#10;Speaker: PhD F&#10;Content: Hmm ?&#10;Speaker: Professor A&#10;Content: Spectral subtraction , I guess .&#10;Speaker: PhD B&#10;Content: or {disfmarker}&#10;Speaker: PhD F&#10;Content: It 's applied before variance normalization . So it 's a good thing ,&#10;Speaker: PhD B&#10;Content: Oh .&#10;Speaker: PhD F&#10;Content: because I guess voice activity detection on this should {disfmarker} could be worse .&#10;Speaker: PhD B">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target=" yeah . When you have noise there is no um {disfmarker} {vocalsound} if {disfmarker} if you have a low frequency noise it could be taken for {disfmarker} for voiced speech and .&#10;Speaker: Professor A&#10;Content: Yeah , you can make these mistakes ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: Professor A&#10;Content: but {disfmarker} but {disfmarker}&#10;Speaker: PhD B&#10;Content: Isn't there some other&#10;Speaker: PhD F&#10;Content: S&#10;Speaker: PhD B&#10;Content: uh d&#10;Speaker: PhD F&#10;Content: So I think that it {disfmarker} it would be good {disfmarker} Yeah , yeah , well , go {disfmarker} go on .&#10;Speaker: PhD B&#10;Content: Uh , I was just gonna say isn't there {disfmarker} {vocalsound} aren't {disfmarker} aren't there lots of ideas for doing voice activity , or speech - nonspeech rather , {comment} um by looking at {vocalsound} um , you know">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="1. Professor A and PhD F plan to resume their work on voice activity detection (VAD) and speech processing when Hynek returns to town the week after next. They aim to organize more visits, connections, and start working towards June. This may suggest that they have a deadline or event in June for which they need to make progress on their VAD research.&#10;2. PhD D suggested working on voiced-unvoiced detection as part of Stephane's ideas. This involves analyzing the difference between speech and nonspeech, focusing on the voiced-unvoiced characteristics. They plan to explore this idea further in order to improve voice activity detection.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="1. The outcome of the experiment was that the French VAD performed significantly better than some other VADs when using only the baseline set of features, which included mel-cepstra. However, the performance varied at times, with the French VAD being better in some instances and worse in others. According to Professor A, the difference in performance between the French VAD and other VADs was substantial enough that it could account for a significant portion of the discrepancy in their own system's performance. PhD D mentioned that they should ask which VAD was used by Pratibha, who conducted the experiment.&#10;&#10;In summary, the French VAD outperformed some other VADs when using only the baseline set of features like mel-cepstra, and this difference in performance could potentially explain a considerable amount of the discrepancy in their own system's performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="1. A convenient way to distribute changes and share information over multiple sites, while minimizing interference with regular work and potentially being useful for other tasks as well, would be to use a version control system like CVS (Concurrent Versions System). This was suggested during the discussion and it is a popular method for managing and distributing changes to software projects across multiple sites. It allows users to easily access the latest versions of files, track revisions, and work collaboratively on the project without interfering with each other's regular work. Additionally, such systems can be used for various types of tasks beyond the initial voice activity detector development, making them a versatile tool for different projects and teams.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="1. Analyzing high resolution spectrum: The team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. They plan to compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech and distinguishing it from non-speech sounds.&#10;2. Voiced-unvoiced characteristics: The team aims to analyze the difference between speech and nonspeech by focusing on the voiced-unvoiced characteristics. This is based on a speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved high correctness rates.&#10;3. Reducing background noise: Spectral subtraction before variance normalization is suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance in distinguishing between voiced speech and non-speech sounds.&#10;4. Low frequency noise consideration: It is important to consider that low frequency noise could be taken for voiced speech, leading to mistakes in distinction. Therefore, appropriate filtering or analysis methods should be used to mitigate this issue.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="1. The result of adding spectral subtraction before the Wall process, which contains LDA (Linear Discriminant Analysis) and online normalization, was that it hurt the performance significantly.&#10;2. The sentence &quot;Due&quot; spoken in an Italian utterance, recorded by two channels (close mic microphone as channel 0 and distant microphone as channel 1), could not be clearly seen when plotted because of the following reasons:&#10;* The plot was a plot of C-zero parameters for one Italian utterance.&#10;* The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;* Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="The speakers observe that when they apply mean normalization to a clean channel (referred to as C-zero), it resembles the second figure, which is a positive outcome. They also note that the noise part is around zero. When both mean and variance normalization are applied, the speech portion of two channels becomes very close in the third figure. This indicates that the visualizations correspond well to their expectations for both clean speech and noisy conditions, suggesting good quality. However, they do not explicitly discuss human vocal sound abilities in this context.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="1. The suggestion to apply mean normalization before Voice Activity Detection (VAD) comes up as the speakers discuss Larry Saul's ideas for developing a voice or voiced-speech detector using the Fast Fourier Transform (FFT) and analyzing high resolution spectrum features. This normalization step might be beneficial for improving the performance of VAD by reducing background noise and making it easier to identify finer details related to speech in the frequency components.&#10;&#10;2. The context of integrating spectral subtraction, as mentioned by &quot;the OGI folk,&quot; implies that they are focusing on methods to minimize background noise and enhance speech recognition. Applying mean normalization before VAD could help achieve these goals by making the voice activity detection process more robust and accurate in various noisy environments.&#10;&#10;3. In summary, applying mean normalization before VAD is significant because it can potentially improve the performance of voice activity detection in adverse acoustic conditions. This is particularly relevant when working on integrating spectral subtraction techniques for speech recognition, where minimizing background noise and accurately identifying speech are crucial.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="The speakers do not directly discuss enhancing harmonic structure to reduce noise during audio processing in the given transcripts. The conversation revolves around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="1. The researchers were discussing a potential experiment using TIMIT data and a &quot;voicing bit&quot; to possibly enhance performance in challenging audio conditions, such as noisy environments or separate audio streams. This conversation was related to Larry Saul's ideas for developing a voice or voiced-speech detector that utilizes the Fast Fourier Transform (FFT) and analyzes high resolution spectrum features.&#10;2. The voicing bit could refer to the voiced-unvoiced characteristics of speech, which are mentioned in the context of differentiating between speech and non-speech. By incorporating this information into their voice activity detector, they aim to improve its performance by better recognizing speech within various types of noise.&#10;3. The researchers also discussed applying mean normalization before Voice Activity Detection (VAD) as part of their plans for minimizing background noise and enhancing speech recognition. This step might help make the VAD more robust and accurate in different noisy conditions, which is particularly relevant when integrating spectral subtraction techniques for speech processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="The transcripts do not provide direct information about measures for ensuring good audio quality when using different microphones or the recommended approach for using a pre-emphasis filter. These topics were not discussed in the meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="The significance of the OGI's focus on integrating spectral subtraction lies in their goal of minimizing background noise and enhancing speech recognition. This focus area aligns with addressing voice activity detection (VAD) issues, as applying mean normalization before VAD could potentially improve its performance in adverse acoustic conditions. By integrating spectral subtraction techniques, the OGI aims to create a more robust and accurate VAD system that can effectively distinguish speech from background noise in various environments.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="1. Spectral subtraction increases the difference between the energy of speech and the energy of the noise portion because it is a technique used to reduce background noise in audio signals. By subtracting an estimate of the noise spectrum from the speech plus noise spectrum, the result highlights the speech components that stand out more than they would against the original noisy background. The difference between the energies becomes larger after spectral subtraction compared to before.&#10;   &#10;2. Spectral subtraction also increases the variance of C-zero (the mean normalized channel). When applying spectral subtraction, it is possible to increase the variability of the signal around its mean value due to a less accurate noise estimation or fluctuations in the speech signal itself. As a result, after spectral subtraction, the variance of C-zero will be larger than before.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="The concern about integrating a different type of spectral subtraction stems from the fact that this method can increase the variance of C-zero (the mean normalized channel) after its application. While spectral subtraction does increase the difference between the energy of speech and noise, it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance. Therefore, careful consideration and adjustments are required when integrating a new spectral subtraction technique to maintain optimal speech recognition and VAD performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discussed the idea of developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum.&#10;2. They plan to take the spectrum and compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech.&#10;3. The voiced-unvoiced characteristics are referenced when discussing the difference between speech and nonspeech. They want to analyze this difference to improve voice activity detection.&#10;4. Spectral subtraction before variance normalization is also suggested as a method for reducing background noise in the voice activity detector.&#10;5. The voiced-unvoiced characteristics are also mentioned when discussing the application of the voice activity detector before variance normalization, suggesting that this placement is beneficial for recognizing speech among different types of noises." target="1. The issue encountered when adding spectral subtraction before the Wall process (which contains LDA and online normalization) was that it significantly hurt the performance. This is because spectral subtraction can increase the difference between the energy of speech and noise, but it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance.&#10;&#10;2. The C-zero parameters for a single Italian utterance in two channels cannot be clearly seen in the plot due to several reasons:&#10;   - The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;   - Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels.&#10;   - The plot being a plot of C-zero parameters for one Italian utterance may not provide enough context or clear differentiation between channels and frequencies to easily distinguish specific words or sounds.">
      <data key="d0">1</data>
    </edge>
    <edge source="i voice activity detection . You also could do it by spect uh spectral subtraction before the {vocalsound} variance normalization , right ?&#10;Speaker: PhD F&#10;Content: Yeah , but it 's not clear , yeah .&#10;Speaker: Professor A&#10;Content: So uh {disfmarker}&#10;Speaker: PhD F&#10;Content: We So . Well . It 's just to&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: the {disfmarker} the number that at that are here are recognition experiments on Italian HM and MM {vocalsound} with these two kinds of parameters . And , {pause} well , it 's better with variance normalization .&#10;Speaker: Professor A&#10;Content: Yeah . Yeah . So it does get better even though it looks ugly .&#10;Speaker: PhD F&#10;Content: Uh {disfmarker}&#10;Speaker: Professor A&#10;Content: OK . but does this have the voice activity detection in it ?&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Um .&#10;Speaker: Professor A" target="1. The result of adding spectral subtraction before the Wall process, which contains LDA (Linear Discriminant Analysis) and online normalization, was that it hurt the performance significantly.&#10;2. The sentence &quot;Due&quot; spoken in an Italian utterance, recorded by two channels (close mic microphone as channel 0 and distant microphone as channel 1), could not be clearly seen when plotted because of the following reasons:&#10;* The plot was a plot of C-zero parameters for one Italian utterance.&#10;* The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;* Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels.">
      <data key="d0">1</data>
    </edge>
    <edge source=" yeah . When you have noise there is no um {disfmarker} {vocalsound} if {disfmarker} if you have a low frequency noise it could be taken for {disfmarker} for voiced speech and .&#10;Speaker: Professor A&#10;Content: Yeah , you can make these mistakes ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: Professor A&#10;Content: but {disfmarker} but {disfmarker}&#10;Speaker: PhD B&#10;Content: Isn't there some other&#10;Speaker: PhD F&#10;Content: S&#10;Speaker: PhD B&#10;Content: uh d&#10;Speaker: PhD F&#10;Content: So I think that it {disfmarker} it would be good {disfmarker} Yeah , yeah , well , go {disfmarker} go on .&#10;Speaker: PhD B&#10;Content: Uh , I was just gonna say isn't there {disfmarker} {vocalsound} aren't {disfmarker} aren't there lots of ideas for doing voice activity , or speech - nonspeech rather , {comment} um by looking at {vocalsound} um , you know" target="1. Analyzing high resolution spectrum: The team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. They plan to compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech and distinguishing it from non-speech sounds.&#10;2. Voiced-unvoiced characteristics: The team aims to analyze the difference between speech and nonspeech by focusing on the voiced-unvoiced characteristics. This is based on a speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved high correctness rates.&#10;3. Reducing background noise: Spectral subtraction before variance normalization is suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance in distinguishing between voiced speech and non-speech sounds.&#10;4. Low frequency noise consideration: It is important to consider that low frequency noise could be taken for voiced speech, leading to mistakes in distinction. Therefore, appropriate filtering or analysis methods should be used to mitigate this issue.">
      <data key="d0">1</data>
    </edge>
    <edge source="The person they were discussing with suggested using a digital filter to preprocess the data before analyzing it. This would be an alternative to changing the capacitor on the input box." target="ound} change a capacitor on the input box for that or whether we should&#10;Speaker: PhD B&#10;Content: Yeah , he suggested a smaller capacitor , right ?&#10;Speaker: Professor A&#10;Content: Right . But then I had some other uh thing discussions with him&#10;Speaker: PhD B&#10;Content: For the P D&#10;Speaker: Professor A&#10;Content: and the feeling was {vocalsound} once we start monk monkeying with that , uh , many other problems could ha happen . And additionally we {disfmarker} we already have a lot of data that 's been collected with that , so .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: A simple thing to do is he {disfmarker} he {disfmarker} he has a {disfmarker} I forget if it {disfmarker} this was in that mail or in the following mail , but he has a {disfmarker} a simple filter , a digital filter that he suggested . We just run over the data before we deal with it .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A">
      <data key="d0">1</data>
    </edge>
    <edge source="The person they were discussing with suggested using a digital filter to preprocess the data before analyzing it. This would be an alternative to changing the capacitor on the input box." target="} I have to be more careful about using that as a {disfmarker} as a {disfmarker} {vocalsound} as a good illustration , uh , in fact it 's not , of uh {disfmarker} {vocalsound} of the effects of room reverberation . It is isn't a bad illustration of the effects of uh room noise . {vocalsound} on {disfmarker} on uh some mikes uh but So . And then we had this other discussion about um {vocalsound} whether this affects the dynamic range , cuz I know , although we start off with thirty two bits , you end up with uh sixteen bits and {vocalsound} you know , are we getting hurt there ? But uh Dan is pretty confident that we 're not , that {disfmarker} that quantization error is not {disfmarker} is still not a significant {vocalsound} factor there . So . So there was a question of whether we should change things here , whether we should {vocalsound} change a capacitor on the input box for that or whether we should&#10;Speaker: PhD B&#10;Content: Yeah , he suggested a smaller capacitor ,">
      <data key="d0">1</data>
    </edge>
    <edge source="The person they were discussing with suggested using a digital filter to preprocess the data before analyzing it. This would be an alternative to changing the capacitor on the input box." target="} that 's a question for this uh you know extending the feature vector versus having different streams .&#10;Speaker: PhD F&#10;Content: Oh . Was it nois noisy condition ? the example that you {disfmarker} you just&#10;Speaker: Professor A&#10;Content: And {disfmarker} and it may not have been noisy conditions .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Yeah . I {disfmarker} I don't remember the example but it was {disfmarker} {vocalsound} it was on some DARPA data and some years ago and so it probably wasn't , actually&#10;Speaker: PhD F&#10;Content: Mm - hmm . Mm - hmm . Yeah . But we were thinking , we discussed with Barry about this , and {vocalsound} perhaps {vocalsound} thinking {disfmarker} we were thinking about some kind of sheet cheating experiment where we would use TIMIT&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: and see if giving the d uh , this voicing bit would help in {disfmarker} in">
      <data key="d0">1</data>
    </edge>
    <edge source="The person they were discussing with suggested using a digital filter to preprocess the data before analyzing it. This would be an alternative to changing the capacitor on the input box." target=" filtering before we {vocalsound} process it . And then again if it 's uh depending on the option that the {disfmarker} our {disfmarker} our software is being run with , it 's {disfmarker} it 's quite possible that 's already being taken care of . uh But I also have to pick a different picture to show the effects of reverberation . uh&#10;Speaker: PhD B&#10;Content: Did somebody notice it during your talk ?&#10;Speaker: Professor A&#10;Content: uh No .&#10;Speaker: PhD B&#10;Content: Huh .&#10;Speaker: Professor A&#10;Content: Well . uh Well . If they made output they were {disfmarker} they were , you know {disfmarker} they were nice .&#10;Speaker: PhD B&#10;Content: Didn't say anything ?&#10;Speaker: Professor A&#10;Content: But . {vocalsound} I mean the thing is it was since I was talking about reverberation and showing this thing that was noise , it wasn't a good match , but it certainly was still uh an indication of the fact that you get noise with distant mikes . uh It 's just not a great example">
      <data key="d0">1</data>
    </edge>
    <edge source="The person they were discussing with suggested using a digital filter to preprocess the data before analyzing it. This would be an alternative to changing the capacitor on the input box." target=" {vocalsound} are moving quite a bit . &quot; And then you look in the other one and they look practically flat .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: So I mean you could {disfmarker} that 's why I was thinking , in a section like that , you could take a look {disfmarker} look at just that part of the spectrogram and you could say &quot; Oh yeah . This {disfmarker} this really distorted it quite a bit . &quot;&#10;Speaker: PhD B&#10;Content: Yeah . The main thing that struck me in looking at those two spectrograms was the difference in the high frequencies . It looked like {vocalsound} for the one that was farther away , you know , it really {disfmarker} everything was attenuated&#10;Speaker: Professor A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: and {disfmarker} I mean that was the main visual thing that I noticed .&#10;Speaker: Professor A&#10;Content: Right . But it 's {disfmarker} it 's uh {disfmarker} So . Yeah">
      <data key="d0">1</data>
    </edge>
    <edge source="The person they were discussing with suggested using a digital filter to preprocess the data before analyzing it. This would be an alternative to changing the capacitor on the input box." target="1. The team plans to continue working on reducing latency, although they acknowledge that their current plan may not be ideal.&#10;2. There is also a possibility of adding a second stream to the project, such as a multi-band or TRAP (Generalized TRAP) system, which both parties are willing to work on.&#10;3. They are currently exchanging emails and discussing significant results regarding this second stream.&#10;4. One party is working on integrating spectral subtraction from Ericsson, while the other party is trying their own spectral subtraction method.&#10;5. Additionally, they are considering using Larry Saul's ideas to develop a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. This could help in identifying finer details related to speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="The person they were discussing with suggested using a digital filter to preprocess the data before analyzing it. This would be an alternative to changing the capacitor on the input box." target="1. A convenient way to distribute changes and share information over multiple sites, while minimizing interference with regular work and potentially being useful for other tasks as well, would be to use a version control system like CVS (Concurrent Versions System). This was suggested during the discussion and it is a popular method for managing and distributing changes to software projects across multiple sites. It allows users to easily access the latest versions of files, track revisions, and work collaboratively on the project without interfering with each other's regular work. Additionally, such systems can be used for various types of tasks beyond the initial voice activity detector development, making them a versatile tool for different projects and teams.">
      <data key="d0">1</data>
    </edge>
    <edge source="The person they were discussing with suggested using a digital filter to preprocess the data before analyzing it. This would be an alternative to changing the capacitor on the input box." target="1. The researchers were discussing a potential experiment using TIMIT data and a &quot;voicing bit&quot; to possibly enhance performance in challenging audio conditions, such as noisy environments or separate audio streams. This conversation was related to Larry Saul's ideas for developing a voice or voiced-speech detector that utilizes the Fast Fourier Transform (FFT) and analyzes high resolution spectrum features.&#10;2. The voicing bit could refer to the voiced-unvoiced characteristics of speech, which are mentioned in the context of differentiating between speech and non-speech. By incorporating this information into their voice activity detector, they aim to improve its performance by better recognizing speech within various types of noise.&#10;3. The researchers also discussed applying mean normalization before Voice Activity Detection (VAD) as part of their plans for minimizing background noise and enhancing speech recognition. This step might help make the VAD more robust and accurate in different noisy conditions, which is particularly relevant when integrating spectral subtraction techniques for speech processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="The person they were discussing with suggested using a digital filter to preprocess the data before analyzing it. This would be an alternative to changing the capacitor on the input box." target="The transcripts do not provide direct information about the method used by the person who worked there some years ago to help reduce noise in signals by utilizing voicing information and estimating pitch and fine harmonic structure. The discussions revolve around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure or estimating pitch for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="The person they were discussing with suggested using a digital filter to preprocess the data before analyzing it. This would be an alternative to changing the capacitor on the input box." target="1. Based on the transcript, there is no explicit statement or confirmation that they have managed to substantially improve the system while also reducing latency. The conversation involves discussions about cutting down the number of iterations, increasing the number of Gaussians, and attempting different methods such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. However, there is no clear indication of success in substantially improving the system while reducing latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="} that 's a question for this uh you know extending the feature vector versus having different streams .&#10;Speaker: PhD F&#10;Content: Oh . Was it nois noisy condition ? the example that you {disfmarker} you just&#10;Speaker: Professor A&#10;Content: And {disfmarker} and it may not have been noisy conditions .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Yeah . I {disfmarker} I don't remember the example but it was {disfmarker} {vocalsound} it was on some DARPA data and some years ago and so it probably wasn't , actually&#10;Speaker: PhD F&#10;Content: Mm - hmm . Mm - hmm . Yeah . But we were thinking , we discussed with Barry about this , and {vocalsound} perhaps {vocalsound} thinking {disfmarker} we were thinking about some kind of sheet cheating experiment where we would use TIMIT&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: and see if giving the d uh , this voicing bit would help in {disfmarker} in" target="1. The researchers were discussing a potential experiment using TIMIT data and a &quot;voicing bit&quot; to possibly enhance performance in challenging audio conditions, such as noisy environments or separate audio streams. This conversation was related to Larry Saul's ideas for developing a voice or voiced-speech detector that utilizes the Fast Fourier Transform (FFT) and analyzes high resolution spectrum features.&#10;2. The voicing bit could refer to the voiced-unvoiced characteristics of speech, which are mentioned in the context of differentiating between speech and non-speech. By incorporating this information into their voice activity detector, they aim to improve its performance by better recognizing speech within various types of noise.&#10;3. The researchers also discussed applying mean normalization before Voice Activity Detection (VAD) as part of their plans for minimizing background noise and enhancing speech recognition. This step might help make the VAD more robust and accurate in different noisy conditions, which is particularly relevant when integrating spectral subtraction techniques for speech processing.">
      <data key="d0">1</data>
    </edge>
    <edge source=" filtering before we {vocalsound} process it . And then again if it 's uh depending on the option that the {disfmarker} our {disfmarker} our software is being run with , it 's {disfmarker} it 's quite possible that 's already being taken care of . uh But I also have to pick a different picture to show the effects of reverberation . uh&#10;Speaker: PhD B&#10;Content: Did somebody notice it during your talk ?&#10;Speaker: Professor A&#10;Content: uh No .&#10;Speaker: PhD B&#10;Content: Huh .&#10;Speaker: Professor A&#10;Content: Well . uh Well . If they made output they were {disfmarker} they were , you know {disfmarker} they were nice .&#10;Speaker: PhD B&#10;Content: Didn't say anything ?&#10;Speaker: Professor A&#10;Content: But . {vocalsound} I mean the thing is it was since I was talking about reverberation and showing this thing that was noise , it wasn't a good match , but it certainly was still uh an indication of the fact that you get noise with distant mikes . uh It 's just not a great example" target="1. During the meeting, Professor A acknowledged that the example they used to demonstrate reverberation effects during their talk was not an ideal match for the topic. The example involved noise from distant microphones, which is not the same as reverberation.&#10;2. However, Professor A also stated that the example was still valuable because it demonstrated the real-world challenge of dealing with background noise in audio recordings. They mentioned that someone later pointed out the discrepancy between the example and the topic, but no one had noticed during the actual talk.&#10;3. In summary, although the example used by Professor A was not perfect for demonstrating reverberation effects, it still served a purpose by highlighting other real-world issues related to audio recordings, such as background noise from distant microphones. The lack of concern during the talk indicates that the audience may have focused more on the main topic rather than the specific example used.">
      <data key="d0">1</data>
    </edge>
    <edge source=" filtering before we {vocalsound} process it . And then again if it 's uh depending on the option that the {disfmarker} our {disfmarker} our software is being run with , it 's {disfmarker} it 's quite possible that 's already being taken care of . uh But I also have to pick a different picture to show the effects of reverberation . uh&#10;Speaker: PhD B&#10;Content: Did somebody notice it during your talk ?&#10;Speaker: Professor A&#10;Content: uh No .&#10;Speaker: PhD B&#10;Content: Huh .&#10;Speaker: Professor A&#10;Content: Well . uh Well . If they made output they were {disfmarker} they were , you know {disfmarker} they were nice .&#10;Speaker: PhD B&#10;Content: Didn't say anything ?&#10;Speaker: Professor A&#10;Content: But . {vocalsound} I mean the thing is it was since I was talking about reverberation and showing this thing that was noise , it wasn't a good match , but it certainly was still uh an indication of the fact that you get noise with distant mikes . uh It 's just not a great example" target="Professionor A mentioned the issue of noise during his talk on reverberation, even though no one seemed to notice it, because he considered it a valuable example of real-world challenges in audio recordings. Specifically, the noise from distant microphones demonstrated background noise that can still be informative and useful for discussion. Although the example was not perfect for illustrating reverberation effects, it served as an opportunity to highlight other relevant issues in audio processing. Furthermore, Professor A used this example to emphasize the importance of filtering and processing techniques to handle such noise.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {vocalsound} are moving quite a bit . &quot; And then you look in the other one and they look practically flat .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: So I mean you could {disfmarker} that 's why I was thinking , in a section like that , you could take a look {disfmarker} look at just that part of the spectrogram and you could say &quot; Oh yeah . This {disfmarker} this really distorted it quite a bit . &quot;&#10;Speaker: PhD B&#10;Content: Yeah . The main thing that struck me in looking at those two spectrograms was the difference in the high frequencies . It looked like {vocalsound} for the one that was farther away , you know , it really {disfmarker} everything was attenuated&#10;Speaker: Professor A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: and {disfmarker} I mean that was the main visual thing that I noticed .&#10;Speaker: Professor A&#10;Content: Right . But it 's {disfmarker} it 's uh {disfmarker} So . Yeah" target="1. During the meeting, Professor A acknowledged that the example they used to demonstrate reverberation effects during their talk was not an ideal match for the topic. The example involved noise from distant microphones, which is not the same as reverberation.&#10;2. However, Professor A also stated that the example was still valuable because it demonstrated the real-world challenge of dealing with background noise in audio recordings. They mentioned that someone later pointed out the discrepancy between the example and the topic, but no one had noticed during the actual talk.&#10;3. In summary, although the example used by Professor A was not perfect for demonstrating reverberation effects, it still served a purpose by highlighting other real-world issues related to audio recordings, such as background noise from distant microphones. The lack of concern during the talk indicates that the audience may have focused more on the main topic rather than the specific example used.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation." target="&#10;Content: R Right . So you have a second neural net .&#10;Speaker: PhD F&#10;Content: and then use a tandem system&#10;Speaker: Professor A&#10;Content: It could be pretty small . Yeah . If you have a tandem system and then you have some kind of {disfmarker} it can be pretty small {disfmarker} net {disfmarker}&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: we used {disfmarker} we d did some of this stuff . Uh I {disfmarker} I did , some years ago ,&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: and the {disfmarker} and {disfmarker} and you use {disfmarker} {vocalsound} the thing is to use information primarily that 's different as you say , it 's more fine - structure - based than {disfmarker} than envelope - based&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: uh so then it you {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation." target=" as the&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: uh {disfmarker} take the log of that or {vocalsound} uh pre pre uh {disfmarker} pre - nonlinearity ,&#10;Speaker: PhD F&#10;Content: Yeah . i if {disfmarker}&#10;Speaker: Professor A&#10;Content: uh and do the KLT on the {disfmarker} on {disfmarker} on that ,&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: then that would {disfmarker} that would I guess be uh a reasonable use of independent information . So maybe that 's what you meant . And then that would be {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah , well , I was not thinking this {disfmarker} yeah , this could be an yeah So you mean have some kind of probability for the v the voicing&#10;Speaker: Professor A&#10;Content: R Right . So you have a second neural net .&#10;Speaker: PhD F&#10;Content: and then use a tandem system&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation." target=" many parameters and how many frames .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: And there are uh almost one point eight million frames of training data and less than forty thousand parameters in the baseline system .&#10;Speaker: Professor A&#10;Content: Hmm .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: So it 's very , very few parameters compared to how much training data .&#10;Speaker: Professor A&#10;Content: Well . Yes .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: So . And that {disfmarker} that says that we could have lots more parameters actually .&#10;Speaker: PhD B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: I did one quick experiment just to make sure I had everything worked out and I just {disfmarker} {vocalsound} uh f for most of the um {disfmarker} For {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation." target="&#10;Speaker: PhD F&#10;Content: Worse , yep .&#10;Speaker: Professor A&#10;Content: Out of what ? I mean . s&#10;Speaker: PhD F&#10;Content: Uh well we start from ninety - four point sixty - four , and we go to ninety - four point O four .&#10;Speaker: Professor A&#10;Content: Uh - huh . So that 's six {disfmarker} six point th&#10;Speaker: PhD F&#10;Content: Uh .&#10;Speaker: PhD B&#10;Content: Ninety - three point six four , right ? is the baseline .&#10;Speaker: PhD F&#10;Content: Oh , no , I 've ninety - four . Oh , the baseline , you mean .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Well I don't {disfmarker} I 'm not talking about the baseline here .&#10;Speaker: PhD B&#10;Content: Oh . Oh . I 'm sorry .&#10;Speaker: PhD F&#10;Content: I uh {disfmarker} My baseline is the submitted system .&#10;Speaker: PhD B&#10;Content: Ah ! OK . Ah , ah .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation." target="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation." target="1. Professor A and PhD F plan to resume their work on voice activity detection (VAD) and speech processing when Hynek returns to town the week after next. They aim to organize more visits, connections, and start working towards June. This may suggest that they have a deadline or event in June for which they need to make progress on their VAD research.&#10;2. PhD D suggested working on voiced-unvoiced detection as part of Stephane's ideas. This involves analyzing the difference between speech and nonspeech, focusing on the voiced-unvoiced characteristics. They plan to explore this idea further in order to improve voice activity detection.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation." target="1. The outcome of the experiment was that the French VAD performed significantly better than some other VADs when using only the baseline set of features, which included mel-cepstra. However, the performance varied at times, with the French VAD being better in some instances and worse in others. According to Professor A, the difference in performance between the French VAD and other VADs was substantial enough that it could account for a significant portion of the discrepancy in their own system's performance. PhD D mentioned that they should ask which VAD was used by Pratibha, who conducted the experiment.&#10;&#10;In summary, the French VAD outperformed some other VADs when using only the baseline set of features like mel-cepstra, and this difference in performance could potentially explain a considerable amount of the discrepancy in their own system's performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation." target="1. A convenient way to distribute changes and share information over multiple sites, while minimizing interference with regular work and potentially being useful for other tasks as well, would be to use a version control system like CVS (Concurrent Versions System). This was suggested during the discussion and it is a popular method for managing and distributing changes to software projects across multiple sites. It allows users to easily access the latest versions of files, track revisions, and work collaboratively on the project without interfering with each other's regular work. Additionally, such systems can be used for various types of tasks beyond the initial voice activity detector development, making them a versatile tool for different projects and teams.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation." target="The speakers observe that when they apply mean normalization to a clean channel (referred to as C-zero), it resembles the second figure, which is a positive outcome. They also note that the noise part is around zero. When both mean and variance normalization are applied, the speech portion of two channels becomes very close in the third figure. This indicates that the visualizations correspond well to their expectations for both clean speech and noisy conditions, suggesting good quality. However, they do not explicitly discuss human vocal sound abilities in this context.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, PhD F and Professor A, discussed the idea of using a tandem system with a small neural network. This means that they will use two systems in tandem (or connected together) to analyze the input data. One system will be a small neural network.&#10;2. The small neural network will primarily use fine-structure-based information. Fine-structure-based information refers to detailed, low-level features of the signal. In this context, it likely means that the neural network will analyze finer details of the speech signal in order to make its predictions. This is contrasted with envelope-based information, which would focus on more global, high-level features of the signal.&#10;3. The purpose of using a small neural network and fine-structure-based information is not explicitly stated in the transcript, but it can be inferred that they are discussing ways to improve voice activity detection or voiced-speech detection, as these concepts are mentioned several times throughout the conversation." target="The transcripts do not provide direct information about measures for ensuring good audio quality when using different microphones or the recommended approach for using a pre-emphasis filter. These topics were not discussed in the meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. During the meeting, Professor A acknowledged that the example they used to demonstrate reverberation effects during their talk was not an ideal match for the topic. The example involved noise from distant microphones, which is not the same as reverberation.&#10;2. However, Professor A also stated that the example was still valuable because it demonstrated the real-world challenge of dealing with background noise in audio recordings. They mentioned that someone later pointed out the discrepancy between the example and the topic, but no one had noticed during the actual talk.&#10;3. In summary, although the example used by Professor A was not perfect for demonstrating reverberation effects, it still served a purpose by highlighting other real-world issues related to audio recordings, such as background noise from distant microphones. The lack of concern during the talk indicates that the audience may have focused more on the main topic rather than the specific example used." target=" that it 's lower amplitude and they hear there 's a second voice ,&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: Professor A&#10;Content: um {vocalsound} but uh that {disfmarker} actually that makes for a perfectly good demo because that 's a real obvious thing , that you hear two voices .&#10;Speaker: PhD B&#10;Content: But not of reverberation .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: A boom .&#10;Speaker: Professor A&#10;Content: Well that {disfmarker} that {disfmarker} that 's OK . But for the {disfmarker} the visual , just , you know , I 'd like to have uh {vocalsound} uh , you know , the spectrogram again ,&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: because you 're {disfmarker} you 're {disfmarker} you 're visual {vocalsound} uh abilities as a human being are so good {vocalsound} you can pick out {disfmarker} you know">
      <data key="d0">1</data>
    </edge>
    <edge source="1. During the meeting, Professor A acknowledged that the example they used to demonstrate reverberation effects during their talk was not an ideal match for the topic. The example involved noise from distant microphones, which is not the same as reverberation.&#10;2. However, Professor A also stated that the example was still valuable because it demonstrated the real-world challenge of dealing with background noise in audio recordings. They mentioned that someone later pointed out the discrepancy between the example and the topic, but no one had noticed during the actual talk.&#10;3. In summary, although the example used by Professor A was not perfect for demonstrating reverberation effects, it still served a purpose by highlighting other real-world issues related to audio recordings, such as background noise from distant microphones. The lack of concern during the talk indicates that the audience may have focused more on the main topic rather than the specific example used." target=" It made a good {disfmarker} good audio demonstration because when we could play that clip the {disfmarker} the {disfmarker} the really {vocalsound} obvious difference is that you can hear two voices and {disfmarker} {vocalsound} {vocalsound} in the second one and only hear {disfmarker}&#10;Speaker: PhD B&#10;Content: Maybe we could just {pause} like , talk into a cup .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Some good reverb .&#10;Speaker: Professor A&#10;Content: No , I mean , it sound {disfmarker} it sounds pretty reverberant , but I mean you can't {disfmarker} when you play it back in a room with a {disfmarker} you know a big room , {vocalsound} nobody can hear that difference really .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: They hear that it 's lower amplitude and they hear there 's a second voice ,&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. During the meeting, Professor A acknowledged that the example they used to demonstrate reverberation effects during their talk was not an ideal match for the topic. The example involved noise from distant microphones, which is not the same as reverberation.&#10;2. However, Professor A also stated that the example was still valuable because it demonstrated the real-world challenge of dealing with background noise in audio recordings. They mentioned that someone later pointed out the discrepancy between the example and the topic, but no one had noticed during the actual talk.&#10;3. In summary, although the example used by Professor A was not perfect for demonstrating reverberation effects, it still served a purpose by highlighting other real-world issues related to audio recordings, such as background noise from distant microphones. The lack of concern during the talk indicates that the audience may have focused more on the main topic rather than the specific example used." target=" good match , but it certainly was still uh an indication of the fact that you get noise with distant mikes . uh It 's just not a great example because not only isn't it reverberation but it 's a noise that we definitely know what to do .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: So , I mean , it doesn't take deep {disfmarker} {vocalsound} a new {disfmarker} bold new methods to get rid of uh five hertz noise , so .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: um {vocalsound} uh But . So it was {disfmarker} it was a bad example in that way , but it 's {disfmarker} it still is {disfmarker} it 's the real thing that we did get out of the microphone at distance , so it wasn't {vocalsound} it w it w wasn't wrong it was inappropriate . So . {vocalsound} So uh , but uh , Yeah , someone noticed it later pointed it out to me , and I went &quot; oh ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. During the meeting, Professor A acknowledged that the example they used to demonstrate reverberation effects during their talk was not an ideal match for the topic. The example involved noise from distant microphones, which is not the same as reverberation.&#10;2. However, Professor A also stated that the example was still valuable because it demonstrated the real-world challenge of dealing with background noise in audio recordings. They mentioned that someone later pointed out the discrepancy between the example and the topic, but no one had noticed during the actual talk.&#10;3. In summary, although the example used by Professor A was not perfect for demonstrating reverberation effects, it still served a purpose by highlighting other real-world issues related to audio recordings, such as background noise from distant microphones. The lack of concern during the talk indicates that the audience may have focused more on the main topic rather than the specific example used." target="The speakers do not directly discuss enhancing harmonic structure to reduce noise during audio processing in the given transcripts. The conversation revolves around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. During the meeting, Professor A acknowledged that the example they used to demonstrate reverberation effects during their talk was not an ideal match for the topic. The example involved noise from distant microphones, which is not the same as reverberation.&#10;2. However, Professor A also stated that the example was still valuable because it demonstrated the real-world challenge of dealing with background noise in audio recordings. They mentioned that someone later pointed out the discrepancy between the example and the topic, but no one had noticed during the actual talk.&#10;3. In summary, although the example used by Professor A was not perfect for demonstrating reverberation effects, it still served a purpose by highlighting other real-world issues related to audio recordings, such as background noise from distant microphones. The lack of concern during the talk indicates that the audience may have focused more on the main topic rather than the specific example used." target="The transcripts do not provide direct information about measures for ensuring good audio quality when using different microphones or the recommended approach for using a pre-emphasis filter. These topics were not discussed in the meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. During the meeting, Professor A acknowledged that the example they used to demonstrate reverberation effects during their talk was not an ideal match for the topic. The example involved noise from distant microphones, which is not the same as reverberation.&#10;2. However, Professor A also stated that the example was still valuable because it demonstrated the real-world challenge of dealing with background noise in audio recordings. They mentioned that someone later pointed out the discrepancy between the example and the topic, but no one had noticed during the actual talk.&#10;3. In summary, although the example used by Professor A was not perfect for demonstrating reverberation effects, it still served a purpose by highlighting other real-world issues related to audio recordings, such as background noise from distant microphones. The lack of concern during the talk indicates that the audience may have focused more on the main topic rather than the specific example used." target="The transcript does not provide a direct answer to this question. However, it includes a comment from PhD F about the Personal Digital Assistant (PDA) being consistently located far away during meetings, with the speakers noting that they have not moved it closer. The reason for this was not discussed in the meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. During the meeting, Professor A acknowledged that the example they used to demonstrate reverberation effects during their talk was not an ideal match for the topic. The example involved noise from distant microphones, which is not the same as reverberation.&#10;2. However, Professor A also stated that the example was still valuable because it demonstrated the real-world challenge of dealing with background noise in audio recordings. They mentioned that someone later pointed out the discrepancy between the example and the topic, but no one had noticed during the actual talk.&#10;3. In summary, although the example used by Professor A was not perfect for demonstrating reverberation effects, it still served a purpose by highlighting other real-world issues related to audio recordings, such as background noise from distant microphones. The lack of concern during the talk indicates that the audience may have focused more on the main topic rather than the specific example used." target="Professionor A mentioned the issue of noise during his talk on reverberation, even though no one seemed to notice it, because he considered it a valuable example of real-world challenges in audio recordings. Specifically, the noise from distant microphones demonstrated background noise that can still be informative and useful for discussion. Although the example was not perfect for illustrating reverberation effects, it served as an opportunity to highlight other relevant issues in audio processing. Furthermore, Professor A used this example to emphasize the importance of filtering and processing techniques to handle such noise.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that it 's lower amplitude and they hear there 's a second voice ,&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: Professor A&#10;Content: um {vocalsound} but uh that {disfmarker} actually that makes for a perfectly good demo because that 's a real obvious thing , that you hear two voices .&#10;Speaker: PhD B&#10;Content: But not of reverberation .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: A boom .&#10;Speaker: Professor A&#10;Content: Well that {disfmarker} that {disfmarker} that 's OK . But for the {disfmarker} the visual , just , you know , I 'd like to have uh {vocalsound} uh , you know , the spectrogram again ,&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: because you 're {disfmarker} you 're {disfmarker} you 're visual {vocalsound} uh abilities as a human being are so good {vocalsound} you can pick out {disfmarker} you know" target="Professionor A mentioned the issue of noise during his talk on reverberation, even though no one seemed to notice it, because he considered it a valuable example of real-world challenges in audio recordings. Specifically, the noise from distant microphones demonstrated background noise that can still be informative and useful for discussion. Although the example was not perfect for illustrating reverberation effects, it served as an opportunity to highlight other relevant issues in audio processing. Furthermore, Professor A used this example to emphasize the importance of filtering and processing techniques to handle such noise.">
      <data key="d0">1</data>
    </edge>
    <edge source=" It made a good {disfmarker} good audio demonstration because when we could play that clip the {disfmarker} the {disfmarker} the really {vocalsound} obvious difference is that you can hear two voices and {disfmarker} {vocalsound} {vocalsound} in the second one and only hear {disfmarker}&#10;Speaker: PhD B&#10;Content: Maybe we could just {pause} like , talk into a cup .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Some good reverb .&#10;Speaker: Professor A&#10;Content: No , I mean , it sound {disfmarker} it sounds pretty reverberant , but I mean you can't {disfmarker} when you play it back in a room with a {disfmarker} you know a big room , {vocalsound} nobody can hear that difference really .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: They hear that it 's lower amplitude and they hear there 's a second voice ,&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker" target="Professionor A mentioned the issue of noise during his talk on reverberation, even though no one seemed to notice it, because he considered it a valuable example of real-world challenges in audio recordings. Specifically, the noise from distant microphones demonstrated background noise that can still be informative and useful for discussion. Although the example was not perfect for illustrating reverberation effects, it served as an opportunity to highlight other relevant issues in audio processing. Furthermore, Professor A used this example to emphasize the importance of filtering and processing techniques to handle such noise.">
      <data key="d0">1</data>
    </edge>
    <edge source=" good match , but it certainly was still uh an indication of the fact that you get noise with distant mikes . uh It 's just not a great example because not only isn't it reverberation but it 's a noise that we definitely know what to do .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: So , I mean , it doesn't take deep {disfmarker} {vocalsound} a new {disfmarker} bold new methods to get rid of uh five hertz noise , so .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: um {vocalsound} uh But . So it was {disfmarker} it was a bad example in that way , but it 's {disfmarker} it still is {disfmarker} it 's the real thing that we did get out of the microphone at distance , so it wasn't {vocalsound} it w it w wasn't wrong it was inappropriate . So . {vocalsound} So uh , but uh , Yeah , someone noticed it later pointed it out to me , and I went &quot; oh ," target="Professionor A mentioned the issue of noise during his talk on reverberation, even though no one seemed to notice it, because he considered it a valuable example of real-world challenges in audio recordings. Specifically, the noise from distant microphones demonstrated background noise that can still be informative and useful for discussion. Although the example was not perfect for illustrating reverberation effects, it served as an opportunity to highlight other relevant issues in audio processing. Furthermore, Professor A used this example to emphasize the importance of filtering and processing techniques to handle such noise.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target=" Well back twenty years ago when I did this voiced - unvoiced stuff , we were getting more like {vocalsound} ninety - seven or ninety - eight percent correct in voicing . But that was {vocalsound} speaker - dependent {vocalsound} actually . We were doing training {vocalsound} on a particular announcer&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: and {disfmarker} and getting a {vocalsound} very good handle on the features .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: And we did this complex feature selection thing where we looked at all the different possible features one could have for voicing and {disfmarker} {vocalsound} and {disfmarker} and uh {disfmarker} and exhaustively searched {vocalsound} all size subsets and {disfmarker} and uh {disfmarker} for {disfmarker} for that particular speaker and you 'd find you know the five or six features which really did well on them .&#10;Speaker: PhD B&#10;Content: Wow">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target=" that particular speaker and you 'd find you know the five or six features which really did well on them .&#10;Speaker: PhD B&#10;Content: Wow !&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: And then doing {disfmarker} doing all of that we could get down to two or three percent error . But that , again , was speaker - dependent with {vocalsound} lots of feature selection&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: and a very complex sort of thing .&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: So I would {disfmarker} I would believe {vocalsound} that uh it was quite likely that um looking at envelope only , that we 'd be {vocalsound} significantly worse than that .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Uh .&#10;Speaker: PhD F&#10;Content: And the {disfmarker} all the {disfmarker} the SpeechCorders ? what '">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target="&#10;Speaker: Professor A&#10;Content: what that said is that , sort of , left to its own devices , like without the {disfmarker} a strong language model and so forth , that you would {disfmarker} {vocalsound} you would make significant number of errors {vocalsound} just with your uh probabilistic machinery in deciding&#10;Speaker: PhD B&#10;Content: It also {disfmarker}&#10;Speaker: Professor A&#10;Content: one oh&#10;Speaker: PhD B&#10;Content: Yeah , the {disfmarker} though I think uh there was one problem with that in that , you know , we used canonical mapping so {vocalsound} our truth may not have really been {pause} true to the acoustics .&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: So .&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: Yeah . Well back twenty years ago when I did this voiced - unvoiced stuff , we were getting more like {vocalsound} ninety - seven or ninety -">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target="1. A convenient way to distribute changes and share information over multiple sites, while minimizing interference with regular work and potentially being useful for other tasks as well, would be to use a version control system like CVS (Concurrent Versions System). This was suggested during the discussion and it is a popular method for managing and distributing changes to software projects across multiple sites. It allows users to easily access the latest versions of files, track revisions, and work collaboratively on the project without interfering with each other's regular work. Additionally, such systems can be used for various types of tasks beyond the initial voice activity detector development, making them a versatile tool for different projects and teams.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target="1. Analyzing high resolution spectrum: The team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. They plan to compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech and distinguishing it from non-speech sounds.&#10;2. Voiced-unvoiced characteristics: The team aims to analyze the difference between speech and nonspeech by focusing on the voiced-unvoiced characteristics. This is based on a speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved high correctness rates.&#10;3. Reducing background noise: Spectral subtraction before variance normalization is suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance in distinguishing between voiced speech and non-speech sounds.&#10;4. Low frequency noise consideration: It is important to consider that low frequency noise could be taken for voiced speech, leading to mistakes in distinction. Therefore, appropriate filtering or analysis methods should be used to mitigate this issue.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target="The speakers observe that when they apply mean normalization to a clean channel (referred to as C-zero), it resembles the second figure, which is a positive outcome. They also note that the noise part is around zero. When both mean and variance normalization are applied, the speech portion of two channels becomes very close in the third figure. This indicates that the visualizations correspond well to their expectations for both clean speech and noisy conditions, suggesting good quality. However, they do not explicitly discuss human vocal sound abilities in this context.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target="1. The suggestion to apply mean normalization before Voice Activity Detection (VAD) comes up as the speakers discuss Larry Saul's ideas for developing a voice or voiced-speech detector using the Fast Fourier Transform (FFT) and analyzing high resolution spectrum features. This normalization step might be beneficial for improving the performance of VAD by reducing background noise and making it easier to identify finer details related to speech in the frequency components.&#10;&#10;2. The context of integrating spectral subtraction, as mentioned by &quot;the OGI folk,&quot; implies that they are focusing on methods to minimize background noise and enhance speech recognition. Applying mean normalization before VAD could help achieve these goals by making the voice activity detection process more robust and accurate in various noisy environments.&#10;&#10;3. In summary, applying mean normalization before VAD is significant because it can potentially improve the performance of voice activity detection in adverse acoustic conditions. This is particularly relevant when working on integrating spectral subtraction techniques for speech recognition, where minimizing background noise and accurately identifying speech are crucial.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target="The speakers do not directly discuss enhancing harmonic structure to reduce noise during audio processing in the given transcripts. The conversation revolves around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target="The transcripts do not provide direct information about measures for ensuring good audio quality when using different microphones or the recommended approach for using a pre-emphasis filter. These topics were not discussed in the meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target="The significance of the OGI's focus on integrating spectral subtraction lies in their goal of minimizing background noise and enhancing speech recognition. This focus area aligns with addressing voice activity detection (VAD) issues, as applying mean normalization before VAD could potentially improve its performance in adverse acoustic conditions. By integrating spectral subtraction techniques, the OGI aims to create a more robust and accurate VAD system that can effectively distinguish speech from background noise in various environments.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target="The transcripts do not provide direct information about the method used by the person who worked there some years ago to help reduce noise in signals by utilizing voicing information and estimating pitch and fine harmonic structure. The discussions revolve around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure or estimating pitch for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target="1. Spectral subtraction increases the difference between the energy of speech and the energy of the noise portion because it is a technique used to reduce background noise in audio signals. By subtracting an estimate of the noise spectrum from the speech plus noise spectrum, the result highlights the speech components that stand out more than they would against the original noisy background. The difference between the energies becomes larger after spectral subtraction compared to before.&#10;   &#10;2. Spectral subtraction also increases the variance of C-zero (the mean normalized channel). When applying spectral subtraction, it is possible to increase the variability of the signal around its mean value due to a less accurate noise estimation or fluctuations in the speech signal itself. As a result, after spectral subtraction, the variance of C-zero will be larger than before.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved approximately ninety-seven to ninety-eight percent correctness involved training on a particular speaker's data. They focused on a specific announcer's voice and determined the most effective features for this speaker through an exhaustive search of all possible feature subsets.&#10;2. This approach relied on the Fast Fourier Transform (FFT) to analyze high-resolution spectrum features in speech. By computing the variance of certain frequency components, they aimed to identify finer details related to speech, which could improve voice activity detection and distinguish between speech and non-speech.&#10;3. Additionally, spectral subtraction before variance normalization was suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance." target="The concern about integrating a different type of spectral subtraction stems from the fact that this method can increase the variance of C-zero (the mean normalized channel) after its application. While spectral subtraction does increase the difference between the energy of speech and noise, it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance. Therefore, careful consideration and adjustments are required when integrating a new spectral subtraction technique to maintain optimal speech recognition and VAD performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor A and PhD F plan to resume their work on voice activity detection (VAD) and speech processing when Hynek returns to town the week after next. They aim to organize more visits, connections, and start working towards June. This may suggest that they have a deadline or event in June for which they need to make progress on their VAD research.&#10;2. PhD D suggested working on voiced-unvoiced detection as part of Stephane's ideas. This involves analyzing the difference between speech and nonspeech, focusing on the voiced-unvoiced characteristics. They plan to explore this idea further in order to improve voice activity detection." target=" {disfmarker} before the VAD . Because {disfmarker} {vocalsound} as {disfmarker} as you 've {pause} mentioned .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: H Hynek will be back in town uh the week after next , back {disfmarker} back in the country . So . And start {disfmarker} start organizing uh {vocalsound} more visits and connections and so forth ,&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: and {disfmarker} uh working towards June .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Also is Stephane was thinking that {vocalsound} maybe it was useful to f to think about uh {vocalsound} voiced - unvoiced {disfmarker}&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: to work uh here in voiced - unvoiced detection .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor A and PhD F plan to resume their work on voice activity detection (VAD) and speech processing when Hynek returns to town the week after next. They aim to organize more visits, connections, and start working towards June. This may suggest that they have a deadline or event in June for which they need to make progress on their VAD research.&#10;2. PhD D suggested working on voiced-unvoiced detection as part of Stephane's ideas. This involves analyzing the difference between speech and nonspeech, focusing on the voiced-unvoiced characteristics. They plan to explore this idea further in order to improve voice activity detection." target="1. The outcome of the experiment was that the French VAD performed significantly better than some other VADs when using only the baseline set of features, which included mel-cepstra. However, the performance varied at times, with the French VAD being better in some instances and worse in others. According to Professor A, the difference in performance between the French VAD and other VADs was substantial enough that it could account for a significant portion of the discrepancy in their own system's performance. PhD D mentioned that they should ask which VAD was used by Pratibha, who conducted the experiment.&#10;&#10;In summary, the French VAD outperformed some other VADs when using only the baseline set of features like mel-cepstra, and this difference in performance could potentially explain a considerable amount of the discrepancy in their own system's performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor A and PhD F plan to resume their work on voice activity detection (VAD) and speech processing when Hynek returns to town the week after next. They aim to organize more visits, connections, and start working towards June. This may suggest that they have a deadline or event in June for which they need to make progress on their VAD research.&#10;2. PhD D suggested working on voiced-unvoiced detection as part of Stephane's ideas. This involves analyzing the difference between speech and nonspeech, focusing on the voiced-unvoiced characteristics. They plan to explore this idea further in order to improve voice activity detection." target="1. Analyzing high resolution spectrum: The team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. They plan to compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech and distinguishing it from non-speech sounds.&#10;2. Voiced-unvoiced characteristics: The team aims to analyze the difference between speech and nonspeech by focusing on the voiced-unvoiced characteristics. This is based on a speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved high correctness rates.&#10;3. Reducing background noise: Spectral subtraction before variance normalization is suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance in distinguishing between voiced speech and non-speech sounds.&#10;4. Low frequency noise consideration: It is important to consider that low frequency noise could be taken for voiced speech, leading to mistakes in distinction. Therefore, appropriate filtering or analysis methods should be used to mitigate this issue.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The team plans to continue working on reducing latency, although they acknowledge that their current plan may not be ideal.&#10;2. There is also a possibility of adding a second stream to the project, such as a multi-band or TRAP (Generalized TRAP) system, which both parties are willing to work on.&#10;3. They are currently exchanging emails and discussing significant results regarding this second stream.&#10;4. One party is working on integrating spectral subtraction from Ericsson, while the other party is trying their own spectral subtraction method.&#10;5. Additionally, they are considering using Larry Saul's ideas to develop a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. This could help in identifying finer details related to speech." target=": Professor A&#10;Content: I don't know . {vocalsound} I mean , we still evidently have a latency reduction plan which {disfmarker} which isn't quite what you 'd like it to be . That {disfmarker} that seems like one prominent thing . And then uh weren't issues of {disfmarker} of having a {disfmarker} a second stream or something ? That was {disfmarker} Was it {disfmarker} There was this business that , you know , we {disfmarker} we could use up the full forty - eight hundred bits , and {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah . But I think they ' I think we want to work on this . They also want to work on this , so . Uh . {vocalsound} yeah . We {disfmarker} we will try MSG , but um , yeah . And they are t I think they want to work on the second stream also , but more with {vocalsound} some kind of multi - band or , well , what they call TRAP or generalized TRAP .&#10;Speaker: Professor A&#10;Content: Mm">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The team plans to continue working on reducing latency, although they acknowledge that their current plan may not be ideal.&#10;2. There is also a possibility of adding a second stream to the project, such as a multi-band or TRAP (Generalized TRAP) system, which both parties are willing to work on.&#10;3. They are currently exchanging emails and discussing significant results regarding this second stream.&#10;4. One party is working on integrating spectral subtraction from Ericsson, while the other party is trying their own spectral subtraction method.&#10;5. Additionally, they are considering using Larry Saul's ideas to develop a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. This could help in identifying finer details related to speech." target=" now , about this and {disfmarker}&#10;Speaker: PhD F&#10;Content: Well , we are exchanging mail as soon as we {disfmarker} {vocalsound} we have significant results .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Um . Yeah . For the moment , they are working on integrating {vocalsound} the um {vocalsound} spectral subtraction apparently from Ericsson .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Um . Yeah . And so . Yeah . We are working on our side on other things like {vocalsound} uh also trying a sup spectral subtraction but of {disfmarker} of our own , I mean , another {vocalsound} spectral substraction .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Um . Yeah . So I think it 's {disfmarker} it 's OK . It 's going {disfmarker}&#10;Speaker: Professor A&#10;Content: Is there any further discussion about this {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The team plans to continue working on reducing latency, although they acknowledge that their current plan may not be ideal.&#10;2. There is also a possibility of adding a second stream to the project, such as a multi-band or TRAP (Generalized TRAP) system, which both parties are willing to work on.&#10;3. They are currently exchanging emails and discussing significant results regarding this second stream.&#10;4. One party is working on integrating spectral subtraction from Ericsson, while the other party is trying their own spectral subtraction method.&#10;5. Additionally, they are considering using Larry Saul's ideas to develop a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. This could help in identifying finer details related to speech." target="aker: PhD B&#10;Content: And so . I 'll let you know what {disfmarker} what happens with that . But if we can {vocalsound} you know , run all of these back - ends f with many fewer iterations and {vocalsound} on Linux boxes we should be able to get a lot more experimenting done .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So . So I wanted to experiment with cutting down the number of iterations before I {vocalsound} increased the number of Gaussians .&#10;Speaker: Professor A&#10;Content: Right . Sorry . So um , how 's it going on the {disfmarker}&#10;Speaker: PhD F&#10;Content: Um .&#10;Speaker: Professor A&#10;Content: So . You {disfmarker} you did some things . They didn't improve things in a way that convinced you you 'd substantially improved anything .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: But they 're not making things worse and we have reduced latency , right ?&#10;Speaker: PhD F&#10;Content: Yeah . But actually">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD B&#10;Content: And so . I 'll let you know what {disfmarker} what happens with that . But if we can {vocalsound} you know , run all of these back - ends f with many fewer iterations and {vocalsound} on Linux boxes we should be able to get a lot more experimenting done .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So . So I wanted to experiment with cutting down the number of iterations before I {vocalsound} increased the number of Gaussians .&#10;Speaker: Professor A&#10;Content: Right . Sorry . So um , how 's it going on the {disfmarker}&#10;Speaker: PhD F&#10;Content: Um .&#10;Speaker: Professor A&#10;Content: So . You {disfmarker} you did some things . They didn't improve things in a way that convinced you you 'd substantially improved anything .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: But they 're not making things worse and we have reduced latency , right ?&#10;Speaker: PhD F&#10;Content: Yeah . But actually" target="1. Based on the transcript, the current level of communication with the OGI gang regarding improving the vocalsound test is through exchanging emails. They are discussing Larry Saul's ideas for developing a voice or voiced-speech detector and analyzing high resolution spectrum features.&#10;&#10;2. There has been no explicit mention of success in approximating old numbers while keeping latency down. However, there have been attempts to implement changes such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. These efforts aim to improve the performance of VAD and reduce latency but their success is not explicitly mentioned in the conversation.&#10;&#10;3. In summary, while there has been ongoing communication with the OGI gang about improving the vocalsound test, there is no clear indication of success in approximating old numbers while keeping latency down. They are still working on different approaches to enhance VAD performance and reduce latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD B&#10;Content: And so . I 'll let you know what {disfmarker} what happens with that . But if we can {vocalsound} you know , run all of these back - ends f with many fewer iterations and {vocalsound} on Linux boxes we should be able to get a lot more experimenting done .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So . So I wanted to experiment with cutting down the number of iterations before I {vocalsound} increased the number of Gaussians .&#10;Speaker: Professor A&#10;Content: Right . Sorry . So um , how 's it going on the {disfmarker}&#10;Speaker: PhD F&#10;Content: Um .&#10;Speaker: Professor A&#10;Content: So . You {disfmarker} you did some things . They didn't improve things in a way that convinced you you 'd substantially improved anything .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: But they 're not making things worse and we have reduced latency , right ?&#10;Speaker: PhD F&#10;Content: Yeah . But actually" target="1. Based on the transcript, there is no explicit statement or confirmation that they have managed to substantially improve the system while also reducing latency. The conversation involves discussions about cutting down the number of iterations, increasing the number of Gaussians, and attempting different methods such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. However, there is no clear indication of success in substantially improving the system while reducing latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The outcome of the experiment was that the French VAD performed significantly better than some other VADs when using only the baseline set of features, which included mel-cepstra. However, the performance varied at times, with the French VAD being better in some instances and worse in others. According to Professor A, the difference in performance between the French VAD and other VADs was substantial enough that it could account for a significant portion of the discrepancy in their own system's performance. PhD D mentioned that they should ask which VAD was used by Pratibha, who conducted the experiment.&#10;&#10;In summary, the French VAD outperformed some other VADs when using only the baseline set of features like mel-cepstra, and this difference in performance could potentially explain a considerable amount of the discrepancy in their own system's performance." target=" of features , just mel cepstra , and you inc incorporate the different V A And it looks like the {disfmarker} the French VAD is actually uh better {disfmarker} significantly better .&#10;Speaker: PhD B&#10;Content: Improves the baseline ?&#10;Speaker: Professor A&#10;Content: Yeah . Yeah .&#10;Speaker: PhD F&#10;Content: Yeah but I don't know which VAD they use . Uh . If the use the small VAD I th I think it 's on {disfmarker} I think it 's easy to do better because it doesn't work at all . So . I {disfmarker} I don't know which {disfmarker} which one . It 's Pratibha that {disfmarker} that did this experiment .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Um . We should ask which VAD she used .&#10;Speaker: PhD D&#10;Content: I don't @ @ . He {disfmarker} Actually , I think that he say with the good VAD of {disfmarker} from OGI and with the Alcatel V">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The outcome of the experiment was that the French VAD performed significantly better than some other VADs when using only the baseline set of features, which included mel-cepstra. However, the performance varied at times, with the French VAD being better in some instances and worse in others. According to Professor A, the difference in performance between the French VAD and other VADs was substantial enough that it could account for a significant portion of the discrepancy in their own system's performance. PhD D mentioned that they should ask which VAD was used by Pratibha, who conducted the experiment.&#10;&#10;In summary, the French VAD outperformed some other VADs when using only the baseline set of features like mel-cepstra, and this difference in performance could potentially explain a considerable amount of the discrepancy in their own system's performance." target="} some kind of multi - band or , well , what they call TRAP or generalized TRAP .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Um . So .&#10;Speaker: Professor A&#10;Content: OK . Do you remember when the next meeting is supposed to be ? the next uh {disfmarker}&#10;Speaker: PhD F&#10;Content: It 's uh in June .&#10;Speaker: Professor A&#10;Content: In June . OK .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Yeah . Um . Yeah , the other thing is that you saw that {disfmarker} that mail about uh the VAD {disfmarker} V A Ds performing quite differently ? That that uh So um . This {disfmarker} there was this experiment of uh &quot; what if we just take the baseline ? &quot;&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: set uh of features , just mel cepstra , and you inc incorporate the different V A And it looks like the {disfmarker} the French VAD">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The outcome of the experiment was that the French VAD performed significantly better than some other VADs when using only the baseline set of features, which included mel-cepstra. However, the performance varied at times, with the French VAD being better in some instances and worse in others. According to Professor A, the difference in performance between the French VAD and other VADs was substantial enough that it could account for a significant portion of the discrepancy in their own system's performance. PhD D mentioned that they should ask which VAD was used by Pratibha, who conducted the experiment.&#10;&#10;In summary, the French VAD outperformed some other VADs when using only the baseline set of features like mel-cepstra, and this difference in performance could potentially explain a considerable amount of the discrepancy in their own system's performance." target="isfmarker} Actually , I think that he say with the good VAD of {disfmarker} from OGI and with the Alcatel VAD . And the experiment was sometime better , sometime worse .&#10;Speaker: PhD F&#10;Content: Yeah but I {disfmarker} it 's uh {disfmarker} I think you were talking about the other mail that used VAD on the reference features .&#10;Speaker: Professor A&#10;Content: Yes .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: And on that one , uh the French one is {disfmarker} was better .&#10;Speaker: PhD D&#10;Content: I don't remember .&#10;Speaker: Professor A&#10;Content: It was just better .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: I mean it was enough better that {disfmarker} that it would {vocalsound} uh account for a fair amount of the difference between our performance , actually .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Mm - h">
      <data key="d0">1</data>
    </edge>
    <edge source="} some kind of multi - band or , well , what they call TRAP or generalized TRAP .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Um . So .&#10;Speaker: Professor A&#10;Content: OK . Do you remember when the next meeting is supposed to be ? the next uh {disfmarker}&#10;Speaker: PhD F&#10;Content: It 's uh in June .&#10;Speaker: Professor A&#10;Content: In June . OK .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Yeah . Um . Yeah , the other thing is that you saw that {disfmarker} that mail about uh the VAD {disfmarker} V A Ds performing quite differently ? That that uh So um . This {disfmarker} there was this experiment of uh &quot; what if we just take the baseline ? &quot;&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: set uh of features , just mel cepstra , and you inc incorporate the different V A And it looks like the {disfmarker} the French VAD" target="The transcript does not provide a direct answer to this question. However, it includes a comment from PhD F about the Personal Digital Assistant (PDA) being consistently located far away during meetings, with the speakers noting that they have not moved it closer. The reason for this was not discussed in the meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. A convenient way to distribute changes and share information over multiple sites, while minimizing interference with regular work and potentially being useful for other tasks as well, would be to use a version control system like CVS (Concurrent Versions System). This was suggested during the discussion and it is a popular method for managing and distributing changes to software projects across multiple sites. It allows users to easily access the latest versions of files, track revisions, and work collaboratively on the project without interfering with each other's regular work. Additionally, such systems can be used for various types of tasks beyond the initial voice activity detector development, making them a versatile tool for different projects and teams." target=" but I could {pause} try to look into like this uh CVS over the web . That seems to be a very popular {vocalsound} way of {pause} people distributing changes and {disfmarker} over , you know , multiple sites and things&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: so maybe {vocalsound} if I can figure out how do that easily and then pass the information on to everybody so that it 's {vocalsound} you know , as easy to do as possible and {disfmarker} and people don't {disfmarker} it won't interfere with {comment} their regular work , then maybe that would be good . And I think we could use it for other things around here too . So .&#10;Speaker: Professor A&#10;Content: Good .&#10;Speaker: Grad C&#10;Content: That 's cool . And if you 're interested in using CVS , I 've set it up here ,&#10;Speaker: PhD B&#10;Content: Oh great .&#10;Speaker: Grad C&#10;Content: so .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Analyzing high resolution spectrum: The team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. They plan to compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech and distinguishing it from non-speech sounds.&#10;2. Voiced-unvoiced characteristics: The team aims to analyze the difference between speech and nonspeech by focusing on the voiced-unvoiced characteristics. This is based on a speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved high correctness rates.&#10;3. Reducing background noise: Spectral subtraction before variance normalization is suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance in distinguishing between voiced speech and non-speech sounds.&#10;4. Low frequency noise consideration: It is important to consider that low frequency noise could be taken for voiced speech, leading to mistakes in distinction. Therefore, appropriate filtering or analysis methods should be used to mitigate this issue." target="The speakers observe that when they apply mean normalization to a clean channel (referred to as C-zero), it resembles the second figure, which is a positive outcome. They also note that the noise part is around zero. When both mean and variance normalization are applied, the speech portion of two channels becomes very close in the third figure. This indicates that the visualizations correspond well to their expectations for both clean speech and noisy conditions, suggesting good quality. However, they do not explicitly discuss human vocal sound abilities in this context.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Analyzing high resolution spectrum: The team is considering developing a voice or voiced-speech detector that works on the Fast Fourier Transform (FFT) and analyzes the high resolution spectrum. They plan to compute the variance of certain frequency components or &quot;high resolution spectrum&quot; features, which could help in identifying finer details related to speech and distinguishing it from non-speech sounds.&#10;2. Voiced-unvoiced characteristics: The team aims to analyze the difference between speech and nonspeech by focusing on the voiced-unvoiced characteristics. This is based on a speaker-dependent voiced-unvoiced speech recognition approach used twenty years ago that achieved high correctness rates.&#10;3. Reducing background noise: Spectral subtraction before variance normalization is suggested as a method for reducing background noise in the voice activity detector, further enhancing its performance in distinguishing between voiced speech and non-speech sounds.&#10;4. Low frequency noise consideration: It is important to consider that low frequency noise could be taken for voiced speech, leading to mistakes in distinction. Therefore, appropriate filtering or analysis methods should be used to mitigate this issue." target="1. The suggestion to apply mean normalization before Voice Activity Detection (VAD) comes up as the speakers discuss Larry Saul's ideas for developing a voice or voiced-speech detector using the Fast Fourier Transform (FFT) and analyzing high resolution spectrum features. This normalization step might be beneficial for improving the performance of VAD by reducing background noise and making it easier to identify finer details related to speech in the frequency components.&#10;&#10;2. The context of integrating spectral subtraction, as mentioned by &quot;the OGI folk,&quot; implies that they are focusing on methods to minimize background noise and enhance speech recognition. Applying mean normalization before VAD could help achieve these goals by making the voice activity detection process more robust and accurate in various noisy environments.&#10;&#10;3. In summary, applying mean normalization before VAD is significant because it can potentially improve the performance of voice activity detection in adverse acoustic conditions. This is particularly relevant when working on integrating spectral subtraction techniques for speech recognition, where minimizing background noise and accurately identifying speech are crucial.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The result of adding spectral subtraction before the Wall process, which contains LDA (Linear Discriminant Analysis) and online normalization, was that it hurt the performance significantly.&#10;2. The sentence &quot;Due&quot; spoken in an Italian utterance, recorded by two channels (close mic microphone as channel 0 and distant microphone as channel 1), could not be clearly seen when plotted because of the following reasons:&#10;* The plot was a plot of C-zero parameters for one Italian utterance.&#10;* The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;* Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels." target=" start {disfmarker} we started to work on spectral subtraction . And {vocalsound} um {vocalsound} the preliminary results were very bad .&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: So the thing that we did is just to add spectral subtraction before this , the Wall uh process , which contains LDA on - line normalization . And it hurts uh a lot .&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: And so we started to look at {disfmarker} at um things like this , which is , well , it 's {disfmarker} Yeah . So you have the C - zero parameters for one uh Italian utterance .&#10;Speaker: PhD D&#10;Content: You can @ @ .&#10;Speaker: PhD F&#10;Content: And I plotted this for two channels . Channel zero is the close mic microphone , and channel one is the distant microphone . And it 's perfectly synchronized , so . And the sentence contain only one word , which is &quot; Due &quot; And it can't clearly be seen . Where {disfmarker} where is it ?&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The result of adding spectral subtraction before the Wall process, which contains LDA (Linear Discriminant Analysis) and online normalization, was that it hurt the performance significantly.&#10;2. The sentence &quot;Due&quot; spoken in an Italian utterance, recorded by two channels (close mic microphone as channel 0 and distant microphone as channel 1), could not be clearly seen when plotted because of the following reasons:&#10;* The plot was a plot of C-zero parameters for one Italian utterance.&#10;* The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;* Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels." target=" the sentence contain only one word , which is &quot; Due &quot; And it can't clearly be seen . Where {disfmarker} where is it ?&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: Where is the word ?&#10;Speaker: PhD B&#10;Content: This is {disfmarker} this is ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: oh , a plot of C - zero ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: PhD B&#10;Content: the energy .&#10;Speaker: PhD F&#10;Content: This is a plot of C - zero , uh when we don't use spectral substraction , and when there is no on - line normalization .&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: So . There is just some filtering with the LDA and {vocalsound} and some downsampling , upsampling .&#10;Speaker: PhD B&#10;Content: C - zero is the close talking ? {disfmarker}&#10;Speaker: PhD F&#10;Content: So .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The result of adding spectral subtraction before the Wall process, which contains LDA (Linear Discriminant Analysis) and online normalization, was that it hurt the performance significantly.&#10;2. The sentence &quot;Due&quot; spoken in an Italian utterance, recorded by two channels (close mic microphone as channel 0 and distant microphone as channel 1), could not be clearly seen when plotted because of the following reasons:&#10;* The plot was a plot of C-zero parameters for one Italian utterance.&#10;* The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;* Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels." target="1. The issue encountered when adding spectral subtraction before the Wall process (which contains LDA and online normalization) was that it significantly hurt the performance. This is because spectral subtraction can increase the difference between the energy of speech and noise, but it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance.&#10;&#10;2. The C-zero parameters for a single Italian utterance in two channels cannot be clearly seen in the plot due to several reasons:&#10;   - The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;   - Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels.&#10;   - The plot being a plot of C-zero parameters for one Italian utterance may not provide enough context or clear differentiation between channels and frequencies to easily distinguish specific words or sounds.">
      <data key="d0">1</data>
    </edge>
    <edge source=" start {disfmarker} we started to work on spectral subtraction . And {vocalsound} um {vocalsound} the preliminary results were very bad .&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: So the thing that we did is just to add spectral subtraction before this , the Wall uh process , which contains LDA on - line normalization . And it hurts uh a lot .&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: And so we started to look at {disfmarker} at um things like this , which is , well , it 's {disfmarker} Yeah . So you have the C - zero parameters for one uh Italian utterance .&#10;Speaker: PhD D&#10;Content: You can @ @ .&#10;Speaker: PhD F&#10;Content: And I plotted this for two channels . Channel zero is the close mic microphone , and channel one is the distant microphone . And it 's perfectly synchronized , so . And the sentence contain only one word , which is &quot; Due &quot; And it can't clearly be seen . Where {disfmarker} where is it ?&#10;" target="1. The issue encountered when adding spectral subtraction before the Wall process (which contains LDA and online normalization) was that it significantly hurt the performance. This is because spectral subtraction can increase the difference between the energy of speech and noise, but it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance.&#10;&#10;2. The C-zero parameters for a single Italian utterance in two channels cannot be clearly seen in the plot due to several reasons:&#10;   - The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;   - Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels.&#10;   - The plot being a plot of C-zero parameters for one Italian utterance may not provide enough context or clear differentiation between channels and frequencies to easily distinguish specific words or sounds.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers observe that when they apply mean normalization to a clean channel (referred to as C-zero), it resembles the second figure, which is a positive outcome. They also note that the noise part is around zero. When both mean and variance normalization are applied, the speech portion of two channels becomes very close in the third figure. This indicates that the visualizations correspond well to their expectations for both clean speech and noisy conditions, suggesting good quality. However, they do not explicitly discuss human vocal sound abilities in this context." target=" PhD B&#10;Content: C - zero is the close talking ? {disfmarker}&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: PhD B&#10;Content: uh the close channel ?&#10;Speaker: PhD F&#10;Content: Yeah . Yeah .&#10;Speaker: PhD B&#10;Content: and s channel one is the {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah . So C - zero is very clean , actually .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Uh then when we apply mean normalization it looks like the second figure , though it is not . Which is good . Well , the noise part is around zero&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: and {disfmarker} {vocalsound} {vocalsound} And then the third figure is what happens when we apply mean normalization and variance normalization . So . What we can clearly see is that on the speech portion {vocalsound} the two channel come {disfmarker} becomes very close , but also what happens on the noisy portion is that the variance of">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers observe that when they apply mean normalization to a clean channel (referred to as C-zero), it resembles the second figure, which is a positive outcome. They also note that the noise part is around zero. When both mean and variance normalization are applied, the speech portion of two channels becomes very close in the third figure. This indicates that the visualizations correspond well to their expectations for both clean speech and noisy conditions, suggesting good quality. However, they do not explicitly discuss human vocal sound abilities in this context." target="1. The suggestion to apply mean normalization before Voice Activity Detection (VAD) comes up as the speakers discuss Larry Saul's ideas for developing a voice or voiced-speech detector using the Fast Fourier Transform (FFT) and analyzing high resolution spectrum features. This normalization step might be beneficial for improving the performance of VAD by reducing background noise and making it easier to identify finer details related to speech in the frequency components.&#10;&#10;2. The context of integrating spectral subtraction, as mentioned by &quot;the OGI folk,&quot; implies that they are focusing on methods to minimize background noise and enhance speech recognition. Applying mean normalization before VAD could help achieve these goals by making the voice activity detection process more robust and accurate in various noisy environments.&#10;&#10;3. In summary, applying mean normalization before VAD is significant because it can potentially improve the performance of voice activity detection in adverse acoustic conditions. This is particularly relevant when working on integrating spectral subtraction techniques for speech recognition, where minimizing background noise and accurately identifying speech are crucial.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers observe that when they apply mean normalization to a clean channel (referred to as C-zero), it resembles the second figure, which is a positive outcome. They also note that the noise part is around zero. When both mean and variance normalization are applied, the speech portion of two channels becomes very close in the third figure. This indicates that the visualizations correspond well to their expectations for both clean speech and noisy conditions, suggesting good quality. However, they do not explicitly discuss human vocal sound abilities in this context." target="The speakers do not directly discuss enhancing harmonic structure to reduce noise during audio processing in the given transcripts. The conversation revolves around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers observe that when they apply mean normalization to a clean channel (referred to as C-zero), it resembles the second figure, which is a positive outcome. They also note that the noise part is around zero. When both mean and variance normalization are applied, the speech portion of two channels becomes very close in the third figure. This indicates that the visualizations correspond well to their expectations for both clean speech and noisy conditions, suggesting good quality. However, they do not explicitly discuss human vocal sound abilities in this context." target="1. Spectral subtraction increases the difference between the energy of speech and the energy of the noise portion because it is a technique used to reduce background noise in audio signals. By subtracting an estimate of the noise spectrum from the speech plus noise spectrum, the result highlights the speech components that stand out more than they would against the original noisy background. The difference between the energies becomes larger after spectral subtraction compared to before.&#10;   &#10;2. Spectral subtraction also increases the variance of C-zero (the mean normalized channel). When applying spectral subtraction, it is possible to increase the variability of the signal around its mean value due to a less accurate noise estimation or fluctuations in the speech signal itself. As a result, after spectral subtraction, the variance of C-zero will be larger than before.">
      <data key="d0">1</data>
    </edge>
    <edge source=" PhD B&#10;Content: C - zero is the close talking ? {disfmarker}&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: PhD B&#10;Content: uh the close channel ?&#10;Speaker: PhD F&#10;Content: Yeah . Yeah .&#10;Speaker: PhD B&#10;Content: and s channel one is the {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah . So C - zero is very clean , actually .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Uh then when we apply mean normalization it looks like the second figure , though it is not . Which is good . Well , the noise part is around zero&#10;Speaker: Professor A&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: and {disfmarker} {vocalsound} {vocalsound} And then the third figure is what happens when we apply mean normalization and variance normalization . So . What we can clearly see is that on the speech portion {vocalsound} the two channel come {disfmarker} becomes very close , but also what happens on the noisy portion is that the variance of" target="Professionor A mentioned the issue of noise during his talk on reverberation, even though no one seemed to notice it, because he considered it a valuable example of real-world challenges in audio recordings. Specifically, the noise from distant microphones demonstrated background noise that can still be informative and useful for discussion. Although the example was not perfect for illustrating reverberation effects, it served as an opportunity to highlight other relevant issues in audio processing. Furthermore, Professor A used this example to emphasize the importance of filtering and processing techniques to handle such noise.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The suggestion to apply mean normalization before Voice Activity Detection (VAD) comes up as the speakers discuss Larry Saul's ideas for developing a voice or voiced-speech detector using the Fast Fourier Transform (FFT) and analyzing high resolution spectrum features. This normalization step might be beneficial for improving the performance of VAD by reducing background noise and making it easier to identify finer details related to speech in the frequency components.&#10;&#10;2. The context of integrating spectral subtraction, as mentioned by &quot;the OGI folk,&quot; implies that they are focusing on methods to minimize background noise and enhance speech recognition. Applying mean normalization before VAD could help achieve these goals by making the voice activity detection process more robust and accurate in various noisy environments.&#10;&#10;3. In summary, applying mean normalization before VAD is significant because it can potentially improve the performance of voice activity detection in adverse acoustic conditions. This is particularly relevant when working on integrating spectral subtraction techniques for speech recognition, where minimizing background noise and accurately identifying speech are crucial." target="1. Based on the transcript, the current level of communication with the OGI gang regarding improving the vocalsound test is through exchanging emails. They are discussing Larry Saul's ideas for developing a voice or voiced-speech detector and analyzing high resolution spectrum features.&#10;&#10;2. There has been no explicit mention of success in approximating old numbers while keeping latency down. However, there have been attempts to implement changes such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. These efforts aim to improve the performance of VAD and reduce latency but their success is not explicitly mentioned in the conversation.&#10;&#10;3. In summary, while there has been ongoing communication with the OGI gang about improving the vocalsound test, there is no clear indication of success in approximating old numbers while keeping latency down. They are still working on different approaches to enhance VAD performance and reduce latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The suggestion to apply mean normalization before Voice Activity Detection (VAD) comes up as the speakers discuss Larry Saul's ideas for developing a voice or voiced-speech detector using the Fast Fourier Transform (FFT) and analyzing high resolution spectrum features. This normalization step might be beneficial for improving the performance of VAD by reducing background noise and making it easier to identify finer details related to speech in the frequency components.&#10;&#10;2. The context of integrating spectral subtraction, as mentioned by &quot;the OGI folk,&quot; implies that they are focusing on methods to minimize background noise and enhance speech recognition. Applying mean normalization before VAD could help achieve these goals by making the voice activity detection process more robust and accurate in various noisy environments.&#10;&#10;3. In summary, applying mean normalization before VAD is significant because it can potentially improve the performance of voice activity detection in adverse acoustic conditions. This is particularly relevant when working on integrating spectral subtraction techniques for speech recognition, where minimizing background noise and accurately identifying speech are crucial." target="The speakers do not directly discuss enhancing harmonic structure to reduce noise during audio processing in the given transcripts. The conversation revolves around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The suggestion to apply mean normalization before Voice Activity Detection (VAD) comes up as the speakers discuss Larry Saul's ideas for developing a voice or voiced-speech detector using the Fast Fourier Transform (FFT) and analyzing high resolution spectrum features. This normalization step might be beneficial for improving the performance of VAD by reducing background noise and making it easier to identify finer details related to speech in the frequency components.&#10;&#10;2. The context of integrating spectral subtraction, as mentioned by &quot;the OGI folk,&quot; implies that they are focusing on methods to minimize background noise and enhance speech recognition. Applying mean normalization before VAD could help achieve these goals by making the voice activity detection process more robust and accurate in various noisy environments.&#10;&#10;3. In summary, applying mean normalization before VAD is significant because it can potentially improve the performance of voice activity detection in adverse acoustic conditions. This is particularly relevant when working on integrating spectral subtraction techniques for speech recognition, where minimizing background noise and accurately identifying speech are crucial." target="1. The researchers were discussing a potential experiment using TIMIT data and a &quot;voicing bit&quot; to possibly enhance performance in challenging audio conditions, such as noisy environments or separate audio streams. This conversation was related to Larry Saul's ideas for developing a voice or voiced-speech detector that utilizes the Fast Fourier Transform (FFT) and analyzes high resolution spectrum features.&#10;2. The voicing bit could refer to the voiced-unvoiced characteristics of speech, which are mentioned in the context of differentiating between speech and non-speech. By incorporating this information into their voice activity detector, they aim to improve its performance by better recognizing speech within various types of noise.&#10;3. The researchers also discussed applying mean normalization before Voice Activity Detection (VAD) as part of their plans for minimizing background noise and enhancing speech recognition. This step might help make the VAD more robust and accurate in different noisy conditions, which is particularly relevant when integrating spectral subtraction techniques for speech processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The suggestion to apply mean normalization before Voice Activity Detection (VAD) comes up as the speakers discuss Larry Saul's ideas for developing a voice or voiced-speech detector using the Fast Fourier Transform (FFT) and analyzing high resolution spectrum features. This normalization step might be beneficial for improving the performance of VAD by reducing background noise and making it easier to identify finer details related to speech in the frequency components.&#10;&#10;2. The context of integrating spectral subtraction, as mentioned by &quot;the OGI folk,&quot; implies that they are focusing on methods to minimize background noise and enhance speech recognition. Applying mean normalization before VAD could help achieve these goals by making the voice activity detection process more robust and accurate in various noisy environments.&#10;&#10;3. In summary, applying mean normalization before VAD is significant because it can potentially improve the performance of voice activity detection in adverse acoustic conditions. This is particularly relevant when working on integrating spectral subtraction techniques for speech recognition, where minimizing background noise and accurately identifying speech are crucial." target="The transcripts do not provide direct information about measures for ensuring good audio quality when using different microphones or the recommended approach for using a pre-emphasis filter. These topics were not discussed in the meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The suggestion to apply mean normalization before Voice Activity Detection (VAD) comes up as the speakers discuss Larry Saul's ideas for developing a voice or voiced-speech detector using the Fast Fourier Transform (FFT) and analyzing high resolution spectrum features. This normalization step might be beneficial for improving the performance of VAD by reducing background noise and making it easier to identify finer details related to speech in the frequency components.&#10;&#10;2. The context of integrating spectral subtraction, as mentioned by &quot;the OGI folk,&quot; implies that they are focusing on methods to minimize background noise and enhance speech recognition. Applying mean normalization before VAD could help achieve these goals by making the voice activity detection process more robust and accurate in various noisy environments.&#10;&#10;3. In summary, applying mean normalization before VAD is significant because it can potentially improve the performance of voice activity detection in adverse acoustic conditions. This is particularly relevant when working on integrating spectral subtraction techniques for speech recognition, where minimizing background noise and accurately identifying speech are crucial." target="The significance of the OGI's focus on integrating spectral subtraction lies in their goal of minimizing background noise and enhancing speech recognition. This focus area aligns with addressing voice activity detection (VAD) issues, as applying mean normalization before VAD could potentially improve its performance in adverse acoustic conditions. By integrating spectral subtraction techniques, the OGI aims to create a more robust and accurate VAD system that can effectively distinguish speech from background noise in various environments.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on the transcript, the current level of communication with the OGI gang regarding improving the vocalsound test is through exchanging emails. They are discussing Larry Saul's ideas for developing a voice or voiced-speech detector and analyzing high resolution spectrum features.&#10;&#10;2. There has been no explicit mention of success in approximating old numbers while keeping latency down. However, there have been attempts to implement changes such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. These efforts aim to improve the performance of VAD and reduce latency but their success is not explicitly mentioned in the conversation.&#10;&#10;3. In summary, while there has been ongoing communication with the OGI gang about improving the vocalsound test, there is no clear indication of success in approximating old numbers while keeping latency down. They are still working on different approaches to enhance VAD performance and reduce latency." target="er} if in all these different cases {vocalsound} it never gets better , and there 's significant number of cases where it gets worse , {vocalsound} then you 're probably {pause} hurting things , {vocalsound} I would say . So um {vocalsound} I mean at the very least that would be a reasonably prediction of what would happen with {disfmarker} with a different test set , that you 're not jiggling things with . So I guess the question is if you can do better than this . If you can {disfmarker} if we can approximate {vocalsound} the old numbers while still keeping the latency down .&#10;Speaker: PhD F&#10;Content: Mmm . Yeah .&#10;Speaker: Professor A&#10;Content: Uh , so . Um . What I was asking , though , is uh {disfmarker} are {disfmarker} what 's {disfmarker} what 's the level of communication with uh {vocalsound} the O G I gang now , about this and {disfmarker}&#10;Speaker: PhD F&#10;Content: Well , we are exchanging mail as soon as we {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on the transcript, the current level of communication with the OGI gang regarding improving the vocalsound test is through exchanging emails. They are discussing Larry Saul's ideas for developing a voice or voiced-speech detector and analyzing high resolution spectrum features.&#10;&#10;2. There has been no explicit mention of success in approximating old numbers while keeping latency down. However, there have been attempts to implement changes such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. These efforts aim to improve the performance of VAD and reduce latency but their success is not explicitly mentioned in the conversation.&#10;&#10;3. In summary, while there has been ongoing communication with the OGI gang about improving the vocalsound test, there is no clear indication of success in approximating old numbers while keeping latency down. They are still working on different approaches to enhance VAD performance and reduce latency." target="aker: PhD F&#10;Content: uh .&#10;Speaker: PhD B&#10;Content: if {vocalsound} um you know , if {disfmarker} if {disfmarker} if you work on something else and {disfmarker} and you 're waiting for them to give you {vocalsound} spectral subtraction {disfmarker} I mean it 's hard to know whether {vocalsound} the effects that you get from the other experiments you do will {vocalsound} carry over once you then bring in their spectral subtraction module . So it 's {disfmarker} it 's almost like everything 's held up waiting for this {vocalsound} one thing . I don't know if that 's true or not , but I could see how {disfmarker}&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: Professor A&#10;Content: I don't know .&#10;Speaker: PhD B&#10;Content: Maybe that 's what you were thinking .&#10;Speaker: Professor A&#10;Content: I don't know . {vocalsound} I mean , we still evidently have a latency reduction plan which {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on the transcript, the current level of communication with the OGI gang regarding improving the vocalsound test is through exchanging emails. They are discussing Larry Saul's ideas for developing a voice or voiced-speech detector and analyzing high resolution spectrum features.&#10;&#10;2. There has been no explicit mention of success in approximating old numbers while keeping latency down. However, there have been attempts to implement changes such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. These efforts aim to improve the performance of VAD and reduce latency but their success is not explicitly mentioned in the conversation.&#10;&#10;3. In summary, while there has been ongoing communication with the OGI gang about improving the vocalsound test, there is no clear indication of success in approximating old numbers while keeping latency down. They are still working on different approaches to enhance VAD performance and reduce latency." target=": Uh . I guess cuz we haven't wanted to move it . We {disfmarker} we could {disfmarker} {vocalsound} we could move us ,&#10;Speaker: PhD F&#10;Content: Yeah ?&#10;Speaker: Professor A&#10;Content: and .&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Grad E&#10;Content: That 's right .&#10;Speaker: PhD F&#10;Content: Well , anyway . Um . Yeah , so . Uh . Since the last meeting we 've {disfmarker} we 've tried to put together um {vocalsound} the clean low - pass um downsampling , upsampling , I mean , Uh the new filter that 's replacing the LDA filters , and also {vocalsound} the um delay issue so that {disfmarker} We considered th the {disfmarker} the delay issue on the {disfmarker} for the on - line normalization . Mmm . So we 've put together all this and then we have results that are not um {vocalsound} {vocalsound} very impressive . Well , there is no {vocalsound} real">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on the transcript, the current level of communication with the OGI gang regarding improving the vocalsound test is through exchanging emails. They are discussing Larry Saul's ideas for developing a voice or voiced-speech detector and analyzing high resolution spectrum features.&#10;&#10;2. There has been no explicit mention of success in approximating old numbers while keeping latency down. However, there have been attempts to implement changes such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. These efforts aim to improve the performance of VAD and reduce latency but their success is not explicitly mentioned in the conversation.&#10;&#10;3. In summary, while there has been ongoing communication with the OGI gang about improving the vocalsound test, there is no clear indication of success in approximating old numbers while keeping latency down. They are still working on different approaches to enhance VAD performance and reduce latency." target="The significance of the OGI's focus on integrating spectral subtraction lies in their goal of minimizing background noise and enhancing speech recognition. This focus area aligns with addressing voice activity detection (VAD) issues, as applying mean normalization before VAD could potentially improve its performance in adverse acoustic conditions. By integrating spectral subtraction techniques, the OGI aims to create a more robust and accurate VAD system that can effectively distinguish speech from background noise in various environments.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on the transcript, the current level of communication with the OGI gang regarding improving the vocalsound test is through exchanging emails. They are discussing Larry Saul's ideas for developing a voice or voiced-speech detector and analyzing high resolution spectrum features.&#10;&#10;2. There has been no explicit mention of success in approximating old numbers while keeping latency down. However, there have been attempts to implement changes such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. These efforts aim to improve the performance of VAD and reduce latency but their success is not explicitly mentioned in the conversation.&#10;&#10;3. In summary, while there has been ongoing communication with the OGI gang about improving the vocalsound test, there is no clear indication of success in approximating old numbers while keeping latency down. They are still working on different approaches to enhance VAD performance and reduce latency." target="1. Based on the transcript, there is no explicit statement or confirmation that they have managed to substantially improve the system while also reducing latency. The conversation involves discussions about cutting down the number of iterations, increasing the number of Gaussians, and attempting different methods such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. However, there is no clear indication of success in substantially improving the system while reducing latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers do not directly discuss enhancing harmonic structure to reduce noise during audio processing in the given transcripts. The conversation revolves around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure for this purpose." target="1. The researchers were discussing a potential experiment using TIMIT data and a &quot;voicing bit&quot; to possibly enhance performance in challenging audio conditions, such as noisy environments or separate audio streams. This conversation was related to Larry Saul's ideas for developing a voice or voiced-speech detector that utilizes the Fast Fourier Transform (FFT) and analyzes high resolution spectrum features.&#10;2. The voicing bit could refer to the voiced-unvoiced characteristics of speech, which are mentioned in the context of differentiating between speech and non-speech. By incorporating this information into their voice activity detector, they aim to improve its performance by better recognizing speech within various types of noise.&#10;3. The researchers also discussed applying mean normalization before Voice Activity Detection (VAD) as part of their plans for minimizing background noise and enhancing speech recognition. This step might help make the VAD more robust and accurate in different noisy conditions, which is particularly relevant when integrating spectral subtraction techniques for speech processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers do not directly discuss enhancing harmonic structure to reduce noise during audio processing in the given transcripts. The conversation revolves around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure for this purpose." target="The transcripts do not provide direct information about measures for ensuring good audio quality when using different microphones or the recommended approach for using a pre-emphasis filter. These topics were not discussed in the meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers do not directly discuss enhancing harmonic structure to reduce noise during audio processing in the given transcripts. The conversation revolves around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure for this purpose." target="The transcripts do not provide direct information about the method used by the person who worked there some years ago to help reduce noise in signals by utilizing voicing information and estimating pitch and fine harmonic structure. The discussions revolve around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure or estimating pitch for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The researchers were discussing a potential experiment using TIMIT data and a &quot;voicing bit&quot; to possibly enhance performance in challenging audio conditions, such as noisy environments or separate audio streams. This conversation was related to Larry Saul's ideas for developing a voice or voiced-speech detector that utilizes the Fast Fourier Transform (FFT) and analyzes high resolution spectrum features.&#10;2. The voicing bit could refer to the voiced-unvoiced characteristics of speech, which are mentioned in the context of differentiating between speech and non-speech. By incorporating this information into their voice activity detector, they aim to improve its performance by better recognizing speech within various types of noise.&#10;3. The researchers also discussed applying mean normalization before Voice Activity Detection (VAD) as part of their plans for minimizing background noise and enhancing speech recognition. This step might help make the VAD more robust and accurate in different noisy conditions, which is particularly relevant when integrating spectral subtraction techniques for speech processing." target="The significance of the OGI's focus on integrating spectral subtraction lies in their goal of minimizing background noise and enhancing speech recognition. This focus area aligns with addressing voice activity detection (VAD) issues, as applying mean normalization before VAD could potentially improve its performance in adverse acoustic conditions. By integrating spectral subtraction techniques, the OGI aims to create a more robust and accurate VAD system that can effectively distinguish speech from background noise in various environments.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The researchers were discussing a potential experiment using TIMIT data and a &quot;voicing bit&quot; to possibly enhance performance in challenging audio conditions, such as noisy environments or separate audio streams. This conversation was related to Larry Saul's ideas for developing a voice or voiced-speech detector that utilizes the Fast Fourier Transform (FFT) and analyzes high resolution spectrum features.&#10;2. The voicing bit could refer to the voiced-unvoiced characteristics of speech, which are mentioned in the context of differentiating between speech and non-speech. By incorporating this information into their voice activity detector, they aim to improve its performance by better recognizing speech within various types of noise.&#10;3. The researchers also discussed applying mean normalization before Voice Activity Detection (VAD) as part of their plans for minimizing background noise and enhancing speech recognition. This step might help make the VAD more robust and accurate in different noisy conditions, which is particularly relevant when integrating spectral subtraction techniques for speech processing." target="The transcripts do not provide direct information about the method used by the person who worked there some years ago to help reduce noise in signals by utilizing voicing information and estimating pitch and fine harmonic structure. The discussions revolve around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure or estimating pitch for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcripts do not provide direct information about measures for ensuring good audio quality when using different microphones or the recommended approach for using a pre-emphasis filter. These topics were not discussed in the meetings." target="The transcript does not provide a direct answer to this question. However, it includes a comment from PhD F about the Personal Digital Assistant (PDA) being consistently located far away during meetings, with the speakers noting that they have not moved it closer. The reason for this was not discussed in the meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcripts do not provide direct information about measures for ensuring good audio quality when using different microphones or the recommended approach for using a pre-emphasis filter. These topics were not discussed in the meetings." target="The transcripts do not provide direct information about the method used by the person who worked there some years ago to help reduce noise in signals by utilizing voicing information and estimating pitch and fine harmonic structure. The discussions revolve around developing a voice or voiced-speech detector using Fast Fourier Transform (FFT) and analyzing high resolution spectrum features, as well as implementing mean normalization before Voice Activity Detection (VAD) to improve its performance by reducing background noise. While these techniques can help with noise reduction and enhancing speech recognition, they do not specifically mention improving harmonic structure or estimating pitch for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="The significance of the OGI's focus on integrating spectral subtraction lies in their goal of minimizing background noise and enhancing speech recognition. This focus area aligns with addressing voice activity detection (VAD) issues, as applying mean normalization before VAD could potentially improve its performance in adverse acoustic conditions. By integrating spectral subtraction techniques, the OGI aims to create a more robust and accurate VAD system that can effectively distinguish speech from background noise in various environments." target="1. Spectral subtraction increases the difference between the energy of speech and the energy of the noise portion because it is a technique used to reduce background noise in audio signals. By subtracting an estimate of the noise spectrum from the speech plus noise spectrum, the result highlights the speech components that stand out more than they would against the original noisy background. The difference between the energies becomes larger after spectral subtraction compared to before.&#10;   &#10;2. Spectral subtraction also increases the variance of C-zero (the mean normalized channel). When applying spectral subtraction, it is possible to increase the variability of the signal around its mean value due to a less accurate noise estimation or fluctuations in the speech signal itself. As a result, after spectral subtraction, the variance of C-zero will be larger than before.">
      <data key="d0">1</data>
    </edge>
    <edge source="The significance of the OGI's focus on integrating spectral subtraction lies in their goal of minimizing background noise and enhancing speech recognition. This focus area aligns with addressing voice activity detection (VAD) issues, as applying mean normalization before VAD could potentially improve its performance in adverse acoustic conditions. By integrating spectral subtraction techniques, the OGI aims to create a more robust and accurate VAD system that can effectively distinguish speech from background noise in various environments." target="The concern about integrating a different type of spectral subtraction stems from the fact that this method can increase the variance of C-zero (the mean normalized channel) after its application. While spectral subtraction does increase the difference between the energy of speech and noise, it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance. Therefore, careful consideration and adjustments are required when integrating a new spectral subtraction technique to maintain optimal speech recognition and VAD performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Spectral Effects: The spectral effects refer to the reduction in high frequencies in the indirect energy. This is likely due to the filtering process mentioned in the transcript, where there is some attenuation of frequencies above a certain cut-off point (approximately twenty hertz). This filtering results in reduced high frequency content in the signal.&#10;&#10;2. Temporal Changes: The temporal changes are alterations in the timing characteristics of the signal. These changes can cause waveforms to appear different, even if the underlying signal is similar. For example, variations in energy levels at different time points or the presence of certain transient elements can result in visible differences between waveforms.&#10;&#10;3. Reduced High Frequencies: The reduction in high frequencies (caused by filtering) affects the overall spectral balance of the signal, resulting in a more pronounced lower frequency content. This change is responsible for some of the observed differences between waveforms and can also contribute to variations in speech perception or recognition performance.&#10;&#10;4. Distinguishing Between E and B: The discussion about distinguishing between E and B refers to trying to separate two sources of variability that may affect the data, potentially introducing bias. One possible explanation for this statement is that if one source (E) was recorded with higher energy levels or more variation compared to another source (B), it could lead to a systematic difference in the perceived signal characteristics. This potential bias can be mitigated by removing sources of variability that are unrelated to the linguistic component being analyzed.&#10;&#10;5. Spectral Subtraction: The application of spectral subtraction increases the difference between speech and noise energy levels, improving the overall signal-to-noise ratio. However, it also increases the variance of C-zero (the residual noise portion after spectral subtraction). This trade-off should be carefully considered during data analysis to balance improvements in signal quality with the increased variability due to the processing technique." target="aker: Professor A&#10;Content: Right . But it 's {disfmarker} it 's uh {disfmarker} So . Yeah . So there are {disfmarker} clearly are spectral effects . Since you 're getting all this indirect energy , then a lot of it does have {disfmarker} have uh {vocalsound} reduced high frequencies . But um the other thing is the temporal courses of things really are changed , and {disfmarker} {vocalsound} and uh we want to show that , in some obvious way . The reason I put the wave forms in there was because {vocalsound} uh they {disfmarker} they do look quite different . Uh . And so I thought &quot; Oh , this is good . &quot; but I {disfmarker} {vocalsound} I just uh {disfmarker} After {disfmarker} after uh they were put in there I didn't really look at them anymore , cuz I just {disfmarker} they were different . So {vocalsound} I want something that has a {disfmarker} is a more interesting explanation for why they 're different . Um .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Spectral Effects: The spectral effects refer to the reduction in high frequencies in the indirect energy. This is likely due to the filtering process mentioned in the transcript, where there is some attenuation of frequencies above a certain cut-off point (approximately twenty hertz). This filtering results in reduced high frequency content in the signal.&#10;&#10;2. Temporal Changes: The temporal changes are alterations in the timing characteristics of the signal. These changes can cause waveforms to appear different, even if the underlying signal is similar. For example, variations in energy levels at different time points or the presence of certain transient elements can result in visible differences between waveforms.&#10;&#10;3. Reduced High Frequencies: The reduction in high frequencies (caused by filtering) affects the overall spectral balance of the signal, resulting in a more pronounced lower frequency content. This change is responsible for some of the observed differences between waveforms and can also contribute to variations in speech perception or recognition performance.&#10;&#10;4. Distinguishing Between E and B: The discussion about distinguishing between E and B refers to trying to separate two sources of variability that may affect the data, potentially introducing bias. One possible explanation for this statement is that if one source (E) was recorded with higher energy levels or more variation compared to another source (B), it could lead to a systematic difference in the perceived signal characteristics. This potential bias can be mitigated by removing sources of variability that are unrelated to the linguistic component being analyzed.&#10;&#10;5. Spectral Subtraction: The application of spectral subtraction increases the difference between speech and noise energy levels, improving the overall signal-to-noise ratio. However, it also increases the variance of C-zero (the residual noise portion after spectral subtraction). This trade-off should be carefully considered during data analysis to balance improvements in signal quality with the increased variability due to the processing technique." target=" filtering and in general we like to do that , because of things like this and {vocalsound} it 's {disfmarker} it 's pretty {disfmarker} it 's not a very severe filter . Doesn't affect speech frequencies , even pretty low speech frequencies , at all , but it 's&#10;Speaker: PhD B&#10;Content: What 's the {pause} cut - off frequency it used ?&#10;Speaker: Professor A&#10;Content: Oh . I don't know I wrote this a while ago&#10;Speaker: PhD B&#10;Content: Is it like twenty ?&#10;Speaker: Professor A&#10;Content: Something like that .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Yeah . I mean I think there 's some effect above twenty but it 's {disfmarker} it 's {disfmarker} it 's {disfmarker} it 's mild . So , I mean it probably {disfmarker} there 's probably some effect up to a hundred hertz or something but it 's {disfmarker} it 's pretty mild . I don't know in the {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Spectral Effects: The spectral effects refer to the reduction in high frequencies in the indirect energy. This is likely due to the filtering process mentioned in the transcript, where there is some attenuation of frequencies above a certain cut-off point (approximately twenty hertz). This filtering results in reduced high frequency content in the signal.&#10;&#10;2. Temporal Changes: The temporal changes are alterations in the timing characteristics of the signal. These changes can cause waveforms to appear different, even if the underlying signal is similar. For example, variations in energy levels at different time points or the presence of certain transient elements can result in visible differences between waveforms.&#10;&#10;3. Reduced High Frequencies: The reduction in high frequencies (caused by filtering) affects the overall spectral balance of the signal, resulting in a more pronounced lower frequency content. This change is responsible for some of the observed differences between waveforms and can also contribute to variations in speech perception or recognition performance.&#10;&#10;4. Distinguishing Between E and B: The discussion about distinguishing between E and B refers to trying to separate two sources of variability that may affect the data, potentially introducing bias. One possible explanation for this statement is that if one source (E) was recorded with higher energy levels or more variation compared to another source (B), it could lead to a systematic difference in the perceived signal characteristics. This potential bias can be mitigated by removing sources of variability that are unrelated to the linguistic component being analyzed.&#10;&#10;5. Spectral Subtraction: The application of spectral subtraction increases the difference between speech and noise energy levels, improving the overall signal-to-noise ratio. However, it also increases the variance of C-zero (the residual noise portion after spectral subtraction). This trade-off should be carefully considered during data analysis to balance improvements in signal quality with the increased variability due to the processing technique." target=" . And so maybe {disfmarker} maybe that would be more obvious .&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Grad C&#10;Content: Spectral slices ?&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: W w what d what do you mean ?&#10;Speaker: Professor A&#10;Content: Well , I mean um all the recognizers look at frames . So they {disfmarker} they look at {disfmarker}&#10;Speaker: PhD B&#10;Content: So like one instant in time .&#10;Speaker: Professor A&#10;Content: Yeah , look at a {disfmarker}&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: So it 's , yeah , at one point in time or uh twenty {disfmarker} over twenty milliseconds or something , {vocalsound} you have a spectrum or a cepstrum .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor A&#10;Content: That 's what I meant by a slice .&#10;Speaker: Grad C&#10;Content: I see .&#10;Speaker: Professor A">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Spectral Effects: The spectral effects refer to the reduction in high frequencies in the indirect energy. This is likely due to the filtering process mentioned in the transcript, where there is some attenuation of frequencies above a certain cut-off point (approximately twenty hertz). This filtering results in reduced high frequency content in the signal.&#10;&#10;2. Temporal Changes: The temporal changes are alterations in the timing characteristics of the signal. These changes can cause waveforms to appear different, even if the underlying signal is similar. For example, variations in energy levels at different time points or the presence of certain transient elements can result in visible differences between waveforms.&#10;&#10;3. Reduced High Frequencies: The reduction in high frequencies (caused by filtering) affects the overall spectral balance of the signal, resulting in a more pronounced lower frequency content. This change is responsible for some of the observed differences between waveforms and can also contribute to variations in speech perception or recognition performance.&#10;&#10;4. Distinguishing Between E and B: The discussion about distinguishing between E and B refers to trying to separate two sources of variability that may affect the data, potentially introducing bias. One possible explanation for this statement is that if one source (E) was recorded with higher energy levels or more variation compared to another source (B), it could lead to a systematic difference in the perceived signal characteristics. This potential bias can be mitigated by removing sources of variability that are unrelated to the linguistic component being analyzed.&#10;&#10;5. Spectral Subtraction: The application of spectral subtraction increases the difference between speech and noise energy levels, improving the overall signal-to-noise ratio. However, it also increases the variance of C-zero (the residual noise portion after spectral subtraction). This trade-off should be carefully considered during data analysis to balance improvements in signal quality with the increased variability due to the processing technique." target=" I mean , because {vocalsound} everything uh {disfmarker} If you have a system based on Gaussians , everything is based on means and variances .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: So if there 's an overall {vocalsound} reason {disfmarker} You know , it 's like uh if you were doing uh image processing and in some of the pictures you were looking at , uh there was a lot of light uh and {disfmarker} and in some , there was low light ,&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: you know , you would want to adjust for that in order to compare things .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: And the variance is just sort of like the next moment , you know ? So uh {vocalsound} what if um one set of pictures was taken uh so that throughout the course it was {disfmarker} went through daylight and night uh {vocalsound} um um ten times , another time it went thr">
      <data key="d0">1</data>
    </edge>
    <edge source=" I mean , because {vocalsound} everything uh {disfmarker} If you have a system based on Gaussians , everything is based on means and variances .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: So if there 's an overall {vocalsound} reason {disfmarker} You know , it 's like uh if you were doing uh image processing and in some of the pictures you were looking at , uh there was a lot of light uh and {disfmarker} and in some , there was low light ,&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: you know , you would want to adjust for that in order to compare things .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: And the variance is just sort of like the next moment , you know ? So uh {vocalsound} what if um one set of pictures was taken uh so that throughout the course it was {disfmarker} went through daylight and night uh {vocalsound} um um ten times , another time it went thr" target="1. Based on the transcript, there is no explicit statement or confirmation that they have managed to substantially improve the system while also reducing latency. The conversation involves discussions about cutting down the number of iterations, increasing the number of Gaussians, and attempting different methods such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. However, there is no clear indication of success in substantially improving the system while reducing latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcript does not provide a direct answer to this question. However, it includes a comment from PhD F about the Personal Digital Assistant (PDA) being consistently located far away during meetings, with the speakers noting that they have not moved it closer. The reason for this was not discussed in the meeting." target=" B&#10;Content: I don't mean a graph . I mean the actual numbers .&#10;Speaker: Professor A&#10;Content: Oh . I see . Oh . That would be lovely , yeah .&#10;Speaker: PhD B&#10;Content: Yeah . &quot; See how different these {vocalsound} sequences of numbers are ? &quot;&#10;Speaker: Professor A&#10;Content: Yeah . Or I could just add them up and get a different total .&#10;Speaker: PhD B&#10;Content: Yeah . It 's not the square .&#10;Speaker: Professor A&#10;Content: OK . Uh . What else {disfmarker} wh what 's {disfmarker} what else is going on ?&#10;Speaker: PhD F&#10;Content: Uh , yeah . Yeah , at first I had a remark why {disfmarker} I am wondering why the PDA is always so far . I mean we are always meeting at the {vocalsound} beginning of the table and {vocalsound} the PDA 's there .&#10;Speaker: Professor A&#10;Content: Uh . I guess cuz we haven't wanted to move it . We {disfmarker} we could {disfmarker} {voc">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Spectral subtraction increases the difference between the energy of speech and the energy of the noise portion because it is a technique used to reduce background noise in audio signals. By subtracting an estimate of the noise spectrum from the speech plus noise spectrum, the result highlights the speech components that stand out more than they would against the original noisy background. The difference between the energies becomes larger after spectral subtraction compared to before.&#10;   &#10;2. Spectral subtraction also increases the variance of C-zero (the mean normalized channel). When applying spectral subtraction, it is possible to increase the variability of the signal around its mean value due to a less accurate noise estimation or fluctuations in the speech signal itself. As a result, after spectral subtraction, the variance of C-zero will be larger than before." target="The concern about integrating a different type of spectral subtraction stems from the fact that this method can increase the variance of C-zero (the mean normalized channel) after its application. While spectral subtraction does increase the difference between the energy of speech and noise, it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance. Therefore, careful consideration and adjustments are required when integrating a new spectral subtraction technique to maintain optimal speech recognition and VAD performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Spectral subtraction increases the difference between the energy of speech and the energy of the noise portion because it is a technique used to reduce background noise in audio signals. By subtracting an estimate of the noise spectrum from the speech plus noise spectrum, the result highlights the speech components that stand out more than they would against the original noisy background. The difference between the energies becomes larger after spectral subtraction compared to before.&#10;   &#10;2. Spectral subtraction also increases the variance of C-zero (the mean normalized channel). When applying spectral subtraction, it is possible to increase the variability of the signal around its mean value due to a less accurate noise estimation or fluctuations in the speech signal itself. As a result, after spectral subtraction, the variance of C-zero will be larger than before." target="1. The issue encountered when adding spectral subtraction before the Wall process (which contains LDA and online normalization) was that it significantly hurt the performance. This is because spectral subtraction can increase the difference between the energy of speech and noise, but it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance.&#10;&#10;2. The C-zero parameters for a single Italian utterance in two channels cannot be clearly seen in the plot due to several reasons:&#10;   - The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;   - Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels.&#10;   - The plot being a plot of C-zero parameters for one Italian utterance may not provide enough context or clear differentiation between channels and frequencies to easily distinguish specific words or sounds.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the transcript, changing the self-loop transition probability by a tenth of a percent causes a ten percent difference in the word error rate. This was stated by PhD B: &quot;and it causes ten percent difference in the word error rate.&quot;" target=" results when Stephane did that&#10;Speaker: PhD F&#10;Content: Well . Eh uh {disfmarker}&#10;Speaker: PhD B&#10;Content: and it 's {disfmarker} it 's really wo really happens .&#10;Speaker: PhD F&#10;Content: This really happens .&#10;Speaker: PhD B&#10;Content: I mean th the only difference is you change the self - loop transition probability by a tenth of a percent&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: and it causes ten percent difference in the word error rate .&#10;Speaker: Professor A&#10;Content: A tenth of a per cent .&#10;Speaker: PhD B&#10;Content: Yeah . From point {disfmarker}&#10;Speaker: PhD F&#10;Content: Even tenth of a percent ?&#10;Speaker: PhD B&#10;Content: I {disfmarker} I 'm sorry&#10;Speaker: PhD F&#10;Content: Well , we tried {disfmarker} we tried point one ,&#10;Speaker: PhD B&#10;Content: f for point {disfmarker} from {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the transcript, changing the self-loop transition probability by a tenth of a percent causes a ten percent difference in the word error rate. This was stated by PhD B: &quot;and it causes ten percent difference in the word error rate.&quot;" target=" the improvement {vocalsound} uh relative to the {disfmarker} the baseline is small {disfmarker}&#10;Speaker: Professor A&#10;Content: So they do improvement in terms of uh accuracy ? rather than word error rate ?&#10;Speaker: PhD F&#10;Content: Uh . Uh improvement ?&#10;Speaker: Professor A&#10;Content: So {disfmarker}&#10;Speaker: PhD F&#10;Content: No , it 's compared to the word er it 's improvement on the word error rate ,&#10;Speaker: Professor A&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: yeah . Sorry .&#10;Speaker: Professor A&#10;Content: So if you have uh ten percent error and you get five percent absolute uh {vocalsound} improvement then that 's fifty percent .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: OK . So what you 're saying then is that if it 's something that has a small word error rate , {vocalsound} then uh a {disfmarker} even a relatively small improvement on it , in absolute terms , {vocalsound} will show up as quite">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the transcript, changing the self-loop transition probability by a tenth of a percent causes a ten percent difference in the word error rate. This was stated by PhD B: &quot;and it causes ten percent difference in the word error rate.&quot;" target="} then uh a {disfmarker} even a relatively small improvement on it , in absolute terms , {vocalsound} will show up as quite {disfmarker} quite large in this .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Professor A&#10;Content: Is that what you 're saying ?&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: Yes .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Professor A&#10;Content: OK . But yeah that 's {disfmarker} that 's {disfmarker} it 's the notion of relative improvement . Word error rate .&#10;Speaker: PhD F&#10;Content: Yeah . Sure , but when we think about the weighting , which is point five , point three , point two , {vocalsound} it 's on absolute on {disfmarker} on relative figures ,&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: not {disfmarker}&#10;Speaker: Professor A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: So when we">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the transcript, changing the self-loop transition probability by a tenth of a percent causes a ten percent difference in the word error rate. This was stated by PhD B: &quot;and it causes ten percent difference in the word error rate.&quot;" target="&#10;Content: But they 're not making things worse and we have reduced latency , right ?&#10;Speaker: PhD F&#10;Content: Yeah . But actually {disfmarker} um actually it seems to do a little bit worse for the well - matched case and we just noticed that {disfmarker} Yeah , actually the way the final score is computed is quite funny . It 's not a mean of word error rate . It 's not a weighted mean of word error rate , it 's a weighted mean of improvements .&#10;Speaker: Professor A&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: So . Which means that {vocalsound} actually the weight on the well - matched is {disfmarker} Well I well what what {disfmarker} What happened is that if you have a small improvement or a small if on the well - matched case {vocalsound} it will have uh huge influence on the improvement compared to the reference because the reference system is {disfmarker} is {disfmarker} is quite good for {disfmarker} for the well - ma well - matched case also .&#10;Speaker: PhD B&#10;Content: So">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the transcript, changing the self-loop transition probability by a tenth of a percent causes a ten percent difference in the word error rate. This was stated by PhD B: &quot;and it causes ten percent difference in the word error rate.&quot;" target="1. Based on the transcript, there is no explicit statement or confirmation that they have managed to substantially improve the system while also reducing latency. The conversation involves discussions about cutting down the number of iterations, increasing the number of Gaussians, and attempting different methods such as applying mean normalization before Voice Activity Detection (VAD) and working on a low-pass downsampling, upsampling filter to replace the LDA filters. However, there is no clear indication of success in substantially improving the system while reducing latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="The concern about integrating a different type of spectral subtraction stems from the fact that this method can increase the variance of C-zero (the mean normalized channel) after its application. While spectral subtraction does increase the difference between the energy of speech and noise, it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance. Therefore, careful consideration and adjustments are required when integrating a new spectral subtraction technique to maintain optimal speech recognition and VAD performance." target="1. The issue encountered when adding spectral subtraction before the Wall process (which contains LDA and online normalization) was that it significantly hurt the performance. This is because spectral subtraction can increase the difference between the energy of speech and noise, but it also raises the variability of the signal around its mean value due to possible inaccuracies in noise estimation or fluctuations in the speech signal itself. This could potentially affect the emergence of speech compared to before by increasing the background noise, which may negatively impact voice activity detection (VAD) performance.&#10;&#10;2. The C-zero parameters for a single Italian utterance in two channels cannot be clearly seen in the plot due to several reasons:&#10;   - The energy of the word &quot;Due&quot; might not have been distinct enough from the background noise or the spectral subtraction residuals, making it difficult to visualize.&#10;   - Since there was no spectral subtraction or online normalization applied at this stage, the word &quot;Due&quot; might have blended in with other frequencies and energy levels.&#10;   - The plot being a plot of C-zero parameters for one Italian utterance may not provide enough context or clear differentiation between channels and frequencies to easily distinguish specific words or sounds.">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
