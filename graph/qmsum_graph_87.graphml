<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." />
    <node id=" backchanneling , automatic backchanneling system .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: It 's a very {disfmarker}&#10;Speaker: PhD D&#10;Content: Oh !&#10;Speaker: PhD G&#10;Content: So , exactly what you describe ,&#10;Speaker: PhD D&#10;Content: Huh .&#10;Speaker: PhD G&#10;Content: but for Japanese . And it 's apparently {disfmarker} for Japa - in Japanese it 's really important that you backchannel . It 's really impolite if you don't , and {disfmarker} So .&#10;Speaker: Professor F&#10;Content: Huh . Actually for a lot of these people I think you could just sort of backchannel continuously and it would {pause} pretty much be fine .&#10;Speaker: PhD D&#10;Content: It wouldn't matter ? Yeah .&#10;Speaker: Grad E&#10;Content: Yeah . That 's w That 's what I do .&#10;Speaker: PhD D&#10;Content: Random intervals .&#10;Speaker: Grad A&#10;Content: There was {disfmarker} there was of course a Monty Python sketch with" />
    <node id=" gives them the backchannel . And so then you {vocalsound} hook that to the phone and go off&#10;Speaker: Grad A&#10;Content: Yeah . Uh - huh .&#10;Speaker: PhD D&#10;Content: and do the {vocalsound} {disfmarker} do whatever you r wanna do ,&#10;Speaker: PhD G&#10;Content: Oh yeah . Well {disfmarker}&#10;Speaker: PhD D&#10;Content: while that thing keeps them busy .&#10;Speaker: PhD G&#10;Content: There 's actually {disfmarker} uh there 's this a former student of here from Berkeley , Nigel {disfmarker} Nigel Ward .&#10;Speaker: PhD D&#10;Content: Uh - huh . Sure .&#10;Speaker: PhD G&#10;Content: Do you know him ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: He did a system uh , in {disfmarker} he {disfmarker} he lives in Japan now , and he did this backchanneling , automatic backchanneling system .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: It '" />
    <node id=": Right .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: You have a lot of {disfmarker} a lot of two - party , subsets within the meeting .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Postdoc C&#10;Content: Uh - huh .&#10;Speaker: Grad A&#10;Content: Well regardless {disfmarker} it 's an interesting result regardless .&#10;Speaker: PhD G&#10;Content: So {disfmarker} Right .&#10;Speaker: Postdoc C&#10;Content: Yes , that 's right .&#10;Speaker: PhD G&#10;Content: And {disfmarker} and {disfmarker} and then {disfmarker} and we also d computed this both with and without backchannels ,&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: so you might think that backchannels have a special status because they 're essentially just {disfmarker}&#10;Speaker: Grad A&#10;Content: Uh - huh . So , did {disf" />
    <node id=" the beeps would be {vocalsound} uh y y y And I 'm not exactly sure how that {disfmarker} how that would work with the {disfmarker} with the backchannels . And , so um {disfmarker} And certainly things that are {vocalsound} intrusions of multiple words , {vocalsound} taken out of context and displaced in time from where they occurred , {vocalsound} that would be hard . So , m my {vocalsound} thought is {pause} i I 'm having this transcriber go through {vocalsound} the EDU - one meeting , and indicate a start time {nonvocalsound} f for each dominant speaker , endpoi end time for each dominant speaker , and the idea that {vocalsound} these units would be generated for the dominant speakers , {vocalsound} and maybe not for the other channels .&#10;Speaker: Grad A&#10;Content: Yeah the only , um , disadvantage of that is , then it 's hard to use an automatic method to do that . The advantage is that it 's probably faster to do that than it is to use the automated method and correct it . So ." />
    <node id=" {disfmarker} the n the backchannel will {disfmarker} will occur at the end of {disfmarker} of those three .&#10;Speaker: Postdoc C&#10;Content: Yes .&#10;Speaker: PhD B&#10;Content: And {disfmarker} and in {disfmarker} in the {disfmarker} in the previous version where in the n which is used now , {vocalsound} there , the backchannel would {disfmarker} would be in - between there somewhere , so .&#10;Speaker: Postdoc C&#10;Content: I see .&#10;Speaker: PhD B&#10;Content: That would be more natural&#10;Speaker: Postdoc C&#10;Content: Yeah . Well ,&#10;Speaker: PhD B&#10;Content: but {disfmarker}&#10;Speaker: Postdoc C&#10;Content: that 's {disfmarker} that 's right , but you know , thi this brings me to the other f stage of this which I discussed with you earlier today ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: which is {vocalsound} the second stage is {" />
    <node id=": So .&#10;Speaker: PhD B&#10;Content: that 's {disfmarker} that 's a second question , &quot; what {disfmarker} what will different transcribers do with {disfmarker} with the laptop sound ? &quot;&#10;Speaker: Postdoc C&#10;Content: Would you {disfmarker} would {disfmarker}&#10;Speaker: Professor F&#10;Content: What was the l what was the laptop sound ?&#10;Speaker: Postdoc C&#10;Content: Yeah , go ahead .&#10;Speaker: Professor F&#10;Content: I mean was it speech ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: or was it {disfmarker}&#10;Speaker: PhD B&#10;Content: It 's speech .&#10;Speaker: Professor F&#10;Content: Great .&#10;Speaker: Postdoc C&#10;Content: Well , so {disfmarker} I mean {disfmarker} So my standard approach has been if it 's not someone close - miked , then , they don't end up on one of the close - miked channels . They end up on a different channel . And we have" />
    <node id="From the meeting transcript, the findings when the speakers rescored and fixed the transcripts were as follows:&#10;&#10;1. They added one number to their analysis, which involved scoring recognition results while ignoring all insertions due to background speech. This change was based on a conjecture from a previous HLT paper that most of the added recognition error is from insertions due to background speech.&#10;2. By scoring the recognition results in this way, they were able to compare the number of overlapped words and spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome.&#10;3. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;4. An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings." />
    <node id=" words to maybe {vocalsound} uh I don't know , eleven percent or something {disfmarker} it 's {disfmarker} it 's not a dramatic change ,&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: so it 's {disfmarker} Anyway , so it 's uh {disfmarker} That was {disfmarker} that was one set of {pause} results , and then the second one was just basically the {disfmarker} {vocalsound} the stuff we had in the {disfmarker} in the HLT paper on how overlaps effect the {pause} recognition performance .&#10;Speaker: Postdoc C&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Nope . Right .&#10;Speaker: Professor F&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: And we rescored things um , a little bit more carefully . We also fixed the transcripts in {disfmarker} in numerous ways . Uh , but mostly we added one {disfmarker} one number , which was what if you {pause" />
    <node id="disfmarker} in numerous ways . Uh , but mostly we added one {disfmarker} one number , which was what if you {pause} uh , basically score ignoring all {disfmarker} So {disfmarker} so the {disfmarker} the conjecture from the HLT results was that {vocalsound} most of the added recognition error is from insertions {vocalsound} due to background speech . So , we scored {vocalsound} all the recognition results , {vocalsound} uh , in such a way that the uh {disfmarker}&#10;Speaker: Grad A&#10;Content: Oh by the way , who 's on channel four ? You 're getting a lot of breath .&#10;Speaker: PhD B&#10;Content: Yeah . I j was just wondering .&#10;Speaker: Grad E&#10;Content: That 's {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: That 's me .&#10;Speaker: PhD G&#10;Content: uh , well Don 's been working hard .&#10;Speaker: Grad E&#10;Content: That 's right .&#10;Speaker: PhD" />
    <node id=" . Yeah , that 's right .&#10;Speaker: PhD G&#10;Content: I think several people {disfmarker} sent this ,&#10;Speaker: Grad A&#10;Content: so .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Uh - huh .&#10;Speaker: PhD G&#10;Content: yeah .&#10;Speaker: Grad A&#10;Content: But any {disfmarker} any help you need I can certainly provide .&#10;Speaker: Professor F&#10;Content: Well ,&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: that 's {disfmarker} that 's a great idea .&#10;Speaker: PhD G&#10;Content: Well {disfmarker} there {disfmarker} there were some interesting results in this paper , though . For instance that Morgan {disfmarker} uh , accounted for fifty - six percent of the Robustness meetings in terms of number of words .&#10;Speaker: Grad A&#10;Content: Wow .&#10;Speaker: Postdoc C&#10;Content: In {disfmarker} in terms of what ? In term&#10;Speaker: PhD G" />
    <node id=" locations where , uh , if you have overlapping speech and someone else starts a sentence , you know , where do these {disfmarker} where do other people start their {vocalsound} turns {disfmarker} not turns really , but you know , sentences ,&#10;Speaker: PhD B&#10;Content: Ah .&#10;Speaker: PhD G&#10;Content: um {disfmarker} So we only looked at cases where there was a foreground speaker and then at the to at the {disfmarker} so the {disfmarker} the foreground speaker started into their sentence and then someone else started later .&#10;Speaker: PhD B&#10;Content: Somewhere in between the start and the end ?&#10;Speaker: PhD G&#10;Content: OK ? And so what {disfmarker}&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: Sorry ?&#10;Speaker: PhD B&#10;Content: Somewhere in between the start and the end of the foreground ?&#10;Speaker: PhD G&#10;Content: Yes . Uh , so that such that there was overlap between the two sentences .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content" />
    <node id="aker: PhD G&#10;Content: right ? So we took the old recognition output and we just scored it differently . So the recognizer didn't have the benefit of knowing where the foreground speech {disfmarker} a start&#10;Speaker: Professor F&#10;Content: Were you including the {disfmarker} the lapel {pause} in this ?&#10;Speaker: PhD G&#10;Content: Yes .&#10;Speaker: Professor F&#10;Content: And did the {disfmarker} did {disfmarker} did the la did the {disfmarker} the problems with the lapel go away also ? Or {disfmarker}&#10;Speaker: PhD G&#10;Content: Um , it {disfmarker} Yeah .&#10;Speaker: Professor F&#10;Content: fray for {disfmarker} for insertions ?&#10;Speaker: PhD G&#10;Content: It u not per {disfmarker} I mean , not completely , but yes ,&#10;Speaker: Professor F&#10;Content: Less so .&#10;Speaker: PhD G&#10;Content: dramatically . So we have to um {disfmarker}&#10;Speaker: Professor F&#10;Content: I mean , you still {" />
    <node id=": PhD G&#10;Content: um , s in terms of {disfmarker} in terms of number of words and also we computed something called a &quot; spurt &quot; , which is essentially a stretch of speech with uh , no pauses exceeding five hundred milliseconds . Um , and we computed how many overlapped i uh spurts there were and how many overlapped words there were . {vocalsound} Um , for four different {pause} corpora , the Meeting Recorder meetings , the Robustness meetings Switchboard and CallHome , and , found {disfmarker} and sort of compared the numbers . Um , and found that the , uh , you know , as you might expect the Meeting Recorder {pause} meetings had the most overlap uh , but next were Switchboard and CallHome , which both had roughly the same , almost identical in fact , and the Robustness meetings were {disfmarker} had the least , so {disfmarker} One sort of unexpected result there is that uh two - party telephone conversations have {vocalsound} about the same amount of overlap ,&#10;Speaker: Grad A&#10;Content: I 'm surprised .&#10;Speaker: PhD G&#10;Content: sort of in gen you" />
    <node id="The conclusion from the HLT (Human Language Technology) results is that most of the added recognition error is caused by insertions due to background speech. To test this conjecture, the speakers rescored the recognition results in such a way that they ignored all insertions. This means that any words or spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) caused by background noise were not taken into account when scoring the recognition results. &#10;&#10;This new scoring method allowed them to compare the number of overlapped words and spurts between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;&#10;An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings. This suggests that background speech, rather than the number of parties or the setting, is a major source of recognition errors." />
    <node id="isfmarker} we {disfmarker} {vocalsound} Uh , Don did some ha hand - checking and {disfmarker} and we think that {disfmarker} based on that , we think that the results are you know , valid , although of course , some error is gonna be in there . But basically what we found is after we take out these regions {disfmarker} so we only score the regions that were certified as foreground speech , {comment} {vocalsound} the recognition error went down to almost {vocalsound} uh , the {pause} level of the non - overlapped {pause} speech . So that means that {vocalsound} even if you do have background speech , if you can somehow separate out or find where it is , {vocalsound} uh , the recognizer does a good job ,&#10;Speaker: Grad A&#10;Content: That 's great .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: even though there is this back&#10;Speaker: Grad A&#10;Content: Yeah , I guess that doesn't surprise me , because , with the close - talking mikes , the {disfmark" />
    <node id="aker: Postdoc C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: I mean I get these AARP things , but I 'm not se really senior yet , but {disfmarker}&#10;Speaker: PhD G&#10;Content: Right&#10;Speaker: Professor F&#10;Content: Um ,&#10;Speaker: PhD G&#10;Content: Hmm .&#10;Speaker: Professor F&#10;Content: but uh , other than that delightful result , what was the rest of the paper about ?&#10;Speaker: PhD G&#10;Content: Um , well it was about {disfmarker} it had three sections&#10;Speaker: Professor F&#10;Content: You sent it to me but I haven't seen it yet .&#10;Speaker: PhD G&#10;Content: uh {disfmarker} three kinds of uh results , if you will . Uh , the one was that the {disfmarker} just the {disfmarker} the amount of overlap&#10;Speaker: Grad A&#10;Content: The good , the bad , and the ugly .&#10;Speaker: PhD G&#10;Content: um , s in terms of {disfmarker} in terms of number of words and also we computed something called a &quot;" />
    <node id="The 'ASR 0.1' directory, as mentioned in the meeting, is meant for speech recognition purposes. The name 'ASR 0.1' stands for &quot;Automatic Speech Recognition&quot; and signifies its version or iteration (0.1). This directory utilizes a quick and dirty form of access control; to access any files within it, one must be told the exact name of the file or subdirectory since it is not directly visible or readable.&#10;&#10;The purpose of this structure is to ensure that only individuals with explicit permission can access the files within the directory. By using a non-standard approach to access control, the creators are able to restrict access and maintain better control over who can view or edit the contents. Additionally, implementing such a system can be beneficial for privacy and security purposes, especially when dealing with sensitive data or information.&#10;&#10;The 'ASR 0.1' directory likely contains various files related to speech recognition research, experiments, or projects that the team is working on. It could include audio files, transcription scripts, software tools, and other resources necessary for ASR development. The access control mechanism discussed in the meeting ensures that only authorized individuals can view and interact with these materials, making collaboration more secure and organized." />
    <node id=" . Um ,&#10;Speaker: Professor F&#10;Content: Yeah&#10;Speaker: PhD G&#10;Content: and then under there {disfmarker} Um actually {disfmarker} Oh and this directory , {vocalsound} is not readable . It 's only uh , accessible . So , {vocalsound} in other words , to access anything under there , you have to {vocalsound} be told what the name is .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: So that 's sort of a g {vocalsound} quick and dirty way of doing access control .&#10;Speaker: Professor F&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: So {disfmarker} uh , and the directory for this I call it I &quot; ASR zero point one &quot; because it 's sort of meant for recognition .&#10;Speaker: Professor F&#10;Content: So anyone who hears this meeting now knows the {disfmarker}&#10;Speaker: Grad A&#10;Content: Beta ?&#10;Speaker: PhD G&#10;Content: And then {disfmarker} then in there I have a file that lists" />
    <node id="comment} {pause} or um , we should just make a note of it .&#10;Speaker: PhD G&#10;Content: OK . Oh . Beca - Well {disfmarker} OK , because in one directory there 's two versions .&#10;Speaker: Grad E&#10;Content: Yeah , that 's the first meeting I cut both versions . Just to check which w if there is a significant difference .&#10;Speaker: PhD G&#10;Content: OK . And so I {disfmarker} but {disfmarker} OK so {disfmarker} but for the other meetings it 's the downsampled version that you have .&#10;Speaker: Grad E&#10;Content: They 're all downsampled , yeah .&#10;Speaker: PhD G&#10;Content: Oh , OK . Oh that 's th important to know , OK so we should probably {disfmarker} uh {pause} give them the non - downsampled versions .&#10;Speaker: Grad E&#10;Content: Yeah . So {disfmarker}&#10;Speaker: PhD G&#10;Content: OK . Alright , then I 'll hold off on that and I 'll wait for you um {disfmarker" />
    <node id="disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah but {disfmarker} but {disfmarker} but he has to {disfmarker}&#10;Speaker: Postdoc C&#10;Content: I 'd hafta add him to Meeting Recorder , I guess ,&#10;Speaker: PhD G&#10;Content: he prefe he said he would prefer FTP&#10;Speaker: Postdoc C&#10;Content: but {disfmarker} OK .&#10;Speaker: PhD G&#10;Content: and also , um , the other person that wants it {disfmarker} There is one person at SRI who wants to look at the {vocalsound} um , you know , the uh {disfmarker} the data we have so far ,&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: and so I figured that FTP is the best {pause} approach . So what I did is I um {disfmarker} {vocalsound} {vocalsound} @ @ {comment} I made a n new directory after Chuck said that would c that was gonna be a good thing . Uh , so it 's &quot;" />
    <node id=" about the IBM transcription process stuff that {disfmarker}&#10;Speaker: Postdoc C&#10;Content: OK . So , um you know that Adam created um , a b a script to generate the beep file ?&#10;Speaker: Professor F&#10;Content: Hmm .&#10;Speaker: Postdoc C&#10;Content: To then create something to send to IBM . And , um , you {disfmarker} you should probably talk about that . But {disfmarker} but you were gonna to use the {pause} originally transcribed file because I tightened the time bins and that 's also the one that they had already {vocalsound} in trying to debug the first stage of this . And uh , my understanding was that , um {disfmarker} I haven't {disfmarker} {vocalsound} I haven't listened to it yet ,&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: Postdoc C&#10;Content: but it sounded very good and {disfmarker} and I understand that you guys {vocalsound} were going to have a meeting today , before this meeting .&#10;Speaker: Grad A&#10;Content: It was just" />
    <node id="disfmarker} we should {vocalsound} leave the {vocalsound} part with the audio in the uh , beep file that we send to IBM for that one , or should we {vocalsound} start after the {disfmarker} that part of the meeting is over in what we send .&#10;Speaker: Professor F&#10;Content: Which part ?&#10;Speaker: PhD B&#10;Content: With {disfmarker}&#10;Speaker: Postdoc C&#10;Content: So , the part where they 're using sounds from their {disfmarker} from their laptops .&#10;Speaker: PhD B&#10;Content: with the laptop sound , or {disfmarker} ? just {disfmarker}&#10;Speaker: Postdoc C&#10;Content: w If we have speech from the laptop should we just uh , excise that from what we send to IBM , or should we {vocalsound} i give it to them and let them do with it what they can ?&#10;Speaker: PhD D&#10;Content: I think we should just {disfmarker} it {disfmarker} it 's gonna be too much work if we hafta {vocalsound" />
    <node id="1. Number of speakers: The speech-nonspeech detection system performs better in meetings with fewer speakers. This is because there are fewer overlapping voices and background noises to confuse the system.&#10;2. Background noise: The system can be affected by background speech, as demonstrated by the fact that Meeting Recorder meetings had more overlap than Robustness meetings, even though both were meeting recordings. Additionally, telephone conversations (Switchboard and CallHome) had about the same amount of overlap as Meeting Recorder meetings, suggesting that background speech is a significant source of recognition errors.&#10;3. Channel quality: The system's performance can be impacted by the quality of the channels being used. If the laptop is in the background or there is breathy channel noise, it can cause false positives for speech detection. This can result in more missed speech, as demonstrated by PhD B's statement that a good meeting has around 1% or less of speech missed by the detector.&#10;4. Speech-nonspeech detector limitations: The speech-nonspeech detector may assign speech randomly to one of the channels when it is unable to accurately detect the source of the speech, which can lead to recognition errors.&#10;5. Adjustments for system performance: To improve the performance of the system, meeting segments with high background noise or multiple speakers might need to be excluded from the analysis. Additionally, efforts could be made to reduce background noise and ensure clearer audio channels during meetings." />
    <node id=" Postdoc C&#10;Content: Now , you were saying that they {disfmarker} they differ in how well they work depending on channel s sys systems and stuff .&#10;Speaker: PhD B&#10;Content: Yeah . So we should perhaps just select meetings on which the speech - nonspeech detection works well ,&#10;Speaker: Postdoc C&#10;Content: But EDU is great .&#10;Speaker: PhD B&#10;Content: and just use , {vocalsound} those meetings to {disfmarker} to {disfmarker} to send to IBM and , do the other ones .&#10;Speaker: Grad A&#10;Content: Release to begin with .&#10;Speaker: Postdoc C&#10;Content: How interesting . You know {disfmarker}&#10;Speaker: Professor F&#10;Content: What 's the problem {disfmarker} the l I forget . Is the problem the lapel , or {disfmarker} or {disfmarker}&#10;Speaker: PhD B&#10;Content: Uh , it really depends . Um , my {disfmarker} my {disfmarker} my impression is that it 's better for meetings with fewer speakers , and it 's" />
    <node id=" {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Hmm !&#10;Speaker: PhD B&#10;Content: Let me think . So the {pause} speech {disfmarker} the amount of speech that is missed by the {pause} detector , for a good meeting , I th is around {pause} or under one percent , I would say . But there can be {disfmarker} Yeah . For {disfmarker} yeah , but there can be more {disfmarker} There 's {disfmarker} There 's more amount speech {disfmarker} uh , more amount of {disfmarker} Yeah well , the detector says there is speech , but there is none . So that {disfmarker} that can be a lot when {disfmarker} when it 's really a breathy channel .&#10;Speaker: Professor F&#10;Content: But I think that 's less of a problem .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: They 'll just listen . It 's just wasted time .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor" />
    <node id=" is playing sound from his laptop .&#10;Speaker: Grad A&#10;Content: Uh - huh&#10;Speaker: PhD B&#10;Content: And i {vocalsound} the speech - nonspeech detector just assigns randomly the speech to {disfmarker} to one of the channels , so . Uh - I haven't - I didn't think of {disfmarker} of s of {vocalsound} this before ,&#10;Speaker: Grad A&#10;Content: What can you do ?&#10;Speaker: PhD B&#10;Content: but what {disfmarker} what shall we do about s things like this ?&#10;Speaker: Postdoc C&#10;Content: Well you were suggesting {disfmarker} You suggested maybe just not sending that part of the meeting .&#10;Speaker: Grad A&#10;Content: Yep . Mmm .&#10;Speaker: Postdoc C&#10;Content: But {disfmarker}&#10;Speaker: PhD B&#10;Content: But , sometimes the {disfmarker} {vocalsound} the {disfmarker} the laptop is in the background and some {disfmarker} somebody is {disfmarker} is talking , and , {vocalsound" />
    <node id="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation." />
    <node id="&#10;Speaker: Professor F&#10;Content: Whereas if they listen {nonvocalsound} to it and there 's {disfmarker} don't hear any speech I think they 'd probably just listen to it once .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: So there 'd {disfmarker} you 'd think there 'd be a {disfmarker} a factor of three or four in {disfmarker} in , uh , cost function ,&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: you know , between them or something .&#10;Speaker: PhD B&#10;Content: Yeah , so {disfmarker} but I think that 's {disfmarker} n that really doesn't happen very often that {disfmarker} that {disfmarker} that a word is cut in the middle or something . That 's {disfmarker} that 's really not {disfmarker} not normal .&#10;Speaker: Professor F&#10;Content: So {disfmarker} so what you 're saying is that nearly always what happens" />
    <node id=" G&#10;Content: dramatically . So we have to um {disfmarker}&#10;Speaker: Professor F&#10;Content: I mean , you still {disfmarker}&#10;Speaker: PhD G&#10;Content: Well I should bring the {disfmarker} should bring the table with results . Maybe we can look at it {pause} Monday .&#10;Speaker: Professor F&#10;Content: I would presume that you still would have somewhat higher error with the lapel for insertions than {disfmarker}&#10;Speaker: PhD G&#10;Content: Yes . It 's {disfmarker} It 's {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Yes . Yeah .&#10;Speaker: Professor F&#10;Content: Cuz again , looking forward to the non - close miked case , I think that we s still {disfmarker}&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: I 'm not looking forward to it .&#10;Speaker: Professor F&#10;Content: i it 's the high signal - to - noise ratio&#10;Speaker:" />
    <node id=" Uh , an order of magnitude notion of {disfmarker} of how {disfmarker} on a good meeting , how often uh , do you get segments that come in the middle of words and so forth , and uh {disfmarker} in a bad meeting how {vocalsound} often ?&#10;Speaker: PhD B&#10;Content: Uh .&#10;Speaker: Postdoc C&#10;Content: Was is it in a {disfmarker} in a {disfmarker} what {disfmarker} what is the t&#10;Speaker: Professor F&#10;Content: Well he 's saying , you know , that the {disfmarker} the EDU meeting was a good {disfmarker} good meeting ,&#10;Speaker: Postdoc C&#10;Content: In a good meeting , what ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: right ?&#10;Speaker: Postdoc C&#10;Content: Oh I see ,&#10;Speaker: Professor F&#10;Content: Uh , and so {disfmarker} so {disfmarker} so it was almost {d" />
    <node id="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." />
    <node id="Content: Yeah .&#10;Speaker: PhD G&#10;Content: so I d shrunk it to eight millimeters and that helped some . And stuff like that .&#10;Speaker: PhD D&#10;Content: Wasn't there {disfmarker} wasn't there some result , Andreas {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah {disfmarker}&#10;Speaker: PhD D&#10;Content: I {disfmarker} I thought maybe Liz presented this at some conference a while ago about {vocalsound} uh , backchannels&#10;Speaker: PhD G&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD D&#10;Content: uh , and that they tend to happen when uh {pause} the pitch drops . You know you get a falling pitch . And so that 's when people tend to backchannel .&#10;Speaker: PhD G&#10;Content: Yeah . Well {disfmarker}&#10;Speaker: PhD D&#10;Content: Uh - i i do you rem&#10;Speaker: PhD G&#10;Content: y We didn't talk about , uh , prosodic , uh , properties at all ,&#10;Speaker: PhD D&#10;Content:" />
    <node id="&#10;Content: So am I recalling correctly ?&#10;Speaker: PhD G&#10;Content: Anyway , so .&#10;Speaker: Postdoc C&#10;Content: Well , I didn't know about Liz 's finding on that ,&#10;Speaker: PhD D&#10;Content: About {disfmarker}&#10;Speaker: Postdoc C&#10;Content: but I know of another paper that talks about something&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Postdoc C&#10;Content: that {disfmarker}&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: I 'd like to see that reference too .&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: It made me think about a cool little device that could be built to uh {disfmarker} to handle those people that call you on the phone and just like to talk and talk and talk . And you just have this little detector that listens for these {vocalsound} drops in pitch and gives them the backchannel . And so then you {vocalsound} hook that to the phone and go off&#10;Speaker: Grad A&#10;Content:" />
    <node id="} y you know , your {disfmarker} your statement about how much overlap there is becomes less , {vocalsound} um , precise ,&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: because you include more of actual pause time into what you consider overlap speech . Um , so , it 's sort of a compromise ,&#10;Speaker: PhD B&#10;Content: Yeah . {comment} {vocalsound} {vocalsound} Yeah , I also used I think something around zero point five seconds for the speech - nonspeech detector {disfmarker}&#10;Speaker: PhD G&#10;Content: and {disfmarker} {vocalsound} it 's also based {disfmarker} I mean Liz suggested that value based on {vocalsound} the distribution of pause times that you see in Switchboard and {disfmarker} and other corpora .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: Um {disfmarker} So {disfmarker}&#10;Speaker: PhD B&#10;Content: for the minimum silence length" />
    <node id=" so , the end of a filled pause and the end of a discourse marker . And we just eyeballed {disfmarker} I mean {vocalsound} we didn't really hand - tag all of these things . We just {pause} looked at the distribution of words , and so every {vocalsound} &quot; so yeah &quot; , and &quot; OK &quot; , uh , and &quot; uh - huh &quot; were {disfmarker} were the {disfmarker} were deemed to be backchannels and {vocalsound} &quot; wow &quot; and &quot; so &quot; and {vocalsound} uh &quot; right &quot; , uh were um {disfmarker} {pause} Not &quot; right &quot; . &quot; Right &quot; is a backchannel . But so , we sort of {disfmarker} just based on the lexical {disfmarker} {vocalsound} um , identity of the words , we {disfmarker} we tagged them as one of these things . And of course the d the interruption points we got from the original transcripts . So , and then we looked at the disti so we looked at the {pause} distribution of these different kinds of tags , overall uh , and {disfmark" />
    <node id="Based on the transcript provided, the proposed solution for increasing the speed of the process being discussed, while still allowing for adjustments and thorough listening, seems to be using an automatic backchanneling system. This system, developed by a former student named Nigel Ward, could help indicate active listening in conversations without requiring as much time as manual backchanneling (e.g., saying &quot;mm-hmm&quot; or &quot;uh-huh&quot;). The professors are interested in exploring how this automatic system could be used to improve communication during their meetings.&#10;&#10;Additionally, Postdoc C mentions that &quot;listening does take time too,&quot; implying that they acknowledge the importance of dedicating sufficient time to listening during conversations. However, no specific solutions for adjusting or optimizing the timing of the process are explicitly mentioned in the transcript.&#10;&#10;Grad A suggests a specific value of &quot;one and a half times real time&quot; in relation to the discussion, but it is not clear whether this number is directly related to the proposed solution for increasing speed while maintaining quality listening and adjustments. It would be beneficial to have more context or information on where this value comes from and how it applies to the process being discussed." />
    <node id="Speaker: PhD B&#10;Content: Nope .&#10;Speaker: Professor F&#10;Content: I {disfmarker} I {disfmarker} I don't know .&#10;Speaker: PhD D&#10;Content: But it would be quick .&#10;Speaker: Professor F&#10;Content: It would be {disfmarker} kind of quick but they 're still listening to everything .&#10;Speaker: PhD D&#10;Content: But there 's no adjusting . And that 's what 's slow . There 's no adjusting of time boundaries .&#10;Speaker: Postdoc C&#10;Content: Well , {vocalsound} eh , listening does take time too .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Yeah . I don't know , I {disfmarker} I think I 'm {disfmarker} I 'm really tending towards {disfmarker}&#10;Speaker: Grad A&#10;Content: One and a half times real time .&#10;Speaker: Professor F&#10;Content: I mean , {vocalsound} what 's the worst that happens ? Do the transcribers {disfmarker} I mean as long" />
    <node id="The best approach for sharing the data with another person at SRI, as determined by PhD G, is using FTP. PhD G implemented this by creating a new directory for sharing the data, which he had previously been advised would be a good idea. The transcript suggests that PhD G has taken steps to make the necessary preparations for sharing the data via FTP, but it does not provide explicit information on whether the data has actually been shared and accessed by the person at SRI." />
    <node id=" PhD G&#10;Content: y We didn't talk about , uh , prosodic , uh , properties at all ,&#10;Speaker: PhD D&#10;Content: Right . Right . But {disfmarker}&#10;Speaker: PhD G&#10;Content: although that 's {disfmarker} I {disfmarker} I take it that 's something that uh Don will {disfmarker} will look at&#10;Speaker: Grad E&#10;Content: Yeah , we 're gonna be looking at that .&#10;Speaker: PhD G&#10;Content: now that we have the data and we have the alignment , so . This is purely based on you know the words&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: and {disfmarker}&#10;Speaker: Postdoc C&#10;Content: I have a reference for that though . Uh - huh .&#10;Speaker: PhD D&#10;Content: Oh you do .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: So am I recalling correctly ?&#10;Speaker: PhD G&#10;Content: Anyway , so .&#10;Speaker: Postdoc C&#10;Content:" />
    <node id=" . Yeah . Yeah .&#10;Speaker: PhD G&#10;Content: and {vocalsound} so {disfmarker} they 're so good that {vocalsound} generally , u the overlapped speech does not {disfmarker} is less than five percent .&#10;Speaker: Postdoc C&#10;Content: Oh , that 's interesting . Yeah .&#10;Speaker: PhD G&#10;Content: So , this is way more than five percent .&#10;Speaker: Grad E&#10;Content: Did he mean face {disfmarker} like face - to - face ? Or {disfmarker} ?&#10;Speaker: PhD G&#10;Content: Well , in real conversations ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: PhD G&#10;Content: everyday conversations .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: It 's s what these conversation analysts have been studying for years and years there .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: But {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Well , of" />
    <node id="1. The paper written by PhD G has three types of results.&#10;2. One result is a measurement of overlap, which was calculated in terms of the number of words.&#10;3. Another result mentioned involves a computation, but further details about this are not provided in the transcript.&#10;4. The paper hasn't been reviewed yet by Professor F." />
    <node id="&#10;Speaker: Professor F&#10;Content: Uh , and you just sent off a Eurospeech paper , so .&#10;Speaker: PhD G&#10;Content: Right . I hope they accept it .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: I mean , I {disfmarker} both actu as {disfmarker} as a submission and {disfmarker} {vocalsound} you know , as a paper . Um {disfmarker} but {disfmarker}&#10;Speaker: Grad A&#10;Content: Well yeah , you sent it in {pause} late .&#10;Speaker: Professor F&#10;Content: Yeah , I guess you {disfmarker} first you have to do the first one ,&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: and then {disfmarker} Yeah .&#10;Speaker: PhD G&#10;Content: We actually exceeded the delayed deadline by o another day , so .&#10;Speaker: PhD B&#10;Content: Oops .&#10;Speaker: Professor F&#10;Content: Oh they {disfmarker} they had some extension that they announced or something ?&#10;" />
    <node id=" sure .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: And otherwise they should mark it so that we can check .&#10;Speaker: PhD B&#10;Content: Mark it . Sure . Yeah . Yeah .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Postdoc C&#10;Content: Well , we have the unintelligibility {pause} convention .&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: Postdoc C&#10;Content: And actually they have one also ,&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Postdoc C&#10;Content: which {disfmarker}&#10;Speaker: Professor F&#10;Content: i Can I maybe have {disfmarker} have an order of {disfmarker} it 's probably in your paper that I haven't looked at lately , but {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Certainty .&#10;Speaker: Professor F&#10;Content: Uh , an order of magnitude notion of {disfmarker} of how {disfmarker} on a good meeting , how often uh , do" />
    <node id=" Wow .&#10;Speaker: Postdoc C&#10;Content: In {disfmarker} in terms of what ? In term&#10;Speaker: PhD G&#10;Content: Number of words .&#10;Speaker: Postdoc C&#10;Content: One ? Wow ! OK .&#10;Speaker: Grad A&#10;Content: That 's just cuz he talks really fast .&#10;Speaker: Postdoc C&#10;Content: Do you mean ,&#10;Speaker: Professor F&#10;Content: n No .&#10;Speaker: Grad A&#10;Content: I know&#10;Speaker: PhD B&#10;Content: Oh . Short words .&#10;Speaker: Postdoc C&#10;Content: because {disfmarker} is it partly , eh , c correctly identified words ? Or is it {disfmarker} or just overall volume ?&#10;Speaker: PhD G&#10;Content: No . Well , according to the transcripts .&#10;Speaker: Grad A&#10;Content: But re well regardless . I think it 's {disfmarker} he 's {disfmarker} he 's in all of them ,&#10;Speaker: Postdoc C&#10;Content: Oh . OK .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker" />
    <node id="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process." />
    <node id=" all the time boundaries&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Yeah ?&#10;Speaker: PhD D&#10;Content: and we send it to IBM . The other one is {vocalsound} we just run his thing and send it to IBM .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: There 's a {disfmarker} a another possibility if we find that there are some problems ,&#10;Speaker: PhD B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD D&#10;Content: and that is {vocalsound} if we go ahead and we {vocalsound} just run his , and we generate the beeps file , then we have somebody listen beeps file .&#10;Speaker: PhD B&#10;Content: Yeah . And erase {disfmarker}&#10;Speaker: PhD D&#10;Content: And they listen to each section and say &quot; yes , no &quot; whether that section is&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Is intelligible .&#10;Speaker: PhD D&#10;Content: i i intelligible or not . And it just" />
    <node id="} the laptop is in the background and some {disfmarker} somebody is {disfmarker} is talking , and , {vocalsound} that 's really a little bit confusing , but {disfmarker}&#10;Speaker: Grad A&#10;Content: It 's a little bit confusing .&#10;Speaker: Professor F&#10;Content: That 's life .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: I mean , {comment} what 're we gonna do ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: Even a hand - transcription would {disfmarker}&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Postdoc C&#10;Content: Do you {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: a hand - transcriber would have trouble with that .&#10;Speaker: PhD B&#10;Content: Yeah ,&#10;Speaker: Grad A&#10;Content: So .&#10;Speaker: PhD B&#10;Content: that 's {disfmarker} that 's a second question , &quot; what {d" />
    <node id="1. Discussion about an automatic backchanneling system: The participants are discussing an automatic backchanneling system developed by a former student named Nigel Ward. They express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even during their current meeting. This system may be particularly useful for Japanese conversations, as backchanneling is crucial and not doing so can be impolite.&#10;&#10;2. Continuously backchanneling at random intervals: The participants mention that backchanneling tends to happen when there is a falling pitch in conversations. They suggest that continuously backchanneling at random intervals could potentially be acceptable in an automatic system, as it would indicate active listening without requiring manual input and might &quot;pretty much be fine&quot; for a lot of people. However, they also note the importance of cultural context, such as Japanese conversations where not backchanneling can be impolite." />
    <node id="1. The reason they cannot include the beep files with the initial submission is that there is a part of the beep file generation process which requires knowledge of the normalization coefficients. These coefficients are necessary for the correct production of the beep files, but they are not yet available at this stage.&#10;2. Normalization coefficients are values used to adjust the volume or amplitude of audio signals during processing. They are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. In the context of generating beep files, these coefficients ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;3. As they are still in the early stages of their project, the normalization coefficients might not yet be determined. Therefore, they cannot generate the beep files and include them in the initial submission to IBM for transcription. They will need to complete this step at a later stage when the necessary information is available." />
    <node id=" I {disfmarker} I think , we won't know until we generate a bunch of beep files automatically , listen to them and see how bad they are .&#10;Speaker: PhD B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: We won't be able to s include it with this first thing ,&#10;Speaker: Grad A&#10;Content: If {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Hmm . Oh , OK .&#10;Speaker: PhD D&#10;Content: because there 's a part of the process of the beep file which requires knowing the normalization coefficients .&#10;Speaker: Postdoc C&#10;Content: Oh , I see .&#10;Speaker: PhD D&#10;Content: And {disfmarker} {vocalsound} So a&#10;Speaker: Grad A&#10;Content: That 's not hard to do . Just {disfmarker} it takes {disfmarker} you know , it just takes five minutes rather than , taking a second .&#10;Speaker: PhD D&#10;Content: OK" />
    <node id=" , that if two , chunks are very close to each other on the same channel we 'll just merge them .&#10;Speaker: Postdoc C&#10;Content: Oh ! OK . Ah , interesting . Yeah . Yeah . Oh , sure . Yeah , sure . Makes sense .&#10;Speaker: Grad A&#10;Content: So , uh , and that will get around the problem of , the , {vocalsound} you know &quot; one word beep , one word beep , one word beep , one word beep &quot; .&#10;Speaker: Postdoc C&#10;Content: Yeah . Ah ! Clever . Yes . Clever . Yeah . Excellent .&#10;Speaker: PhD D&#10;Content: Yeah , in fact after our meeting uh , this morning Thilo came in and said that {vocalsound} um , there could be {pause} other differences between {vocalsound} the uh {pause} already transcribed meeting with the beeps in it and one that has {pause} just r been run through his process .&#10;Speaker: Postdoc C&#10;Content: And that 's the purpose . Yeah .&#10;Speaker: PhD D&#10;Content: So tomorrow , {vocalsound} when we go to make the um {pause" />
    <node id="Content: So if it 's only inserting &quot; mm - hmm &quot;s here and there , then , wouldn't that be something that would be just as efficient to do at this end , instead of having it go through I B M , then be patched together , then be double checked here .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Right .&#10;Speaker: PhD B&#10;Content: Yeah . But {disfmarker} But then we could just use the {disfmarker} the output of the detector , and do the beeping on it , and send it to I B&#10;Speaker: PhD D&#10;Content: Without having her check anything .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: Postdoc C&#10;Content: Well , I guess {disfmarker}&#10;Speaker: Grad A&#10;Content: I think we just {disfmarker} we just have to listen to it and see how good they are .&#10;Speaker: PhD B&#10;Content: For some meetings , I 'm {disfmarker} I 'm sure it {disfmarker} i n&#10;Speaker" />
    <node id="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content." />
    <node id="1. The plan is to compare the two processes of creating a chunked file for IBM transcription services and applying the beep-ify method (adding beeps at regular intervals) on both versions. They will listen to the output of both methods and look for major differences between them.&#10;   &#10;2. When doing the process by hand, they will pay attention to factors such as consistent volume levels and beep duration throughout the recording. This is where normalization coefficients come into play, as these values are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. They ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;&#10;3. One major difference they might look for is whether there's any confusion caused by including a section where some participants were playing sounds from their laptops during the meeting. This could potentially cause issues with differentiating between actual speech and background noise, as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels.&#10;&#10;4. Ultimately, they have decided to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also plan on having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;&#10;5. This discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work." />
    <node id="s the purpose . Yeah .&#10;Speaker: PhD D&#10;Content: So tomorrow , {vocalsound} when we go to make the um {pause} uh , chunked file {vocalsound} for IBM , we 're going to actually compare the two . So he 's gonna run his process on that same meeting ,&#10;Speaker: Postdoc C&#10;Content: Great idea !&#10;Speaker: PhD D&#10;Content: and then we 're gonna do the beep - ify on both , and listen to them and see if we notice any real differences .&#10;Speaker: PhD G&#10;Content: Beep - ify !&#10;Speaker: Postdoc C&#10;Content: OK , now one thing that prevented us from apply you {disfmarker} you from applying {disfmarker} Exactly . The training {disfmarker} So that is the training meeting . OK .&#10;Speaker: PhD D&#10;Content: Yeah , w and we know that . Wel - uh we just wanna if {disfmarker} if there 're any major differences between {vocalsound} doing it on the hand&#10;Speaker: Postdoc C&#10;Content: Uh - huh . Oh , interesting ." />
    <node id="} Cuz I {disfmarker} I don't think the staggered mixed channel is awfully good as a way of handling overlaps .&#10;Speaker: Professor F&#10;Content: Yeah . Uh - huh .&#10;Speaker: Postdoc C&#10;Content: But {disfmarker} but uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Well good . That {disfmarker} that really simplifies thing then .&#10;Speaker: Postdoc C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: And we can just , you know , get the meeting , process it , put the beeps file , send it off to IBM .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: You know ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: With very little {pause} work on our side .&#10;Speaker: PhD B&#10;Content: Process it , hear into it . I would {disfmarker}&#10;Speaker: PhD D&#10;Content: Do what ?&#10;Speaker: PhD B&#10;Content: Um , {pause} listen to it ," />
    <node id="The interruptions of hearing a word followed by a beep in the same sentence in the original file are likely caused by the manual application of the &quot;beep-ify&quot; method, where beeps are added at regular intervals throughout the audio recording. If the beep duration and placement is not consistent or well-coordinated with the speech segments, it may result in hearing a word followed by a beep within the same sentence. This issue highlights the importance of maintaining consistent volume levels and beep duration throughout the recording when applying this method manually." />
    <node id=" do that ?&#10;Speaker: Professor F&#10;Content: no .&#10;Speaker: PhD D&#10;Content: I {disfmarker} I {disfmarker}&#10;Speaker: Professor F&#10;Content: Uh , concentration .&#10;Speaker: PhD B&#10;Content: Perhaps there are {vocalsound} lots of errors in it&#10;Speaker: PhD D&#10;Content: Gah !&#10;Speaker: Grad A&#10;Content: Total concentration . Are you guys ready ?&#10;Speaker: PhD D&#10;Content: You hate to have your ears plugged ?&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Really ?" />
    <node id="&#10;Speaker: Professor F&#10;Content: And {disfmarker}&#10;Speaker: PhD B&#10;Content: So {disfmarker}&#10;Speaker: Postdoc C&#10;Content: And I guess we could just indicate to the transcribers not to {pause} encode that if they {disfmarker} We could still do the beep file .&#10;Speaker: Professor F&#10;Content: Yeah again I {disfmarker} I think that that is probably less of a problem because if you 're {disfmarker} if there 's {disfmarker} {vocalsound} If {disfmarker} if a {disfmarker} if a word is {disfmarker} is split , then they might have to listen to it a few times to really understand that they can't quite get it .&#10;Speaker: Postdoc C&#10;Content: OK . OK .&#10;Speaker: PhD B&#10;Content: But {disfmarker}&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: Whereas if they listen {nonvocalsound} to it and there 's {disfmarker} don" />
    <node id="1. Professor F realized that adding a random bit in RASTA's option was equivalent to adding flat spectrum because it added energy across all frequencies, similar to adding a constant to the spectrum. This made it faster and more efficient than adding a random bit.&#10;2. The transcript does not provide enough information to answer this question fully. However, it can be inferred that the paper written by PhD G presents three types of results, one of which is a measurement of overlap calculated in terms of the number of words. Another result mentioned involves a computation, and there may have been errors in the paper according to PhD B.&#10;3. The paper written by PhD G has not been reviewed by Professor F yet.&#10;4. The discussion about concentration and turning microphones on or off suggests that they are discussing issues related to sound quality and clarity in their research, possibly related to audio recordings or transcriptions of meetings." />
    <node id=" {disfmarker} for {disfmarker} for {disfmarker} when we use the ICSI front - end ,&#10;Speaker: Grad A&#10;Content: As an argument .&#10;Speaker: Professor F&#10;Content: but um , there is an {disfmarker} there is an o an option in {disfmarker} in RASTA , which , um , {vocalsound} in when I first put it in , uh , back in the days when I actually wrote things , uh , {vocalsound} I {pause} did actually put in a random bit or so that was in it ,&#10;Speaker: Grad E&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: but {vocalsound} then I realized that putting in a random bit was equivalent to adding uh {disfmarker} adding flat spectrum ,&#10;Speaker: Grad E&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: and it was a lot faster to just add a constant to the {disfmarker} {vocalsound} to the spectrum . So then I just started doing that&#10;Speaker: Grad E&#10;Content: Mmm . OK ." />
    <node id="mm ! {comment} {vocalsound} Gar - darn !&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Channel two .&#10;Speaker: Grad A&#10;Content: Do we use square brackets for anything ?&#10;Speaker: Postdoc C&#10;Content: Yeah . Uh {disfmarker}&#10;Speaker: Grad E&#10;Content: These poor transcribers .&#10;Speaker: Professor F&#10;Content: u&#10;Speaker: Postdoc C&#10;Content: Not ri not right now . I mean {disfmarker} No .&#10;Speaker: PhD D&#10;Content: There 's gonna be some zeros from this morning 's meeting because I noticed that&#10;Speaker: Professor F&#10;Content: u&#10;Speaker: PhD D&#10;Content: Barry , I think maybe you turned your mike off before the digits were {disfmarker} Oh , was it during digits ? Oh , so it doesn't matter .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: It 's still not a good idea .&#10;Speaker: PhD B&#10;Content: So it 's not {disfmarker" />
    <node id="m not looking forward to it .&#10;Speaker: Professor F&#10;Content: i it 's the high signal - to - noise ratio&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: here that {disfmarker} that helps you .&#10;Speaker: PhD G&#10;Content: u s Right . So {disfmarker} so that was number {disfmarker} that was the second set of {disfmarker} uh , the second section . And then , {vocalsound} the third thing was , we looked at , {vocalsound} {vocalsound} uh , what we call &quot; interrupts &quot; , although that 's {disfmarker} that may be {vocalsound} a misnomer , but basically {vocalsound} we looked at cases where {disfmarker} Uh , so we {disfmarker} we used the punctuation from the original transcripts and we inferred the beginnings and ends of sentences . So , you know {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Di - did you use upper - lower case also , or not ?&#10;Speaker: PhD G&#10;Content" />
    <node id="1. {disfmarker}: This symbol, mentioned by PhD G and Grad A, appears to be a placeholder for some sort of discourse marker or verbal tic that they are correcting or discussing in the context of analyzing conversational data. The disagreement revolves around how specific they should be when referring to the person using these markers; PhD G initially mentions &quot;Morgan&quot; but then changes it to &quot;one participant,&quot; while Grad A wonders if they identified him as a senior member.&#10;   &#10;2. {pause}: This symbol, used by Grad A, likely represents an actual pause in conversation. There is no disagreement about its use.&#10;&#10;3. The laptop sounds and potential transcription issues were discussed, but these are not symbols or unique markers specifically mentioned in the conversation." />
    <node id=" all of them ,&#10;Speaker: Postdoc C&#10;Content: Oh . OK .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: I mean , we didn't mention Morgan by name&#10;Speaker: Grad A&#10;Content: and he talks a lot .&#10;Speaker: PhD G&#10;Content: we just {disfmarker}&#10;Speaker: Grad A&#10;Content: One participant .&#10;Speaker: Professor F&#10;Content: Well {disfmarker} we have now , but {disfmarker}&#10;Speaker: PhD G&#10;Content: We {disfmarker} we {disfmarker} we {disfmarker} something about {disfmarker}&#10;Speaker: Grad A&#10;Content: Did you identify him as a senior {pause} member ?&#10;Speaker: PhD G&#10;Content: No , we as identify him as the person dominating the conversation .&#10;Speaker: Professor F&#10;Content: Well .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: I mean I get these AARP things , but I 'm" />
    <node id="Based on the transcript provided, the group is open to the idea of normalizing the comparison of robustness between multi-party and two-party conversations by the number of speakers involved. This suggestion was raised by PhD D and supported by other participants, including PhD B, Postdoc C, and Professor F. They believe that this normalization could provide a more fair and accurate comparison between different types of conversations. However, the group does not make a definitive decision on this matter during the discussion, so further conversation may be necessary to reach a conclusion." />
    <node id=" Meeting Recorder meetings and {vocalsound} about half that for , {vocalsound} uh , the Robustness .&#10;Speaker: Professor F&#10;Content: Maybe ten percent ?&#10;Speaker: Grad A&#10;Content: But I don't know if that 's really a fair way of comparing between , multi - party , conversations and two - party conversations . Yeah . I {disfmarker} I {disfmarker} I don't know .&#10;Speaker: PhD B&#10;Content: Then {disfmarker} then {disfmarker} then you have to {disfmarker}&#10;Speaker: Grad A&#10;Content: I mean that 's just something {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah , I just wonder if you have to normalize by the numbers of speakers or something .&#10;Speaker: Postdoc C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Then {disfmarker} Yeah , then normalize by {disfmarker} by something like that ,&#10;Speaker: Postdoc C&#10;Content: Yeah , that 's a good point .&#10;Speaker: PhD G&#10;Content: Well ," />
    <node id="Based on the meeting transcript, Don will be looking at prosodic properties as part of the team's focus now that they have the data and alignment. Prosodic properties likely include aspects such as intonation, stress, and rhythm in speech. This task was assigned by PhD G to Don during their discussion. Grad E also confirmed that they will be looking at prosodic properties." />
    <node id="&#10;Content: uh , well Don 's been working hard .&#10;Speaker: Grad E&#10;Content: That 's right .&#10;Speaker: PhD G&#10;Content: OK , so {disfmarker} {vocalsound} so if you have the foreground speaker speaking here , and then there 's some background speech , may be overlapping it somehow , um , and this is the time bin that we used , then of course you 're gonna get insertion errors here and here .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: Right ? So we scored everything , and I must say the NIST scoring tools are pretty nice for this , where you just basically ignore everything outside of the , {vocalsound} uh , region that was deemed to be foreground speech . And where that was we had to use the t forced alignment , uh , results from s for {disfmarker} so {disfmarker} That 's somewhat {disfmarker} that 's somewhat subject to error , but still we {disfmarker} we {disfmarker} {vocalsound} Uh , Don did some ha hand - checking and {disfmarker}" />
    <node id="1. The status of the Eurospeech paper submission is that it has been sent off, but it was sent in late, exceeding the delayed deadline by another day. The reason for its late submission is not explicitly mentioned in the transcript, but it appears to be unrelated to any issues with the paper's content.&#10;2. There is no direct information given about why the paper was sent in late. Grad A simply states, &quot;Well yeah, you sent it in late.&quot; No further explanation or context is provided in the transcript.&#10;3. The transcript does indicate that there was a delayed deadline and the paper exceeded it by another day. Professor F asks, &quot;Oh they had some extension that they announced or something?&quot; to which PhD G responds, &quot;Well yeah. Liz had sent them a note saying 'could we please have another three days' or something, and they said yes.&quot; It seems there was an agreed-upon extension, but the paper was still submitted later than the extended deadline.&#10;4. The exact reason for the late submission remains unclear from the transcript. It could be due to unforeseen circumstances, last-minute revisions, or other factors not mentioned in the conversation. However, it is evident that the paper has been sent off and the authors are now waiting for the review process to begin." />
    <node id="Content: Oops .&#10;Speaker: Professor F&#10;Content: Oh they {disfmarker} they had some extension that they announced or something ?&#10;Speaker: PhD G&#10;Content: Well yeah . Liz had sent them a note saying &quot; could we please {pause} have another &quot; {comment} {pause} I don't know , &quot; three days &quot; or something , and they said yes .&#10;Speaker: PhD D&#10;Content: And then she said &quot; Did I say three ?&#10;Speaker: Grad A&#10;Content: Oh ,&#10;Speaker: PhD D&#10;Content: I meant four . &quot;&#10;Speaker: Grad A&#10;Content: that was the other thing uh ,&#10;Speaker: PhD G&#10;Content: But u&#10;Speaker: Grad A&#10;Content: uh , Dave Gelbart sent me email , I think he sent it to you too , {comment} that um , there 's a special topic , section in si in Eurospeech on new , corp corpors corpora . And it 's not due until like May fifteenth .&#10;Speaker: Professor F&#10;Content: Oh this isn't the Aurora one ?&#10;Speaker: Grad A&#10;Content: No .&#10;Speaker: Professor F&#10;" />
    <node id="1. The group decided to include the section with participants playing sounds from their laptops and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They agreed not to manually insert &quot;mm-hmm&quot;s at this end but rather consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;2. The group did not explicitly discuss sending the output of the detector directly to IBM without any human checking involved. The consensus leans more towards including human intervention, either by having IBM handle the laptop sound section or by listening to the beep file generated by their script for manual indications." />
    <node id="Content: So far we haven't . So this is not gonna be a major part of the process , at least {disfmarker} least not in {disfmarker} not on ones that {disfmarker} that really {disfmarker}&#10;Speaker: PhD D&#10;Content: So if you don't have to adjust the bins , why not just do what it {disfmarker} for all the channels ?&#10;Speaker: Postdoc C&#10;Content: Mm - hmm ?&#10;Speaker: PhD D&#10;Content: Why not just throw all the channels to IBM ?&#10;Speaker: Postdoc C&#10;Content: Well there 's the question o of {pause} whether {disfmarker} Well , OK . She i It 's a question of how much time we want our transcriber to invest here {vocalsound} when she 's gonna have to invest that when it comes back from IBM anyway .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Postdoc C&#10;Content: So if it 's only inserting &quot; mm - hmm &quot;s here and there , then , wouldn't that be something that would be just as" />
    <node id="er} uh , for our purposes for the {disfmarker} for the IBM preparation , {vocalsound} uh , n having these {pause} joined together ,&#10;Speaker: PhD B&#10;Content: Yeah . Yeah .&#10;Speaker: Postdoc C&#10;Content: and uh {disfmarker} It makes a lot of sense . And in terms of transcription , it would be easy to do it that way .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: The way that they have with the longer units ,&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: not having to fuss with adding these units at this time .&#10;Speaker: PhD B&#10;Content: Yeah . Whi - which could have one drawback . If there is uh a backchannel in between those three things ,&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: the {disfmarker} the n the backchannel will {disfmarker} will occur at the end of {disfmarker} of those" />
    <node id="1. The speakers mention that &quot;one EDU (Educational Unit) is good enough&quot; and consider including it in the set of files they will send for transcription. They see value in generating multiple &quot;beep&quot; files to assess their quality, as this might help them determine how bad the audio with laptop sounds is and whether their automatic backchanneling system can improve communication.&#10;2. However, they also express concerns about the time and resources required to review and analyze transcribed content, potential privacy issues, and the accuracy of transcription, especially given the challenges discussed in the transcript.&#10;3. The beep files they plan on generating will serve as a means to compare different processing methods and identify major differences between them. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually, with normalization coefficients being essential for maintaining consistent volume levels across different sections of an audio file and between multiple files.&#10;4. The discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work." />
    <node id=" PhD D&#10;Content: just {disfmarker}&#10;Speaker: Grad A&#10;Content: I mean it just depends on how {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Well EDU {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah ,&#10;Speaker: Grad A&#10;Content: Sorry , go ahead .&#10;Speaker: PhD B&#10;Content: u u u&#10;Speaker: Postdoc C&#10;Content: So I was gonna say , EDU - one is good enough ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: maybe we could include it in this {disfmarker} in this set of uh , this stuff we send .&#10;Speaker: PhD B&#10;Content: Yeah there 's {disfmarker} I {disfmarker} I think there are some meetings where it would {disfmarker} would {disfmarker} It 's possible like this .&#10;Speaker: Grad A&#10;Content: Yeah I {disfmarker} I think , we won't know until we generate a bunch of beep files automatically , listen to them and see how bad" />
    <node id="1. The proposed solution to address the issue of consecutive beeps in the transcription process is to maintain consistent volume levels and beep duration throughout the recording when applying the &quot;beep-ify&quot; method manually. This involves paying close attention to factors such as normalization coefficients, which are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. By ensuring that the inserted beeps have a uniform volume and duration throughout the recording, it can help prevent hearing a word followed by a beep within the same sentence.&#10;&#10;2. Implementing this solution might affect the outcome of the meeting that has already been run through a certain process by improving the overall quality of the transcription. The beep files generated from the original audio recording can serve as a means to compare different processing methods and identify major differences between them. By addressing common challenges in audio recordings, such as consistent volume levels and beep duration, the group can enhance the IBM transcription process and produce more accurate results." />
    <node id="The proposed approach for sharing data with another person at SRI, as discussed by PhD G, is using File Transfer Protocol (FTP). PhD G believes that FTP is the best method for several reasons. First, he has created a new directory specifically for sharing the data after being advised that it would be a good idea. This new directory will allow the other person at SRI to access the necessary data easily. While the transcript does not explicitly confirm that the data has been shared and accessed via FTP, PhD G's preparations indicate that he is actively working on implementing this method for sharing the data." />
    <node id="1. The old recognition output was scored differently, but the recognizer was not provided with information about where the foreground speech starts.&#10;2. The old recognition output was modified to include the lapel microphone, which was previously excluded.&#10;3. Although the issues with the lapel microphone were not explicitly stated as resolved, they were discussed less frequently in comparison to other topics. This could imply that the problems were mitigated but may not have been completely eliminated.&#10;&#10;In summary, the old recognition output underwent a change in scoring and began including data from the lapel microphone. While the resolution of issues related to the lapel microphone was not explicitly confirmed, there was less discussion about those problems." />
    <node id=" bit of a problem&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: That 's great .&#10;Speaker: PhD B&#10;Content: as it really switches around between {vocalsound} two different channels , I think .&#10;Speaker: Grad A&#10;Content: Mm - hmm , and {disfmarker} and they 're very {disfmarker} it 's very audible ? on the close - talking channels ?&#10;Speaker: PhD B&#10;Content: What {disfmarker} what I would {disfmarker} Yeah .&#10;Speaker: Grad A&#10;Content: Oh well . I mean , it 's the same problem as the lapel mike .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: But {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Oh , interesting .&#10;Speaker: PhD B&#10;Content: Comparable , yeah .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Postdoc C&#10;Content:" />
    <node id="} you know , we {disfmarker} we uh , {vocalsound} make all the features have zero mean and unit variance .&#10;Speaker: Grad A&#10;Content: Over an entire utterance ?&#10;Speaker: Professor F&#10;Content: And {disfmarker}&#10;Speaker: Grad A&#10;Content: Or windowed ?&#10;Speaker: PhD G&#10;Content: Over {disfmarker} over the entire c over the entire channel .&#10;Speaker: PhD B&#10;Content: Don't {pause} train {disfmarker}&#10;Speaker: PhD G&#10;Content: Over the {disfmarker}&#10;Speaker: Grad A&#10;Content: Hmm .&#10;Speaker: PhD G&#10;Content: but you know . Um , now we didn't re - align the recognizer for this . We just took the old {disfmarker} So this is actually a sub - optimal way of doing it ,&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: right ? So we took the old recognition output and we just scored it differently . So the recognizer didn't have the benefit" />
    <node id=" any major differences between {vocalsound} doing it on the hand&#10;Speaker: Postdoc C&#10;Content: Uh - huh . Oh , interesting . Ah !&#10;Speaker: Grad A&#10;Content: Hmm .&#10;Speaker: Postdoc C&#10;Content: OK . Interesting idea . Great .&#10;Speaker: PhD G&#10;Content: So this training meeting , uh w un is that uh {pause} some data where we have uh very um , {vocalsound} you know , accurate {pause} time marks ? for {disfmarker}&#10;Speaker: Postdoc C&#10;Content: I went back and hand - marked the {pause} ba the bins , I ment I mentioned that last week .&#10;Speaker: PhD G&#10;Content: OK , yeah .&#10;Speaker: PhD D&#10;Content: But the {disfmarker} but there 's {disfmarker} yeah , but there is this one issue with them in that there 're {disfmarker} {vocalsound} there are time boundaries in there that occur in the middle of speech .&#10;Speaker: PhD G&#10;Content: Because {disfmarker}&#10;Speaker: PhD D&#10;Content: So {" />
    <node id="The transcript does not provide direct information on why Liz decided to use the downsampled versions instead of the full bandwidth version. However, it is mentioned that there was no significant difference between them and the downsampled versions even performed better than the original versions in terms of communication. It can be speculated that Liz might have chosen to use the downsampled versions to save disk space or due to other practical considerations, such as easier handling or faster processing times. Ultimately, the reason for her decision is not explicitly stated in the given transcript." />
    <node id="aker: PhD G&#10;Content: OK . Alright , then I 'll hold off on that and I 'll wait for you um {disfmarker}&#10;Speaker: Grad E&#10;Content: Probably by tomorrow&#10;Speaker: PhD G&#10;Content: gen&#10;Speaker: Grad E&#10;Content: I can {disfmarker} I 'll send you an email .&#10;Speaker: PhD G&#10;Content: OK . Alright . OK . Yeah , definitely they should have the full bandwidth version ,&#10;Speaker: Grad E&#10;Content: Yeah , because I mean {disfmarker} I I think Liz decided to go ahead with the {pause} downsampled versions cuz we can {disfmarker} There was no s like , r significant difference .&#10;Speaker: PhD G&#10;Content: yeah . OK . Well , it takes {disfmarker} it takes up less disk space , for one thing .&#10;Speaker: Grad E&#10;Content: It does take up less disk space , and apparently it did even better {pause} than the original {disfmarker} than the original versions ,&#10;Speaker: PhD G&#10;Content: Yeah . Yeah .&#10;Speaker: Grad E" />
    <node id="The benefits of using a channelized interface for encoding categorized speech include ease of handling and improved clarity when the context is apparent. This method would make it easier to distinguish between different types of speech, such as channeled or close-miked speakers.&#10;&#10;However, there are also potential challenges associated with this approach. If the context is not clear, listeners might struggle to determine whether a given sound is part of the channelized speech or an independent audio source. This confusion could lead to misunderstandings and make transcription more difficult for both automated systems and human transcribers. The speakers in the conversation acknowledge these challenges and express concern that such ambiguity could be confusing." />
    <node id="&#10;Content: So th&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: there was a category for @ @ {comment} speech .&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Yeah , the default .&#10;Speaker: Postdoc C&#10;Content: Yeah , s a&#10;Speaker: Grad A&#10;Content: No , not default .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Postdoc C&#10;Content: Well , as it comes back , we have a uh {disfmarker} when we can use the channelized interface for encoding it , then it 'll be easy for us to handle .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: But {disfmarker} {vocalsound} but if {disfmarker} if out of context , they can't tell if it 's a channeled speak uh , you know , a close - miked speaker or not , {vocalsound} then that would be confusing to them .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Grad A&#10;Content" />
    <node id="1. The speakers decide to include a portion of the audio recording containing laptop sounds in the file they will send to IBM for transcription, despite potential challenges in distinguishing these sounds from actual speech. They also consider having someone manually indicate which sections of the generated beep file are intelligible or not.&#10;2. To handle words that are cut off and other acoustic problems in a transcript, the speakers plan on generating multiple &quot;beep&quot; files to assess their quality and determine how bad the audio with laptop sounds is. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually.&#10;3. The speakers express interest in exploring an automatic backchanneling system developed by a former student named Nigel Ward, which could be used to improve communication and indicate active listening in conversations. This suggests that they may be open to using such tools to help address acoustic problems and improve the overall quality of their transcripts." />
    <node id="1. The specific content of the paper that Graduate Student A printed out but hasn't read yet is not mentioned in the transcript. However, it is clear that the paper is related to a Monty Python sketch and has been submitted for Eurospeech.&#10;2. PhD Student G is planning to send an email to Brian Kingbury about where he can find the related material because Brian had previously requested this material for a speech recognition experiment. PhD G was in the process of sending the location of the shared directory when his desktop froze, causing him to delay the email. The transcript suggests that PhD G has prepared a shared directory via FTP to send to Brian, but it does not explicitly confirm whether the email has been sent or the material has been accessed by Brian at SRI." />
    <node id="Content: Random intervals .&#10;Speaker: Grad A&#10;Content: There was {disfmarker} there was of course a Monty Python sketch with that . Where the barber who was afraid of scissors was playing a {disfmarker} a tape of clipping sounds , and saying &quot; uh - huh &quot; , &quot; yeah &quot; , &quot; how about them sports teams ? &quot;&#10;Speaker: PhD G&#10;Content: Anyway . So the paper 's on - line and y I {disfmarker} I think I uh {disfmarker} I CC ' ed a message to Meeting Recorder with the URL so you can get it .&#10;Speaker: Grad A&#10;Content: Yep .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: Printed it out , haven't read it yet .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Um , uh one more thing . So I {disfmarker} I 'm actually {disfmarker} {vocalsound} about to send Brian Kingbury an email saying where he can find the {disfmarker} the s the m the material he wanted for the" />
    <node id="ound} about to send Brian Kingbury an email saying where he can find the {disfmarker} the s the m the material he wanted for the s for the speech recognition experiment , so {disfmarker} but I haven't sent it out yet because actually my desktop locked up , like I can't type anything . Uh b so if there 's any suggestions you have for that I was just gonna send him the {disfmarker}&#10;Speaker: PhD D&#10;Content: Is it the same directory that you had suggested ?&#10;Speaker: PhD G&#10;Content: I made a directory . I called it um {disfmarker}&#10;Speaker: Postdoc C&#10;Content: He still has his Unix account here , you know .&#10;Speaker: PhD G&#10;Content: Well this isn't {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: He does ?&#10;Speaker: Postdoc C&#10;Content: And he {disfmarker} and he 's {disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah but {disfmarker} but {disfmarker} but he has" />
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target=" backchanneling , automatic backchanneling system .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: It 's a very {disfmarker}&#10;Speaker: PhD D&#10;Content: Oh !&#10;Speaker: PhD G&#10;Content: So , exactly what you describe ,&#10;Speaker: PhD D&#10;Content: Huh .&#10;Speaker: PhD G&#10;Content: but for Japanese . And it 's apparently {disfmarker} for Japa - in Japanese it 's really important that you backchannel . It 's really impolite if you don't , and {disfmarker} So .&#10;Speaker: Professor F&#10;Content: Huh . Actually for a lot of these people I think you could just sort of backchannel continuously and it would {pause} pretty much be fine .&#10;Speaker: PhD D&#10;Content: It wouldn't matter ? Yeah .&#10;Speaker: Grad E&#10;Content: Yeah . That 's w That 's what I do .&#10;Speaker: PhD D&#10;Content: Random intervals .&#10;Speaker: Grad A&#10;Content: There was {disfmarker} there was of course a Monty Python sketch with">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target=" gives them the backchannel . And so then you {vocalsound} hook that to the phone and go off&#10;Speaker: Grad A&#10;Content: Yeah . Uh - huh .&#10;Speaker: PhD D&#10;Content: and do the {vocalsound} {disfmarker} do whatever you r wanna do ,&#10;Speaker: PhD G&#10;Content: Oh yeah . Well {disfmarker}&#10;Speaker: PhD D&#10;Content: while that thing keeps them busy .&#10;Speaker: PhD G&#10;Content: There 's actually {disfmarker} uh there 's this a former student of here from Berkeley , Nigel {disfmarker} Nigel Ward .&#10;Speaker: PhD D&#10;Content: Uh - huh . Sure .&#10;Speaker: PhD G&#10;Content: Do you know him ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: He did a system uh , in {disfmarker} he {disfmarker} he lives in Japan now , and he did this backchanneling , automatic backchanneling system .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: It '">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target=": Right .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: You have a lot of {disfmarker} a lot of two - party , subsets within the meeting .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Postdoc C&#10;Content: Uh - huh .&#10;Speaker: Grad A&#10;Content: Well regardless {disfmarker} it 's an interesting result regardless .&#10;Speaker: PhD G&#10;Content: So {disfmarker} Right .&#10;Speaker: Postdoc C&#10;Content: Yes , that 's right .&#10;Speaker: PhD G&#10;Content: And {disfmarker} and {disfmarker} and then {disfmarker} and we also d computed this both with and without backchannels ,&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: so you might think that backchannels have a special status because they 're essentially just {disfmarker}&#10;Speaker: Grad A&#10;Content: Uh - huh . So , did {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target=" the beeps would be {vocalsound} uh y y y And I 'm not exactly sure how that {disfmarker} how that would work with the {disfmarker} with the backchannels . And , so um {disfmarker} And certainly things that are {vocalsound} intrusions of multiple words , {vocalsound} taken out of context and displaced in time from where they occurred , {vocalsound} that would be hard . So , m my {vocalsound} thought is {pause} i I 'm having this transcriber go through {vocalsound} the EDU - one meeting , and indicate a start time {nonvocalsound} f for each dominant speaker , endpoi end time for each dominant speaker , and the idea that {vocalsound} these units would be generated for the dominant speakers , {vocalsound} and maybe not for the other channels .&#10;Speaker: Grad A&#10;Content: Yeah the only , um , disadvantage of that is , then it 's hard to use an automatic method to do that . The advantage is that it 's probably faster to do that than it is to use the automated method and correct it . So .">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target=" {disfmarker} the n the backchannel will {disfmarker} will occur at the end of {disfmarker} of those three .&#10;Speaker: Postdoc C&#10;Content: Yes .&#10;Speaker: PhD B&#10;Content: And {disfmarker} and in {disfmarker} in the {disfmarker} in the previous version where in the n which is used now , {vocalsound} there , the backchannel would {disfmarker} would be in - between there somewhere , so .&#10;Speaker: Postdoc C&#10;Content: I see .&#10;Speaker: PhD B&#10;Content: That would be more natural&#10;Speaker: Postdoc C&#10;Content: Yeah . Well ,&#10;Speaker: PhD B&#10;Content: but {disfmarker}&#10;Speaker: Postdoc C&#10;Content: that 's {disfmarker} that 's right , but you know , thi this brings me to the other f stage of this which I discussed with you earlier today ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: which is {vocalsound} the second stage is {">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target=": So .&#10;Speaker: PhD B&#10;Content: that 's {disfmarker} that 's a second question , &quot; what {disfmarker} what will different transcribers do with {disfmarker} with the laptop sound ? &quot;&#10;Speaker: Postdoc C&#10;Content: Would you {disfmarker} would {disfmarker}&#10;Speaker: Professor F&#10;Content: What was the l what was the laptop sound ?&#10;Speaker: Postdoc C&#10;Content: Yeah , go ahead .&#10;Speaker: Professor F&#10;Content: I mean was it speech ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: or was it {disfmarker}&#10;Speaker: PhD B&#10;Content: It 's speech .&#10;Speaker: Professor F&#10;Content: Great .&#10;Speaker: Postdoc C&#10;Content: Well , so {disfmarker} I mean {disfmarker} So my standard approach has been if it 's not someone close - miked , then , they don't end up on one of the close - miked channels . They end up on a different channel . And we have">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target="The 'ASR 0.1' directory, as mentioned in the meeting, is meant for speech recognition purposes. The name 'ASR 0.1' stands for &quot;Automatic Speech Recognition&quot; and signifies its version or iteration (0.1). This directory utilizes a quick and dirty form of access control; to access any files within it, one must be told the exact name of the file or subdirectory since it is not directly visible or readable.&#10;&#10;The purpose of this structure is to ensure that only individuals with explicit permission can access the files within the directory. By using a non-standard approach to access control, the creators are able to restrict access and maintain better control over who can view or edit the contents. Additionally, implementing such a system can be beneficial for privacy and security purposes, especially when dealing with sensitive data or information.&#10;&#10;The 'ASR 0.1' directory likely contains various files related to speech recognition research, experiments, or projects that the team is working on. It could include audio files, transcription scripts, software tools, and other resources necessary for ASR development. The access control mechanism discussed in the meeting ensures that only authorized individuals can view and interact with these materials, making collaboration more secure and organized.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target="1. Number of speakers: The speech-nonspeech detection system performs better in meetings with fewer speakers. This is because there are fewer overlapping voices and background noises to confuse the system.&#10;2. Background noise: The system can be affected by background speech, as demonstrated by the fact that Meeting Recorder meetings had more overlap than Robustness meetings, even though both were meeting recordings. Additionally, telephone conversations (Switchboard and CallHome) had about the same amount of overlap as Meeting Recorder meetings, suggesting that background speech is a significant source of recognition errors.&#10;3. Channel quality: The system's performance can be impacted by the quality of the channels being used. If the laptop is in the background or there is breathy channel noise, it can cause false positives for speech detection. This can result in more missed speech, as demonstrated by PhD B's statement that a good meeting has around 1% or less of speech missed by the detector.&#10;4. Speech-nonspeech detector limitations: The speech-nonspeech detector may assign speech randomly to one of the channels when it is unable to accurately detect the source of the speech, which can lead to recognition errors.&#10;5. Adjustments for system performance: To improve the performance of the system, meeting segments with high background noise or multiple speakers might need to be excluded from the analysis. Additionally, efforts could be made to reduce background noise and ensure clearer audio channels during meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target="Based on the transcript provided, the proposed solution for increasing the speed of the process being discussed, while still allowing for adjustments and thorough listening, seems to be using an automatic backchanneling system. This system, developed by a former student named Nigel Ward, could help indicate active listening in conversations without requiring as much time as manual backchanneling (e.g., saying &quot;mm-hmm&quot; or &quot;uh-huh&quot;). The professors are interested in exploring how this automatic system could be used to improve communication during their meetings.&#10;&#10;Additionally, Postdoc C mentions that &quot;listening does take time too,&quot; implying that they acknowledge the importance of dedicating sufficient time to listening during conversations. However, no specific solutions for adjusting or optimizing the timing of the process are explicitly mentioned in the transcript.&#10;&#10;Grad A suggests a specific value of &quot;one and a half times real time&quot; in relation to the discussion, but it is not clear whether this number is directly related to the proposed solution for increasing speed while maintaining quality listening and adjustments. It would be beneficial to have more context or information on where this value comes from and how it applies to the process being discussed.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target="1. Discussion about an automatic backchanneling system: The participants are discussing an automatic backchanneling system developed by a former student named Nigel Ward. They express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even during their current meeting. This system may be particularly useful for Japanese conversations, as backchanneling is crucial and not doing so can be impolite.&#10;&#10;2. Continuously backchanneling at random intervals: The participants mention that backchanneling tends to happen when there is a falling pitch in conversations. They suggest that continuously backchanneling at random intervals could potentially be acceptable in an automatic system, as it would indicate active listening without requiring manual input and might &quot;pretty much be fine&quot; for a lot of people. However, they also note the importance of cultural context, such as Japanese conversations where not backchanneling can be impolite.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target="1. {disfmarker}: This symbol, mentioned by PhD G and Grad A, appears to be a placeholder for some sort of discourse marker or verbal tic that they are correcting or discussing in the context of analyzing conversational data. The disagreement revolves around how specific they should be when referring to the person using these markers; PhD G initially mentions &quot;Morgan&quot; but then changes it to &quot;one participant,&quot; while Grad A wonders if they identified him as a senior member.&#10;   &#10;2. {pause}: This symbol, used by Grad A, likely represents an actual pause in conversation. There is no disagreement about its use.&#10;&#10;3. The laptop sounds and potential transcription issues were discussed, but these are not symbols or unique markers specifically mentioned in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target="Based on the transcript provided, the group is open to the idea of normalizing the comparison of robustness between multi-party and two-party conversations by the number of speakers involved. This suggestion was raised by PhD D and supported by other participants, including PhD B, Postdoc C, and Professor F. They believe that this normalization could provide a more fair and accurate comparison between different types of conversations. However, the group does not make a definitive decision on this matter during the discussion, so further conversation may be necessary to reach a conclusion.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target="The benefits of using a channelized interface for encoding categorized speech include ease of handling and improved clarity when the context is apparent. This method would make it easier to distinguish between different types of speech, such as channeled or close-miked speakers.&#10;&#10;However, there are also potential challenges associated with this approach. If the context is not clear, listeners might struggle to determine whether a given sound is part of the channelized speech or an independent audio source. This confusion could lead to misunderstandings and make transcription more difficult for both automated systems and human transcribers. The speakers in the conversation acknowledge these challenges and express concern that such ambiguity could be confusing.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the importance of backchanneling, which are sounds or words that indicate active listening, such as &quot;mm-hmm&quot; or &quot;uh-huh.&quot; In the transcript, the participants discuss how backchanneling is crucial in Japanese communication, and not doing so can be impolite. They also talk about an automatic backchanneling system developed by a former student named Nigel Ward. The professors express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even using it during the current meeting." target="1. The speakers decide to include a portion of the audio recording containing laptop sounds in the file they will send to IBM for transcription, despite potential challenges in distinguishing these sounds from actual speech. They also consider having someone manually indicate which sections of the generated beep file are intelligible or not.&#10;2. To handle words that are cut off and other acoustic problems in a transcript, the speakers plan on generating multiple &quot;beep&quot; files to assess their quality and determine how bad the audio with laptop sounds is. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually.&#10;3. The speakers express interest in exploring an automatic backchanneling system developed by a former student named Nigel Ward, which could be used to improve communication and indicate active listening in conversations. This suggests that they may be open to using such tools to help address acoustic problems and improve the overall quality of their transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source=" backchanneling , automatic backchanneling system .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: It 's a very {disfmarker}&#10;Speaker: PhD D&#10;Content: Oh !&#10;Speaker: PhD G&#10;Content: So , exactly what you describe ,&#10;Speaker: PhD D&#10;Content: Huh .&#10;Speaker: PhD G&#10;Content: but for Japanese . And it 's apparently {disfmarker} for Japa - in Japanese it 's really important that you backchannel . It 's really impolite if you don't , and {disfmarker} So .&#10;Speaker: Professor F&#10;Content: Huh . Actually for a lot of these people I think you could just sort of backchannel continuously and it would {pause} pretty much be fine .&#10;Speaker: PhD D&#10;Content: It wouldn't matter ? Yeah .&#10;Speaker: Grad E&#10;Content: Yeah . That 's w That 's what I do .&#10;Speaker: PhD D&#10;Content: Random intervals .&#10;Speaker: Grad A&#10;Content: There was {disfmarker} there was of course a Monty Python sketch with" target="1. Discussion about an automatic backchanneling system: The participants are discussing an automatic backchanneling system developed by a former student named Nigel Ward. They express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even during their current meeting. This system may be particularly useful for Japanese conversations, as backchanneling is crucial and not doing so can be impolite.&#10;&#10;2. Continuously backchanneling at random intervals: The participants mention that backchanneling tends to happen when there is a falling pitch in conversations. They suggest that continuously backchanneling at random intervals could potentially be acceptable in an automatic system, as it would indicate active listening without requiring manual input and might &quot;pretty much be fine&quot; for a lot of people. However, they also note the importance of cultural context, such as Japanese conversations where not backchanneling can be impolite.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Right .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: You have a lot of {disfmarker} a lot of two - party , subsets within the meeting .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Postdoc C&#10;Content: Uh - huh .&#10;Speaker: Grad A&#10;Content: Well regardless {disfmarker} it 's an interesting result regardless .&#10;Speaker: PhD G&#10;Content: So {disfmarker} Right .&#10;Speaker: Postdoc C&#10;Content: Yes , that 's right .&#10;Speaker: PhD G&#10;Content: And {disfmarker} and {disfmarker} and then {disfmarker} and we also d computed this both with and without backchannels ,&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: so you might think that backchannels have a special status because they 're essentially just {disfmarker}&#10;Speaker: Grad A&#10;Content: Uh - huh . So , did {disf" target="The best approach for sharing the data with another person at SRI, as determined by PhD G, is using FTP. PhD G implemented this by creating a new directory for sharing the data, which he had previously been advised would be a good idea. The transcript suggests that PhD G has taken steps to make the necessary preparations for sharing the data via FTP, but it does not provide explicit information on whether the data has actually been shared and accessed by the person at SRI.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Right .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: You have a lot of {disfmarker} a lot of two - party , subsets within the meeting .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Postdoc C&#10;Content: Uh - huh .&#10;Speaker: Grad A&#10;Content: Well regardless {disfmarker} it 's an interesting result regardless .&#10;Speaker: PhD G&#10;Content: So {disfmarker} Right .&#10;Speaker: Postdoc C&#10;Content: Yes , that 's right .&#10;Speaker: PhD G&#10;Content: And {disfmarker} and {disfmarker} and then {disfmarker} and we also d computed this both with and without backchannels ,&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: so you might think that backchannels have a special status because they 're essentially just {disfmarker}&#10;Speaker: Grad A&#10;Content: Uh - huh . So , did {disf" target="1. Discussion about an automatic backchanneling system: The participants are discussing an automatic backchanneling system developed by a former student named Nigel Ward. They express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even during their current meeting. This system may be particularly useful for Japanese conversations, as backchanneling is crucial and not doing so can be impolite.&#10;&#10;2. Continuously backchanneling at random intervals: The participants mention that backchanneling tends to happen when there is a falling pitch in conversations. They suggest that continuously backchanneling at random intervals could potentially be acceptable in an automatic system, as it would indicate active listening without requiring manual input and might &quot;pretty much be fine&quot; for a lot of people. However, they also note the importance of cultural context, such as Japanese conversations where not backchanneling can be impolite.">
      <data key="d0">1</data>
    </edge>
    <edge source=" the beeps would be {vocalsound} uh y y y And I 'm not exactly sure how that {disfmarker} how that would work with the {disfmarker} with the backchannels . And , so um {disfmarker} And certainly things that are {vocalsound} intrusions of multiple words , {vocalsound} taken out of context and displaced in time from where they occurred , {vocalsound} that would be hard . So , m my {vocalsound} thought is {pause} i I 'm having this transcriber go through {vocalsound} the EDU - one meeting , and indicate a start time {nonvocalsound} f for each dominant speaker , endpoi end time for each dominant speaker , and the idea that {vocalsound} these units would be generated for the dominant speakers , {vocalsound} and maybe not for the other channels .&#10;Speaker: Grad A&#10;Content: Yeah the only , um , disadvantage of that is , then it 's hard to use an automatic method to do that . The advantage is that it 's probably faster to do that than it is to use the automated method and correct it . So ." target="1. The reason they cannot include the beep files with the initial submission is that there is a part of the beep file generation process which requires knowledge of the normalization coefficients. These coefficients are necessary for the correct production of the beep files, but they are not yet available at this stage.&#10;2. Normalization coefficients are values used to adjust the volume or amplitude of audio signals during processing. They are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. In the context of generating beep files, these coefficients ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;3. As they are still in the early stages of their project, the normalization coefficients might not yet be determined. Therefore, they cannot generate the beep files and include them in the initial submission to IBM for transcription. They will need to complete this step at a later stage when the necessary information is available.">
      <data key="d0">1</data>
    </edge>
    <edge source=": So .&#10;Speaker: PhD B&#10;Content: that 's {disfmarker} that 's a second question , &quot; what {disfmarker} what will different transcribers do with {disfmarker} with the laptop sound ? &quot;&#10;Speaker: Postdoc C&#10;Content: Would you {disfmarker} would {disfmarker}&#10;Speaker: Professor F&#10;Content: What was the l what was the laptop sound ?&#10;Speaker: Postdoc C&#10;Content: Yeah , go ahead .&#10;Speaker: Professor F&#10;Content: I mean was it speech ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: or was it {disfmarker}&#10;Speaker: PhD B&#10;Content: It 's speech .&#10;Speaker: Professor F&#10;Content: Great .&#10;Speaker: Postdoc C&#10;Content: Well , so {disfmarker} I mean {disfmarker} So my standard approach has been if it 's not someone close - miked , then , they don't end up on one of the close - miked channels . They end up on a different channel . And we have" target="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published.">
      <data key="d0">1</data>
    </edge>
    <edge source=": So .&#10;Speaker: PhD B&#10;Content: that 's {disfmarker} that 's a second question , &quot; what {disfmarker} what will different transcribers do with {disfmarker} with the laptop sound ? &quot;&#10;Speaker: Postdoc C&#10;Content: Would you {disfmarker} would {disfmarker}&#10;Speaker: Professor F&#10;Content: What was the l what was the laptop sound ?&#10;Speaker: Postdoc C&#10;Content: Yeah , go ahead .&#10;Speaker: Professor F&#10;Content: I mean was it speech ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: or was it {disfmarker}&#10;Speaker: PhD B&#10;Content: It 's speech .&#10;Speaker: Professor F&#10;Content: Great .&#10;Speaker: Postdoc C&#10;Content: Well , so {disfmarker} I mean {disfmarker} So my standard approach has been if it 's not someone close - miked , then , they don't end up on one of the close - miked channels . They end up on a different channel . And we have" target="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process.">
      <data key="d0">1</data>
    </edge>
    <edge source="From the meeting transcript, the findings when the speakers rescored and fixed the transcripts were as follows:&#10;&#10;1. They added one number to their analysis, which involved scoring recognition results while ignoring all insertions due to background speech. This change was based on a conjecture from a previous HLT paper that most of the added recognition error is from insertions due to background speech.&#10;2. By scoring the recognition results in this way, they were able to compare the number of overlapped words and spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome.&#10;3. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;4. An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings." target=" words to maybe {vocalsound} uh I don't know , eleven percent or something {disfmarker} it 's {disfmarker} it 's not a dramatic change ,&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: so it 's {disfmarker} Anyway , so it 's uh {disfmarker} That was {disfmarker} that was one set of {pause} results , and then the second one was just basically the {disfmarker} {vocalsound} the stuff we had in the {disfmarker} in the HLT paper on how overlaps effect the {pause} recognition performance .&#10;Speaker: Postdoc C&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Nope . Right .&#10;Speaker: Professor F&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: And we rescored things um , a little bit more carefully . We also fixed the transcripts in {disfmarker} in numerous ways . Uh , but mostly we added one {disfmarker} one number , which was what if you {pause">
      <data key="d0">1</data>
    </edge>
    <edge source="From the meeting transcript, the findings when the speakers rescored and fixed the transcripts were as follows:&#10;&#10;1. They added one number to their analysis, which involved scoring recognition results while ignoring all insertions due to background speech. This change was based on a conjecture from a previous HLT paper that most of the added recognition error is from insertions due to background speech.&#10;2. By scoring the recognition results in this way, they were able to compare the number of overlapped words and spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome.&#10;3. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;4. An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings." target="disfmarker} in numerous ways . Uh , but mostly we added one {disfmarker} one number , which was what if you {pause} uh , basically score ignoring all {disfmarker} So {disfmarker} so the {disfmarker} the conjecture from the HLT results was that {vocalsound} most of the added recognition error is from insertions {vocalsound} due to background speech . So , we scored {vocalsound} all the recognition results , {vocalsound} uh , in such a way that the uh {disfmarker}&#10;Speaker: Grad A&#10;Content: Oh by the way , who 's on channel four ? You 're getting a lot of breath .&#10;Speaker: PhD B&#10;Content: Yeah . I j was just wondering .&#10;Speaker: Grad E&#10;Content: That 's {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: That 's me .&#10;Speaker: PhD G&#10;Content: uh , well Don 's been working hard .&#10;Speaker: Grad E&#10;Content: That 's right .&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="From the meeting transcript, the findings when the speakers rescored and fixed the transcripts were as follows:&#10;&#10;1. They added one number to their analysis, which involved scoring recognition results while ignoring all insertions due to background speech. This change was based on a conjecture from a previous HLT paper that most of the added recognition error is from insertions due to background speech.&#10;2. By scoring the recognition results in this way, they were able to compare the number of overlapped words and spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome.&#10;3. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;4. An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings." target=" . Yeah , that 's right .&#10;Speaker: PhD G&#10;Content: I think several people {disfmarker} sent this ,&#10;Speaker: Grad A&#10;Content: so .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Uh - huh .&#10;Speaker: PhD G&#10;Content: yeah .&#10;Speaker: Grad A&#10;Content: But any {disfmarker} any help you need I can certainly provide .&#10;Speaker: Professor F&#10;Content: Well ,&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: that 's {disfmarker} that 's a great idea .&#10;Speaker: PhD G&#10;Content: Well {disfmarker} there {disfmarker} there were some interesting results in this paper , though . For instance that Morgan {disfmarker} uh , accounted for fifty - six percent of the Robustness meetings in terms of number of words .&#10;Speaker: Grad A&#10;Content: Wow .&#10;Speaker: Postdoc C&#10;Content: In {disfmarker} in terms of what ? In term&#10;Speaker: PhD G">
      <data key="d0">1</data>
    </edge>
    <edge source="From the meeting transcript, the findings when the speakers rescored and fixed the transcripts were as follows:&#10;&#10;1. They added one number to their analysis, which involved scoring recognition results while ignoring all insertions due to background speech. This change was based on a conjecture from a previous HLT paper that most of the added recognition error is from insertions due to background speech.&#10;2. By scoring the recognition results in this way, they were able to compare the number of overlapped words and spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome.&#10;3. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;4. An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings." target=" locations where , uh , if you have overlapping speech and someone else starts a sentence , you know , where do these {disfmarker} where do other people start their {vocalsound} turns {disfmarker} not turns really , but you know , sentences ,&#10;Speaker: PhD B&#10;Content: Ah .&#10;Speaker: PhD G&#10;Content: um {disfmarker} So we only looked at cases where there was a foreground speaker and then at the to at the {disfmarker} so the {disfmarker} the foreground speaker started into their sentence and then someone else started later .&#10;Speaker: PhD B&#10;Content: Somewhere in between the start and the end ?&#10;Speaker: PhD G&#10;Content: OK ? And so what {disfmarker}&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: Sorry ?&#10;Speaker: PhD B&#10;Content: Somewhere in between the start and the end of the foreground ?&#10;Speaker: PhD G&#10;Content: Yes . Uh , so that such that there was overlap between the two sentences .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="From the meeting transcript, the findings when the speakers rescored and fixed the transcripts were as follows:&#10;&#10;1. They added one number to their analysis, which involved scoring recognition results while ignoring all insertions due to background speech. This change was based on a conjecture from a previous HLT paper that most of the added recognition error is from insertions due to background speech.&#10;2. By scoring the recognition results in this way, they were able to compare the number of overlapped words and spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome.&#10;3. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;4. An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings." target="aker: PhD G&#10;Content: right ? So we took the old recognition output and we just scored it differently . So the recognizer didn't have the benefit of knowing where the foreground speech {disfmarker} a start&#10;Speaker: Professor F&#10;Content: Were you including the {disfmarker} the lapel {pause} in this ?&#10;Speaker: PhD G&#10;Content: Yes .&#10;Speaker: Professor F&#10;Content: And did the {disfmarker} did {disfmarker} did the la did the {disfmarker} the problems with the lapel go away also ? Or {disfmarker}&#10;Speaker: PhD G&#10;Content: Um , it {disfmarker} Yeah .&#10;Speaker: Professor F&#10;Content: fray for {disfmarker} for insertions ?&#10;Speaker: PhD G&#10;Content: It u not per {disfmarker} I mean , not completely , but yes ,&#10;Speaker: Professor F&#10;Content: Less so .&#10;Speaker: PhD G&#10;Content: dramatically . So we have to um {disfmarker}&#10;Speaker: Professor F&#10;Content: I mean , you still {">
      <data key="d0">1</data>
    </edge>
    <edge source="From the meeting transcript, the findings when the speakers rescored and fixed the transcripts were as follows:&#10;&#10;1. They added one number to their analysis, which involved scoring recognition results while ignoring all insertions due to background speech. This change was based on a conjecture from a previous HLT paper that most of the added recognition error is from insertions due to background speech.&#10;2. By scoring the recognition results in this way, they were able to compare the number of overlapped words and spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome.&#10;3. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;4. An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings." target=": PhD G&#10;Content: um , s in terms of {disfmarker} in terms of number of words and also we computed something called a &quot; spurt &quot; , which is essentially a stretch of speech with uh , no pauses exceeding five hundred milliseconds . Um , and we computed how many overlapped i uh spurts there were and how many overlapped words there were . {vocalsound} Um , for four different {pause} corpora , the Meeting Recorder meetings , the Robustness meetings Switchboard and CallHome , and , found {disfmarker} and sort of compared the numbers . Um , and found that the , uh , you know , as you might expect the Meeting Recorder {pause} meetings had the most overlap uh , but next were Switchboard and CallHome , which both had roughly the same , almost identical in fact , and the Robustness meetings were {disfmarker} had the least , so {disfmarker} One sort of unexpected result there is that uh two - party telephone conversations have {vocalsound} about the same amount of overlap ,&#10;Speaker: Grad A&#10;Content: I 'm surprised .&#10;Speaker: PhD G&#10;Content: sort of in gen you">
      <data key="d0">1</data>
    </edge>
    <edge source="From the meeting transcript, the findings when the speakers rescored and fixed the transcripts were as follows:&#10;&#10;1. They added one number to their analysis, which involved scoring recognition results while ignoring all insertions due to background speech. This change was based on a conjecture from a previous HLT paper that most of the added recognition error is from insertions due to background speech.&#10;2. By scoring the recognition results in this way, they were able to compare the number of overlapped words and spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome.&#10;3. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;4. An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings." target="The conclusion from the HLT (Human Language Technology) results is that most of the added recognition error is caused by insertions due to background speech. To test this conjecture, the speakers rescored the recognition results in such a way that they ignored all insertions. This means that any words or spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) caused by background noise were not taken into account when scoring the recognition results. &#10;&#10;This new scoring method allowed them to compare the number of overlapped words and spurts between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;&#10;An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings. This suggests that background speech, rather than the number of parties or the setting, is a major source of recognition errors.">
      <data key="d0">1</data>
    </edge>
    <edge source="From the meeting transcript, the findings when the speakers rescored and fixed the transcripts were as follows:&#10;&#10;1. They added one number to their analysis, which involved scoring recognition results while ignoring all insertions due to background speech. This change was based on a conjecture from a previous HLT paper that most of the added recognition error is from insertions due to background speech.&#10;2. By scoring the recognition results in this way, they were able to compare the number of overlapped words and spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome.&#10;3. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;4. An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings." target="1. Number of speakers: The speech-nonspeech detection system performs better in meetings with fewer speakers. This is because there are fewer overlapping voices and background noises to confuse the system.&#10;2. Background noise: The system can be affected by background speech, as demonstrated by the fact that Meeting Recorder meetings had more overlap than Robustness meetings, even though both were meeting recordings. Additionally, telephone conversations (Switchboard and CallHome) had about the same amount of overlap as Meeting Recorder meetings, suggesting that background speech is a significant source of recognition errors.&#10;3. Channel quality: The system's performance can be impacted by the quality of the channels being used. If the laptop is in the background or there is breathy channel noise, it can cause false positives for speech detection. This can result in more missed speech, as demonstrated by PhD B's statement that a good meeting has around 1% or less of speech missed by the detector.&#10;4. Speech-nonspeech detector limitations: The speech-nonspeech detector may assign speech randomly to one of the channels when it is unable to accurately detect the source of the speech, which can lead to recognition errors.&#10;5. Adjustments for system performance: To improve the performance of the system, meeting segments with high background noise or multiple speakers might need to be excluded from the analysis. Additionally, efforts could be made to reduce background noise and ensure clearer audio channels during meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="From the meeting transcript, the findings when the speakers rescored and fixed the transcripts were as follows:&#10;&#10;1. They added one number to their analysis, which involved scoring recognition results while ignoring all insertions due to background speech. This change was based on a conjecture from a previous HLT paper that most of the added recognition error is from insertions due to background speech.&#10;2. By scoring the recognition results in this way, they were able to compare the number of overlapped words and spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome.&#10;3. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;4. An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings." target="Based on the transcript provided, the proposed solution for increasing the speed of the process being discussed, while still allowing for adjustments and thorough listening, seems to be using an automatic backchanneling system. This system, developed by a former student named Nigel Ward, could help indicate active listening in conversations without requiring as much time as manual backchanneling (e.g., saying &quot;mm-hmm&quot; or &quot;uh-huh&quot;). The professors are interested in exploring how this automatic system could be used to improve communication during their meetings.&#10;&#10;Additionally, Postdoc C mentions that &quot;listening does take time too,&quot; implying that they acknowledge the importance of dedicating sufficient time to listening during conversations. However, no specific solutions for adjusting or optimizing the timing of the process are explicitly mentioned in the transcript.&#10;&#10;Grad A suggests a specific value of &quot;one and a half times real time&quot; in relation to the discussion, but it is not clear whether this number is directly related to the proposed solution for increasing speed while maintaining quality listening and adjustments. It would be beneficial to have more context or information on where this value comes from and how it applies to the process being discussed.">
      <data key="d0">1</data>
    </edge>
    <edge source="From the meeting transcript, the findings when the speakers rescored and fixed the transcripts were as follows:&#10;&#10;1. They added one number to their analysis, which involved scoring recognition results while ignoring all insertions due to background speech. This change was based on a conjecture from a previous HLT paper that most of the added recognition error is from insertions due to background speech.&#10;2. By scoring the recognition results in this way, they were able to compare the number of overlapped words and spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome.&#10;3. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;4. An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings." target="Based on the meeting transcript, Don will be looking at prosodic properties as part of the team's focus now that they have the data and alignment. Prosodic properties likely include aspects such as intonation, stress, and rhythm in speech. This task was assigned by PhD G to Don during their discussion. Grad E also confirmed that they will be looking at prosodic properties.">
      <data key="d0">1</data>
    </edge>
    <edge source=" words to maybe {vocalsound} uh I don't know , eleven percent or something {disfmarker} it 's {disfmarker} it 's not a dramatic change ,&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: so it 's {disfmarker} Anyway , so it 's uh {disfmarker} That was {disfmarker} that was one set of {pause} results , and then the second one was just basically the {disfmarker} {vocalsound} the stuff we had in the {disfmarker} in the HLT paper on how overlaps effect the {pause} recognition performance .&#10;Speaker: Postdoc C&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Nope . Right .&#10;Speaker: Professor F&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: And we rescored things um , a little bit more carefully . We also fixed the transcripts in {disfmarker} in numerous ways . Uh , but mostly we added one {disfmarker} one number , which was what if you {pause" target="The conclusion from the HLT (Human Language Technology) results is that most of the added recognition error is caused by insertions due to background speech. To test this conjecture, the speakers rescored the recognition results in such a way that they ignored all insertions. This means that any words or spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) caused by background noise were not taken into account when scoring the recognition results. &#10;&#10;This new scoring method allowed them to compare the number of overlapped words and spurts between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;&#10;An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings. This suggests that background speech, rather than the number of parties or the setting, is a major source of recognition errors.">
      <data key="d0">1</data>
    </edge>
    <edge source=" words to maybe {vocalsound} uh I don't know , eleven percent or something {disfmarker} it 's {disfmarker} it 's not a dramatic change ,&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: so it 's {disfmarker} Anyway , so it 's uh {disfmarker} That was {disfmarker} that was one set of {pause} results , and then the second one was just basically the {disfmarker} {vocalsound} the stuff we had in the {disfmarker} in the HLT paper on how overlaps effect the {pause} recognition performance .&#10;Speaker: Postdoc C&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Nope . Right .&#10;Speaker: Professor F&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: And we rescored things um , a little bit more carefully . We also fixed the transcripts in {disfmarker} in numerous ways . Uh , but mostly we added one {disfmarker} one number , which was what if you {pause" target="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="disfmarker} in numerous ways . Uh , but mostly we added one {disfmarker} one number , which was what if you {pause} uh , basically score ignoring all {disfmarker} So {disfmarker} so the {disfmarker} the conjecture from the HLT results was that {vocalsound} most of the added recognition error is from insertions {vocalsound} due to background speech . So , we scored {vocalsound} all the recognition results , {vocalsound} uh , in such a way that the uh {disfmarker}&#10;Speaker: Grad A&#10;Content: Oh by the way , who 's on channel four ? You 're getting a lot of breath .&#10;Speaker: PhD B&#10;Content: Yeah . I j was just wondering .&#10;Speaker: Grad E&#10;Content: That 's {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: That 's me .&#10;Speaker: PhD G&#10;Content: uh , well Don 's been working hard .&#10;Speaker: Grad E&#10;Content: That 's right .&#10;Speaker: PhD" target="The conclusion from the HLT (Human Language Technology) results is that most of the added recognition error is caused by insertions due to background speech. To test this conjecture, the speakers rescored the recognition results in such a way that they ignored all insertions. This means that any words or spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) caused by background noise were not taken into account when scoring the recognition results. &#10;&#10;This new scoring method allowed them to compare the number of overlapped words and spurts between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;&#10;An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings. This suggests that background speech, rather than the number of parties or the setting, is a major source of recognition errors.">
      <data key="d0">1</data>
    </edge>
    <edge source=" . Yeah , that 's right .&#10;Speaker: PhD G&#10;Content: I think several people {disfmarker} sent this ,&#10;Speaker: Grad A&#10;Content: so .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Uh - huh .&#10;Speaker: PhD G&#10;Content: yeah .&#10;Speaker: Grad A&#10;Content: But any {disfmarker} any help you need I can certainly provide .&#10;Speaker: Professor F&#10;Content: Well ,&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: that 's {disfmarker} that 's a great idea .&#10;Speaker: PhD G&#10;Content: Well {disfmarker} there {disfmarker} there were some interesting results in this paper , though . For instance that Morgan {disfmarker} uh , accounted for fifty - six percent of the Robustness meetings in terms of number of words .&#10;Speaker: Grad A&#10;Content: Wow .&#10;Speaker: Postdoc C&#10;Content: In {disfmarker} in terms of what ? In term&#10;Speaker: PhD G" target="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source=" . Yeah , that 's right .&#10;Speaker: PhD G&#10;Content: I think several people {disfmarker} sent this ,&#10;Speaker: Grad A&#10;Content: so .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Uh - huh .&#10;Speaker: PhD G&#10;Content: yeah .&#10;Speaker: Grad A&#10;Content: But any {disfmarker} any help you need I can certainly provide .&#10;Speaker: Professor F&#10;Content: Well ,&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: that 's {disfmarker} that 's a great idea .&#10;Speaker: PhD G&#10;Content: Well {disfmarker} there {disfmarker} there were some interesting results in this paper , though . For instance that Morgan {disfmarker} uh , accounted for fifty - six percent of the Robustness meetings in terms of number of words .&#10;Speaker: Grad A&#10;Content: Wow .&#10;Speaker: Postdoc C&#10;Content: In {disfmarker} in terms of what ? In term&#10;Speaker: PhD G" target="The best approach for sharing the data with another person at SRI, as determined by PhD G, is using FTP. PhD G implemented this by creating a new directory for sharing the data, which he had previously been advised would be a good idea. The transcript suggests that PhD G has taken steps to make the necessary preparations for sharing the data via FTP, but it does not provide explicit information on whether the data has actually been shared and accessed by the person at SRI.">
      <data key="d0">1</data>
    </edge>
    <edge source=" . Yeah , that 's right .&#10;Speaker: PhD G&#10;Content: I think several people {disfmarker} sent this ,&#10;Speaker: Grad A&#10;Content: so .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Uh - huh .&#10;Speaker: PhD G&#10;Content: yeah .&#10;Speaker: Grad A&#10;Content: But any {disfmarker} any help you need I can certainly provide .&#10;Speaker: Professor F&#10;Content: Well ,&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: that 's {disfmarker} that 's a great idea .&#10;Speaker: PhD G&#10;Content: Well {disfmarker} there {disfmarker} there were some interesting results in this paper , though . For instance that Morgan {disfmarker} uh , accounted for fifty - six percent of the Robustness meetings in terms of number of words .&#10;Speaker: Grad A&#10;Content: Wow .&#10;Speaker: Postdoc C&#10;Content: In {disfmarker} in terms of what ? In term&#10;Speaker: PhD G" target="1. The paper written by PhD G has three types of results.&#10;2. One result is a measurement of overlap, which was calculated in terms of the number of words.&#10;3. Another result mentioned involves a computation, but further details about this are not provided in the transcript.&#10;4. The paper hasn't been reviewed yet by Professor F.">
      <data key="d0">1</data>
    </edge>
    <edge source=" . Yeah , that 's right .&#10;Speaker: PhD G&#10;Content: I think several people {disfmarker} sent this ,&#10;Speaker: Grad A&#10;Content: so .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Uh - huh .&#10;Speaker: PhD G&#10;Content: yeah .&#10;Speaker: Grad A&#10;Content: But any {disfmarker} any help you need I can certainly provide .&#10;Speaker: Professor F&#10;Content: Well ,&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: that 's {disfmarker} that 's a great idea .&#10;Speaker: PhD G&#10;Content: Well {disfmarker} there {disfmarker} there were some interesting results in this paper , though . For instance that Morgan {disfmarker} uh , accounted for fifty - six percent of the Robustness meetings in terms of number of words .&#10;Speaker: Grad A&#10;Content: Wow .&#10;Speaker: Postdoc C&#10;Content: In {disfmarker} in terms of what ? In term&#10;Speaker: PhD G" target="Based on the transcript provided, the group is open to the idea of normalizing the comparison of robustness between multi-party and two-party conversations by the number of speakers involved. This suggestion was raised by PhD D and supported by other participants, including PhD B, Postdoc C, and Professor F. They believe that this normalization could provide a more fair and accurate comparison between different types of conversations. However, the group does not make a definitive decision on this matter during the discussion, so further conversation may be necessary to reach a conclusion.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD G&#10;Content: right ? So we took the old recognition output and we just scored it differently . So the recognizer didn't have the benefit of knowing where the foreground speech {disfmarker} a start&#10;Speaker: Professor F&#10;Content: Were you including the {disfmarker} the lapel {pause} in this ?&#10;Speaker: PhD G&#10;Content: Yes .&#10;Speaker: Professor F&#10;Content: And did the {disfmarker} did {disfmarker} did the la did the {disfmarker} the problems with the lapel go away also ? Or {disfmarker}&#10;Speaker: PhD G&#10;Content: Um , it {disfmarker} Yeah .&#10;Speaker: Professor F&#10;Content: fray for {disfmarker} for insertions ?&#10;Speaker: PhD G&#10;Content: It u not per {disfmarker} I mean , not completely , but yes ,&#10;Speaker: Professor F&#10;Content: Less so .&#10;Speaker: PhD G&#10;Content: dramatically . So we have to um {disfmarker}&#10;Speaker: Professor F&#10;Content: I mean , you still {" target="The conclusion from the HLT (Human Language Technology) results is that most of the added recognition error is caused by insertions due to background speech. To test this conjecture, the speakers rescored the recognition results in such a way that they ignored all insertions. This means that any words or spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) caused by background noise were not taken into account when scoring the recognition results. &#10;&#10;This new scoring method allowed them to compare the number of overlapped words and spurts between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;&#10;An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings. This suggests that background speech, rather than the number of parties or the setting, is a major source of recognition errors.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD G&#10;Content: right ? So we took the old recognition output and we just scored it differently . So the recognizer didn't have the benefit of knowing where the foreground speech {disfmarker} a start&#10;Speaker: Professor F&#10;Content: Were you including the {disfmarker} the lapel {pause} in this ?&#10;Speaker: PhD G&#10;Content: Yes .&#10;Speaker: Professor F&#10;Content: And did the {disfmarker} did {disfmarker} did the la did the {disfmarker} the problems with the lapel go away also ? Or {disfmarker}&#10;Speaker: PhD G&#10;Content: Um , it {disfmarker} Yeah .&#10;Speaker: Professor F&#10;Content: fray for {disfmarker} for insertions ?&#10;Speaker: PhD G&#10;Content: It u not per {disfmarker} I mean , not completely , but yes ,&#10;Speaker: Professor F&#10;Content: Less so .&#10;Speaker: PhD G&#10;Content: dramatically . So we have to um {disfmarker}&#10;Speaker: Professor F&#10;Content: I mean , you still {" target="1. The old recognition output was scored differently, but the recognizer was not provided with information about where the foreground speech starts.&#10;2. The old recognition output was modified to include the lapel microphone, which was previously excluded.&#10;3. Although the issues with the lapel microphone were not explicitly stated as resolved, they were discussed less frequently in comparison to other topics. This could imply that the problems were mitigated but may not have been completely eliminated.&#10;&#10;In summary, the old recognition output underwent a change in scoring and began including data from the lapel microphone. While the resolution of issues related to the lapel microphone was not explicitly confirmed, there was less discussion about those problems.">
      <data key="d0">1</data>
    </edge>
    <edge source="The conclusion from the HLT (Human Language Technology) results is that most of the added recognition error is caused by insertions due to background speech. To test this conjecture, the speakers rescored the recognition results in such a way that they ignored all insertions. This means that any words or spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) caused by background noise were not taken into account when scoring the recognition results. &#10;&#10;This new scoring method allowed them to compare the number of overlapped words and spurts between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;&#10;An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings. This suggests that background speech, rather than the number of parties or the setting, is a major source of recognition errors." target="isfmarker} we {disfmarker} {vocalsound} Uh , Don did some ha hand - checking and {disfmarker} and we think that {disfmarker} based on that , we think that the results are you know , valid , although of course , some error is gonna be in there . But basically what we found is after we take out these regions {disfmarker} so we only score the regions that were certified as foreground speech , {comment} {vocalsound} the recognition error went down to almost {vocalsound} uh , the {pause} level of the non - overlapped {pause} speech . So that means that {vocalsound} even if you do have background speech , if you can somehow separate out or find where it is , {vocalsound} uh , the recognizer does a good job ,&#10;Speaker: Grad A&#10;Content: That 's great .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: even though there is this back&#10;Speaker: Grad A&#10;Content: Yeah , I guess that doesn't surprise me , because , with the close - talking mikes , the {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="The conclusion from the HLT (Human Language Technology) results is that most of the added recognition error is caused by insertions due to background speech. To test this conjecture, the speakers rescored the recognition results in such a way that they ignored all insertions. This means that any words or spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) caused by background noise were not taken into account when scoring the recognition results. &#10;&#10;This new scoring method allowed them to compare the number of overlapped words and spurts between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;&#10;An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings. This suggests that background speech, rather than the number of parties or the setting, is a major source of recognition errors." target="aker: Postdoc C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: I mean I get these AARP things , but I 'm not se really senior yet , but {disfmarker}&#10;Speaker: PhD G&#10;Content: Right&#10;Speaker: Professor F&#10;Content: Um ,&#10;Speaker: PhD G&#10;Content: Hmm .&#10;Speaker: Professor F&#10;Content: but uh , other than that delightful result , what was the rest of the paper about ?&#10;Speaker: PhD G&#10;Content: Um , well it was about {disfmarker} it had three sections&#10;Speaker: Professor F&#10;Content: You sent it to me but I haven't seen it yet .&#10;Speaker: PhD G&#10;Content: uh {disfmarker} three kinds of uh results , if you will . Uh , the one was that the {disfmarker} just the {disfmarker} the amount of overlap&#10;Speaker: Grad A&#10;Content: The good , the bad , and the ugly .&#10;Speaker: PhD G&#10;Content: um , s in terms of {disfmarker} in terms of number of words and also we computed something called a &quot;">
      <data key="d0">1</data>
    </edge>
    <edge source="The conclusion from the HLT (Human Language Technology) results is that most of the added recognition error is caused by insertions due to background speech. To test this conjecture, the speakers rescored the recognition results in such a way that they ignored all insertions. This means that any words or spurts (a stretch of speech with no pauses exceeding five hundred milliseconds) caused by background noise were not taken into account when scoring the recognition results. &#10;&#10;This new scoring method allowed them to compare the number of overlapped words and spurts between four different corpora: Meeting Recorder meetings, Robustness meetings, Switchboard, and CallHome. They found that Meeting Recorder meetings had the most overlap, followed by Switchboard and CallHome, which both had roughly the same amount of overlap. The Robustness meetings had the least overlap.&#10;&#10;An unexpected result was that two-party telephone conversations (Switchboard and CallHome) had about the same amount of overlap as the Meeting Recorder meetings. This suggests that background speech, rather than the number of parties or the setting, is a major source of recognition errors." target="1. Number of speakers: The speech-nonspeech detection system performs better in meetings with fewer speakers. This is because there are fewer overlapping voices and background noises to confuse the system.&#10;2. Background noise: The system can be affected by background speech, as demonstrated by the fact that Meeting Recorder meetings had more overlap than Robustness meetings, even though both were meeting recordings. Additionally, telephone conversations (Switchboard and CallHome) had about the same amount of overlap as Meeting Recorder meetings, suggesting that background speech is a significant source of recognition errors.&#10;3. Channel quality: The system's performance can be impacted by the quality of the channels being used. If the laptop is in the background or there is breathy channel noise, it can cause false positives for speech detection. This can result in more missed speech, as demonstrated by PhD B's statement that a good meeting has around 1% or less of speech missed by the detector.&#10;4. Speech-nonspeech detector limitations: The speech-nonspeech detector may assign speech randomly to one of the channels when it is unable to accurately detect the source of the speech, which can lead to recognition errors.&#10;5. Adjustments for system performance: To improve the performance of the system, meeting segments with high background noise or multiple speakers might need to be excluded from the analysis. Additionally, efforts could be made to reduce background noise and ensure clearer audio channels during meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker} we {disfmarker} {vocalsound} Uh , Don did some ha hand - checking and {disfmarker} and we think that {disfmarker} based on that , we think that the results are you know , valid , although of course , some error is gonna be in there . But basically what we found is after we take out these regions {disfmarker} so we only score the regions that were certified as foreground speech , {comment} {vocalsound} the recognition error went down to almost {vocalsound} uh , the {pause} level of the non - overlapped {pause} speech . So that means that {vocalsound} even if you do have background speech , if you can somehow separate out or find where it is , {vocalsound} uh , the recognizer does a good job ,&#10;Speaker: Grad A&#10;Content: That 's great .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: even though there is this back&#10;Speaker: Grad A&#10;Content: Yeah , I guess that doesn't surprise me , because , with the close - talking mikes , the {disfmark" target="1. The old recognition output was scored differently, but the recognizer was not provided with information about where the foreground speech starts.&#10;2. The old recognition output was modified to include the lapel microphone, which was previously excluded.&#10;3. Although the issues with the lapel microphone were not explicitly stated as resolved, they were discussed less frequently in comparison to other topics. This could imply that the problems were mitigated but may not have been completely eliminated.&#10;&#10;In summary, the old recognition output underwent a change in scoring and began including data from the lapel microphone. While the resolution of issues related to the lapel microphone was not explicitly confirmed, there was less discussion about those problems.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: Postdoc C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: I mean I get these AARP things , but I 'm not se really senior yet , but {disfmarker}&#10;Speaker: PhD G&#10;Content: Right&#10;Speaker: Professor F&#10;Content: Um ,&#10;Speaker: PhD G&#10;Content: Hmm .&#10;Speaker: Professor F&#10;Content: but uh , other than that delightful result , what was the rest of the paper about ?&#10;Speaker: PhD G&#10;Content: Um , well it was about {disfmarker} it had three sections&#10;Speaker: Professor F&#10;Content: You sent it to me but I haven't seen it yet .&#10;Speaker: PhD G&#10;Content: uh {disfmarker} three kinds of uh results , if you will . Uh , the one was that the {disfmarker} just the {disfmarker} the amount of overlap&#10;Speaker: Grad A&#10;Content: The good , the bad , and the ugly .&#10;Speaker: PhD G&#10;Content: um , s in terms of {disfmarker} in terms of number of words and also we computed something called a &quot;" target="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: Postdoc C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: I mean I get these AARP things , but I 'm not se really senior yet , but {disfmarker}&#10;Speaker: PhD G&#10;Content: Right&#10;Speaker: Professor F&#10;Content: Um ,&#10;Speaker: PhD G&#10;Content: Hmm .&#10;Speaker: Professor F&#10;Content: but uh , other than that delightful result , what was the rest of the paper about ?&#10;Speaker: PhD G&#10;Content: Um , well it was about {disfmarker} it had three sections&#10;Speaker: Professor F&#10;Content: You sent it to me but I haven't seen it yet .&#10;Speaker: PhD G&#10;Content: uh {disfmarker} three kinds of uh results , if you will . Uh , the one was that the {disfmarker} just the {disfmarker} the amount of overlap&#10;Speaker: Grad A&#10;Content: The good , the bad , and the ugly .&#10;Speaker: PhD G&#10;Content: um , s in terms of {disfmarker} in terms of number of words and also we computed something called a &quot;" target="The best approach for sharing the data with another person at SRI, as determined by PhD G, is using FTP. PhD G implemented this by creating a new directory for sharing the data, which he had previously been advised would be a good idea. The transcript suggests that PhD G has taken steps to make the necessary preparations for sharing the data via FTP, but it does not provide explicit information on whether the data has actually been shared and accessed by the person at SRI.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: Postdoc C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: I mean I get these AARP things , but I 'm not se really senior yet , but {disfmarker}&#10;Speaker: PhD G&#10;Content: Right&#10;Speaker: Professor F&#10;Content: Um ,&#10;Speaker: PhD G&#10;Content: Hmm .&#10;Speaker: Professor F&#10;Content: but uh , other than that delightful result , what was the rest of the paper about ?&#10;Speaker: PhD G&#10;Content: Um , well it was about {disfmarker} it had three sections&#10;Speaker: Professor F&#10;Content: You sent it to me but I haven't seen it yet .&#10;Speaker: PhD G&#10;Content: uh {disfmarker} three kinds of uh results , if you will . Uh , the one was that the {disfmarker} just the {disfmarker} the amount of overlap&#10;Speaker: Grad A&#10;Content: The good , the bad , and the ugly .&#10;Speaker: PhD G&#10;Content: um , s in terms of {disfmarker} in terms of number of words and also we computed something called a &quot;" target="1. The paper written by PhD G has three types of results.&#10;2. One result is a measurement of overlap, which was calculated in terms of the number of words.&#10;3. Another result mentioned involves a computation, but further details about this are not provided in the transcript.&#10;4. The paper hasn't been reviewed yet by Professor F.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: Postdoc C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: I mean I get these AARP things , but I 'm not se really senior yet , but {disfmarker}&#10;Speaker: PhD G&#10;Content: Right&#10;Speaker: Professor F&#10;Content: Um ,&#10;Speaker: PhD G&#10;Content: Hmm .&#10;Speaker: Professor F&#10;Content: but uh , other than that delightful result , what was the rest of the paper about ?&#10;Speaker: PhD G&#10;Content: Um , well it was about {disfmarker} it had three sections&#10;Speaker: Professor F&#10;Content: You sent it to me but I haven't seen it yet .&#10;Speaker: PhD G&#10;Content: uh {disfmarker} three kinds of uh results , if you will . Uh , the one was that the {disfmarker} just the {disfmarker} the amount of overlap&#10;Speaker: Grad A&#10;Content: The good , the bad , and the ugly .&#10;Speaker: PhD G&#10;Content: um , s in terms of {disfmarker} in terms of number of words and also we computed something called a &quot;" target="1. The specific content of the paper that Graduate Student A printed out but hasn't read yet is not mentioned in the transcript. However, it is clear that the paper is related to a Monty Python sketch and has been submitted for Eurospeech.&#10;2. PhD Student G is planning to send an email to Brian Kingbury about where he can find the related material because Brian had previously requested this material for a speech recognition experiment. PhD G was in the process of sending the location of the shared directory when his desktop froze, causing him to delay the email. The transcript suggests that PhD G has prepared a shared directory via FTP to send to Brian, but it does not explicitly confirm whether the email has been sent or the material has been accessed by Brian at SRI.">
      <data key="d0">1</data>
    </edge>
    <edge source="The 'ASR 0.1' directory, as mentioned in the meeting, is meant for speech recognition purposes. The name 'ASR 0.1' stands for &quot;Automatic Speech Recognition&quot; and signifies its version or iteration (0.1). This directory utilizes a quick and dirty form of access control; to access any files within it, one must be told the exact name of the file or subdirectory since it is not directly visible or readable.&#10;&#10;The purpose of this structure is to ensure that only individuals with explicit permission can access the files within the directory. By using a non-standard approach to access control, the creators are able to restrict access and maintain better control over who can view or edit the contents. Additionally, implementing such a system can be beneficial for privacy and security purposes, especially when dealing with sensitive data or information.&#10;&#10;The 'ASR 0.1' directory likely contains various files related to speech recognition research, experiments, or projects that the team is working on. It could include audio files, transcription scripts, software tools, and other resources necessary for ASR development. The access control mechanism discussed in the meeting ensures that only authorized individuals can view and interact with these materials, making collaboration more secure and organized." target=" . Um ,&#10;Speaker: Professor F&#10;Content: Yeah&#10;Speaker: PhD G&#10;Content: and then under there {disfmarker} Um actually {disfmarker} Oh and this directory , {vocalsound} is not readable . It 's only uh , accessible . So , {vocalsound} in other words , to access anything under there , you have to {vocalsound} be told what the name is .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: So that 's sort of a g {vocalsound} quick and dirty way of doing access control .&#10;Speaker: Professor F&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: So {disfmarker} uh , and the directory for this I call it I &quot; ASR zero point one &quot; because it 's sort of meant for recognition .&#10;Speaker: Professor F&#10;Content: So anyone who hears this meeting now knows the {disfmarker}&#10;Speaker: Grad A&#10;Content: Beta ?&#10;Speaker: PhD G&#10;Content: And then {disfmarker} then in there I have a file that lists">
      <data key="d0">1</data>
    </edge>
    <edge source="The 'ASR 0.1' directory, as mentioned in the meeting, is meant for speech recognition purposes. The name 'ASR 0.1' stands for &quot;Automatic Speech Recognition&quot; and signifies its version or iteration (0.1). This directory utilizes a quick and dirty form of access control; to access any files within it, one must be told the exact name of the file or subdirectory since it is not directly visible or readable.&#10;&#10;The purpose of this structure is to ensure that only individuals with explicit permission can access the files within the directory. By using a non-standard approach to access control, the creators are able to restrict access and maintain better control over who can view or edit the contents. Additionally, implementing such a system can be beneficial for privacy and security purposes, especially when dealing with sensitive data or information.&#10;&#10;The 'ASR 0.1' directory likely contains various files related to speech recognition research, experiments, or projects that the team is working on. It could include audio files, transcription scripts, software tools, and other resources necessary for ASR development. The access control mechanism discussed in the meeting ensures that only authorized individuals can view and interact with these materials, making collaboration more secure and organized." target="comment} {pause} or um , we should just make a note of it .&#10;Speaker: PhD G&#10;Content: OK . Oh . Beca - Well {disfmarker} OK , because in one directory there 's two versions .&#10;Speaker: Grad E&#10;Content: Yeah , that 's the first meeting I cut both versions . Just to check which w if there is a significant difference .&#10;Speaker: PhD G&#10;Content: OK . And so I {disfmarker} but {disfmarker} OK so {disfmarker} but for the other meetings it 's the downsampled version that you have .&#10;Speaker: Grad E&#10;Content: They 're all downsampled , yeah .&#10;Speaker: PhD G&#10;Content: Oh , OK . Oh that 's th important to know , OK so we should probably {disfmarker} uh {pause} give them the non - downsampled versions .&#10;Speaker: Grad E&#10;Content: Yeah . So {disfmarker}&#10;Speaker: PhD G&#10;Content: OK . Alright , then I 'll hold off on that and I 'll wait for you um {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="The 'ASR 0.1' directory, as mentioned in the meeting, is meant for speech recognition purposes. The name 'ASR 0.1' stands for &quot;Automatic Speech Recognition&quot; and signifies its version or iteration (0.1). This directory utilizes a quick and dirty form of access control; to access any files within it, one must be told the exact name of the file or subdirectory since it is not directly visible or readable.&#10;&#10;The purpose of this structure is to ensure that only individuals with explicit permission can access the files within the directory. By using a non-standard approach to access control, the creators are able to restrict access and maintain better control over who can view or edit the contents. Additionally, implementing such a system can be beneficial for privacy and security purposes, especially when dealing with sensitive data or information.&#10;&#10;The 'ASR 0.1' directory likely contains various files related to speech recognition research, experiments, or projects that the team is working on. It could include audio files, transcription scripts, software tools, and other resources necessary for ASR development. The access control mechanism discussed in the meeting ensures that only authorized individuals can view and interact with these materials, making collaboration more secure and organized." target="disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah but {disfmarker} but {disfmarker} but he has to {disfmarker}&#10;Speaker: Postdoc C&#10;Content: I 'd hafta add him to Meeting Recorder , I guess ,&#10;Speaker: PhD G&#10;Content: he prefe he said he would prefer FTP&#10;Speaker: Postdoc C&#10;Content: but {disfmarker} OK .&#10;Speaker: PhD G&#10;Content: and also , um , the other person that wants it {disfmarker} There is one person at SRI who wants to look at the {vocalsound} um , you know , the uh {disfmarker} the data we have so far ,&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: and so I figured that FTP is the best {pause} approach . So what I did is I um {disfmarker} {vocalsound} {vocalsound} @ @ {comment} I made a n new directory after Chuck said that would c that was gonna be a good thing . Uh , so it 's &quot;">
      <data key="d0">1</data>
    </edge>
    <edge source="The 'ASR 0.1' directory, as mentioned in the meeting, is meant for speech recognition purposes. The name 'ASR 0.1' stands for &quot;Automatic Speech Recognition&quot; and signifies its version or iteration (0.1). This directory utilizes a quick and dirty form of access control; to access any files within it, one must be told the exact name of the file or subdirectory since it is not directly visible or readable.&#10;&#10;The purpose of this structure is to ensure that only individuals with explicit permission can access the files within the directory. By using a non-standard approach to access control, the creators are able to restrict access and maintain better control over who can view or edit the contents. Additionally, implementing such a system can be beneficial for privacy and security purposes, especially when dealing with sensitive data or information.&#10;&#10;The 'ASR 0.1' directory likely contains various files related to speech recognition research, experiments, or projects that the team is working on. It could include audio files, transcription scripts, software tools, and other resources necessary for ASR development. The access control mechanism discussed in the meeting ensures that only authorized individuals can view and interact with these materials, making collaboration more secure and organized." target=" about the IBM transcription process stuff that {disfmarker}&#10;Speaker: Postdoc C&#10;Content: OK . So , um you know that Adam created um , a b a script to generate the beep file ?&#10;Speaker: Professor F&#10;Content: Hmm .&#10;Speaker: Postdoc C&#10;Content: To then create something to send to IBM . And , um , you {disfmarker} you should probably talk about that . But {disfmarker} but you were gonna to use the {pause} originally transcribed file because I tightened the time bins and that 's also the one that they had already {vocalsound} in trying to debug the first stage of this . And uh , my understanding was that , um {disfmarker} I haven't {disfmarker} {vocalsound} I haven't listened to it yet ,&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: Postdoc C&#10;Content: but it sounded very good and {disfmarker} and I understand that you guys {vocalsound} were going to have a meeting today , before this meeting .&#10;Speaker: Grad A&#10;Content: It was just">
      <data key="d0">1</data>
    </edge>
    <edge source="The 'ASR 0.1' directory, as mentioned in the meeting, is meant for speech recognition purposes. The name 'ASR 0.1' stands for &quot;Automatic Speech Recognition&quot; and signifies its version or iteration (0.1). This directory utilizes a quick and dirty form of access control; to access any files within it, one must be told the exact name of the file or subdirectory since it is not directly visible or readable.&#10;&#10;The purpose of this structure is to ensure that only individuals with explicit permission can access the files within the directory. By using a non-standard approach to access control, the creators are able to restrict access and maintain better control over who can view or edit the contents. Additionally, implementing such a system can be beneficial for privacy and security purposes, especially when dealing with sensitive data or information.&#10;&#10;The 'ASR 0.1' directory likely contains various files related to speech recognition research, experiments, or projects that the team is working on. It could include audio files, transcription scripts, software tools, and other resources necessary for ASR development. The access control mechanism discussed in the meeting ensures that only authorized individuals can view and interact with these materials, making collaboration more secure and organized." target="disfmarker} we should {vocalsound} leave the {vocalsound} part with the audio in the uh , beep file that we send to IBM for that one , or should we {vocalsound} start after the {disfmarker} that part of the meeting is over in what we send .&#10;Speaker: Professor F&#10;Content: Which part ?&#10;Speaker: PhD B&#10;Content: With {disfmarker}&#10;Speaker: Postdoc C&#10;Content: So , the part where they 're using sounds from their {disfmarker} from their laptops .&#10;Speaker: PhD B&#10;Content: with the laptop sound , or {disfmarker} ? just {disfmarker}&#10;Speaker: Postdoc C&#10;Content: w If we have speech from the laptop should we just uh , excise that from what we send to IBM , or should we {vocalsound} i give it to them and let them do with it what they can ?&#10;Speaker: PhD D&#10;Content: I think we should just {disfmarker} it {disfmarker} it 's gonna be too much work if we hafta {vocalsound">
      <data key="d0">1</data>
    </edge>
    <edge source="The 'ASR 0.1' directory, as mentioned in the meeting, is meant for speech recognition purposes. The name 'ASR 0.1' stands for &quot;Automatic Speech Recognition&quot; and signifies its version or iteration (0.1). This directory utilizes a quick and dirty form of access control; to access any files within it, one must be told the exact name of the file or subdirectory since it is not directly visible or readable.&#10;&#10;The purpose of this structure is to ensure that only individuals with explicit permission can access the files within the directory. By using a non-standard approach to access control, the creators are able to restrict access and maintain better control over who can view or edit the contents. Additionally, implementing such a system can be beneficial for privacy and security purposes, especially when dealing with sensitive data or information.&#10;&#10;The 'ASR 0.1' directory likely contains various files related to speech recognition research, experiments, or projects that the team is working on. It could include audio files, transcription scripts, software tools, and other resources necessary for ASR development. The access control mechanism discussed in the meeting ensures that only authorized individuals can view and interact with these materials, making collaboration more secure and organized." target="Based on the transcript provided, the proposed solution for increasing the speed of the process being discussed, while still allowing for adjustments and thorough listening, seems to be using an automatic backchanneling system. This system, developed by a former student named Nigel Ward, could help indicate active listening in conversations without requiring as much time as manual backchanneling (e.g., saying &quot;mm-hmm&quot; or &quot;uh-huh&quot;). The professors are interested in exploring how this automatic system could be used to improve communication during their meetings.&#10;&#10;Additionally, Postdoc C mentions that &quot;listening does take time too,&quot; implying that they acknowledge the importance of dedicating sufficient time to listening during conversations. However, no specific solutions for adjusting or optimizing the timing of the process are explicitly mentioned in the transcript.&#10;&#10;Grad A suggests a specific value of &quot;one and a half times real time&quot; in relation to the discussion, but it is not clear whether this number is directly related to the proposed solution for increasing speed while maintaining quality listening and adjustments. It would be beneficial to have more context or information on where this value comes from and how it applies to the process being discussed.">
      <data key="d0">1</data>
    </edge>
    <edge source="disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah but {disfmarker} but {disfmarker} but he has to {disfmarker}&#10;Speaker: Postdoc C&#10;Content: I 'd hafta add him to Meeting Recorder , I guess ,&#10;Speaker: PhD G&#10;Content: he prefe he said he would prefer FTP&#10;Speaker: Postdoc C&#10;Content: but {disfmarker} OK .&#10;Speaker: PhD G&#10;Content: and also , um , the other person that wants it {disfmarker} There is one person at SRI who wants to look at the {vocalsound} um , you know , the uh {disfmarker} the data we have so far ,&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: and so I figured that FTP is the best {pause} approach . So what I did is I um {disfmarker} {vocalsound} {vocalsound} @ @ {comment} I made a n new directory after Chuck said that would c that was gonna be a good thing . Uh , so it 's &quot;" target="The best approach for sharing the data with another person at SRI, as determined by PhD G, is using FTP. PhD G implemented this by creating a new directory for sharing the data, which he had previously been advised would be a good idea. The transcript suggests that PhD G has taken steps to make the necessary preparations for sharing the data via FTP, but it does not provide explicit information on whether the data has actually been shared and accessed by the person at SRI.">
      <data key="d0">1</data>
    </edge>
    <edge source="disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah but {disfmarker} but {disfmarker} but he has to {disfmarker}&#10;Speaker: Postdoc C&#10;Content: I 'd hafta add him to Meeting Recorder , I guess ,&#10;Speaker: PhD G&#10;Content: he prefe he said he would prefer FTP&#10;Speaker: Postdoc C&#10;Content: but {disfmarker} OK .&#10;Speaker: PhD G&#10;Content: and also , um , the other person that wants it {disfmarker} There is one person at SRI who wants to look at the {vocalsound} um , you know , the uh {disfmarker} the data we have so far ,&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: and so I figured that FTP is the best {pause} approach . So what I did is I um {disfmarker} {vocalsound} {vocalsound} @ @ {comment} I made a n new directory after Chuck said that would c that was gonna be a good thing . Uh , so it 's &quot;" target="The proposed approach for sharing data with another person at SRI, as discussed by PhD G, is using File Transfer Protocol (FTP). PhD G believes that FTP is the best method for several reasons. First, he has created a new directory specifically for sharing the data after being advised that it would be a good idea. This new directory will allow the other person at SRI to access the necessary data easily. While the transcript does not explicitly confirm that the data has been shared and accessed via FTP, PhD G's preparations indicate that he is actively working on implementing this method for sharing the data.">
      <data key="d0">1</data>
    </edge>
    <edge source=" about the IBM transcription process stuff that {disfmarker}&#10;Speaker: Postdoc C&#10;Content: OK . So , um you know that Adam created um , a b a script to generate the beep file ?&#10;Speaker: Professor F&#10;Content: Hmm .&#10;Speaker: Postdoc C&#10;Content: To then create something to send to IBM . And , um , you {disfmarker} you should probably talk about that . But {disfmarker} but you were gonna to use the {pause} originally transcribed file because I tightened the time bins and that 's also the one that they had already {vocalsound} in trying to debug the first stage of this . And uh , my understanding was that , um {disfmarker} I haven't {disfmarker} {vocalsound} I haven't listened to it yet ,&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: Postdoc C&#10;Content: but it sounded very good and {disfmarker} and I understand that you guys {vocalsound} were going to have a meeting today , before this meeting .&#10;Speaker: Grad A&#10;Content: It was just" target="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process.">
      <data key="d0">1</data>
    </edge>
    <edge source=" about the IBM transcription process stuff that {disfmarker}&#10;Speaker: Postdoc C&#10;Content: OK . So , um you know that Adam created um , a b a script to generate the beep file ?&#10;Speaker: Professor F&#10;Content: Hmm .&#10;Speaker: Postdoc C&#10;Content: To then create something to send to IBM . And , um , you {disfmarker} you should probably talk about that . But {disfmarker} but you were gonna to use the {pause} originally transcribed file because I tightened the time bins and that 's also the one that they had already {vocalsound} in trying to debug the first stage of this . And uh , my understanding was that , um {disfmarker} I haven't {disfmarker} {vocalsound} I haven't listened to it yet ,&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: Postdoc C&#10;Content: but it sounded very good and {disfmarker} and I understand that you guys {vocalsound} were going to have a meeting today , before this meeting .&#10;Speaker: Grad A&#10;Content: It was just" target="1. The plan is to compare the two processes of creating a chunked file for IBM transcription services and applying the beep-ify method (adding beeps at regular intervals) on both versions. They will listen to the output of both methods and look for major differences between them.&#10;   &#10;2. When doing the process by hand, they will pay attention to factors such as consistent volume levels and beep duration throughout the recording. This is where normalization coefficients come into play, as these values are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. They ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;&#10;3. One major difference they might look for is whether there's any confusion caused by including a section where some participants were playing sounds from their laptops during the meeting. This could potentially cause issues with differentiating between actual speech and background noise, as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels.&#10;&#10;4. Ultimately, they have decided to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also plan on having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;&#10;5. This discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work.">
      <data key="d0">1</data>
    </edge>
    <edge source="disfmarker} we should {vocalsound} leave the {vocalsound} part with the audio in the uh , beep file that we send to IBM for that one , or should we {vocalsound} start after the {disfmarker} that part of the meeting is over in what we send .&#10;Speaker: Professor F&#10;Content: Which part ?&#10;Speaker: PhD B&#10;Content: With {disfmarker}&#10;Speaker: Postdoc C&#10;Content: So , the part where they 're using sounds from their {disfmarker} from their laptops .&#10;Speaker: PhD B&#10;Content: with the laptop sound , or {disfmarker} ? just {disfmarker}&#10;Speaker: Postdoc C&#10;Content: w If we have speech from the laptop should we just uh , excise that from what we send to IBM , or should we {vocalsound} i give it to them and let them do with it what they can ?&#10;Speaker: PhD D&#10;Content: I think we should just {disfmarker} it {disfmarker} it 's gonna be too much work if we hafta {vocalsound" target="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Number of speakers: The speech-nonspeech detection system performs better in meetings with fewer speakers. This is because there are fewer overlapping voices and background noises to confuse the system.&#10;2. Background noise: The system can be affected by background speech, as demonstrated by the fact that Meeting Recorder meetings had more overlap than Robustness meetings, even though both were meeting recordings. Additionally, telephone conversations (Switchboard and CallHome) had about the same amount of overlap as Meeting Recorder meetings, suggesting that background speech is a significant source of recognition errors.&#10;3. Channel quality: The system's performance can be impacted by the quality of the channels being used. If the laptop is in the background or there is breathy channel noise, it can cause false positives for speech detection. This can result in more missed speech, as demonstrated by PhD B's statement that a good meeting has around 1% or less of speech missed by the detector.&#10;4. Speech-nonspeech detector limitations: The speech-nonspeech detector may assign speech randomly to one of the channels when it is unable to accurately detect the source of the speech, which can lead to recognition errors.&#10;5. Adjustments for system performance: To improve the performance of the system, meeting segments with high background noise or multiple speakers might need to be excluded from the analysis. Additionally, efforts could be made to reduce background noise and ensure clearer audio channels during meetings." target=" Postdoc C&#10;Content: Now , you were saying that they {disfmarker} they differ in how well they work depending on channel s sys systems and stuff .&#10;Speaker: PhD B&#10;Content: Yeah . So we should perhaps just select meetings on which the speech - nonspeech detection works well ,&#10;Speaker: Postdoc C&#10;Content: But EDU is great .&#10;Speaker: PhD B&#10;Content: and just use , {vocalsound} those meetings to {disfmarker} to {disfmarker} to send to IBM and , do the other ones .&#10;Speaker: Grad A&#10;Content: Release to begin with .&#10;Speaker: Postdoc C&#10;Content: How interesting . You know {disfmarker}&#10;Speaker: Professor F&#10;Content: What 's the problem {disfmarker} the l I forget . Is the problem the lapel , or {disfmarker} or {disfmarker}&#10;Speaker: PhD B&#10;Content: Uh , it really depends . Um , my {disfmarker} my {disfmarker} my impression is that it 's better for meetings with fewer speakers , and it 's">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Number of speakers: The speech-nonspeech detection system performs better in meetings with fewer speakers. This is because there are fewer overlapping voices and background noises to confuse the system.&#10;2. Background noise: The system can be affected by background speech, as demonstrated by the fact that Meeting Recorder meetings had more overlap than Robustness meetings, even though both were meeting recordings. Additionally, telephone conversations (Switchboard and CallHome) had about the same amount of overlap as Meeting Recorder meetings, suggesting that background speech is a significant source of recognition errors.&#10;3. Channel quality: The system's performance can be impacted by the quality of the channels being used. If the laptop is in the background or there is breathy channel noise, it can cause false positives for speech detection. This can result in more missed speech, as demonstrated by PhD B's statement that a good meeting has around 1% or less of speech missed by the detector.&#10;4. Speech-nonspeech detector limitations: The speech-nonspeech detector may assign speech randomly to one of the channels when it is unable to accurately detect the source of the speech, which can lead to recognition errors.&#10;5. Adjustments for system performance: To improve the performance of the system, meeting segments with high background noise or multiple speakers might need to be excluded from the analysis. Additionally, efforts could be made to reduce background noise and ensure clearer audio channels during meetings." target=" {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Hmm !&#10;Speaker: PhD B&#10;Content: Let me think . So the {pause} speech {disfmarker} the amount of speech that is missed by the {pause} detector , for a good meeting , I th is around {pause} or under one percent , I would say . But there can be {disfmarker} Yeah . For {disfmarker} yeah , but there can be more {disfmarker} There 's {disfmarker} There 's more amount speech {disfmarker} uh , more amount of {disfmarker} Yeah well , the detector says there is speech , but there is none . So that {disfmarker} that can be a lot when {disfmarker} when it 's really a breathy channel .&#10;Speaker: Professor F&#10;Content: But I think that 's less of a problem .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: They 'll just listen . It 's just wasted time .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Number of speakers: The speech-nonspeech detection system performs better in meetings with fewer speakers. This is because there are fewer overlapping voices and background noises to confuse the system.&#10;2. Background noise: The system can be affected by background speech, as demonstrated by the fact that Meeting Recorder meetings had more overlap than Robustness meetings, even though both were meeting recordings. Additionally, telephone conversations (Switchboard and CallHome) had about the same amount of overlap as Meeting Recorder meetings, suggesting that background speech is a significant source of recognition errors.&#10;3. Channel quality: The system's performance can be impacted by the quality of the channels being used. If the laptop is in the background or there is breathy channel noise, it can cause false positives for speech detection. This can result in more missed speech, as demonstrated by PhD B's statement that a good meeting has around 1% or less of speech missed by the detector.&#10;4. Speech-nonspeech detector limitations: The speech-nonspeech detector may assign speech randomly to one of the channels when it is unable to accurately detect the source of the speech, which can lead to recognition errors.&#10;5. Adjustments for system performance: To improve the performance of the system, meeting segments with high background noise or multiple speakers might need to be excluded from the analysis. Additionally, efforts could be made to reduce background noise and ensure clearer audio channels during meetings." target=" is playing sound from his laptop .&#10;Speaker: Grad A&#10;Content: Uh - huh&#10;Speaker: PhD B&#10;Content: And i {vocalsound} the speech - nonspeech detector just assigns randomly the speech to {disfmarker} to one of the channels , so . Uh - I haven't - I didn't think of {disfmarker} of s of {vocalsound} this before ,&#10;Speaker: Grad A&#10;Content: What can you do ?&#10;Speaker: PhD B&#10;Content: but what {disfmarker} what shall we do about s things like this ?&#10;Speaker: Postdoc C&#10;Content: Well you were suggesting {disfmarker} You suggested maybe just not sending that part of the meeting .&#10;Speaker: Grad A&#10;Content: Yep . Mmm .&#10;Speaker: Postdoc C&#10;Content: But {disfmarker}&#10;Speaker: PhD B&#10;Content: But , sometimes the {disfmarker} {vocalsound} the {disfmarker} the laptop is in the background and some {disfmarker} somebody is {disfmarker} is talking , and , {vocalsound">
      <data key="d0">1</data>
    </edge>
    <edge source=" is playing sound from his laptop .&#10;Speaker: Grad A&#10;Content: Uh - huh&#10;Speaker: PhD B&#10;Content: And i {vocalsound} the speech - nonspeech detector just assigns randomly the speech to {disfmarker} to one of the channels , so . Uh - I haven't - I didn't think of {disfmarker} of s of {vocalsound} this before ,&#10;Speaker: Grad A&#10;Content: What can you do ?&#10;Speaker: PhD B&#10;Content: but what {disfmarker} what shall we do about s things like this ?&#10;Speaker: Postdoc C&#10;Content: Well you were suggesting {disfmarker} You suggested maybe just not sending that part of the meeting .&#10;Speaker: Grad A&#10;Content: Yep . Mmm .&#10;Speaker: Postdoc C&#10;Content: But {disfmarker}&#10;Speaker: PhD B&#10;Content: But , sometimes the {disfmarker} {vocalsound} the {disfmarker} the laptop is in the background and some {disfmarker} somebody is {disfmarker} is talking , and , {vocalsound" target="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation." target="&#10;Speaker: Professor F&#10;Content: Whereas if they listen {nonvocalsound} to it and there 's {disfmarker} don't hear any speech I think they 'd probably just listen to it once .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: So there 'd {disfmarker} you 'd think there 'd be a {disfmarker} a factor of three or four in {disfmarker} in , uh , cost function ,&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: you know , between them or something .&#10;Speaker: PhD B&#10;Content: Yeah , so {disfmarker} but I think that 's {disfmarker} n that really doesn't happen very often that {disfmarker} that {disfmarker} that a word is cut in the middle or something . That 's {disfmarker} that 's really not {disfmarker} not normal .&#10;Speaker: Professor F&#10;Content: So {disfmarker} so what you 're saying is that nearly always what happens">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation." target=" G&#10;Content: dramatically . So we have to um {disfmarker}&#10;Speaker: Professor F&#10;Content: I mean , you still {disfmarker}&#10;Speaker: PhD G&#10;Content: Well I should bring the {disfmarker} should bring the table with results . Maybe we can look at it {pause} Monday .&#10;Speaker: Professor F&#10;Content: I would presume that you still would have somewhat higher error with the lapel for insertions than {disfmarker}&#10;Speaker: PhD G&#10;Content: Yes . It 's {disfmarker} It 's {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Yes . Yeah .&#10;Speaker: Professor F&#10;Content: Cuz again , looking forward to the non - close miked case , I think that we s still {disfmarker}&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: I 'm not looking forward to it .&#10;Speaker: Professor F&#10;Content: i it 's the high signal - to - noise ratio&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation." target=" Uh , an order of magnitude notion of {disfmarker} of how {disfmarker} on a good meeting , how often uh , do you get segments that come in the middle of words and so forth , and uh {disfmarker} in a bad meeting how {vocalsound} often ?&#10;Speaker: PhD B&#10;Content: Uh .&#10;Speaker: Postdoc C&#10;Content: Was is it in a {disfmarker} in a {disfmarker} what {disfmarker} what is the t&#10;Speaker: Professor F&#10;Content: Well he 's saying , you know , that the {disfmarker} the EDU meeting was a good {disfmarker} good meeting ,&#10;Speaker: Postdoc C&#10;Content: In a good meeting , what ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: right ?&#10;Speaker: Postdoc C&#10;Content: Oh I see ,&#10;Speaker: Professor F&#10;Content: Uh , and so {disfmarker} so {disfmarker} so it was almost {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation." target="Based on the transcript provided, the proposed solution for increasing the speed of the process being discussed, while still allowing for adjustments and thorough listening, seems to be using an automatic backchanneling system. This system, developed by a former student named Nigel Ward, could help indicate active listening in conversations without requiring as much time as manual backchanneling (e.g., saying &quot;mm-hmm&quot; or &quot;uh-huh&quot;). The professors are interested in exploring how this automatic system could be used to improve communication during their meetings.&#10;&#10;Additionally, Postdoc C mentions that &quot;listening does take time too,&quot; implying that they acknowledge the importance of dedicating sufficient time to listening during conversations. However, no specific solutions for adjusting or optimizing the timing of the process are explicitly mentioned in the transcript.&#10;&#10;Grad A suggests a specific value of &quot;one and a half times real time&quot; in relation to the discussion, but it is not clear whether this number is directly related to the proposed solution for increasing speed while maintaining quality listening and adjustments. It would be beneficial to have more context or information on where this value comes from and how it applies to the process being discussed.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation." target="1. Discussion about an automatic backchanneling system: The participants are discussing an automatic backchanneling system developed by a former student named Nigel Ward. They express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even during their current meeting. This system may be particularly useful for Japanese conversations, as backchanneling is crucial and not doing so can be impolite.&#10;&#10;2. Continuously backchanneling at random intervals: The participants mention that backchanneling tends to happen when there is a falling pitch in conversations. They suggest that continuously backchanneling at random intervals could potentially be acceptable in an automatic system, as it would indicate active listening without requiring manual input and might &quot;pretty much be fine&quot; for a lot of people. However, they also note the importance of cultural context, such as Japanese conversations where not backchanneling can be impolite.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation." target="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation." target="1. {disfmarker}: This symbol, mentioned by PhD G and Grad A, appears to be a placeholder for some sort of discourse marker or verbal tic that they are correcting or discussing in the context of analyzing conversational data. The disagreement revolves around how specific they should be when referring to the person using these markers; PhD G initially mentions &quot;Morgan&quot; but then changes it to &quot;one participant,&quot; while Grad A wonders if they identified him as a senior member.&#10;   &#10;2. {pause}: This symbol, used by Grad A, likely represents an actual pause in conversation. There is no disagreement about its use.&#10;&#10;3. The laptop sounds and potential transcription issues were discussed, but these are not symbols or unique markers specifically mentioned in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation." target="Based on the transcript provided, the group is open to the idea of normalizing the comparison of robustness between multi-party and two-party conversations by the number of speakers involved. This suggestion was raised by PhD D and supported by other participants, including PhD B, Postdoc C, and Professor F. They believe that this normalization could provide a more fair and accurate comparison between different types of conversations. However, the group does not make a definitive decision on this matter during the discussion, so further conversation may be necessary to reach a conclusion.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The first result is about the amount of overlap in terms of the number of words between different speakers. A specific example given was that one speaker, Morgan, accounted for fifty-six percent of the Robustness meetings in terms of number of words.&#10;2. The second set of results is about the impact of overlaps on speech recognition performance, as previously discussed in a HLT paper.&#10;3. The third result involves rescoring transcripts and fixing them in various ways, as well as adding one specific number related to the pauses between words. However, the details of this result are not provided in the conversation." target="Based on the meeting transcript, Don will be looking at prosodic properties as part of the team's focus now that they have the data and alignment. Prosodic properties likely include aspects such as intonation, stress, and rhythm in speech. This task was assigned by PhD G to Don during their discussion. Grad E also confirmed that they will be looking at prosodic properties.">
      <data key="d0">1</data>
    </edge>
    <edge source=" G&#10;Content: dramatically . So we have to um {disfmarker}&#10;Speaker: Professor F&#10;Content: I mean , you still {disfmarker}&#10;Speaker: PhD G&#10;Content: Well I should bring the {disfmarker} should bring the table with results . Maybe we can look at it {pause} Monday .&#10;Speaker: Professor F&#10;Content: I would presume that you still would have somewhat higher error with the lapel for insertions than {disfmarker}&#10;Speaker: PhD G&#10;Content: Yes . It 's {disfmarker} It 's {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Yes . Yeah .&#10;Speaker: Professor F&#10;Content: Cuz again , looking forward to the non - close miked case , I think that we s still {disfmarker}&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: I 'm not looking forward to it .&#10;Speaker: Professor F&#10;Content: i it 's the high signal - to - noise ratio&#10;Speaker:" target="1. The paper written by PhD G has three types of results.&#10;2. One result is a measurement of overlap, which was calculated in terms of the number of words.&#10;3. Another result mentioned involves a computation, but further details about this are not provided in the transcript.&#10;4. The paper hasn't been reviewed yet by Professor F.">
      <data key="d0">1</data>
    </edge>
    <edge source=" G&#10;Content: dramatically . So we have to um {disfmarker}&#10;Speaker: Professor F&#10;Content: I mean , you still {disfmarker}&#10;Speaker: PhD G&#10;Content: Well I should bring the {disfmarker} should bring the table with results . Maybe we can look at it {pause} Monday .&#10;Speaker: Professor F&#10;Content: I would presume that you still would have somewhat higher error with the lapel for insertions than {disfmarker}&#10;Speaker: PhD G&#10;Content: Yes . It 's {disfmarker} It 's {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Yes . Yeah .&#10;Speaker: Professor F&#10;Content: Cuz again , looking forward to the non - close miked case , I think that we s still {disfmarker}&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: I 'm not looking forward to it .&#10;Speaker: Professor F&#10;Content: i it 's the high signal - to - noise ratio&#10;Speaker:" target="1. Professor F realized that adding a random bit in RASTA's option was equivalent to adding flat spectrum because it added energy across all frequencies, similar to adding a constant to the spectrum. This made it faster and more efficient than adding a random bit.&#10;2. The transcript does not provide enough information to answer this question fully. However, it can be inferred that the paper written by PhD G presents three types of results, one of which is a measurement of overlap calculated in terms of the number of words. Another result mentioned involves a computation, and there may have been errors in the paper according to PhD B.&#10;3. The paper written by PhD G has not been reviewed by Professor F yet.&#10;4. The discussion about concentration and turning microphones on or off suggests that they are discussing issues related to sound quality and clarity in their research, possibly related to audio recordings or transcriptions of meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." target="Content: Yeah .&#10;Speaker: PhD G&#10;Content: so I d shrunk it to eight millimeters and that helped some . And stuff like that .&#10;Speaker: PhD D&#10;Content: Wasn't there {disfmarker} wasn't there some result , Andreas {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah {disfmarker}&#10;Speaker: PhD D&#10;Content: I {disfmarker} I thought maybe Liz presented this at some conference a while ago about {vocalsound} uh , backchannels&#10;Speaker: PhD G&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD D&#10;Content: uh , and that they tend to happen when uh {pause} the pitch drops . You know you get a falling pitch . And so that 's when people tend to backchannel .&#10;Speaker: PhD G&#10;Content: Yeah . Well {disfmarker}&#10;Speaker: PhD D&#10;Content: Uh - i i do you rem&#10;Speaker: PhD G&#10;Content: y We didn't talk about , uh , prosodic , uh , properties at all ,&#10;Speaker: PhD D&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." target="&#10;Content: So am I recalling correctly ?&#10;Speaker: PhD G&#10;Content: Anyway , so .&#10;Speaker: Postdoc C&#10;Content: Well , I didn't know about Liz 's finding on that ,&#10;Speaker: PhD D&#10;Content: About {disfmarker}&#10;Speaker: Postdoc C&#10;Content: but I know of another paper that talks about something&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Postdoc C&#10;Content: that {disfmarker}&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: I 'd like to see that reference too .&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: It made me think about a cool little device that could be built to uh {disfmarker} to handle those people that call you on the phone and just like to talk and talk and talk . And you just have this little detector that listens for these {vocalsound} drops in pitch and gives them the backchannel . And so then you {vocalsound} hook that to the phone and go off&#10;Speaker: Grad A&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." target="} y you know , your {disfmarker} your statement about how much overlap there is becomes less , {vocalsound} um , precise ,&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: because you include more of actual pause time into what you consider overlap speech . Um , so , it 's sort of a compromise ,&#10;Speaker: PhD B&#10;Content: Yeah . {comment} {vocalsound} {vocalsound} Yeah , I also used I think something around zero point five seconds for the speech - nonspeech detector {disfmarker}&#10;Speaker: PhD G&#10;Content: and {disfmarker} {vocalsound} it 's also based {disfmarker} I mean Liz suggested that value based on {vocalsound} the distribution of pause times that you see in Switchboard and {disfmarker} and other corpora .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: Um {disfmarker} So {disfmarker}&#10;Speaker: PhD B&#10;Content: for the minimum silence length">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." target=" so , the end of a filled pause and the end of a discourse marker . And we just eyeballed {disfmarker} I mean {vocalsound} we didn't really hand - tag all of these things . We just {pause} looked at the distribution of words , and so every {vocalsound} &quot; so yeah &quot; , and &quot; OK &quot; , uh , and &quot; uh - huh &quot; were {disfmarker} were the {disfmarker} were deemed to be backchannels and {vocalsound} &quot; wow &quot; and &quot; so &quot; and {vocalsound} uh &quot; right &quot; , uh were um {disfmarker} {pause} Not &quot; right &quot; . &quot; Right &quot; is a backchannel . But so , we sort of {disfmarker} just based on the lexical {disfmarker} {vocalsound} um , identity of the words , we {disfmarker} we tagged them as one of these things . And of course the d the interruption points we got from the original transcripts . So , and then we looked at the disti so we looked at the {pause} distribution of these different kinds of tags , overall uh , and {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." target="Based on the transcript provided, the proposed solution for increasing the speed of the process being discussed, while still allowing for adjustments and thorough listening, seems to be using an automatic backchanneling system. This system, developed by a former student named Nigel Ward, could help indicate active listening in conversations without requiring as much time as manual backchanneling (e.g., saying &quot;mm-hmm&quot; or &quot;uh-huh&quot;). The professors are interested in exploring how this automatic system could be used to improve communication during their meetings.&#10;&#10;Additionally, Postdoc C mentions that &quot;listening does take time too,&quot; implying that they acknowledge the importance of dedicating sufficient time to listening during conversations. However, no specific solutions for adjusting or optimizing the timing of the process are explicitly mentioned in the transcript.&#10;&#10;Grad A suggests a specific value of &quot;one and a half times real time&quot; in relation to the discussion, but it is not clear whether this number is directly related to the proposed solution for increasing speed while maintaining quality listening and adjustments. It would be beneficial to have more context or information on where this value comes from and how it applies to the process being discussed.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." target="1. Discussion about an automatic backchanneling system: The participants are discussing an automatic backchanneling system developed by a former student named Nigel Ward. They express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even during their current meeting. This system may be particularly useful for Japanese conversations, as backchanneling is crucial and not doing so can be impolite.&#10;&#10;2. Continuously backchanneling at random intervals: The participants mention that backchanneling tends to happen when there is a falling pitch in conversations. They suggest that continuously backchanneling at random intervals could potentially be acceptable in an automatic system, as it would indicate active listening without requiring manual input and might &quot;pretty much be fine&quot; for a lot of people. However, they also note the importance of cultural context, such as Japanese conversations where not backchanneling can be impolite.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." target="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." target="1. {disfmarker}: This symbol, mentioned by PhD G and Grad A, appears to be a placeholder for some sort of discourse marker or verbal tic that they are correcting or discussing in the context of analyzing conversational data. The disagreement revolves around how specific they should be when referring to the person using these markers; PhD G initially mentions &quot;Morgan&quot; but then changes it to &quot;one participant,&quot; while Grad A wonders if they identified him as a senior member.&#10;   &#10;2. {pause}: This symbol, used by Grad A, likely represents an actual pause in conversation. There is no disagreement about its use.&#10;&#10;3. The laptop sounds and potential transcription issues were discussed, but these are not symbols or unique markers specifically mentioned in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." target="Based on the transcript provided, the group is open to the idea of normalizing the comparison of robustness between multi-party and two-party conversations by the number of speakers involved. This suggestion was raised by PhD D and supported by other participants, including PhD B, Postdoc C, and Professor F. They believe that this normalization could provide a more fair and accurate comparison between different types of conversations. However, the group does not make a definitive decision on this matter during the discussion, so further conversation may be necessary to reach a conclusion.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." target="1. The status of the Eurospeech paper submission is that it has been sent off, but it was sent in late, exceeding the delayed deadline by another day. The reason for its late submission is not explicitly mentioned in the transcript, but it appears to be unrelated to any issues with the paper's content.&#10;2. There is no direct information given about why the paper was sent in late. Grad A simply states, &quot;Well yeah, you sent it in late.&quot; No further explanation or context is provided in the transcript.&#10;3. The transcript does indicate that there was a delayed deadline and the paper exceeded it by another day. Professor F asks, &quot;Oh they had some extension that they announced or something?&quot; to which PhD G responds, &quot;Well yeah. Liz had sent them a note saying 'could we please have another three days' or something, and they said yes.&quot; It seems there was an agreed-upon extension, but the paper was still submitted later than the extended deadline.&#10;4. The exact reason for the late submission remains unclear from the transcript. It could be due to unforeseen circumstances, last-minute revisions, or other factors not mentioned in the conversation. However, it is evident that the paper has been sent off and the authors are now waiting for the review process to begin.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." target="The transcript does not provide direct information on why Liz decided to use the downsampled versions instead of the full bandwidth version. However, it is mentioned that there was no significant difference between them and the downsampled versions even performed better than the original versions in terms of communication. It can be speculated that Liz might have chosen to use the downsampled versions to save disk space or due to other practical considerations, such as easier handling or faster processing times. Ultimately, the reason for her decision is not explicitly stated in the given transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, there is no direct mention or presentation of specific numerical results from Liz's research regarding the relationship between backchannels and pitch changes in conversations. However, the discussion does reference that Liz presented this topic at some conference, and the professors express interest in exploring how her automatic backchanneling system could be used to improve communication. They also mention that backchanneling tends to happen when there is a falling pitch in conversations. To get more details about Liz's findings, it is suggested to refer to the source where her research was presented or published." target="The benefits of using a channelized interface for encoding categorized speech include ease of handling and improved clarity when the context is apparent. This method would make it easier to distinguish between different types of speech, such as channeled or close-miked speakers.&#10;&#10;However, there are also potential challenges associated with this approach. If the context is not clear, listeners might struggle to determine whether a given sound is part of the channelized speech or an independent audio source. This confusion could lead to misunderstandings and make transcription more difficult for both automated systems and human transcribers. The speakers in the conversation acknowledge these challenges and express concern that such ambiguity could be confusing.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Yeah .&#10;Speaker: PhD G&#10;Content: so I d shrunk it to eight millimeters and that helped some . And stuff like that .&#10;Speaker: PhD D&#10;Content: Wasn't there {disfmarker} wasn't there some result , Andreas {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah {disfmarker}&#10;Speaker: PhD D&#10;Content: I {disfmarker} I thought maybe Liz presented this at some conference a while ago about {vocalsound} uh , backchannels&#10;Speaker: PhD G&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD D&#10;Content: uh , and that they tend to happen when uh {pause} the pitch drops . You know you get a falling pitch . And so that 's when people tend to backchannel .&#10;Speaker: PhD G&#10;Content: Yeah . Well {disfmarker}&#10;Speaker: PhD D&#10;Content: Uh - i i do you rem&#10;Speaker: PhD G&#10;Content: y We didn't talk about , uh , prosodic , uh , properties at all ,&#10;Speaker: PhD D&#10;Content:" target="The transcript does not provide direct information on why Liz decided to use the downsampled versions instead of the full bandwidth version. However, it is mentioned that there was no significant difference between them and the downsampled versions even performed better than the original versions in terms of communication. It can be speculated that Liz might have chosen to use the downsampled versions to save disk space or due to other practical considerations, such as easier handling or faster processing times. Ultimately, the reason for her decision is not explicitly stated in the given transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, the proposed solution for increasing the speed of the process being discussed, while still allowing for adjustments and thorough listening, seems to be using an automatic backchanneling system. This system, developed by a former student named Nigel Ward, could help indicate active listening in conversations without requiring as much time as manual backchanneling (e.g., saying &quot;mm-hmm&quot; or &quot;uh-huh&quot;). The professors are interested in exploring how this automatic system could be used to improve communication during their meetings.&#10;&#10;Additionally, Postdoc C mentions that &quot;listening does take time too,&quot; implying that they acknowledge the importance of dedicating sufficient time to listening during conversations. However, no specific solutions for adjusting or optimizing the timing of the process are explicitly mentioned in the transcript.&#10;&#10;Grad A suggests a specific value of &quot;one and a half times real time&quot; in relation to the discussion, but it is not clear whether this number is directly related to the proposed solution for increasing speed while maintaining quality listening and adjustments. It would be beneficial to have more context or information on where this value comes from and how it applies to the process being discussed." target="Speaker: PhD B&#10;Content: Nope .&#10;Speaker: Professor F&#10;Content: I {disfmarker} I {disfmarker} I don't know .&#10;Speaker: PhD D&#10;Content: But it would be quick .&#10;Speaker: Professor F&#10;Content: It would be {disfmarker} kind of quick but they 're still listening to everything .&#10;Speaker: PhD D&#10;Content: But there 's no adjusting . And that 's what 's slow . There 's no adjusting of time boundaries .&#10;Speaker: Postdoc C&#10;Content: Well , {vocalsound} eh , listening does take time too .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Yeah . I don't know , I {disfmarker} I think I 'm {disfmarker} I 'm really tending towards {disfmarker}&#10;Speaker: Grad A&#10;Content: One and a half times real time .&#10;Speaker: Professor F&#10;Content: I mean , {vocalsound} what 's the worst that happens ? Do the transcribers {disfmarker} I mean as long">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, the proposed solution for increasing the speed of the process being discussed, while still allowing for adjustments and thorough listening, seems to be using an automatic backchanneling system. This system, developed by a former student named Nigel Ward, could help indicate active listening in conversations without requiring as much time as manual backchanneling (e.g., saying &quot;mm-hmm&quot; or &quot;uh-huh&quot;). The professors are interested in exploring how this automatic system could be used to improve communication during their meetings.&#10;&#10;Additionally, Postdoc C mentions that &quot;listening does take time too,&quot; implying that they acknowledge the importance of dedicating sufficient time to listening during conversations. However, no specific solutions for adjusting or optimizing the timing of the process are explicitly mentioned in the transcript.&#10;&#10;Grad A suggests a specific value of &quot;one and a half times real time&quot; in relation to the discussion, but it is not clear whether this number is directly related to the proposed solution for increasing speed while maintaining quality listening and adjustments. It would be beneficial to have more context or information on where this value comes from and how it applies to the process being discussed." target="1. Discussion about an automatic backchanneling system: The participants are discussing an automatic backchanneling system developed by a former student named Nigel Ward. They express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even during their current meeting. This system may be particularly useful for Japanese conversations, as backchanneling is crucial and not doing so can be impolite.&#10;&#10;2. Continuously backchanneling at random intervals: The participants mention that backchanneling tends to happen when there is a falling pitch in conversations. They suggest that continuously backchanneling at random intervals could potentially be acceptable in an automatic system, as it would indicate active listening without requiring manual input and might &quot;pretty much be fine&quot; for a lot of people. However, they also note the importance of cultural context, such as Japanese conversations where not backchanneling can be impolite.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, the proposed solution for increasing the speed of the process being discussed, while still allowing for adjustments and thorough listening, seems to be using an automatic backchanneling system. This system, developed by a former student named Nigel Ward, could help indicate active listening in conversations without requiring as much time as manual backchanneling (e.g., saying &quot;mm-hmm&quot; or &quot;uh-huh&quot;). The professors are interested in exploring how this automatic system could be used to improve communication during their meetings.&#10;&#10;Additionally, Postdoc C mentions that &quot;listening does take time too,&quot; implying that they acknowledge the importance of dedicating sufficient time to listening during conversations. However, no specific solutions for adjusting or optimizing the timing of the process are explicitly mentioned in the transcript.&#10;&#10;Grad A suggests a specific value of &quot;one and a half times real time&quot; in relation to the discussion, but it is not clear whether this number is directly related to the proposed solution for increasing speed while maintaining quality listening and adjustments. It would be beneficial to have more context or information on where this value comes from and how it applies to the process being discussed." target="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content.">
      <data key="d0">1</data>
    </edge>
    <edge source="The best approach for sharing the data with another person at SRI, as determined by PhD G, is using FTP. PhD G implemented this by creating a new directory for sharing the data, which he had previously been advised would be a good idea. The transcript suggests that PhD G has taken steps to make the necessary preparations for sharing the data via FTP, but it does not provide explicit information on whether the data has actually been shared and accessed by the person at SRI." target=" PhD G&#10;Content: y We didn't talk about , uh , prosodic , uh , properties at all ,&#10;Speaker: PhD D&#10;Content: Right . Right . But {disfmarker}&#10;Speaker: PhD G&#10;Content: although that 's {disfmarker} I {disfmarker} I take it that 's something that uh Don will {disfmarker} will look at&#10;Speaker: Grad E&#10;Content: Yeah , we 're gonna be looking at that .&#10;Speaker: PhD G&#10;Content: now that we have the data and we have the alignment , so . This is purely based on you know the words&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: and {disfmarker}&#10;Speaker: Postdoc C&#10;Content: I have a reference for that though . Uh - huh .&#10;Speaker: PhD D&#10;Content: Oh you do .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: So am I recalling correctly ?&#10;Speaker: PhD G&#10;Content: Anyway , so .&#10;Speaker: Postdoc C&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="The best approach for sharing the data with another person at SRI, as determined by PhD G, is using FTP. PhD G implemented this by creating a new directory for sharing the data, which he had previously been advised would be a good idea. The transcript suggests that PhD G has taken steps to make the necessary preparations for sharing the data via FTP, but it does not provide explicit information on whether the data has actually been shared and accessed by the person at SRI." target=" . Yeah . Yeah .&#10;Speaker: PhD G&#10;Content: and {vocalsound} so {disfmarker} they 're so good that {vocalsound} generally , u the overlapped speech does not {disfmarker} is less than five percent .&#10;Speaker: Postdoc C&#10;Content: Oh , that 's interesting . Yeah .&#10;Speaker: PhD G&#10;Content: So , this is way more than five percent .&#10;Speaker: Grad E&#10;Content: Did he mean face {disfmarker} like face - to - face ? Or {disfmarker} ?&#10;Speaker: PhD G&#10;Content: Well , in real conversations ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: PhD G&#10;Content: everyday conversations .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: It 's s what these conversation analysts have been studying for years and years there .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: But {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Well , of">
      <data key="d0">1</data>
    </edge>
    <edge source="The best approach for sharing the data with another person at SRI, as determined by PhD G, is using FTP. PhD G implemented this by creating a new directory for sharing the data, which he had previously been advised would be a good idea. The transcript suggests that PhD G has taken steps to make the necessary preparations for sharing the data via FTP, but it does not provide explicit information on whether the data has actually been shared and accessed by the person at SRI." target="1. The status of the Eurospeech paper submission is that it has been sent off, but it was sent in late, exceeding the delayed deadline by another day. The reason for its late submission is not explicitly mentioned in the transcript, but it appears to be unrelated to any issues with the paper's content.&#10;2. There is no direct information given about why the paper was sent in late. Grad A simply states, &quot;Well yeah, you sent it in late.&quot; No further explanation or context is provided in the transcript.&#10;3. The transcript does indicate that there was a delayed deadline and the paper exceeded it by another day. Professor F asks, &quot;Oh they had some extension that they announced or something?&quot; to which PhD G responds, &quot;Well yeah. Liz had sent them a note saying 'could we please have another three days' or something, and they said yes.&quot; It seems there was an agreed-upon extension, but the paper was still submitted later than the extended deadline.&#10;4. The exact reason for the late submission remains unclear from the transcript. It could be due to unforeseen circumstances, last-minute revisions, or other factors not mentioned in the conversation. However, it is evident that the paper has been sent off and the authors are now waiting for the review process to begin.">
      <data key="d0">1</data>
    </edge>
    <edge source="The best approach for sharing the data with another person at SRI, as determined by PhD G, is using FTP. PhD G implemented this by creating a new directory for sharing the data, which he had previously been advised would be a good idea. The transcript suggests that PhD G has taken steps to make the necessary preparations for sharing the data via FTP, but it does not provide explicit information on whether the data has actually been shared and accessed by the person at SRI." target="The proposed approach for sharing data with another person at SRI, as discussed by PhD G, is using File Transfer Protocol (FTP). PhD G believes that FTP is the best method for several reasons. First, he has created a new directory specifically for sharing the data after being advised that it would be a good idea. This new directory will allow the other person at SRI to access the necessary data easily. While the transcript does not explicitly confirm that the data has been shared and accessed via FTP, PhD G's preparations indicate that he is actively working on implementing this method for sharing the data.">
      <data key="d0">1</data>
    </edge>
    <edge source="The best approach for sharing the data with another person at SRI, as determined by PhD G, is using FTP. PhD G implemented this by creating a new directory for sharing the data, which he had previously been advised would be a good idea. The transcript suggests that PhD G has taken steps to make the necessary preparations for sharing the data via FTP, but it does not provide explicit information on whether the data has actually been shared and accessed by the person at SRI." target="The transcript does not provide direct information on why Liz decided to use the downsampled versions instead of the full bandwidth version. However, it is mentioned that there was no significant difference between them and the downsampled versions even performed better than the original versions in terms of communication. It can be speculated that Liz might have chosen to use the downsampled versions to save disk space or due to other practical considerations, such as easier handling or faster processing times. Ultimately, the reason for her decision is not explicitly stated in the given transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="The best approach for sharing the data with another person at SRI, as determined by PhD G, is using FTP. PhD G implemented this by creating a new directory for sharing the data, which he had previously been advised would be a good idea. The transcript suggests that PhD G has taken steps to make the necessary preparations for sharing the data via FTP, but it does not provide explicit information on whether the data has actually been shared and accessed by the person at SRI." target="1. The specific content of the paper that Graduate Student A printed out but hasn't read yet is not mentioned in the transcript. However, it is clear that the paper is related to a Monty Python sketch and has been submitted for Eurospeech.&#10;2. PhD Student G is planning to send an email to Brian Kingbury about where he can find the related material because Brian had previously requested this material for a speech recognition experiment. PhD G was in the process of sending the location of the shared directory when his desktop froze, causing him to delay the email. The transcript suggests that PhD G has prepared a shared directory via FTP to send to Brian, but it does not explicitly confirm whether the email has been sent or the material has been accessed by Brian at SRI.">
      <data key="d0">1</data>
    </edge>
    <edge source=" PhD G&#10;Content: y We didn't talk about , uh , prosodic , uh , properties at all ,&#10;Speaker: PhD D&#10;Content: Right . Right . But {disfmarker}&#10;Speaker: PhD G&#10;Content: although that 's {disfmarker} I {disfmarker} I take it that 's something that uh Don will {disfmarker} will look at&#10;Speaker: Grad E&#10;Content: Yeah , we 're gonna be looking at that .&#10;Speaker: PhD G&#10;Content: now that we have the data and we have the alignment , so . This is purely based on you know the words&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: and {disfmarker}&#10;Speaker: Postdoc C&#10;Content: I have a reference for that though . Uh - huh .&#10;Speaker: PhD D&#10;Content: Oh you do .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: So am I recalling correctly ?&#10;Speaker: PhD G&#10;Content: Anyway , so .&#10;Speaker: Postdoc C&#10;Content:" target="Based on the meeting transcript, Don will be looking at prosodic properties as part of the team's focus now that they have the data and alignment. Prosodic properties likely include aspects such as intonation, stress, and rhythm in speech. This task was assigned by PhD G to Don during their discussion. Grad E also confirmed that they will be looking at prosodic properties.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The paper written by PhD G has three types of results.&#10;2. One result is a measurement of overlap, which was calculated in terms of the number of words.&#10;3. Another result mentioned involves a computation, but further details about this are not provided in the transcript.&#10;4. The paper hasn't been reviewed yet by Professor F." target="&#10;Speaker: Professor F&#10;Content: Uh , and you just sent off a Eurospeech paper , so .&#10;Speaker: PhD G&#10;Content: Right . I hope they accept it .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: I mean , I {disfmarker} both actu as {disfmarker} as a submission and {disfmarker} {vocalsound} you know , as a paper . Um {disfmarker} but {disfmarker}&#10;Speaker: Grad A&#10;Content: Well yeah , you sent it in {pause} late .&#10;Speaker: Professor F&#10;Content: Yeah , I guess you {disfmarker} first you have to do the first one ,&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: and then {disfmarker} Yeah .&#10;Speaker: PhD G&#10;Content: We actually exceeded the delayed deadline by o another day , so .&#10;Speaker: PhD B&#10;Content: Oops .&#10;Speaker: Professor F&#10;Content: Oh they {disfmarker} they had some extension that they announced or something ?&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The paper written by PhD G has three types of results.&#10;2. One result is a measurement of overlap, which was calculated in terms of the number of words.&#10;3. Another result mentioned involves a computation, but further details about this are not provided in the transcript.&#10;4. The paper hasn't been reviewed yet by Professor F." target=" sure .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: And otherwise they should mark it so that we can check .&#10;Speaker: PhD B&#10;Content: Mark it . Sure . Yeah . Yeah .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Postdoc C&#10;Content: Well , we have the unintelligibility {pause} convention .&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: Postdoc C&#10;Content: And actually they have one also ,&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Postdoc C&#10;Content: which {disfmarker}&#10;Speaker: Professor F&#10;Content: i Can I maybe have {disfmarker} have an order of {disfmarker} it 's probably in your paper that I haven't looked at lately , but {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Certainty .&#10;Speaker: Professor F&#10;Content: Uh , an order of magnitude notion of {disfmarker} of how {disfmarker} on a good meeting , how often uh , do">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The paper written by PhD G has three types of results.&#10;2. One result is a measurement of overlap, which was calculated in terms of the number of words.&#10;3. Another result mentioned involves a computation, but further details about this are not provided in the transcript.&#10;4. The paper hasn't been reviewed yet by Professor F." target=" Wow .&#10;Speaker: Postdoc C&#10;Content: In {disfmarker} in terms of what ? In term&#10;Speaker: PhD G&#10;Content: Number of words .&#10;Speaker: Postdoc C&#10;Content: One ? Wow ! OK .&#10;Speaker: Grad A&#10;Content: That 's just cuz he talks really fast .&#10;Speaker: Postdoc C&#10;Content: Do you mean ,&#10;Speaker: Professor F&#10;Content: n No .&#10;Speaker: Grad A&#10;Content: I know&#10;Speaker: PhD B&#10;Content: Oh . Short words .&#10;Speaker: Postdoc C&#10;Content: because {disfmarker} is it partly , eh , c correctly identified words ? Or is it {disfmarker} or just overall volume ?&#10;Speaker: PhD G&#10;Content: No . Well , according to the transcripts .&#10;Speaker: Grad A&#10;Content: But re well regardless . I think it 's {disfmarker} he 's {disfmarker} he 's in all of them ,&#10;Speaker: Postdoc C&#10;Content: Oh . OK .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The paper written by PhD G has three types of results.&#10;2. One result is a measurement of overlap, which was calculated in terms of the number of words.&#10;3. Another result mentioned involves a computation, but further details about this are not provided in the transcript.&#10;4. The paper hasn't been reviewed yet by Professor F." target="1. Professor F realized that adding a random bit in RASTA's option was equivalent to adding flat spectrum because it added energy across all frequencies, similar to adding a constant to the spectrum. This made it faster and more efficient than adding a random bit.&#10;2. The transcript does not provide enough information to answer this question fully. However, it can be inferred that the paper written by PhD G presents three types of results, one of which is a measurement of overlap calculated in terms of the number of words. Another result mentioned involves a computation, and there may have been errors in the paper according to PhD B.&#10;3. The paper written by PhD G has not been reviewed by Professor F yet.&#10;4. The discussion about concentration and turning microphones on or off suggests that they are discussing issues related to sound quality and clarity in their research, possibly related to audio recordings or transcriptions of meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The paper written by PhD G has three types of results.&#10;2. One result is a measurement of overlap, which was calculated in terms of the number of words.&#10;3. Another result mentioned involves a computation, but further details about this are not provided in the transcript.&#10;4. The paper hasn't been reviewed yet by Professor F." target="Based on the meeting transcript, Don will be looking at prosodic properties as part of the team's focus now that they have the data and alignment. Prosodic properties likely include aspects such as intonation, stress, and rhythm in speech. This task was assigned by PhD G to Don during their discussion. Grad E also confirmed that they will be looking at prosodic properties.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The paper written by PhD G has three types of results.&#10;2. One result is a measurement of overlap, which was calculated in terms of the number of words.&#10;3. Another result mentioned involves a computation, but further details about this are not provided in the transcript.&#10;4. The paper hasn't been reviewed yet by Professor F." target="1. The status of the Eurospeech paper submission is that it has been sent off, but it was sent in late, exceeding the delayed deadline by another day. The reason for its late submission is not explicitly mentioned in the transcript, but it appears to be unrelated to any issues with the paper's content.&#10;2. There is no direct information given about why the paper was sent in late. Grad A simply states, &quot;Well yeah, you sent it in late.&quot; No further explanation or context is provided in the transcript.&#10;3. The transcript does indicate that there was a delayed deadline and the paper exceeded it by another day. Professor F asks, &quot;Oh they had some extension that they announced or something?&quot; to which PhD G responds, &quot;Well yeah. Liz had sent them a note saying 'could we please have another three days' or something, and they said yes.&quot; It seems there was an agreed-upon extension, but the paper was still submitted later than the extended deadline.&#10;4. The exact reason for the late submission remains unclear from the transcript. It could be due to unforeseen circumstances, last-minute revisions, or other factors not mentioned in the conversation. However, it is evident that the paper has been sent off and the authors are now waiting for the review process to begin.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The paper written by PhD G has three types of results.&#10;2. One result is a measurement of overlap, which was calculated in terms of the number of words.&#10;3. Another result mentioned involves a computation, but further details about this are not provided in the transcript.&#10;4. The paper hasn't been reviewed yet by Professor F." target="1. The specific content of the paper that Graduate Student A printed out but hasn't read yet is not mentioned in the transcript. However, it is clear that the paper is related to a Monty Python sketch and has been submitted for Eurospeech.&#10;2. PhD Student G is planning to send an email to Brian Kingbury about where he can find the related material because Brian had previously requested this material for a speech recognition experiment. PhD G was in the process of sending the location of the shared directory when his desktop froze, causing him to delay the email. The transcript suggests that PhD G has prepared a shared directory via FTP to send to Brian, but it does not explicitly confirm whether the email has been sent or the material has been accessed by Brian at SRI.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: Professor F&#10;Content: Uh , and you just sent off a Eurospeech paper , so .&#10;Speaker: PhD G&#10;Content: Right . I hope they accept it .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: I mean , I {disfmarker} both actu as {disfmarker} as a submission and {disfmarker} {vocalsound} you know , as a paper . Um {disfmarker} but {disfmarker}&#10;Speaker: Grad A&#10;Content: Well yeah , you sent it in {pause} late .&#10;Speaker: Professor F&#10;Content: Yeah , I guess you {disfmarker} first you have to do the first one ,&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: and then {disfmarker} Yeah .&#10;Speaker: PhD G&#10;Content: We actually exceeded the delayed deadline by o another day , so .&#10;Speaker: PhD B&#10;Content: Oops .&#10;Speaker: Professor F&#10;Content: Oh they {disfmarker} they had some extension that they announced or something ?&#10;" target="1. The status of the Eurospeech paper submission is that it has been sent off, but it was sent in late, exceeding the delayed deadline by another day. The reason for its late submission is not explicitly mentioned in the transcript, but it appears to be unrelated to any issues with the paper's content.&#10;2. There is no direct information given about why the paper was sent in late. Grad A simply states, &quot;Well yeah, you sent it in late.&quot; No further explanation or context is provided in the transcript.&#10;3. The transcript does indicate that there was a delayed deadline and the paper exceeded it by another day. Professor F asks, &quot;Oh they had some extension that they announced or something?&quot; to which PhD G responds, &quot;Well yeah. Liz had sent them a note saying 'could we please have another three days' or something, and they said yes.&quot; It seems there was an agreed-upon extension, but the paper was still submitted later than the extended deadline.&#10;4. The exact reason for the late submission remains unclear from the transcript. It could be due to unforeseen circumstances, last-minute revisions, or other factors not mentioned in the conversation. However, it is evident that the paper has been sent off and the authors are now waiting for the review process to begin.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process." target=" all the time boundaries&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Yeah ?&#10;Speaker: PhD D&#10;Content: and we send it to IBM . The other one is {vocalsound} we just run his thing and send it to IBM .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: There 's a {disfmarker} a another possibility if we find that there are some problems ,&#10;Speaker: PhD B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD D&#10;Content: and that is {vocalsound} if we go ahead and we {vocalsound} just run his , and we generate the beeps file , then we have somebody listen beeps file .&#10;Speaker: PhD B&#10;Content: Yeah . And erase {disfmarker}&#10;Speaker: PhD D&#10;Content: And they listen to each section and say &quot; yes , no &quot; whether that section is&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Is intelligible .&#10;Speaker: PhD D&#10;Content: i i intelligible or not . And it just">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process." target="} the laptop is in the background and some {disfmarker} somebody is {disfmarker} is talking , and , {vocalsound} that 's really a little bit confusing , but {disfmarker}&#10;Speaker: Grad A&#10;Content: It 's a little bit confusing .&#10;Speaker: Professor F&#10;Content: That 's life .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: I mean , {comment} what 're we gonna do ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: Even a hand - transcription would {disfmarker}&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Postdoc C&#10;Content: Do you {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: a hand - transcriber would have trouble with that .&#10;Speaker: PhD B&#10;Content: Yeah ,&#10;Speaker: Grad A&#10;Content: So .&#10;Speaker: PhD B&#10;Content: that 's {disfmarker} that 's a second question , &quot; what {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process." target="1. The reason they cannot include the beep files with the initial submission is that there is a part of the beep file generation process which requires knowledge of the normalization coefficients. These coefficients are necessary for the correct production of the beep files, but they are not yet available at this stage.&#10;2. Normalization coefficients are values used to adjust the volume or amplitude of audio signals during processing. They are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. In the context of generating beep files, these coefficients ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;3. As they are still in the early stages of their project, the normalization coefficients might not yet be determined. Therefore, they cannot generate the beep files and include them in the initial submission to IBM for transcription. They will need to complete this step at a later stage when the necessary information is available.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process." target="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process." target="1. The plan is to compare the two processes of creating a chunked file for IBM transcription services and applying the beep-ify method (adding beeps at regular intervals) on both versions. They will listen to the output of both methods and look for major differences between them.&#10;   &#10;2. When doing the process by hand, they will pay attention to factors such as consistent volume levels and beep duration throughout the recording. This is where normalization coefficients come into play, as these values are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. They ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;&#10;3. One major difference they might look for is whether there's any confusion caused by including a section where some participants were playing sounds from their laptops during the meeting. This could potentially cause issues with differentiating between actual speech and background noise, as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels.&#10;&#10;4. Ultimately, they have decided to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also plan on having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;&#10;5. This discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process." target="The interruptions of hearing a word followed by a beep in the same sentence in the original file are likely caused by the manual application of the &quot;beep-ify&quot; method, where beeps are added at regular intervals throughout the audio recording. If the beep duration and placement is not consistent or well-coordinated with the speech segments, it may result in hearing a word followed by a beep within the same sentence. This issue highlights the importance of maintaining consistent volume levels and beep duration throughout the recording when applying this method manually.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process." target="1. The group decided to include the section with participants playing sounds from their laptops and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They agreed not to manually insert &quot;mm-hmm&quot;s at this end but rather consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;2. The group did not explicitly discuss sending the output of the detector directly to IBM without any human checking involved. The consensus leans more towards including human intervention, either by having IBM handle the laptop sound section or by listening to the beep file generated by their script for manual indications.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process." target="1. The proposed solution to address the issue of consecutive beeps in the transcription process is to maintain consistent volume levels and beep duration throughout the recording when applying the &quot;beep-ify&quot; method manually. This involves paying close attention to factors such as normalization coefficients, which are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. By ensuring that the inserted beeps have a uniform volume and duration throughout the recording, it can help prevent hearing a word followed by a beep within the same sentence.&#10;&#10;2. Implementing this solution might affect the outcome of the meeting that has already been run through a certain process by improving the overall quality of the transcription. The beep files generated from the original audio recording can serve as a means to compare different processing methods and identify major differences between them. By addressing common challenges in audio recordings, such as consistent volume levels and beep duration, the group can enhance the IBM transcription process and produce more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers are discussing whether to include or exclude a portion of the audio recording in the file they will send to IBM for transcription. The part in question is where some participants were playing sounds from their laptops during the meeting.&#10;2. There are concerns that including this section might cause confusion as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels, making it difficult to differentiate between actual speech and background noise.&#10;3. Some suggestions include removing the portion before sending it to IBM or including it and letting them handle it. However, there are concerns that even human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;4. Ultimately, they decide to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;5. The group agrees that this is a common challenge in audio recordings and that they will include this issue in their discussion about the IBM transcription process." target="1. The speakers decide to include a portion of the audio recording containing laptop sounds in the file they will send to IBM for transcription, despite potential challenges in distinguishing these sounds from actual speech. They also consider having someone manually indicate which sections of the generated beep file are intelligible or not.&#10;2. To handle words that are cut off and other acoustic problems in a transcript, the speakers plan on generating multiple &quot;beep&quot; files to assess their quality and determine how bad the audio with laptop sounds is. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually.&#10;3. The speakers express interest in exploring an automatic backchanneling system developed by a former student named Nigel Ward, which could be used to improve communication and indicate active listening in conversations. This suggests that they may be open to using such tools to help address acoustic problems and improve the overall quality of their transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source=" all the time boundaries&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Yeah ?&#10;Speaker: PhD D&#10;Content: and we send it to IBM . The other one is {vocalsound} we just run his thing and send it to IBM .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: There 's a {disfmarker} a another possibility if we find that there are some problems ,&#10;Speaker: PhD B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD D&#10;Content: and that is {vocalsound} if we go ahead and we {vocalsound} just run his , and we generate the beeps file , then we have somebody listen beeps file .&#10;Speaker: PhD B&#10;Content: Yeah . And erase {disfmarker}&#10;Speaker: PhD D&#10;Content: And they listen to each section and say &quot; yes , no &quot; whether that section is&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Is intelligible .&#10;Speaker: PhD D&#10;Content: i i intelligible or not . And it just" target="1. The reason they cannot include the beep files with the initial submission is that there is a part of the beep file generation process which requires knowledge of the normalization coefficients. These coefficients are necessary for the correct production of the beep files, but they are not yet available at this stage.&#10;2. Normalization coefficients are values used to adjust the volume or amplitude of audio signals during processing. They are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. In the context of generating beep files, these coefficients ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;3. As they are still in the early stages of their project, the normalization coefficients might not yet be determined. Therefore, they cannot generate the beep files and include them in the initial submission to IBM for transcription. They will need to complete this step at a later stage when the necessary information is available.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Discussion about an automatic backchanneling system: The participants are discussing an automatic backchanneling system developed by a former student named Nigel Ward. They express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even during their current meeting. This system may be particularly useful for Japanese conversations, as backchanneling is crucial and not doing so can be impolite.&#10;&#10;2. Continuously backchanneling at random intervals: The participants mention that backchanneling tends to happen when there is a falling pitch in conversations. They suggest that continuously backchanneling at random intervals could potentially be acceptable in an automatic system, as it would indicate active listening without requiring manual input and might &quot;pretty much be fine&quot; for a lot of people. However, they also note the importance of cultural context, such as Japanese conversations where not backchanneling can be impolite." target="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Discussion about an automatic backchanneling system: The participants are discussing an automatic backchanneling system developed by a former student named Nigel Ward. They express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even during their current meeting. This system may be particularly useful for Japanese conversations, as backchanneling is crucial and not doing so can be impolite.&#10;&#10;2. Continuously backchanneling at random intervals: The participants mention that backchanneling tends to happen when there is a falling pitch in conversations. They suggest that continuously backchanneling at random intervals could potentially be acceptable in an automatic system, as it would indicate active listening without requiring manual input and might &quot;pretty much be fine&quot; for a lot of people. However, they also note the importance of cultural context, such as Japanese conversations where not backchanneling can be impolite." target="1. {disfmarker}: This symbol, mentioned by PhD G and Grad A, appears to be a placeholder for some sort of discourse marker or verbal tic that they are correcting or discussing in the context of analyzing conversational data. The disagreement revolves around how specific they should be when referring to the person using these markers; PhD G initially mentions &quot;Morgan&quot; but then changes it to &quot;one participant,&quot; while Grad A wonders if they identified him as a senior member.&#10;   &#10;2. {pause}: This symbol, used by Grad A, likely represents an actual pause in conversation. There is no disagreement about its use.&#10;&#10;3. The laptop sounds and potential transcription issues were discussed, but these are not symbols or unique markers specifically mentioned in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Discussion about an automatic backchanneling system: The participants are discussing an automatic backchanneling system developed by a former student named Nigel Ward. They express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even during their current meeting. This system may be particularly useful for Japanese conversations, as backchanneling is crucial and not doing so can be impolite.&#10;&#10;2. Continuously backchanneling at random intervals: The participants mention that backchanneling tends to happen when there is a falling pitch in conversations. They suggest that continuously backchanneling at random intervals could potentially be acceptable in an automatic system, as it would indicate active listening without requiring manual input and might &quot;pretty much be fine&quot; for a lot of people. However, they also note the importance of cultural context, such as Japanese conversations where not backchanneling can be impolite." target="Based on the transcript provided, the group is open to the idea of normalizing the comparison of robustness between multi-party and two-party conversations by the number of speakers involved. This suggestion was raised by PhD D and supported by other participants, including PhD B, Postdoc C, and Professor F. They believe that this normalization could provide a more fair and accurate comparison between different types of conversations. However, the group does not make a definitive decision on this matter during the discussion, so further conversation may be necessary to reach a conclusion.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Discussion about an automatic backchanneling system: The participants are discussing an automatic backchanneling system developed by a former student named Nigel Ward. They express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even during their current meeting. This system may be particularly useful for Japanese conversations, as backchanneling is crucial and not doing so can be impolite.&#10;&#10;2. Continuously backchanneling at random intervals: The participants mention that backchanneling tends to happen when there is a falling pitch in conversations. They suggest that continuously backchanneling at random intervals could potentially be acceptable in an automatic system, as it would indicate active listening without requiring manual input and might &quot;pretty much be fine&quot; for a lot of people. However, they also note the importance of cultural context, such as Japanese conversations where not backchanneling can be impolite." target="The proposed approach for sharing data with another person at SRI, as discussed by PhD G, is using File Transfer Protocol (FTP). PhD G believes that FTP is the best method for several reasons. First, he has created a new directory specifically for sharing the data after being advised that it would be a good idea. This new directory will allow the other person at SRI to access the necessary data easily. While the transcript does not explicitly confirm that the data has been shared and accessed via FTP, PhD G's preparations indicate that he is actively working on implementing this method for sharing the data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Discussion about an automatic backchanneling system: The participants are discussing an automatic backchanneling system developed by a former student named Nigel Ward. They express interest in exploring how this system could be used to improve communication and indicate active listening in conversations, possibly even during their current meeting. This system may be particularly useful for Japanese conversations, as backchanneling is crucial and not doing so can be impolite.&#10;&#10;2. Continuously backchanneling at random intervals: The participants mention that backchanneling tends to happen when there is a falling pitch in conversations. They suggest that continuously backchanneling at random intervals could potentially be acceptable in an automatic system, as it would indicate active listening without requiring manual input and might &quot;pretty much be fine&quot; for a lot of people. However, they also note the importance of cultural context, such as Japanese conversations where not backchanneling can be impolite." target="The benefits of using a channelized interface for encoding categorized speech include ease of handling and improved clarity when the context is apparent. This method would make it easier to distinguish between different types of speech, such as channeled or close-miked speakers.&#10;&#10;However, there are also potential challenges associated with this approach. If the context is not clear, listeners might struggle to determine whether a given sound is part of the channelized speech or an independent audio source. This confusion could lead to misunderstandings and make transcription more difficult for both automated systems and human transcribers. The speakers in the conversation acknowledge these challenges and express concern that such ambiguity could be confusing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The reason they cannot include the beep files with the initial submission is that there is a part of the beep file generation process which requires knowledge of the normalization coefficients. These coefficients are necessary for the correct production of the beep files, but they are not yet available at this stage.&#10;2. Normalization coefficients are values used to adjust the volume or amplitude of audio signals during processing. They are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. In the context of generating beep files, these coefficients ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;3. As they are still in the early stages of their project, the normalization coefficients might not yet be determined. Therefore, they cannot generate the beep files and include them in the initial submission to IBM for transcription. They will need to complete this step at a later stage when the necessary information is available." target=" I {disfmarker} I think , we won't know until we generate a bunch of beep files automatically , listen to them and see how bad they are .&#10;Speaker: PhD B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: We won't be able to s include it with this first thing ,&#10;Speaker: Grad A&#10;Content: If {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Hmm . Oh , OK .&#10;Speaker: PhD D&#10;Content: because there 's a part of the process of the beep file which requires knowing the normalization coefficients .&#10;Speaker: Postdoc C&#10;Content: Oh , I see .&#10;Speaker: PhD D&#10;Content: And {disfmarker} {vocalsound} So a&#10;Speaker: Grad A&#10;Content: That 's not hard to do . Just {disfmarker} it takes {disfmarker} you know , it just takes five minutes rather than , taking a second .&#10;Speaker: PhD D&#10;Content: OK">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The reason they cannot include the beep files with the initial submission is that there is a part of the beep file generation process which requires knowledge of the normalization coefficients. These coefficients are necessary for the correct production of the beep files, but they are not yet available at this stage.&#10;2. Normalization coefficients are values used to adjust the volume or amplitude of audio signals during processing. They are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. In the context of generating beep files, these coefficients ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;3. As they are still in the early stages of their project, the normalization coefficients might not yet be determined. Therefore, they cannot generate the beep files and include them in the initial submission to IBM for transcription. They will need to complete this step at a later stage when the necessary information is available." target=" , that if two , chunks are very close to each other on the same channel we 'll just merge them .&#10;Speaker: Postdoc C&#10;Content: Oh ! OK . Ah , interesting . Yeah . Yeah . Oh , sure . Yeah , sure . Makes sense .&#10;Speaker: Grad A&#10;Content: So , uh , and that will get around the problem of , the , {vocalsound} you know &quot; one word beep , one word beep , one word beep , one word beep &quot; .&#10;Speaker: Postdoc C&#10;Content: Yeah . Ah ! Clever . Yes . Clever . Yeah . Excellent .&#10;Speaker: PhD D&#10;Content: Yeah , in fact after our meeting uh , this morning Thilo came in and said that {vocalsound} um , there could be {pause} other differences between {vocalsound} the uh {pause} already transcribed meeting with the beeps in it and one that has {pause} just r been run through his process .&#10;Speaker: Postdoc C&#10;Content: And that 's the purpose . Yeah .&#10;Speaker: PhD D&#10;Content: So tomorrow , {vocalsound} when we go to make the um {pause">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The reason they cannot include the beep files with the initial submission is that there is a part of the beep file generation process which requires knowledge of the normalization coefficients. These coefficients are necessary for the correct production of the beep files, but they are not yet available at this stage.&#10;2. Normalization coefficients are values used to adjust the volume or amplitude of audio signals during processing. They are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. In the context of generating beep files, these coefficients ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;3. As they are still in the early stages of their project, the normalization coefficients might not yet be determined. Therefore, they cannot generate the beep files and include them in the initial submission to IBM for transcription. They will need to complete this step at a later stage when the necessary information is available." target="Content: So if it 's only inserting &quot; mm - hmm &quot;s here and there , then , wouldn't that be something that would be just as efficient to do at this end , instead of having it go through I B M , then be patched together , then be double checked here .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Right .&#10;Speaker: PhD B&#10;Content: Yeah . But {disfmarker} But then we could just use the {disfmarker} the output of the detector , and do the beeping on it , and send it to I B&#10;Speaker: PhD D&#10;Content: Without having her check anything .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: Postdoc C&#10;Content: Well , I guess {disfmarker}&#10;Speaker: Grad A&#10;Content: I think we just {disfmarker} we just have to listen to it and see how good they are .&#10;Speaker: PhD B&#10;Content: For some meetings , I 'm {disfmarker} I 'm sure it {disfmarker} i n&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The reason they cannot include the beep files with the initial submission is that there is a part of the beep file generation process which requires knowledge of the normalization coefficients. These coefficients are necessary for the correct production of the beep files, but they are not yet available at this stage.&#10;2. Normalization coefficients are values used to adjust the volume or amplitude of audio signals during processing. They are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. In the context of generating beep files, these coefficients ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;3. As they are still in the early stages of their project, the normalization coefficients might not yet be determined. Therefore, they cannot generate the beep files and include them in the initial submission to IBM for transcription. They will need to complete this step at a later stage when the necessary information is available." target="1. The plan is to compare the two processes of creating a chunked file for IBM transcription services and applying the beep-ify method (adding beeps at regular intervals) on both versions. They will listen to the output of both methods and look for major differences between them.&#10;   &#10;2. When doing the process by hand, they will pay attention to factors such as consistent volume levels and beep duration throughout the recording. This is where normalization coefficients come into play, as these values are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. They ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;&#10;3. One major difference they might look for is whether there's any confusion caused by including a section where some participants were playing sounds from their laptops during the meeting. This could potentially cause issues with differentiating between actual speech and background noise, as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels.&#10;&#10;4. Ultimately, they have decided to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also plan on having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;&#10;5. This discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The reason they cannot include the beep files with the initial submission is that there is a part of the beep file generation process which requires knowledge of the normalization coefficients. These coefficients are necessary for the correct production of the beep files, but they are not yet available at this stage.&#10;2. Normalization coefficients are values used to adjust the volume or amplitude of audio signals during processing. They are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. In the context of generating beep files, these coefficients ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;3. As they are still in the early stages of their project, the normalization coefficients might not yet be determined. Therefore, they cannot generate the beep files and include them in the initial submission to IBM for transcription. They will need to complete this step at a later stage when the necessary information is available." target="The interruptions of hearing a word followed by a beep in the same sentence in the original file are likely caused by the manual application of the &quot;beep-ify&quot; method, where beeps are added at regular intervals throughout the audio recording. If the beep duration and placement is not consistent or well-coordinated with the speech segments, it may result in hearing a word followed by a beep within the same sentence. This issue highlights the importance of maintaining consistent volume levels and beep duration throughout the recording when applying this method manually.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The reason they cannot include the beep files with the initial submission is that there is a part of the beep file generation process which requires knowledge of the normalization coefficients. These coefficients are necessary for the correct production of the beep files, but they are not yet available at this stage.&#10;2. Normalization coefficients are values used to adjust the volume or amplitude of audio signals during processing. They are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. In the context of generating beep files, these coefficients ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;3. As they are still in the early stages of their project, the normalization coefficients might not yet be determined. Therefore, they cannot generate the beep files and include them in the initial submission to IBM for transcription. They will need to complete this step at a later stage when the necessary information is available." target="1. The proposed solution to address the issue of consecutive beeps in the transcription process is to maintain consistent volume levels and beep duration throughout the recording when applying the &quot;beep-ify&quot; method manually. This involves paying close attention to factors such as normalization coefficients, which are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. By ensuring that the inserted beeps have a uniform volume and duration throughout the recording, it can help prevent hearing a word followed by a beep within the same sentence.&#10;&#10;2. Implementing this solution might affect the outcome of the meeting that has already been run through a certain process by improving the overall quality of the transcription. The beep files generated from the original audio recording can serve as a means to compare different processing methods and identify major differences between them. By addressing common challenges in audio recordings, such as consistent volume levels and beep duration, the group can enhance the IBM transcription process and produce more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I {disfmarker} I think , we won't know until we generate a bunch of beep files automatically , listen to them and see how bad they are .&#10;Speaker: PhD B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: We won't be able to s include it with this first thing ,&#10;Speaker: Grad A&#10;Content: If {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Hmm . Oh , OK .&#10;Speaker: PhD D&#10;Content: because there 's a part of the process of the beep file which requires knowing the normalization coefficients .&#10;Speaker: Postdoc C&#10;Content: Oh , I see .&#10;Speaker: PhD D&#10;Content: And {disfmarker} {vocalsound} So a&#10;Speaker: Grad A&#10;Content: That 's not hard to do . Just {disfmarker} it takes {disfmarker} you know , it just takes five minutes rather than , taking a second .&#10;Speaker: PhD D&#10;Content: OK" target="The interruptions of hearing a word followed by a beep in the same sentence in the original file are likely caused by the manual application of the &quot;beep-ify&quot; method, where beeps are added at regular intervals throughout the audio recording. If the beep duration and placement is not consistent or well-coordinated with the speech segments, it may result in hearing a word followed by a beep within the same sentence. This issue highlights the importance of maintaining consistent volume levels and beep duration throughout the recording when applying this method manually.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I {disfmarker} I think , we won't know until we generate a bunch of beep files automatically , listen to them and see how bad they are .&#10;Speaker: PhD B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: We won't be able to s include it with this first thing ,&#10;Speaker: Grad A&#10;Content: If {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Hmm . Oh , OK .&#10;Speaker: PhD D&#10;Content: because there 's a part of the process of the beep file which requires knowing the normalization coefficients .&#10;Speaker: Postdoc C&#10;Content: Oh , I see .&#10;Speaker: PhD D&#10;Content: And {disfmarker} {vocalsound} So a&#10;Speaker: Grad A&#10;Content: That 's not hard to do . Just {disfmarker} it takes {disfmarker} you know , it just takes five minutes rather than , taking a second .&#10;Speaker: PhD D&#10;Content: OK" target="1. The speakers mention that &quot;one EDU (Educational Unit) is good enough&quot; and consider including it in the set of files they will send for transcription. They see value in generating multiple &quot;beep&quot; files to assess their quality, as this might help them determine how bad the audio with laptop sounds is and whether their automatic backchanneling system can improve communication.&#10;2. However, they also express concerns about the time and resources required to review and analyze transcribed content, potential privacy issues, and the accuracy of transcription, especially given the challenges discussed in the transcript.&#10;3. The beep files they plan on generating will serve as a means to compare different processing methods and identify major differences between them. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually, with normalization coefficients being essential for maintaining consistent volume levels across different sections of an audio file and between multiple files.&#10;4. The discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , that if two , chunks are very close to each other on the same channel we 'll just merge them .&#10;Speaker: Postdoc C&#10;Content: Oh ! OK . Ah , interesting . Yeah . Yeah . Oh , sure . Yeah , sure . Makes sense .&#10;Speaker: Grad A&#10;Content: So , uh , and that will get around the problem of , the , {vocalsound} you know &quot; one word beep , one word beep , one word beep , one word beep &quot; .&#10;Speaker: Postdoc C&#10;Content: Yeah . Ah ! Clever . Yes . Clever . Yeah . Excellent .&#10;Speaker: PhD D&#10;Content: Yeah , in fact after our meeting uh , this morning Thilo came in and said that {vocalsound} um , there could be {pause} other differences between {vocalsound} the uh {pause} already transcribed meeting with the beeps in it and one that has {pause} just r been run through his process .&#10;Speaker: Postdoc C&#10;Content: And that 's the purpose . Yeah .&#10;Speaker: PhD D&#10;Content: So tomorrow , {vocalsound} when we go to make the um {pause" target="1. The plan is to compare the two processes of creating a chunked file for IBM transcription services and applying the beep-ify method (adding beeps at regular intervals) on both versions. They will listen to the output of both methods and look for major differences between them.&#10;   &#10;2. When doing the process by hand, they will pay attention to factors such as consistent volume levels and beep duration throughout the recording. This is where normalization coefficients come into play, as these values are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. They ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;&#10;3. One major difference they might look for is whether there's any confusion caused by including a section where some participants were playing sounds from their laptops during the meeting. This could potentially cause issues with differentiating between actual speech and background noise, as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels.&#10;&#10;4. Ultimately, they have decided to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also plan on having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;&#10;5. This discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: So if it 's only inserting &quot; mm - hmm &quot;s here and there , then , wouldn't that be something that would be just as efficient to do at this end , instead of having it go through I B M , then be patched together , then be double checked here .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Right .&#10;Speaker: PhD B&#10;Content: Yeah . But {disfmarker} But then we could just use the {disfmarker} the output of the detector , and do the beeping on it , and send it to I B&#10;Speaker: PhD D&#10;Content: Without having her check anything .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: Postdoc C&#10;Content: Well , I guess {disfmarker}&#10;Speaker: Grad A&#10;Content: I think we just {disfmarker} we just have to listen to it and see how good they are .&#10;Speaker: PhD B&#10;Content: For some meetings , I 'm {disfmarker} I 'm sure it {disfmarker} i n&#10;Speaker" target="1. The group decided to include the section with participants playing sounds from their laptops and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They agreed not to manually insert &quot;mm-hmm&quot;s at this end but rather consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;2. The group did not explicitly discuss sending the output of the detector directly to IBM without any human checking involved. The consensus leans more towards including human intervention, either by having IBM handle the laptop sound section or by listening to the beep file generated by their script for manual indications.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content." target="1. {disfmarker}: This symbol, mentioned by PhD G and Grad A, appears to be a placeholder for some sort of discourse marker or verbal tic that they are correcting or discussing in the context of analyzing conversational data. The disagreement revolves around how specific they should be when referring to the person using these markers; PhD G initially mentions &quot;Morgan&quot; but then changes it to &quot;one participant,&quot; while Grad A wonders if they identified him as a senior member.&#10;   &#10;2. {pause}: This symbol, used by Grad A, likely represents an actual pause in conversation. There is no disagreement about its use.&#10;&#10;3. The laptop sounds and potential transcription issues were discussed, but these are not symbols or unique markers specifically mentioned in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content." target="Based on the meeting transcript, Don will be looking at prosodic properties as part of the team's focus now that they have the data and alignment. Prosodic properties likely include aspects such as intonation, stress, and rhythm in speech. This task was assigned by PhD G to Don during their discussion. Grad E also confirmed that they will be looking at prosodic properties.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content." target="1. The speakers mention that &quot;one EDU (Educational Unit) is good enough&quot; and consider including it in the set of files they will send for transcription. They see value in generating multiple &quot;beep&quot; files to assess their quality, as this might help them determine how bad the audio with laptop sounds is and whether their automatic backchanneling system can improve communication.&#10;2. However, they also express concerns about the time and resources required to review and analyze transcribed content, potential privacy issues, and the accuracy of transcription, especially given the challenges discussed in the transcript.&#10;3. The beep files they plan on generating will serve as a means to compare different processing methods and identify major differences between them. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually, with normalization coefficients being essential for maintaining consistent volume levels across different sections of an audio file and between multiple files.&#10;4. The discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content." target="1. The proposed solution to address the issue of consecutive beeps in the transcription process is to maintain consistent volume levels and beep duration throughout the recording when applying the &quot;beep-ify&quot; method manually. This involves paying close attention to factors such as normalization coefficients, which are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. By ensuring that the inserted beeps have a uniform volume and duration throughout the recording, it can help prevent hearing a word followed by a beep within the same sentence.&#10;&#10;2. Implementing this solution might affect the outcome of the meeting that has already been run through a certain process by improving the overall quality of the transcription. The beep files generated from the original audio recording can serve as a means to compare different processing methods and identify major differences between them. By addressing common challenges in audio recordings, such as consistent volume levels and beep duration, the group can enhance the IBM transcription process and produce more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content." target="The proposed approach for sharing data with another person at SRI, as discussed by PhD G, is using File Transfer Protocol (FTP). PhD G believes that FTP is the best method for several reasons. First, he has created a new directory specifically for sharing the data after being advised that it would be a good idea. This new directory will allow the other person at SRI to access the necessary data easily. While the transcript does not explicitly confirm that the data has been shared and accessed via FTP, PhD G's preparations indicate that he is actively working on implementing this method for sharing the data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content." target="The transcript does not provide direct information on why Liz decided to use the downsampled versions instead of the full bandwidth version. However, it is mentioned that there was no significant difference between them and the downsampled versions even performed better than the original versions in terms of communication. It can be speculated that Liz might have chosen to use the downsampled versions to save disk space or due to other practical considerations, such as easier handling or faster processing times. Ultimately, the reason for her decision is not explicitly stated in the given transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content." target="The benefits of using a channelized interface for encoding categorized speech include ease of handling and improved clarity when the context is apparent. This method would make it easier to distinguish between different types of speech, such as channeled or close-miked speakers.&#10;&#10;However, there are also potential challenges associated with this approach. If the context is not clear, listeners might struggle to determine whether a given sound is part of the channelized speech or an independent audio source. This confusion could lead to misunderstandings and make transcription more difficult for both automated systems and human transcribers. The speakers in the conversation acknowledge these challenges and express concern that such ambiguity could be confusing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers decide to include a portion of the audio recording in the file they will send to IBM for transcription, despite it containing laptop sounds made by some participants during the meeting. They choose this option because excluding the section would require extra work in determining the exact boundaries of what to remove. However, they are aware that both automatic and human transcribers might struggle with distinguishing the laptop sounds from actual speech in this section.&#10;2. While the transcript does not provide specific opinions on having a transcribed version of their conversation evaluated, it is mentioned that Liz presented her research on backchannels and pitch changes in conversations at a conference. The professors express interest in exploring how her automatic backchanneling system could be used to improve communication. This interest suggests that they may be open to evaluating a transcribed version of their conversation, particularly if it can help them understand and improve their communication dynamics.&#10;3. Potential concerns about having a transcribed version of their conversation evaluated might include privacy issues, the accuracy of the transcription (especially given the challenges discussed in the transcript), and the time and resources required to review and analyze the transcribed content." target="1. The speakers decide to include a portion of the audio recording containing laptop sounds in the file they will send to IBM for transcription, despite potential challenges in distinguishing these sounds from actual speech. They also consider having someone manually indicate which sections of the generated beep file are intelligible or not.&#10;2. To handle words that are cut off and other acoustic problems in a transcript, the speakers plan on generating multiple &quot;beep&quot; files to assess their quality and determine how bad the audio with laptop sounds is. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually.&#10;3. The speakers express interest in exploring an automatic backchanneling system developed by a former student named Nigel Ward, which could be used to improve communication and indicate active listening in conversations. This suggests that they may be open to using such tools to help address acoustic problems and improve the overall quality of their transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan is to compare the two processes of creating a chunked file for IBM transcription services and applying the beep-ify method (adding beeps at regular intervals) on both versions. They will listen to the output of both methods and look for major differences between them.&#10;   &#10;2. When doing the process by hand, they will pay attention to factors such as consistent volume levels and beep duration throughout the recording. This is where normalization coefficients come into play, as these values are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. They ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;&#10;3. One major difference they might look for is whether there's any confusion caused by including a section where some participants were playing sounds from their laptops during the meeting. This could potentially cause issues with differentiating between actual speech and background noise, as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels.&#10;&#10;4. Ultimately, they have decided to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also plan on having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;&#10;5. This discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work." target="s the purpose . Yeah .&#10;Speaker: PhD D&#10;Content: So tomorrow , {vocalsound} when we go to make the um {pause} uh , chunked file {vocalsound} for IBM , we 're going to actually compare the two . So he 's gonna run his process on that same meeting ,&#10;Speaker: Postdoc C&#10;Content: Great idea !&#10;Speaker: PhD D&#10;Content: and then we 're gonna do the beep - ify on both , and listen to them and see if we notice any real differences .&#10;Speaker: PhD G&#10;Content: Beep - ify !&#10;Speaker: Postdoc C&#10;Content: OK , now one thing that prevented us from apply you {disfmarker} you from applying {disfmarker} Exactly . The training {disfmarker} So that is the training meeting . OK .&#10;Speaker: PhD D&#10;Content: Yeah , w and we know that . Wel - uh we just wanna if {disfmarker} if there 're any major differences between {vocalsound} doing it on the hand&#10;Speaker: Postdoc C&#10;Content: Uh - huh . Oh , interesting .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan is to compare the two processes of creating a chunked file for IBM transcription services and applying the beep-ify method (adding beeps at regular intervals) on both versions. They will listen to the output of both methods and look for major differences between them.&#10;   &#10;2. When doing the process by hand, they will pay attention to factors such as consistent volume levels and beep duration throughout the recording. This is where normalization coefficients come into play, as these values are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. They ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;&#10;3. One major difference they might look for is whether there's any confusion caused by including a section where some participants were playing sounds from their laptops during the meeting. This could potentially cause issues with differentiating between actual speech and background noise, as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels.&#10;&#10;4. Ultimately, they have decided to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also plan on having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;&#10;5. This discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work." target="} Cuz I {disfmarker} I don't think the staggered mixed channel is awfully good as a way of handling overlaps .&#10;Speaker: Professor F&#10;Content: Yeah . Uh - huh .&#10;Speaker: Postdoc C&#10;Content: But {disfmarker} but uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Well good . That {disfmarker} that really simplifies thing then .&#10;Speaker: Postdoc C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: And we can just , you know , get the meeting , process it , put the beeps file , send it off to IBM .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: You know ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: With very little {pause} work on our side .&#10;Speaker: PhD B&#10;Content: Process it , hear into it . I would {disfmarker}&#10;Speaker: PhD D&#10;Content: Do what ?&#10;Speaker: PhD B&#10;Content: Um , {pause} listen to it ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan is to compare the two processes of creating a chunked file for IBM transcription services and applying the beep-ify method (adding beeps at regular intervals) on both versions. They will listen to the output of both methods and look for major differences between them.&#10;   &#10;2. When doing the process by hand, they will pay attention to factors such as consistent volume levels and beep duration throughout the recording. This is where normalization coefficients come into play, as these values are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. They ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;&#10;3. One major difference they might look for is whether there's any confusion caused by including a section where some participants were playing sounds from their laptops during the meeting. This could potentially cause issues with differentiating between actual speech and background noise, as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels.&#10;&#10;4. Ultimately, they have decided to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also plan on having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;&#10;5. This discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work." target="The interruptions of hearing a word followed by a beep in the same sentence in the original file are likely caused by the manual application of the &quot;beep-ify&quot; method, where beeps are added at regular intervals throughout the audio recording. If the beep duration and placement is not consistent or well-coordinated with the speech segments, it may result in hearing a word followed by a beep within the same sentence. This issue highlights the importance of maintaining consistent volume levels and beep duration throughout the recording when applying this method manually.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan is to compare the two processes of creating a chunked file for IBM transcription services and applying the beep-ify method (adding beeps at regular intervals) on both versions. They will listen to the output of both methods and look for major differences between them.&#10;   &#10;2. When doing the process by hand, they will pay attention to factors such as consistent volume levels and beep duration throughout the recording. This is where normalization coefficients come into play, as these values are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. They ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;&#10;3. One major difference they might look for is whether there's any confusion caused by including a section where some participants were playing sounds from their laptops during the meeting. This could potentially cause issues with differentiating between actual speech and background noise, as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels.&#10;&#10;4. Ultimately, they have decided to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also plan on having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;&#10;5. This discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work." target="1. The group decided to include the section with participants playing sounds from their laptops and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They agreed not to manually insert &quot;mm-hmm&quot;s at this end but rather consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;2. The group did not explicitly discuss sending the output of the detector directly to IBM without any human checking involved. The consensus leans more towards including human intervention, either by having IBM handle the laptop sound section or by listening to the beep file generated by their script for manual indications.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan is to compare the two processes of creating a chunked file for IBM transcription services and applying the beep-ify method (adding beeps at regular intervals) on both versions. They will listen to the output of both methods and look for major differences between them.&#10;   &#10;2. When doing the process by hand, they will pay attention to factors such as consistent volume levels and beep duration throughout the recording. This is where normalization coefficients come into play, as these values are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. They ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;&#10;3. One major difference they might look for is whether there's any confusion caused by including a section where some participants were playing sounds from their laptops during the meeting. This could potentially cause issues with differentiating between actual speech and background noise, as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels.&#10;&#10;4. Ultimately, they have decided to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also plan on having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;&#10;5. This discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work." target="1. The speakers mention that &quot;one EDU (Educational Unit) is good enough&quot; and consider including it in the set of files they will send for transcription. They see value in generating multiple &quot;beep&quot; files to assess their quality, as this might help them determine how bad the audio with laptop sounds is and whether their automatic backchanneling system can improve communication.&#10;2. However, they also express concerns about the time and resources required to review and analyze transcribed content, potential privacy issues, and the accuracy of transcription, especially given the challenges discussed in the transcript.&#10;3. The beep files they plan on generating will serve as a means to compare different processing methods and identify major differences between them. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually, with normalization coefficients being essential for maintaining consistent volume levels across different sections of an audio file and between multiple files.&#10;4. The discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan is to compare the two processes of creating a chunked file for IBM transcription services and applying the beep-ify method (adding beeps at regular intervals) on both versions. They will listen to the output of both methods and look for major differences between them.&#10;   &#10;2. When doing the process by hand, they will pay attention to factors such as consistent volume levels and beep duration throughout the recording. This is where normalization coefficients come into play, as these values are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. They ensure that the inserted beeps have a uniform volume and duration throughout the recording.&#10;&#10;3. One major difference they might look for is whether there's any confusion caused by including a section where some participants were playing sounds from their laptops during the meeting. This could potentially cause issues with differentiating between actual speech and background noise, as the speech-nonspeech detector may randomly assign the laptop sound to one of the channels.&#10;&#10;4. Ultimately, they have decided to include the section with the laptop sounds and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They also plan on having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;&#10;5. This discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work." target="1. The proposed solution to address the issue of consecutive beeps in the transcription process is to maintain consistent volume levels and beep duration throughout the recording when applying the &quot;beep-ify&quot; method manually. This involves paying close attention to factors such as normalization coefficients, which are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. By ensuring that the inserted beeps have a uniform volume and duration throughout the recording, it can help prevent hearing a word followed by a beep within the same sentence.&#10;&#10;2. Implementing this solution might affect the outcome of the meeting that has already been run through a certain process by improving the overall quality of the transcription. The beep files generated from the original audio recording can serve as a means to compare different processing methods and identify major differences between them. By addressing common challenges in audio recordings, such as consistent volume levels and beep duration, the group can enhance the IBM transcription process and produce more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="} Cuz I {disfmarker} I don't think the staggered mixed channel is awfully good as a way of handling overlaps .&#10;Speaker: Professor F&#10;Content: Yeah . Uh - huh .&#10;Speaker: Postdoc C&#10;Content: But {disfmarker} but uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Well good . That {disfmarker} that really simplifies thing then .&#10;Speaker: Postdoc C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: And we can just , you know , get the meeting , process it , put the beeps file , send it off to IBM .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: You know ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: With very little {pause} work on our side .&#10;Speaker: PhD B&#10;Content: Process it , hear into it . I would {disfmarker}&#10;Speaker: PhD D&#10;Content: Do what ?&#10;Speaker: PhD B&#10;Content: Um , {pause} listen to it ," target="1. The group decided to include the section with participants playing sounds from their laptops and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They agreed not to manually insert &quot;mm-hmm&quot;s at this end but rather consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;2. The group did not explicitly discuss sending the output of the detector directly to IBM without any human checking involved. The consensus leans more towards including human intervention, either by having IBM handle the laptop sound section or by listening to the beep file generated by their script for manual indications.">
      <data key="d0">1</data>
    </edge>
    <edge source="The interruptions of hearing a word followed by a beep in the same sentence in the original file are likely caused by the manual application of the &quot;beep-ify&quot; method, where beeps are added at regular intervals throughout the audio recording. If the beep duration and placement is not consistent or well-coordinated with the speech segments, it may result in hearing a word followed by a beep within the same sentence. This issue highlights the importance of maintaining consistent volume levels and beep duration throughout the recording when applying this method manually." target=" do that ?&#10;Speaker: Professor F&#10;Content: no .&#10;Speaker: PhD D&#10;Content: I {disfmarker} I {disfmarker}&#10;Speaker: Professor F&#10;Content: Uh , concentration .&#10;Speaker: PhD B&#10;Content: Perhaps there are {vocalsound} lots of errors in it&#10;Speaker: PhD D&#10;Content: Gah !&#10;Speaker: Grad A&#10;Content: Total concentration . Are you guys ready ?&#10;Speaker: PhD D&#10;Content: You hate to have your ears plugged ?&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Really ?">
      <data key="d0">1</data>
    </edge>
    <edge source="The interruptions of hearing a word followed by a beep in the same sentence in the original file are likely caused by the manual application of the &quot;beep-ify&quot; method, where beeps are added at regular intervals throughout the audio recording. If the beep duration and placement is not consistent or well-coordinated with the speech segments, it may result in hearing a word followed by a beep within the same sentence. This issue highlights the importance of maintaining consistent volume levels and beep duration throughout the recording when applying this method manually." target="&#10;Speaker: Professor F&#10;Content: And {disfmarker}&#10;Speaker: PhD B&#10;Content: So {disfmarker}&#10;Speaker: Postdoc C&#10;Content: And I guess we could just indicate to the transcribers not to {pause} encode that if they {disfmarker} We could still do the beep file .&#10;Speaker: Professor F&#10;Content: Yeah again I {disfmarker} I think that that is probably less of a problem because if you 're {disfmarker} if there 's {disfmarker} {vocalsound} If {disfmarker} if a {disfmarker} if a word is {disfmarker} is split , then they might have to listen to it a few times to really understand that they can't quite get it .&#10;Speaker: Postdoc C&#10;Content: OK . OK .&#10;Speaker: PhD B&#10;Content: But {disfmarker}&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: Whereas if they listen {nonvocalsound} to it and there 's {disfmarker} don">
      <data key="d0">1</data>
    </edge>
    <edge source="The interruptions of hearing a word followed by a beep in the same sentence in the original file are likely caused by the manual application of the &quot;beep-ify&quot; method, where beeps are added at regular intervals throughout the audio recording. If the beep duration and placement is not consistent or well-coordinated with the speech segments, it may result in hearing a word followed by a beep within the same sentence. This issue highlights the importance of maintaining consistent volume levels and beep duration throughout the recording when applying this method manually." target="1. The speakers mention that &quot;one EDU (Educational Unit) is good enough&quot; and consider including it in the set of files they will send for transcription. They see value in generating multiple &quot;beep&quot; files to assess their quality, as this might help them determine how bad the audio with laptop sounds is and whether their automatic backchanneling system can improve communication.&#10;2. However, they also express concerns about the time and resources required to review and analyze transcribed content, potential privacy issues, and the accuracy of transcription, especially given the challenges discussed in the transcript.&#10;3. The beep files they plan on generating will serve as a means to compare different processing methods and identify major differences between them. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually, with normalization coefficients being essential for maintaining consistent volume levels across different sections of an audio file and between multiple files.&#10;4. The discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work.">
      <data key="d0">1</data>
    </edge>
    <edge source="The interruptions of hearing a word followed by a beep in the same sentence in the original file are likely caused by the manual application of the &quot;beep-ify&quot; method, where beeps are added at regular intervals throughout the audio recording. If the beep duration and placement is not consistent or well-coordinated with the speech segments, it may result in hearing a word followed by a beep within the same sentence. This issue highlights the importance of maintaining consistent volume levels and beep duration throughout the recording when applying this method manually." target="1. The proposed solution to address the issue of consecutive beeps in the transcription process is to maintain consistent volume levels and beep duration throughout the recording when applying the &quot;beep-ify&quot; method manually. This involves paying close attention to factors such as normalization coefficients, which are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. By ensuring that the inserted beeps have a uniform volume and duration throughout the recording, it can help prevent hearing a word followed by a beep within the same sentence.&#10;&#10;2. Implementing this solution might affect the outcome of the meeting that has already been run through a certain process by improving the overall quality of the transcription. The beep files generated from the original audio recording can serve as a means to compare different processing methods and identify major differences between them. By addressing common challenges in audio recordings, such as consistent volume levels and beep duration, the group can enhance the IBM transcription process and produce more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="The interruptions of hearing a word followed by a beep in the same sentence in the original file are likely caused by the manual application of the &quot;beep-ify&quot; method, where beeps are added at regular intervals throughout the audio recording. If the beep duration and placement is not consistent or well-coordinated with the speech segments, it may result in hearing a word followed by a beep within the same sentence. This issue highlights the importance of maintaining consistent volume levels and beep duration throughout the recording when applying this method manually." target="1. The speakers decide to include a portion of the audio recording containing laptop sounds in the file they will send to IBM for transcription, despite potential challenges in distinguishing these sounds from actual speech. They also consider having someone manually indicate which sections of the generated beep file are intelligible or not.&#10;2. To handle words that are cut off and other acoustic problems in a transcript, the speakers plan on generating multiple &quot;beep&quot; files to assess their quality and determine how bad the audio with laptop sounds is. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually.&#10;3. The speakers express interest in exploring an automatic backchanneling system developed by a former student named Nigel Ward, which could be used to improve communication and indicate active listening in conversations. This suggests that they may be open to using such tools to help address acoustic problems and improve the overall quality of their transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do that ?&#10;Speaker: Professor F&#10;Content: no .&#10;Speaker: PhD D&#10;Content: I {disfmarker} I {disfmarker}&#10;Speaker: Professor F&#10;Content: Uh , concentration .&#10;Speaker: PhD B&#10;Content: Perhaps there are {vocalsound} lots of errors in it&#10;Speaker: PhD D&#10;Content: Gah !&#10;Speaker: Grad A&#10;Content: Total concentration . Are you guys ready ?&#10;Speaker: PhD D&#10;Content: You hate to have your ears plugged ?&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Really ?" target="1. Professor F realized that adding a random bit in RASTA's option was equivalent to adding flat spectrum because it added energy across all frequencies, similar to adding a constant to the spectrum. This made it faster and more efficient than adding a random bit.&#10;2. The transcript does not provide enough information to answer this question fully. However, it can be inferred that the paper written by PhD G presents three types of results, one of which is a measurement of overlap calculated in terms of the number of words. Another result mentioned involves a computation, and there may have been errors in the paper according to PhD B.&#10;3. The paper written by PhD G has not been reviewed by Professor F yet.&#10;4. The discussion about concentration and turning microphones on or off suggests that they are discussing issues related to sound quality and clarity in their research, possibly related to audio recordings or transcriptions of meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do that ?&#10;Speaker: Professor F&#10;Content: no .&#10;Speaker: PhD D&#10;Content: I {disfmarker} I {disfmarker}&#10;Speaker: Professor F&#10;Content: Uh , concentration .&#10;Speaker: PhD B&#10;Content: Perhaps there are {vocalsound} lots of errors in it&#10;Speaker: PhD D&#10;Content: Gah !&#10;Speaker: Grad A&#10;Content: Total concentration . Are you guys ready ?&#10;Speaker: PhD D&#10;Content: You hate to have your ears plugged ?&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Really ?" target="1. The speakers mention that &quot;one EDU (Educational Unit) is good enough&quot; and consider including it in the set of files they will send for transcription. They see value in generating multiple &quot;beep&quot; files to assess their quality, as this might help them determine how bad the audio with laptop sounds is and whether their automatic backchanneling system can improve communication.&#10;2. However, they also express concerns about the time and resources required to review and analyze transcribed content, potential privacy issues, and the accuracy of transcription, especially given the challenges discussed in the transcript.&#10;3. The beep files they plan on generating will serve as a means to compare different processing methods and identify major differences between them. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually, with normalization coefficients being essential for maintaining consistent volume levels across different sections of an audio file and between multiple files.&#10;4. The discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do that ?&#10;Speaker: Professor F&#10;Content: no .&#10;Speaker: PhD D&#10;Content: I {disfmarker} I {disfmarker}&#10;Speaker: Professor F&#10;Content: Uh , concentration .&#10;Speaker: PhD B&#10;Content: Perhaps there are {vocalsound} lots of errors in it&#10;Speaker: PhD D&#10;Content: Gah !&#10;Speaker: Grad A&#10;Content: Total concentration . Are you guys ready ?&#10;Speaker: PhD D&#10;Content: You hate to have your ears plugged ?&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Really ?" target="The proposed approach for sharing data with another person at SRI, as discussed by PhD G, is using File Transfer Protocol (FTP). PhD G believes that FTP is the best method for several reasons. First, he has created a new directory specifically for sharing the data after being advised that it would be a good idea. This new directory will allow the other person at SRI to access the necessary data easily. While the transcript does not explicitly confirm that the data has been shared and accessed via FTP, PhD G's preparations indicate that he is actively working on implementing this method for sharing the data.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do that ?&#10;Speaker: Professor F&#10;Content: no .&#10;Speaker: PhD D&#10;Content: I {disfmarker} I {disfmarker}&#10;Speaker: Professor F&#10;Content: Uh , concentration .&#10;Speaker: PhD B&#10;Content: Perhaps there are {vocalsound} lots of errors in it&#10;Speaker: PhD D&#10;Content: Gah !&#10;Speaker: Grad A&#10;Content: Total concentration . Are you guys ready ?&#10;Speaker: PhD D&#10;Content: You hate to have your ears plugged ?&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Really ?" target="1. The old recognition output was scored differently, but the recognizer was not provided with information about where the foreground speech starts.&#10;2. The old recognition output was modified to include the lapel microphone, which was previously excluded.&#10;3. Although the issues with the lapel microphone were not explicitly stated as resolved, they were discussed less frequently in comparison to other topics. This could imply that the problems were mitigated but may not have been completely eliminated.&#10;&#10;In summary, the old recognition output underwent a change in scoring and began including data from the lapel microphone. While the resolution of issues related to the lapel microphone was not explicitly confirmed, there was less discussion about those problems.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do that ?&#10;Speaker: Professor F&#10;Content: no .&#10;Speaker: PhD D&#10;Content: I {disfmarker} I {disfmarker}&#10;Speaker: Professor F&#10;Content: Uh , concentration .&#10;Speaker: PhD B&#10;Content: Perhaps there are {vocalsound} lots of errors in it&#10;Speaker: PhD D&#10;Content: Gah !&#10;Speaker: Grad A&#10;Content: Total concentration . Are you guys ready ?&#10;Speaker: PhD D&#10;Content: You hate to have your ears plugged ?&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Really ?" target="1. The speakers decide to include a portion of the audio recording containing laptop sounds in the file they will send to IBM for transcription, despite potential challenges in distinguishing these sounds from actual speech. They also consider having someone manually indicate which sections of the generated beep file are intelligible or not.&#10;2. To handle words that are cut off and other acoustic problems in a transcript, the speakers plan on generating multiple &quot;beep&quot; files to assess their quality and determine how bad the audio with laptop sounds is. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually.&#10;3. The speakers express interest in exploring an automatic backchanneling system developed by a former student named Nigel Ward, which could be used to improve communication and indicate active listening in conversations. This suggests that they may be open to using such tools to help address acoustic problems and improve the overall quality of their transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor F realized that adding a random bit in RASTA's option was equivalent to adding flat spectrum because it added energy across all frequencies, similar to adding a constant to the spectrum. This made it faster and more efficient than adding a random bit.&#10;2. The transcript does not provide enough information to answer this question fully. However, it can be inferred that the paper written by PhD G presents three types of results, one of which is a measurement of overlap calculated in terms of the number of words. Another result mentioned involves a computation, and there may have been errors in the paper according to PhD B.&#10;3. The paper written by PhD G has not been reviewed by Professor F yet.&#10;4. The discussion about concentration and turning microphones on or off suggests that they are discussing issues related to sound quality and clarity in their research, possibly related to audio recordings or transcriptions of meetings." target=" {disfmarker} for {disfmarker} for {disfmarker} when we use the ICSI front - end ,&#10;Speaker: Grad A&#10;Content: As an argument .&#10;Speaker: Professor F&#10;Content: but um , there is an {disfmarker} there is an o an option in {disfmarker} in RASTA , which , um , {vocalsound} in when I first put it in , uh , back in the days when I actually wrote things , uh , {vocalsound} I {pause} did actually put in a random bit or so that was in it ,&#10;Speaker: Grad E&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: but {vocalsound} then I realized that putting in a random bit was equivalent to adding uh {disfmarker} adding flat spectrum ,&#10;Speaker: Grad E&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: and it was a lot faster to just add a constant to the {disfmarker} {vocalsound} to the spectrum . So then I just started doing that&#10;Speaker: Grad E&#10;Content: Mmm . OK .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor F realized that adding a random bit in RASTA's option was equivalent to adding flat spectrum because it added energy across all frequencies, similar to adding a constant to the spectrum. This made it faster and more efficient than adding a random bit.&#10;2. The transcript does not provide enough information to answer this question fully. However, it can be inferred that the paper written by PhD G presents three types of results, one of which is a measurement of overlap calculated in terms of the number of words. Another result mentioned involves a computation, and there may have been errors in the paper according to PhD B.&#10;3. The paper written by PhD G has not been reviewed by Professor F yet.&#10;4. The discussion about concentration and turning microphones on or off suggests that they are discussing issues related to sound quality and clarity in their research, possibly related to audio recordings or transcriptions of meetings." target="mm ! {comment} {vocalsound} Gar - darn !&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: Channel two .&#10;Speaker: Grad A&#10;Content: Do we use square brackets for anything ?&#10;Speaker: Postdoc C&#10;Content: Yeah . Uh {disfmarker}&#10;Speaker: Grad E&#10;Content: These poor transcribers .&#10;Speaker: Professor F&#10;Content: u&#10;Speaker: Postdoc C&#10;Content: Not ri not right now . I mean {disfmarker} No .&#10;Speaker: PhD D&#10;Content: There 's gonna be some zeros from this morning 's meeting because I noticed that&#10;Speaker: Professor F&#10;Content: u&#10;Speaker: PhD D&#10;Content: Barry , I think maybe you turned your mike off before the digits were {disfmarker} Oh , was it during digits ? Oh , so it doesn't matter .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: It 's still not a good idea .&#10;Speaker: PhD B&#10;Content: So it 's not {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor F realized that adding a random bit in RASTA's option was equivalent to adding flat spectrum because it added energy across all frequencies, similar to adding a constant to the spectrum. This made it faster and more efficient than adding a random bit.&#10;2. The transcript does not provide enough information to answer this question fully. However, it can be inferred that the paper written by PhD G presents three types of results, one of which is a measurement of overlap calculated in terms of the number of words. Another result mentioned involves a computation, and there may have been errors in the paper according to PhD B.&#10;3. The paper written by PhD G has not been reviewed by Professor F yet.&#10;4. The discussion about concentration and turning microphones on or off suggests that they are discussing issues related to sound quality and clarity in their research, possibly related to audio recordings or transcriptions of meetings." target="m not looking forward to it .&#10;Speaker: Professor F&#10;Content: i it 's the high signal - to - noise ratio&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: here that {disfmarker} that helps you .&#10;Speaker: PhD G&#10;Content: u s Right . So {disfmarker} so that was number {disfmarker} that was the second set of {disfmarker} uh , the second section . And then , {vocalsound} the third thing was , we looked at , {vocalsound} {vocalsound} uh , what we call &quot; interrupts &quot; , although that 's {disfmarker} that may be {vocalsound} a misnomer , but basically {vocalsound} we looked at cases where {disfmarker} Uh , so we {disfmarker} we used the punctuation from the original transcripts and we inferred the beginnings and ends of sentences . So , you know {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Di - did you use upper - lower case also , or not ?&#10;Speaker: PhD G&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor F realized that adding a random bit in RASTA's option was equivalent to adding flat spectrum because it added energy across all frequencies, similar to adding a constant to the spectrum. This made it faster and more efficient than adding a random bit.&#10;2. The transcript does not provide enough information to answer this question fully. However, it can be inferred that the paper written by PhD G presents three types of results, one of which is a measurement of overlap calculated in terms of the number of words. Another result mentioned involves a computation, and there may have been errors in the paper according to PhD B.&#10;3. The paper written by PhD G has not been reviewed by Professor F yet.&#10;4. The discussion about concentration and turning microphones on or off suggests that they are discussing issues related to sound quality and clarity in their research, possibly related to audio recordings or transcriptions of meetings." target="1. The status of the Eurospeech paper submission is that it has been sent off, but it was sent in late, exceeding the delayed deadline by another day. The reason for its late submission is not explicitly mentioned in the transcript, but it appears to be unrelated to any issues with the paper's content.&#10;2. There is no direct information given about why the paper was sent in late. Grad A simply states, &quot;Well yeah, you sent it in late.&quot; No further explanation or context is provided in the transcript.&#10;3. The transcript does indicate that there was a delayed deadline and the paper exceeded it by another day. Professor F asks, &quot;Oh they had some extension that they announced or something?&quot; to which PhD G responds, &quot;Well yeah. Liz had sent them a note saying 'could we please have another three days' or something, and they said yes.&quot; It seems there was an agreed-upon extension, but the paper was still submitted later than the extended deadline.&#10;4. The exact reason for the late submission remains unclear from the transcript. It could be due to unforeseen circumstances, last-minute revisions, or other factors not mentioned in the conversation. However, it is evident that the paper has been sent off and the authors are now waiting for the review process to begin.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. {disfmarker}: This symbol, mentioned by PhD G and Grad A, appears to be a placeholder for some sort of discourse marker or verbal tic that they are correcting or discussing in the context of analyzing conversational data. The disagreement revolves around how specific they should be when referring to the person using these markers; PhD G initially mentions &quot;Morgan&quot; but then changes it to &quot;one participant,&quot; while Grad A wonders if they identified him as a senior member.&#10;   &#10;2. {pause}: This symbol, used by Grad A, likely represents an actual pause in conversation. There is no disagreement about its use.&#10;&#10;3. The laptop sounds and potential transcription issues were discussed, but these are not symbols or unique markers specifically mentioned in the conversation." target=" all of them ,&#10;Speaker: Postdoc C&#10;Content: Oh . OK .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: I mean , we didn't mention Morgan by name&#10;Speaker: Grad A&#10;Content: and he talks a lot .&#10;Speaker: PhD G&#10;Content: we just {disfmarker}&#10;Speaker: Grad A&#10;Content: One participant .&#10;Speaker: Professor F&#10;Content: Well {disfmarker} we have now , but {disfmarker}&#10;Speaker: PhD G&#10;Content: We {disfmarker} we {disfmarker} we {disfmarker} something about {disfmarker}&#10;Speaker: Grad A&#10;Content: Did you identify him as a senior {pause} member ?&#10;Speaker: PhD G&#10;Content: No , we as identify him as the person dominating the conversation .&#10;Speaker: Professor F&#10;Content: Well .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: I mean I get these AARP things , but I 'm">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, the group is open to the idea of normalizing the comparison of robustness between multi-party and two-party conversations by the number of speakers involved. This suggestion was raised by PhD D and supported by other participants, including PhD B, Postdoc C, and Professor F. They believe that this normalization could provide a more fair and accurate comparison between different types of conversations. However, the group does not make a definitive decision on this matter during the discussion, so further conversation may be necessary to reach a conclusion." target=" Meeting Recorder meetings and {vocalsound} about half that for , {vocalsound} uh , the Robustness .&#10;Speaker: Professor F&#10;Content: Maybe ten percent ?&#10;Speaker: Grad A&#10;Content: But I don't know if that 's really a fair way of comparing between , multi - party , conversations and two - party conversations . Yeah . I {disfmarker} I {disfmarker} I don't know .&#10;Speaker: PhD B&#10;Content: Then {disfmarker} then {disfmarker} then you have to {disfmarker}&#10;Speaker: Grad A&#10;Content: I mean that 's just something {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah , I just wonder if you have to normalize by the numbers of speakers or something .&#10;Speaker: Postdoc C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Then {disfmarker} Yeah , then normalize by {disfmarker} by something like that ,&#10;Speaker: Postdoc C&#10;Content: Yeah , that 's a good point .&#10;Speaker: PhD G&#10;Content: Well ,">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, the group is open to the idea of normalizing the comparison of robustness between multi-party and two-party conversations by the number of speakers involved. This suggestion was raised by PhD D and supported by other participants, including PhD B, Postdoc C, and Professor F. They believe that this normalization could provide a more fair and accurate comparison between different types of conversations. However, the group does not make a definitive decision on this matter during the discussion, so further conversation may be necessary to reach a conclusion." target="The benefits of using a channelized interface for encoding categorized speech include ease of handling and improved clarity when the context is apparent. This method would make it easier to distinguish between different types of speech, such as channeled or close-miked speakers.&#10;&#10;However, there are also potential challenges associated with this approach. If the context is not clear, listeners might struggle to determine whether a given sound is part of the channelized speech or an independent audio source. This confusion could lead to misunderstandings and make transcription more difficult for both automated systems and human transcribers. The speakers in the conversation acknowledge these challenges and express concern that such ambiguity could be confusing.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the meeting transcript, Don will be looking at prosodic properties as part of the team's focus now that they have the data and alignment. Prosodic properties likely include aspects such as intonation, stress, and rhythm in speech. This task was assigned by PhD G to Don during their discussion. Grad E also confirmed that they will be looking at prosodic properties." target="&#10;Content: uh , well Don 's been working hard .&#10;Speaker: Grad E&#10;Content: That 's right .&#10;Speaker: PhD G&#10;Content: OK , so {disfmarker} {vocalsound} so if you have the foreground speaker speaking here , and then there 's some background speech , may be overlapping it somehow , um , and this is the time bin that we used , then of course you 're gonna get insertion errors here and here .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: Right ? So we scored everything , and I must say the NIST scoring tools are pretty nice for this , where you just basically ignore everything outside of the , {vocalsound} uh , region that was deemed to be foreground speech . And where that was we had to use the t forced alignment , uh , results from s for {disfmarker} so {disfmarker} That 's somewhat {disfmarker} that 's somewhat subject to error , but still we {disfmarker} we {disfmarker} {vocalsound} Uh , Don did some ha hand - checking and {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The status of the Eurospeech paper submission is that it has been sent off, but it was sent in late, exceeding the delayed deadline by another day. The reason for its late submission is not explicitly mentioned in the transcript, but it appears to be unrelated to any issues with the paper's content.&#10;2. There is no direct information given about why the paper was sent in late. Grad A simply states, &quot;Well yeah, you sent it in late.&quot; No further explanation or context is provided in the transcript.&#10;3. The transcript does indicate that there was a delayed deadline and the paper exceeded it by another day. Professor F asks, &quot;Oh they had some extension that they announced or something?&quot; to which PhD G responds, &quot;Well yeah. Liz had sent them a note saying 'could we please have another three days' or something, and they said yes.&quot; It seems there was an agreed-upon extension, but the paper was still submitted later than the extended deadline.&#10;4. The exact reason for the late submission remains unclear from the transcript. It could be due to unforeseen circumstances, last-minute revisions, or other factors not mentioned in the conversation. However, it is evident that the paper has been sent off and the authors are now waiting for the review process to begin." target="Content: Oops .&#10;Speaker: Professor F&#10;Content: Oh they {disfmarker} they had some extension that they announced or something ?&#10;Speaker: PhD G&#10;Content: Well yeah . Liz had sent them a note saying &quot; could we please {pause} have another &quot; {comment} {pause} I don't know , &quot; three days &quot; or something , and they said yes .&#10;Speaker: PhD D&#10;Content: And then she said &quot; Did I say three ?&#10;Speaker: Grad A&#10;Content: Oh ,&#10;Speaker: PhD D&#10;Content: I meant four . &quot;&#10;Speaker: Grad A&#10;Content: that was the other thing uh ,&#10;Speaker: PhD G&#10;Content: But u&#10;Speaker: Grad A&#10;Content: uh , Dave Gelbart sent me email , I think he sent it to you too , {comment} that um , there 's a special topic , section in si in Eurospeech on new , corp corpors corpora . And it 's not due until like May fifteenth .&#10;Speaker: Professor F&#10;Content: Oh this isn't the Aurora one ?&#10;Speaker: Grad A&#10;Content: No .&#10;Speaker: Professor F&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The status of the Eurospeech paper submission is that it has been sent off, but it was sent in late, exceeding the delayed deadline by another day. The reason for its late submission is not explicitly mentioned in the transcript, but it appears to be unrelated to any issues with the paper's content.&#10;2. There is no direct information given about why the paper was sent in late. Grad A simply states, &quot;Well yeah, you sent it in late.&quot; No further explanation or context is provided in the transcript.&#10;3. The transcript does indicate that there was a delayed deadline and the paper exceeded it by another day. Professor F asks, &quot;Oh they had some extension that they announced or something?&quot; to which PhD G responds, &quot;Well yeah. Liz had sent them a note saying 'could we please have another three days' or something, and they said yes.&quot; It seems there was an agreed-upon extension, but the paper was still submitted later than the extended deadline.&#10;4. The exact reason for the late submission remains unclear from the transcript. It could be due to unforeseen circumstances, last-minute revisions, or other factors not mentioned in the conversation. However, it is evident that the paper has been sent off and the authors are now waiting for the review process to begin." target="The transcript does not provide direct information on why Liz decided to use the downsampled versions instead of the full bandwidth version. However, it is mentioned that there was no significant difference between them and the downsampled versions even performed better than the original versions in terms of communication. It can be speculated that Liz might have chosen to use the downsampled versions to save disk space or due to other practical considerations, such as easier handling or faster processing times. Ultimately, the reason for her decision is not explicitly stated in the given transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The status of the Eurospeech paper submission is that it has been sent off, but it was sent in late, exceeding the delayed deadline by another day. The reason for its late submission is not explicitly mentioned in the transcript, but it appears to be unrelated to any issues with the paper's content.&#10;2. There is no direct information given about why the paper was sent in late. Grad A simply states, &quot;Well yeah, you sent it in late.&quot; No further explanation or context is provided in the transcript.&#10;3. The transcript does indicate that there was a delayed deadline and the paper exceeded it by another day. Professor F asks, &quot;Oh they had some extension that they announced or something?&quot; to which PhD G responds, &quot;Well yeah. Liz had sent them a note saying 'could we please have another three days' or something, and they said yes.&quot; It seems there was an agreed-upon extension, but the paper was still submitted later than the extended deadline.&#10;4. The exact reason for the late submission remains unclear from the transcript. It could be due to unforeseen circumstances, last-minute revisions, or other factors not mentioned in the conversation. However, it is evident that the paper has been sent off and the authors are now waiting for the review process to begin." target="1. The specific content of the paper that Graduate Student A printed out but hasn't read yet is not mentioned in the transcript. However, it is clear that the paper is related to a Monty Python sketch and has been submitted for Eurospeech.&#10;2. PhD Student G is planning to send an email to Brian Kingbury about where he can find the related material because Brian had previously requested this material for a speech recognition experiment. PhD G was in the process of sending the location of the shared directory when his desktop froze, causing him to delay the email. The transcript suggests that PhD G has prepared a shared directory via FTP to send to Brian, but it does not explicitly confirm whether the email has been sent or the material has been accessed by Brian at SRI.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group decided to include the section with participants playing sounds from their laptops and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They agreed not to manually insert &quot;mm-hmm&quot;s at this end but rather consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;2. The group did not explicitly discuss sending the output of the detector directly to IBM without any human checking involved. The consensus leans more towards including human intervention, either by having IBM handle the laptop sound section or by listening to the beep file generated by their script for manual indications." target="Content: So far we haven't . So this is not gonna be a major part of the process , at least {disfmarker} least not in {disfmarker} not on ones that {disfmarker} that really {disfmarker}&#10;Speaker: PhD D&#10;Content: So if you don't have to adjust the bins , why not just do what it {disfmarker} for all the channels ?&#10;Speaker: Postdoc C&#10;Content: Mm - hmm ?&#10;Speaker: PhD D&#10;Content: Why not just throw all the channels to IBM ?&#10;Speaker: Postdoc C&#10;Content: Well there 's the question o of {pause} whether {disfmarker} Well , OK . She i It 's a question of how much time we want our transcriber to invest here {vocalsound} when she 's gonna have to invest that when it comes back from IBM anyway .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Postdoc C&#10;Content: So if it 's only inserting &quot; mm - hmm &quot;s here and there , then , wouldn't that be something that would be just as">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group decided to include the section with participants playing sounds from their laptops and let IBM handle it, as excluding it would require extra work in determining the exact boundaries of what to remove. They agreed not to manually insert &quot;mm-hmm&quot;s at this end but rather consider having someone listen to the beep file generated by their script and manually indicate which sections are intelligible or not.&#10;2. The group did not explicitly discuss sending the output of the detector directly to IBM without any human checking involved. The consensus leans more towards including human intervention, either by having IBM handle the laptop sound section or by listening to the beep file generated by their script for manual indications." target="er} uh , for our purposes for the {disfmarker} for the IBM preparation , {vocalsound} uh , n having these {pause} joined together ,&#10;Speaker: PhD B&#10;Content: Yeah . Yeah .&#10;Speaker: Postdoc C&#10;Content: and uh {disfmarker} It makes a lot of sense . And in terms of transcription , it would be easy to do it that way .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: The way that they have with the longer units ,&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: not having to fuss with adding these units at this time .&#10;Speaker: PhD B&#10;Content: Yeah . Whi - which could have one drawback . If there is uh a backchannel in between those three things ,&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Postdoc C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: the {disfmarker} the n the backchannel will {disfmarker} will occur at the end of {disfmarker} of those">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers mention that &quot;one EDU (Educational Unit) is good enough&quot; and consider including it in the set of files they will send for transcription. They see value in generating multiple &quot;beep&quot; files to assess their quality, as this might help them determine how bad the audio with laptop sounds is and whether their automatic backchanneling system can improve communication.&#10;2. However, they also express concerns about the time and resources required to review and analyze transcribed content, potential privacy issues, and the accuracy of transcription, especially given the challenges discussed in the transcript.&#10;3. The beep files they plan on generating will serve as a means to compare different processing methods and identify major differences between them. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually, with normalization coefficients being essential for maintaining consistent volume levels across different sections of an audio file and between multiple files.&#10;4. The discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work." target=" PhD D&#10;Content: just {disfmarker}&#10;Speaker: Grad A&#10;Content: I mean it just depends on how {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Well EDU {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah ,&#10;Speaker: Grad A&#10;Content: Sorry , go ahead .&#10;Speaker: PhD B&#10;Content: u u u&#10;Speaker: Postdoc C&#10;Content: So I was gonna say , EDU - one is good enough ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: maybe we could include it in this {disfmarker} in this set of uh , this stuff we send .&#10;Speaker: PhD B&#10;Content: Yeah there 's {disfmarker} I {disfmarker} I think there are some meetings where it would {disfmarker} would {disfmarker} It 's possible like this .&#10;Speaker: Grad A&#10;Content: Yeah I {disfmarker} I think , we won't know until we generate a bunch of beep files automatically , listen to them and see how bad">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers mention that &quot;one EDU (Educational Unit) is good enough&quot; and consider including it in the set of files they will send for transcription. They see value in generating multiple &quot;beep&quot; files to assess their quality, as this might help them determine how bad the audio with laptop sounds is and whether their automatic backchanneling system can improve communication.&#10;2. However, they also express concerns about the time and resources required to review and analyze transcribed content, potential privacy issues, and the accuracy of transcription, especially given the challenges discussed in the transcript.&#10;3. The beep files they plan on generating will serve as a means to compare different processing methods and identify major differences between them. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually, with normalization coefficients being essential for maintaining consistent volume levels across different sections of an audio file and between multiple files.&#10;4. The discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work." target="1. The proposed solution to address the issue of consecutive beeps in the transcription process is to maintain consistent volume levels and beep duration throughout the recording when applying the &quot;beep-ify&quot; method manually. This involves paying close attention to factors such as normalization coefficients, which are essential for maintaining consistent volume levels across different sections of an audio file and between multiple files. By ensuring that the inserted beeps have a uniform volume and duration throughout the recording, it can help prevent hearing a word followed by a beep within the same sentence.&#10;&#10;2. Implementing this solution might affect the outcome of the meeting that has already been run through a certain process by improving the overall quality of the transcription. The beep files generated from the original audio recording can serve as a means to compare different processing methods and identify major differences between them. By addressing common challenges in audio recordings, such as consistent volume levels and beep duration, the group can enhance the IBM transcription process and produce more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers mention that &quot;one EDU (Educational Unit) is good enough&quot; and consider including it in the set of files they will send for transcription. They see value in generating multiple &quot;beep&quot; files to assess their quality, as this might help them determine how bad the audio with laptop sounds is and whether their automatic backchanneling system can improve communication.&#10;2. However, they also express concerns about the time and resources required to review and analyze transcribed content, potential privacy issues, and the accuracy of transcription, especially given the challenges discussed in the transcript.&#10;3. The beep files they plan on generating will serve as a means to compare different processing methods and identify major differences between them. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually, with normalization coefficients being essential for maintaining consistent volume levels across different sections of an audio file and between multiple files.&#10;4. The discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work." target="The proposed approach for sharing data with another person at SRI, as discussed by PhD G, is using File Transfer Protocol (FTP). PhD G believes that FTP is the best method for several reasons. First, he has created a new directory specifically for sharing the data after being advised that it would be a good idea. This new directory will allow the other person at SRI to access the necessary data easily. While the transcript does not explicitly confirm that the data has been shared and accessed via FTP, PhD G's preparations indicate that he is actively working on implementing this method for sharing the data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers mention that &quot;one EDU (Educational Unit) is good enough&quot; and consider including it in the set of files they will send for transcription. They see value in generating multiple &quot;beep&quot; files to assess their quality, as this might help them determine how bad the audio with laptop sounds is and whether their automatic backchanneling system can improve communication.&#10;2. However, they also express concerns about the time and resources required to review and analyze transcribed content, potential privacy issues, and the accuracy of transcription, especially given the challenges discussed in the transcript.&#10;3. The beep files they plan on generating will serve as a means to compare different processing methods and identify major differences between them. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually, with normalization coefficients being essential for maintaining consistent volume levels across different sections of an audio file and between multiple files.&#10;4. The discussion about comparing processes and identifying major differences is an important part of the IBM transcription process, as they aim to address common challenges in audio recordings and improve the overall quality of their work." target="1. The speakers decide to include a portion of the audio recording containing laptop sounds in the file they will send to IBM for transcription, despite potential challenges in distinguishing these sounds from actual speech. They also consider having someone manually indicate which sections of the generated beep file are intelligible or not.&#10;2. To handle words that are cut off and other acoustic problems in a transcript, the speakers plan on generating multiple &quot;beep&quot; files to assess their quality and determine how bad the audio with laptop sounds is. They will pay attention to factors such as consistent volume levels and beep duration throughout the recording when applying the beep-ify method manually.&#10;3. The speakers express interest in exploring an automatic backchanneling system developed by a former student named Nigel Ward, which could be used to improve communication and indicate active listening in conversations. This suggests that they may be open to using such tools to help address acoustic problems and improve the overall quality of their transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The old recognition output was scored differently, but the recognizer was not provided with information about where the foreground speech starts.&#10;2. The old recognition output was modified to include the lapel microphone, which was previously excluded.&#10;3. Although the issues with the lapel microphone were not explicitly stated as resolved, they were discussed less frequently in comparison to other topics. This could imply that the problems were mitigated but may not have been completely eliminated.&#10;&#10;In summary, the old recognition output underwent a change in scoring and began including data from the lapel microphone. While the resolution of issues related to the lapel microphone was not explicitly confirmed, there was less discussion about those problems." target=" bit of a problem&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: That 's great .&#10;Speaker: PhD B&#10;Content: as it really switches around between {vocalsound} two different channels , I think .&#10;Speaker: Grad A&#10;Content: Mm - hmm , and {disfmarker} and they 're very {disfmarker} it 's very audible ? on the close - talking channels ?&#10;Speaker: PhD B&#10;Content: What {disfmarker} what I would {disfmarker} Yeah .&#10;Speaker: Grad A&#10;Content: Oh well . I mean , it 's the same problem as the lapel mike .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: But {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Oh , interesting .&#10;Speaker: PhD B&#10;Content: Comparable , yeah .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Postdoc C&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The old recognition output was scored differently, but the recognizer was not provided with information about where the foreground speech starts.&#10;2. The old recognition output was modified to include the lapel microphone, which was previously excluded.&#10;3. Although the issues with the lapel microphone were not explicitly stated as resolved, they were discussed less frequently in comparison to other topics. This could imply that the problems were mitigated but may not have been completely eliminated.&#10;&#10;In summary, the old recognition output underwent a change in scoring and began including data from the lapel microphone. While the resolution of issues related to the lapel microphone was not explicitly confirmed, there was less discussion about those problems." target="} you know , we {disfmarker} we uh , {vocalsound} make all the features have zero mean and unit variance .&#10;Speaker: Grad A&#10;Content: Over an entire utterance ?&#10;Speaker: Professor F&#10;Content: And {disfmarker}&#10;Speaker: Grad A&#10;Content: Or windowed ?&#10;Speaker: PhD G&#10;Content: Over {disfmarker} over the entire c over the entire channel .&#10;Speaker: PhD B&#10;Content: Don't {pause} train {disfmarker}&#10;Speaker: PhD G&#10;Content: Over the {disfmarker}&#10;Speaker: Grad A&#10;Content: Hmm .&#10;Speaker: PhD G&#10;Content: but you know . Um , now we didn't re - align the recognizer for this . We just took the old {disfmarker} So this is actually a sub - optimal way of doing it ,&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: right ? So we took the old recognition output and we just scored it differently . So the recognizer didn't have the benefit">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The old recognition output was scored differently, but the recognizer was not provided with information about where the foreground speech starts.&#10;2. The old recognition output was modified to include the lapel microphone, which was previously excluded.&#10;3. Although the issues with the lapel microphone were not explicitly stated as resolved, they were discussed less frequently in comparison to other topics. This could imply that the problems were mitigated but may not have been completely eliminated.&#10;&#10;In summary, the old recognition output underwent a change in scoring and began including data from the lapel microphone. While the resolution of issues related to the lapel microphone was not explicitly confirmed, there was less discussion about those problems." target=" any major differences between {vocalsound} doing it on the hand&#10;Speaker: Postdoc C&#10;Content: Uh - huh . Oh , interesting . Ah !&#10;Speaker: Grad A&#10;Content: Hmm .&#10;Speaker: Postdoc C&#10;Content: OK . Interesting idea . Great .&#10;Speaker: PhD G&#10;Content: So this training meeting , uh w un is that uh {pause} some data where we have uh very um , {vocalsound} you know , accurate {pause} time marks ? for {disfmarker}&#10;Speaker: Postdoc C&#10;Content: I went back and hand - marked the {pause} ba the bins , I ment I mentioned that last week .&#10;Speaker: PhD G&#10;Content: OK , yeah .&#10;Speaker: PhD D&#10;Content: But the {disfmarker} but there 's {disfmarker} yeah , but there is this one issue with them in that there 're {disfmarker} {vocalsound} there are time boundaries in there that occur in the middle of speech .&#10;Speaker: PhD G&#10;Content: Because {disfmarker}&#10;Speaker: PhD D&#10;Content: So {">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcript does not provide direct information on why Liz decided to use the downsampled versions instead of the full bandwidth version. However, it is mentioned that there was no significant difference between them and the downsampled versions even performed better than the original versions in terms of communication. It can be speculated that Liz might have chosen to use the downsampled versions to save disk space or due to other practical considerations, such as easier handling or faster processing times. Ultimately, the reason for her decision is not explicitly stated in the given transcript." target="aker: PhD G&#10;Content: OK . Alright , then I 'll hold off on that and I 'll wait for you um {disfmarker}&#10;Speaker: Grad E&#10;Content: Probably by tomorrow&#10;Speaker: PhD G&#10;Content: gen&#10;Speaker: Grad E&#10;Content: I can {disfmarker} I 'll send you an email .&#10;Speaker: PhD G&#10;Content: OK . Alright . OK . Yeah , definitely they should have the full bandwidth version ,&#10;Speaker: Grad E&#10;Content: Yeah , because I mean {disfmarker} I I think Liz decided to go ahead with the {pause} downsampled versions cuz we can {disfmarker} There was no s like , r significant difference .&#10;Speaker: PhD G&#10;Content: yeah . OK . Well , it takes {disfmarker} it takes up less disk space , for one thing .&#10;Speaker: Grad E&#10;Content: It does take up less disk space , and apparently it did even better {pause} than the original {disfmarker} than the original versions ,&#10;Speaker: PhD G&#10;Content: Yeah . Yeah .&#10;Speaker: Grad E">
      <data key="d0">1</data>
    </edge>
    <edge source="The benefits of using a channelized interface for encoding categorized speech include ease of handling and improved clarity when the context is apparent. This method would make it easier to distinguish between different types of speech, such as channeled or close-miked speakers.&#10;&#10;However, there are also potential challenges associated with this approach. If the context is not clear, listeners might struggle to determine whether a given sound is part of the channelized speech or an independent audio source. This confusion could lead to misunderstandings and make transcription more difficult for both automated systems and human transcribers. The speakers in the conversation acknowledge these challenges and express concern that such ambiguity could be confusing." target="&#10;Content: So th&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: there was a category for @ @ {comment} speech .&#10;Speaker: Postdoc C&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Yeah , the default .&#10;Speaker: Postdoc C&#10;Content: Yeah , s a&#10;Speaker: Grad A&#10;Content: No , not default .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Postdoc C&#10;Content: Well , as it comes back , we have a uh {disfmarker} when we can use the channelized interface for encoding it , then it 'll be easy for us to handle .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Postdoc C&#10;Content: But {disfmarker} {vocalsound} but if {disfmarker} if out of context , they can't tell if it 's a channeled speak uh , you know , a close - miked speaker or not , {vocalsound} then that would be confusing to them .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Grad A&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The specific content of the paper that Graduate Student A printed out but hasn't read yet is not mentioned in the transcript. However, it is clear that the paper is related to a Monty Python sketch and has been submitted for Eurospeech.&#10;2. PhD Student G is planning to send an email to Brian Kingbury about where he can find the related material because Brian had previously requested this material for a speech recognition experiment. PhD G was in the process of sending the location of the shared directory when his desktop froze, causing him to delay the email. The transcript suggests that PhD G has prepared a shared directory via FTP to send to Brian, but it does not explicitly confirm whether the email has been sent or the material has been accessed by Brian at SRI." target="Content: Random intervals .&#10;Speaker: Grad A&#10;Content: There was {disfmarker} there was of course a Monty Python sketch with that . Where the barber who was afraid of scissors was playing a {disfmarker} a tape of clipping sounds , and saying &quot; uh - huh &quot; , &quot; yeah &quot; , &quot; how about them sports teams ? &quot;&#10;Speaker: PhD G&#10;Content: Anyway . So the paper 's on - line and y I {disfmarker} I think I uh {disfmarker} I CC ' ed a message to Meeting Recorder with the URL so you can get it .&#10;Speaker: Grad A&#10;Content: Yep .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: Printed it out , haven't read it yet .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Um , uh one more thing . So I {disfmarker} I 'm actually {disfmarker} {vocalsound} about to send Brian Kingbury an email saying where he can find the {disfmarker} the s the m the material he wanted for the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The specific content of the paper that Graduate Student A printed out but hasn't read yet is not mentioned in the transcript. However, it is clear that the paper is related to a Monty Python sketch and has been submitted for Eurospeech.&#10;2. PhD Student G is planning to send an email to Brian Kingbury about where he can find the related material because Brian had previously requested this material for a speech recognition experiment. PhD G was in the process of sending the location of the shared directory when his desktop froze, causing him to delay the email. The transcript suggests that PhD G has prepared a shared directory via FTP to send to Brian, but it does not explicitly confirm whether the email has been sent or the material has been accessed by Brian at SRI." target="ound} about to send Brian Kingbury an email saying where he can find the {disfmarker} the s the m the material he wanted for the s for the speech recognition experiment , so {disfmarker} but I haven't sent it out yet because actually my desktop locked up , like I can't type anything . Uh b so if there 's any suggestions you have for that I was just gonna send him the {disfmarker}&#10;Speaker: PhD D&#10;Content: Is it the same directory that you had suggested ?&#10;Speaker: PhD G&#10;Content: I made a directory . I called it um {disfmarker}&#10;Speaker: Postdoc C&#10;Content: He still has his Unix account here , you know .&#10;Speaker: PhD G&#10;Content: Well this isn't {disfmarker}&#10;Speaker: Postdoc C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: He does ?&#10;Speaker: Postdoc C&#10;Content: And he {disfmarker} and he 's {disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah but {disfmarker} but {disfmarker} but he has">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
