<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="Based on the discussion, the person being referred to (it's not clear who this is exactly, as there are several people mentioned) is interested in or potentially working on a project that involves removing high-level constraints and approaching it from a bottom-up perspective. This project seems to be related to using less data and infrastructure than might typically be used for similar projects. Professor B also suggests that if someone with a strong interest in this area were to become involved, they could drive the direction of the project. However, there are no specific details about the nature or scope of the project itself." />
    <node id="Speaker: Professor B&#10;Content: Are we on ? We 're on . OK .&#10;Speaker: PhD E&#10;Content: Is it on ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . OK ,&#10;Speaker: PhD D&#10;Content: One , two {disfmarker} u OK .&#10;Speaker: PhD A&#10;Content: Why is it so cold in here ?&#10;Speaker: Professor B&#10;Content: so , uh , we haven't sent around the agenda . So , i uh , any agenda items anybody has , wants to talk about , what 's going on ?&#10;Speaker: Postdoc G&#10;Content: I c I could talk about the meeting .&#10;Speaker: Grad H&#10;Content: Does everyone {disfmarker} has everyone met Don ?&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: It 's on ?&#10;Speaker: PhD C&#10;Content: Now , yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: Yeah ? OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker:" />
    <node id=" .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: So .&#10;Speaker: Grad H&#10;Content: Yeah , and you {disfmarker} and you signed a form .&#10;Speaker: Grad F&#10;Content: Oh , I think so .&#10;Speaker: Postdoc G&#10;Content: Did you sign a form ?&#10;Speaker: Grad F&#10;Content: Did I ? I don't know .&#10;Speaker: Grad H&#10;Content: I 'm pretty sure . Well I 'll {disfmarker} I 'll get another one before the end of the meeting .&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: Thank you .&#10;Speaker: Grad F&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: You don't {disfmarker} you don't have to leave for it .&#10;Speaker: Professor B&#10;Content: Yeah , we {disfmarker} we {disfmarker}" />
    <node id=" Professor B&#10;Content: It 's pretty tough , uh , this group . Yeah .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So , uh , what about {disfmarker} what about people who involved in some artistic endeavor ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean , film - making or something like that .&#10;Speaker: PhD A&#10;Content: Exactly , that 's what I was {disfmarker}&#10;Speaker: Professor B&#10;Content: You 'd think like they would be {disfmarker}&#10;Speaker: PhD D&#10;Content: A film - maker .&#10;Speaker: PhD A&#10;Content: something where there {disfmarker} there is actually discussion where there 's no right or wrong answer but {disfmarker} but it 's a matter of opinion kind of thing . Uh , anyway , if you {disfmarker} if you have ideas {disfmarker}&#10;Speaker: Postdoc G&#10;Content: It 's be fun .&#10;Speaker: Grad" />
    <node id=": Grad H&#10;Content: Well , I don't know if they meet regularly or not but they are no longer recording .&#10;Speaker: Professor B&#10;Content: But I mean , ha ha have they said they don't want to anymore or {disfmarker} ?&#10;Speaker: Grad H&#10;Content: Um , ugh , what was his name ?&#10;Speaker: Professor B&#10;Content: Uh , i i&#10;Speaker: Postdoc G&#10;Content: Joe Sokol ?&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: When {disfmarker} with him gone , it sorta trickled off .&#10;Speaker: Professor B&#10;Content: OK , so they 're down to three or four people&#10;Speaker: Grad H&#10;Content: They {disfmarker} and they stopped {disfmarker} Yeah .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: but the thing is three or four people is OK .&#10;Speaker: Grad H&#10;Content: Yep .&#10;Speaker: Postdoc G&#10;Content:" />
    <node id="&#10;Speaker: Postdoc G&#10;Content: Mm - hmm , OK .&#10;Speaker: Grad H&#10;Content: Yeah , but {disfmarker} but just saying what the {disfmarker}&#10;Speaker: Professor B&#10;Content: because it 's {disfmarker} you know , there 's this movement from here to here&#10;Speaker: Postdoc G&#10;Content: Yeah , I 'm sure . Uh , yeah , I {disfmarker} I know .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: and {disfmarker} and {disfmarker} and it 's {disfmarker} so I&#10;Speaker: PhD E&#10;Content: You 're saying r sort of remove the high level constraints and go bottom - up .&#10;Speaker: Professor B&#10;Content: Yeah , describe {disfmarker} describe it .&#10;Speaker: PhD E&#10;Content: Then just say {disfmarker}&#10;Speaker: Grad H&#10;Content: Yep , just features .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content:" />
    <node id=" not use all the data ?&#10;Speaker: Grad H&#10;Content: It 's {disfmarker} it 's just a lot of infrastructure that for our particular purpose we felt we didn't need to set up .&#10;Speaker: Postdoc G&#10;Content: I see .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Fine .&#10;Speaker: Professor B&#10;Content: Yeah , if ninety - nine percent of what you 're doing is c is shutting off most of the mikes , then going through the {disfmarker}&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: But if you get somebody who 's {disfmarker} who {disfmarker} who has that as a primary interest then that put {disfmarker} then that drives it in that direction .&#10;Speaker: Grad H&#10;Content: That 's right , I mean if someone {disfmarker} if someone came in and said we really want to do it ,&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Grad H&#10;Content: I mean , we don't care" />
    <node id="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." />
    <node id="'t know , at least half of them probably {comment} on average are g are ha are {disfmarker} have a lot of cross - ta sorry , some of the segments have a lot of cross - talk . Um , it 's good to get sort of short segments if you 're gonna do recognition , especially forced alignment . So , uh , Don has been taking a first stab actually using Jane 's first {disfmarker} the fir the meeting that Jane transcribed which we did have some problems with , and Thilo , uh , I think told me why this was , but that people were switching microphones around {comment} in the very beginning , so {disfmarker} the SRI re&#10;Speaker: PhD C&#10;Content: No , th Yeah . No . They {disfmarker} they were not switching them but what they were {disfmarker} they were adjusting them ,&#10;Speaker: PhD A&#10;Content: and they {disfmarker} They were not {disfmarker}&#10;Speaker: PhD C&#10;Content: so .&#10;Speaker: Grad F&#10;Content: Mmm .&#10;Speaker: Grad H&#10;Content: Adjusting . Oh .&#10;" />
    <node id=" research is going on we 're also experimenting with different ASR , uh , techniques .&#10;Speaker: Grad H&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: And so it 'd be w good to know about it .&#10;Speaker: PhD E&#10;Content: So the problem is like , uh , on the microphone of somebody who 's not talking they 're picking up signals from other people {comment} and that 's {vocalsound} causing problems ?&#10;Speaker: PhD A&#10;Content: R right , although if they 're not talking , using the {disfmarker} the inhouse transcriptions , were sort of O K because the t no one transcribed any words there and we throw it out .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: But if they 're talking at all and they 're not talking the whole time , so you get some speech and then a &quot; mm - hmm &quot; , and some more speech , so that whole thing is one chunk . And the person in the middle who said only a little bit is picking up the speech around it , that 's where it" />
    <node id=" and putting them in that format , and see how that works out . I {disfmarker} I {disfmarker} I explained to him in {disfmarker} in detail the , uh , conventions that we 're using here in this {disfmarker} in this word level transcript . And , um , you know , I {disfmarker} I explained , you know , the reasons that {disfmarker} that we were not coding more elaborately and {disfmarker} and the focus on reliability . He expressed a lot of interest in reliability . It 's like he 's {disfmarker} he 's really up on these things . He 's {disfmarker} he 's very {disfmarker} Um , independently he asked , &quot; well what about reliability ? &quot; So , {vocalsound} he 's interested in the consistency of the encoding and that sort of thing . OK , um {disfmarker}&#10;Speaker: PhD A&#10;Content: Sorry , can you explain what the ATLAS {disfmarker} I 'm not familiar with this ATLAS system .&#10;Speaker: Postdoc G&#10;Content:" />
    <node id=" what I had read was , uh , they had a uh very large amount of software infrastructure for coordinating all this , both in terms of recording and also live room where you 're interacting {disfmarker} the participants are interacting with the computer , and with the video , and lots of other stuff .&#10;Speaker: Professor B&#10;Content: Well , I 'm {disfmarker} I 'm {disfmarker} I 'm not sure .&#10;Speaker: Grad H&#10;Content: So .&#10;Speaker: Professor B&#10;Content: All {disfmarker} all I know is that they 've been talking to me about a project that they 're going to start up recording people meet in meetings .&#10;Speaker: Grad H&#10;Content: OK . Well {disfmarker}&#10;Speaker: Professor B&#10;Content: And , uh , it is related to ours . They were interested in ours . They wanted to get some uniformity with us , uh , about the transcriptions and so on .&#10;Speaker: Grad H&#10;Content: Alright .&#10;Speaker: Professor B&#10;Content: And one {disfmarker} one notable difference {disfmarker} u u actually" />
    <node id=": so {disfmarker} yeah , uniformity would be great .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Is it because {disfmarker} You {disfmarker} you 're saying the {disfmarker} for dialogue purposes , so that means that the transcribers are having trouble with those mikes ? Is that what you mean ?&#10;Speaker: PhD A&#10;Content: Well Jane would know more about the transcribers .&#10;Speaker: PhD E&#10;Content: Or {disfmarker} ?&#10;Speaker: Postdoc G&#10;Content: And that 's true . I mean , I {disfmarker} we did discuss this . Uh , and {disfmarker} and {disfmarker}&#10;Speaker: Grad H&#10;Content: Yep . Couple times .&#10;Speaker: Postdoc G&#10;Content: a couple times , so , um , yeah , the transcribers notice {disfmarker} And in fact there 're some where , um {disfmarker} ugh well , I mean there 's {disfmarker} it 's the double thing" />
    <node id=" Professor B&#10;Content: Yeah . No , I mean , that 's what all this is about . They {disfmarker} they haven't done it yet . They wanted to do it {disfmarker}&#10;Speaker: Grad H&#10;Content: OK . I had read some papers that looked like they had already done some work .&#10;Speaker: Professor B&#10;Content: Uh , well I think they 've instrumented a room but I don't {pause} think they {disfmarker} they haven't started recordings yet . They don't have the t the transcription standards . They don't have the {disfmarker}&#10;Speaker: PhD E&#10;Content: Are they going to do video as well ?&#10;Speaker: Grad H&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Yeah . I think .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: I think they are .&#10;Speaker: Grad H&#10;Content: Oh , cuz what {disfmarker} what I had read was , uh , they had a uh very large amount of software infrastructure for coordinating all this , both in terms of recording and also live room" />
    <node id="1. The speakers seem open to the idea of allowing students or filmmakers to exercise creative control over a potential one-hour film, as they discuss the possibility of having student groups or filmmakers participate in the project. They mention that this could add some color and creativity to the proceedings. (PhD A, Postdoc G, Grad F, Grad H, PhD D, Professor B)&#10;&#10;2. However, there are concerns about the commercial value of such a film. The speakers discuss the possibility that focusing on artistic aspects might detract from the main objectives of the project and reduce its commercial potential. (Professor B, Postdoc G, PhD D)&#10;&#10;3. Another concern is the length of the film. The participants mention that allowing students or filmmakers to create a one-hour film might result in excessive footage, making it difficult to manage and utilize effectively. They discuss the need to strike a balance between artistic freedom and practical constraints. (Postdoc G, PhD D)&#10;&#10;In summary, while the speakers are generally open to the idea of involving students or filmmakers in the project, they have concerns about the commercial value and length of the potential film. They acknowledge the need to balance artistic aspects with practical considerations to ensure the success of the project." />
    <node id="aker: PhD A&#10;Content: No , but maybe stu student , uh , groups or , um , film - makers , or som Something a little bit colorful .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Yeah . {vocalsound} Exactly .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . {vocalsound} {vocalsound} {vocalsound} {vocalsound} Yeah , th there 's a problem there in terms of , uh , the um commercial value of {disfmarker} of st uh ,&#10;Speaker: Postdoc G&#10;Content: Yeah , of course there is this problem though , that if we give them the chance to excise later we e {vocalsound} might end up with like five minutes out of a f {comment} {pause} of m one hour&#10;Speaker: PhD D&#10;Content: Film - maker .&#10;Speaker: Grad H&#10;Content: Of beeps ,&#10;Speaker: PhD C&#10;Content: Yeah . Yeah .&#10;" />
    <node id=" and {disfmarker} or emotion , and things like that . And so I was thinking if there 's any like Berkeley political groups or something . I mean , that 'd be perfect . Some group , &quot; yes , we must {disfmarker} &quot;&#10;Speaker: Grad H&#10;Content: Who 's willing to get recorded and distributed ?&#10;Speaker: PhD A&#10;Content: Well , you know , something {disfmarker}&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Yeah , I don't think the more political argumentative ones would be willing to {disfmarker}&#10;Speaker: PhD A&#10;Content: Um {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , with {disfmarker} with {disfmarker} with potential use from the defense department .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Well , OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: No , but maybe stu student , uh , groups or , um , film - makers , or som Something a little bit" />
    <node id=" I I 've heard comments about this before , &quot; why don't you just put on a video camera ? &quot; But you know , it 's sort of like saying , &quot; uh , well we 're primarily interested in {disfmarker} in some dialogue things , uh , but , uh , why don't we just throw a microphone out there . &quot; I mean , the thing is , once you actually have serious interest in any of these things then you actually have to put a lot of effort in .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: And , uh , you really want to do it right .&#10;Speaker: Grad H&#10;Content: I know . Yep .&#10;Speaker: Professor B&#10;Content: So I think NIST or LDC , or somebody like that I think is much better shape to do all that . We {disfmarker} there will be other meeting recordings . We won't be the only place doing meeting recordings . We are doing what we 're doing .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh , hopefully it 'll be useful .&#10;Spe" />
    <node id=" a completely different set up than we have ,&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: one that would go up to thirty - two channels or something .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So basically {disfmarker}&#10;Speaker: Grad H&#10;Content: Or a hundred thirty - two .&#10;Speaker: Professor B&#10;Content: or a hun Yeah . So , I 'm kinda skeptical , but um I think that {disfmarker}&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So , uh , I don't think we can share the resource in that way . But what we could do is if there was someone else who 's interested they could have a separate set up which they wouldn't be trying to synch with ours which might be useful for {disfmarker} for them .&#10;Speaker: PhD A&#10;Content: Right , I mean at least they 'd have the data and the transcripts ,&#10;Speaker: Professor B&#10;Content: And then we can offer up the room ,&#10;Speaker: PhD A&#10;" />
    <node id="From the transcript, it is indicated that each transcriber is given their own &quot;meeting&quot; to transcribe. The size of the data set is not explicitly stated in terms of word count or file size. However, it is mentioned that each meeting lasts for an hour to an hour and a half. Therefore, we can infer that the data sets given to the transcribers are roughly an hour to an hour and a half long worth of audio data per meeting. Additionally, there are eight transcribers in total." />
    <node id=" during these meetings ,&#10;Speaker: Grad F&#10;Content: I don't care . You can do whatever you want with it .&#10;Speaker: Professor B&#10;Content: and {disfmarker}&#10;Speaker: PhD E&#10;Content: Usually .&#10;Speaker: Grad F&#10;Content: That 's fine .&#10;Speaker: Professor B&#10;Content: Yeah . OK . Uh , transcriptions .&#10;Speaker: Postdoc G&#10;Content: Transcriptions , OK . Um , I thought about {disfmarker} there are maybe three aspects of this . So first of all , um , I 've got eight transcribers . Uh , seven of them are linguists . One of them is a graduate student in psychology . Um , Each {disfmarker} I gave each of them , uh , their own data set . Two of them have already finished the data sets . And {pause} the meetings run , you know , let 's say an hour . Sometimes as man much as an hour and a half .&#10;Speaker: PhD E&#10;Content: How big is the data set ?&#10;Speaker: Postdoc G&#10;Content: Oh , it 's {disfmarker} what I mean" />
    <node id=": How big is the data set ?&#10;Speaker: Postdoc G&#10;Content: Oh , it 's {disfmarker} what I mean is one meeting .&#10;Speaker: PhD E&#10;Content: Ah , OK .&#10;Speaker: Postdoc G&#10;Content: Each {disfmarker} each person got their own meeting . I didn't want to have any conflicts of , you know , of {disfmarker} of when to stop transcribing this one or {disfmarker} So I wanted to keep it clear whose data were whose , and {disfmarker} and {disfmarker} and so {disfmarker}&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Postdoc G&#10;Content: And , uh , meetings , you know , I think that they 're {disfmarker} they go as long as a {disfmarker} almost two hours in some {disfmarker} in some cases . So , you know , that means {disfmarker} you know , if we 've got two already finished and they 're working on {disfmarker} Uh , right now all eight of" />
    <node id="}&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: I think there 's a second pass and I don't really know what would exist in it . But there 's definitely a second pass worth doing to maybe encode some kinds of , you know , is it a question or not ,&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: or {disfmarker} um , that maybe these transcribers could do . So {disfmarker} Yeah .&#10;Speaker: Postdoc G&#10;Content: They 'd be really good . They 're {disfmarker} they 're very {disfmarker} they 're very consistent .&#10;Speaker: PhD A&#10;Content: That 'd be great .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc G&#10;Content: Uh , I wanted to {disfmarker} whi while we 're {disfmarker} Uh , so , to return just briefly to this question of more meeting data , um {disfmarker} I have two questions . One of" />
    <node id=" that .&#10;Speaker: PhD E&#10;Content: Just out of curiosity , I mean {disfmarker}&#10;Speaker: Grad H&#10;Content: They also all have h heavy accents .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: The networks group meetings are all {disfmarker}&#10;Speaker: PhD E&#10;Content: Given all of the effort that is going on here in transcribing why do we have I B M doing it ? Why not just do it all ourselves ?&#10;Speaker: Professor B&#10;Content: Um , it 's historical . I mean , uh , some point ago we thought that uh , it {disfmarker} &quot; boy , we 'd really have to ramp up to do that &quot; ,&#10;Speaker: PhD C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: No , just {disfmarker}&#10;Speaker: Professor B&#10;Content: you know , like we just did , and , um , here 's , uh , a {disfmarker} a , uh" />
    <node id=" H&#10;Content: Oh .&#10;Speaker: Postdoc G&#10;Content: Oh , I see .&#10;Speaker: PhD C&#10;Content: It 's a little bit at odd to {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Oh , darn . Of course , of course , of course . Yeah , OK .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: And actually as you get transcripts just , um , for new meetings , {comment} um , we can try {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: I mean , the {disfmarker} the more data we have to try the {disfmarker} the alignments on , um , the better . So it 'd be good for {disfmarker} just to know as transcriptions are coming through the pipeline from the transcribers , just to sort of {disfmarker} we 're playing around with sort of uh , parameters f on the recognizer ,&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A" />
    <node id="Based on the meeting transcript, the level of detail in the time information provided by Brian Kingsbury's team will not be as fine-grained as the current transcription methods used. The exact difference in the level of detail is not specified in the transcript. However, it is mentioned that the current transcriptions are created by transcribers who are given individual &quot;meetings&quot; to transcribe, with each meeting lasting for an hour to an hour and a half. Therefore, the current transcription methods likely include detailed time information for each speaker in the meeting. In contrast, Brian Kingsbury's team will not provide as much detail in terms of time information, but it is unclear exactly how much less detailed their transcriptions will be." />
    <node id=" Right .&#10;Speaker: Postdoc G&#10;Content: And he was , you know , very interested . And , &quot; oh , and how 'd you handle this ? &quot; And I said , &quot; well , you know , this way &quot; and {disfmarker} And {disfmarker} and we had a really nice conversation . Um , OK , now I also wanted to say in a different {disfmarker} a different direction is , Brian Kingsbury . So , um , I corresponded briefly with him . I , uh , c I {disfmarker} He still has an account here . I told him he could SSH on and use multi - trans , and have a look at the already done , uh , transcription . And he {disfmarker} and he did . And what he said was that , um , what they 'll be providing is {disfmarker} will not be as fine grained in terms of the time information . And , um , that 's , uh {disfmarker} You know , I need to get back to him and {disfmarker} and , uh , you know , explore that a little bit more and see what they 'll be giving" />
    <node id="isfmarker}&#10;Speaker: Grad H&#10;Content: Well we can talk about more details later .&#10;Speaker: PhD A&#10;Content: um , you know , yeah , whether to {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . Yeah . Yeah , so .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: We 'll see . I mean , I think , th you know , they {disfmarker} they {disfmarker} they 've proceeded along a bit . Let 's see what comes out of it , and {disfmarker} and , uh , you know , have some more discussions with them .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm . It 's very {disfmarker} a real benefit having Brian involved because of his knowledge of what the {disfmarker} how the data need to be used and so what 's useful to have in the format .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Grad H&#10;Content: So , um ," />
    <node id=" get back to him and {disfmarker} and , uh , you know , explore that a little bit more and see what they 'll be giving us in specific ,&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD E&#10;Content: The p the people {disfmarker}&#10;Speaker: Postdoc G&#10;Content: but I just haven't had time yet .&#10;Speaker: PhD E&#10;Content: The {disfmarker} the folks that they 're , uh , subcontracting out the transcription to , are they like court reporters&#10;Speaker: Postdoc G&#10;Content: Sorry , what ? Yes .&#10;Speaker: PhD E&#10;Content: or {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Apparently {disfmarker} Well , I get the sense they 're kind of like that . Like it 's like a pool of {disfmarker} of somewhat uh , secretarial {disfmarker} I don't think that they 're court reporters . I don't think they have the special keyboards and that {disfmarker} and that type of training .&#10;Speaker: PhD E&#10;Content: Mm" />
    <node id="PHP A's concern is that the current method of determining word segments in the transcripts, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. A forced alignment can help address this issue by creating a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio. This would improve the accuracy of prosodic feature computation and overall transcription quality." />
    <node id=" it 's all pretty good sized for the recognizer also .&#10;Speaker: PhD A&#10;Content: Right , and it {disfmarker} it helps that it 's made based on sort of heuristics and human ear I think .&#10;Speaker: Postdoc G&#10;Content: Good . Oh good .&#10;Speaker: PhD A&#10;Content: Th - but there 's going to be a real problem , uh , even if we chop up based on speech silence these , uh , the transcripts from I B M , we don't actually know where the words were , which segment they belonged to .&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: So that 's sort of what I 'm {pause} worried about right now .&#10;Speaker: PhD E&#10;Content: Why not do a {disfmarker} a {disfmarker} a forced alignment ?&#10;Speaker: Grad H&#10;Content: That 's what she 's saying , is that you can't .&#10;Speaker: PhD A&#10;Content: If you do a forced alignment on something really {disfmarker}&#10;Speaker: Grad H&#10;Content: Got uh six sixty" />
    <node id=" ratio is too low , um , you {disfmarker} you 'll get , a uh {disfmarker} an alignment with the wrong duration pattern or it {disfmarker}&#10;Speaker: PhD E&#10;Content: Oh , so that 's the problem , is the {disfmarker} the signal - to - noise ratio .&#10;Speaker: PhD A&#10;Content: Yeah . It 's not the fact that you have like {disfmarker} I mean , what he did is allow you to have , uh , words that were in another segment move over to the {disfmarker} at the edges of {disfmarker} of segmentations .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Or even words inserted that weren't {disfmarker} weren't there .&#10;Speaker: PhD A&#10;Content: Right , things {disfmarker} things near the boundaries where if you got your alignment wrong {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: cuz what they had done there is align and then chop .&#10;Speaker: PhD" />
    <node id="marker}&#10;Speaker: PhD E&#10;Content: Well you would need to {disfmarker} like a forced alignment before you did the chopping , right ?&#10;Speaker: PhD A&#10;Content: No , we used the fact that {disfmarker} So when Jane transcribes them the way she has transcribers doing this , whether it 's with the pre - segmentation or not ,&#10;Speaker: Grad H&#10;Content: It 's already chunked .&#10;Speaker: PhD A&#10;Content: they have a chunk and then they transcribes {comment} the words in the chunk . And maybe they choose the chunk or now they use a pre - segmentation and then correct it if necessary . But there 's first a chunk and then a transcription .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Then a chunk , then a transcription . That 's great , cuz the recognizer can {disfmarker}&#10;Speaker: Grad H&#10;Content: Uh , it 's all pretty good sized for the recognizer also .&#10;Speaker: PhD A&#10;Content: Right , and it {disfmarker}" />
    <node id=" the analysis that you 're doing involves taking segments which are of a particular type and putting them together .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: And th so if you have like a p a s you know , speech from one speaker , {pause} then you cut out the part that 's not that speaker ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Postdoc G&#10;Content: and you combine segments from {pause} that same speaker to {disfmarker} {comment} and run them through the recognizer . Is that {pause} right ?&#10;Speaker: PhD A&#10;Content: Well we try to find as close of start and end time of {disfmarker} as we can to the speech from an individual speaker ,&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: because then we {disfmarker} we 're more guaranteed that the recognizer will {disfmarker} for the forced alignment which is just to give us the time boundaries , because from those time boundaries then the plan is to compute prosodic features ." />
    <node id="During the discussion, Professors B and A mentioned some proposed categories that would allow for overlapping and different timing changes. While specific details about these categories were not provided in the transcript, they seem to be related to a project or research endeavor that involves removing high-level constraints and approaching it from a bottom-up perspective. This project may also involve using less data and infrastructure than typical projects in this field. To further explore this topic, Professor B suggests involving John Ohala, who is likely an expert or key figure with valuable insights on the subject matter. However, the transcript does not offer enough context to provide a more precise description of these categories." />
    <node id="Content: Yeah .&#10;Speaker: Professor B&#10;Content: some sort of categories but {disfmarker} but something that allows for overlapping change of these things and then this would give some more ground work for people who were building statistical models that allowed for overlapping changes , different timing changes as opposed to just &quot; click , you 're now in this state , which corresponds to this speech sound &quot; and so on .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD A&#10;Content: So this is like gestural {disfmarker} uh , these g&#10;Speaker: Professor B&#10;Content: Yeah , something like that .&#10;Speaker: PhD A&#10;Content: Right . OK .&#10;Speaker: Professor B&#10;Content: I mean , actually if we get into that it might be good to , uh , uh , haul John Ohala into this&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: and ask his {disfmarker} his views on it I think .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: But is {disfmarker" />
    <node id="&#10;Content: because that {disfmarker} that 'll really help us .&#10;Speaker: Professor B&#10;Content: O K , uh tea has started out there I suggest we c run through our digits and ,&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Uh , So , OK , we 're done ." />
    <node id=" it 's not finished .&#10;Speaker: Professor B&#10;Content: OK . Alright , that seems like a {disfmarker} a good collection of things . And we 'll undoubtedly think of {pause} other things .&#10;Speaker: Postdoc G&#10;Content: I had thought under my topic that I would mention the , uh , four items that I {disfmarker} I , uh , put out for being on the agenda f on that meeting , which includes like the pre - segmentation and the {disfmarker} and the developments in multitrans .&#10;Speaker: Professor B&#10;Content: Oh , under the NIST meeting .&#10;Speaker: Postdoc G&#10;Content: Yeah , under the NIST thing .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Alright , why don't we start off with this , u u I guess the order we brought them up seems fine .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Um , so , better quality close talking mikes . So the one issue was that the {disfmarker" />
    <node id="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results." />
    <node id=" 's not enough interest to overcome all of {disfmarker}&#10;Speaker: Grad H&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Right . Internally , but I know there is interest from other places that are interested in looking at meeting data and having the video . So it 's just {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Yeah , w although I {disfmarker} m {vocalsound} I {disfmarker} I have to u u mention the human subjects problems , {pause} that i increase with video .&#10;Speaker: PhD A&#10;Content: Right , that 's true .&#10;Speaker: Professor B&#10;Content: Yeah , so it 's , uh , people {disfmarker} people getting shy about it .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: There 's this human subjects problem . There 's the fact that then um , if {disfmarker} i I I 've heard comments about this before , &quot; why don't you just put on a video camera ? &quot; But you know , it 's sort of" />
    <node id="According to Professor B, the purpose of using meeting data is not specifically for studying gestures or for comparing it to far-field studies. Instead, meetings are seen as a natural way to get people talking and providing a source of naturally occurring speech data. This is inferred from her statements where she mentions viewing meetings as a neat way to get people talking naturally and seeing the value of meeting data as a source of data to help their research. The discussion about gestures was initiated by PhD A, but Professor B does not explicitly connect meeting data to gesture studies in her responses." />
    <node id=" on it I think .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: But is {disfmarker} is the goal there to have this on meeting data ,&#10;Speaker: Postdoc G&#10;Content: Excellent .&#10;Speaker: PhD A&#10;Content: like so that you can do far field studies {comment} of those gestures or {disfmarker} um , or is it because you think there 's a different kind of actual production in meetings {comment} that people use ? Or {disfmarker} ?&#10;Speaker: Professor B&#10;Content: No , I think {disfmarker} I think it 's {disfmarker} for {disfmarker} for {disfmarker} for that purpose I 'm just viewing meetings as being a {disfmarker} a neat way to get people talking naturally . And then you have i and then {disfmarker} and then it 's natural in all senses ,&#10;Speaker: PhD E&#10;Content: Just a source of data ?&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: in the sense that you" />
    <node id=" with that is right now , um , some of the Jimlets aren't working . The little {disfmarker} the boxes under the table .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: And so , w Uh , I 've only been able to find three jacks that are working .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Can we get these , wireless ?&#10;Speaker: Grad H&#10;Content: So {disfmarker}&#10;Speaker: Professor B&#10;Content: No , but my point is {disfmarker}&#10;Speaker: PhD A&#10;Content: But y we could just record these signals separately and time align them with the start of the meeting .&#10;Speaker: Professor B&#10;Content: R r right {disfmarker}&#10;Speaker: Grad H&#10;Content: I {disfmarker} I 'm not sure I 'm follow . Say that again ?&#10;Speaker: Professor B&#10;Content: Right now , we 've got , uh , two microphones in the room , that are not quote - unquote standard . So why don't we replace those {d" />
    <node id="er} Uh , so , to return just briefly to this question of more meeting data , um {disfmarker} I have two questions . One of them is , um , Jerry Feldman 's group , they {disfmarker} they , uh , are they {disfmarker} I know that they recorded one meeting . Are they willing ?&#10;Speaker: Professor B&#10;Content: I think they 're open to it . I think , you know , all these things are {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , yeah .&#10;Speaker: Professor B&#10;Content: I think there 's {disfmarker} we should go beyond , uh , ICSI but , I mean , there 's a lot of stuff happening at ICSI that we 're not getting now that we could .&#10;Speaker: PhD A&#10;Content: Oh , that we could .&#10;Speaker: Professor B&#10;Content: So it 's just {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: OK . I thought that all these people had sort of said &quot; no &quot; twice already" />
    <node id="1. Segmentation: The initial step involves segmenting the continuous audio stream based on speech-silence transitions. This results in separate speech segments.&#10;&#10;2. Speaker Identification: Within these segments, speakers are identified using automatic speech recognition (ASR) techniques. Adjustments to microphones and cross-talk can create inconsistencies and errors in transcriptions, so it is crucial to accurately associate each word with its corresponding time boundaries in the original audio.&#10;&#10;3. Forced Alignment: Before dividing the audio into smaller segments, a forced alignment process takes place. This involves determining as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This allows for improved accuracy in associating words with their time boundaries.&#10;&#10;4. Word-level Segmentation: After performing the forced alignment, the audio is divided into smaller segments containing single words or phrases. Each segment is associated with its corresponding transcription and time boundaries.&#10;&#10;5. Combining Segments: The speech segments from the same speaker are combined to create longer stretches of continuous speech for improved accuracy in recognition and feature computation. This consolidation process helps ensure that a speaker's unique characteristics and patterns can be better analyzed by the recognizer.&#10;&#10;6. Running through Recognizer: The combined speech segments from each speaker are then passed through a recognizer, which uses automatic speech recognition techniques to identify phonemes, words, or phrases within the audio. This generates a more accurate transcription and time alignment for further analysis.&#10;&#10;7. Computing Prosodic Features: With the accurately aligned transcriptions and time boundaries obtained from the previous steps, prosodic features such as pitch, intensity, and speaking rate can be computed to better understand the speaker's emotions, fluency, or other characteristics of their speech patterns." />
    <node id=": You know , just sort of say , OK , it 's voiced from here to here , there 's {disfmarker} it 's nasal from here to here , and so forth . Um , as opposed to doing phonetic {disfmarker} uh , you know , phonemic and the phonetic analysis ,&#10;Speaker: Grad H&#10;Content: &#10;Speaker: Professor B&#10;Content: and , uh , assuming , uh , articulatory feature values for those {disfmarker} those things . Um , obviously that 's extremely time - consuming . Uh {disfmarker}&#10;Speaker: PhD E&#10;Content: That would be really valuable I think .&#10;Speaker: Professor B&#10;Content: but , uh , we could do it on some small subset .&#10;Speaker: Postdoc G&#10;Content: Also if you 're dealing with consonants that would be easier than vowels , wouldn't it ? I mean , I would think that {disfmarker} that , uh , being able to code that there 's a {disfmarker} a fricative extending from here to here would be a lot easier than classifying precisely which vowel that was .&#10;Spe" />
    <node id="doc G&#10;Content: Or some other kind of thing ?&#10;Speaker: PhD D&#10;Content: No ? To {disfmarker} to mark {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Well , I wouldn't say {vocalsound} symbols so much . The {disfmarker} the main change that I {disfmarker} that I see in the interface is {disfmarker} is just that we 'll be able to more finely c uh , time things .&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Postdoc G&#10;Content: But I {disfmarker} I also st there was another aspect of your work that I was thinking about when I was talking to you&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Postdoc G&#10;Content: which is that it sounded to me , Liz , as though you {disfmarker} and , uh , maybe I didn't q understand this , but it sounded to me as though part of the analysis that you 're doing involves taking segments which are of a particular type and putting them together .&#10;Speaker: PhD D&#10;Content: Yeah ." />
    <node id=" some very high level labelings ,&#10;Speaker: Postdoc G&#10;Content: Yeah . The types of overlaps {disfmarker}&#10;Speaker: Professor B&#10;Content: types of overlaps , and so forth that {disfmarker} that someone could do . Second was , uh , somewhat lower level&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: just doing these more precise timings . And the third one is {disfmarker} is , uh , just a completely wild hair brained idea that I have which is that , um , if , uh {disfmarker} if we have time and people are able to do it , to take some subset of the data and do some very fine grained analysis of the speech . For instance , uh , marking in some overlapping {disfmarker} potentially overlapping fashion , uh , the value of , uh , ar articulatory features .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: You know , just sort of say , OK , it 's voiced from here to here , there 's {disfmarker} it 's" />
    <node id="Based on the transcript, there are no specific measures explicitly discussed to deal with people losing interest or becoming frustrated during the meeting. However, some related issues are mentioned that can help address the question:&#10;&#10;1. Microphone difficulties: The participants bring up the problem of inconsistent microphone usage leading to cross-talk and inaccurate transcriptions. Improving audio quality can help maintain engagement and make it easier for participants to follow the conversation.&#10;2. Transcription standards: Discussants emphasize the importance of having uniformity and transcription standards, particularly when multiple people are speaking simultaneously or using specialized terminology. Clear communication and understanding can prevent frustration and misunderstandings.&#10;3. Human subjects concerns: The group acknowledges that video recordings may make participants uncomfortable due to human subjects issues, which could contribute to waning interest or frustration. Addressing these concerns by ensuring privacy and informed consent is crucial for maintaining engagement.&#10;4. Meeting data usage: While not directly related to addressing frustration during the meeting itself, it's mentioned that the primary purpose of using meeting data is to gather naturally occurring speech data, rather than focusing on individual behaviors like gestures or comparing it to far-field studies. Clarifying the goals and intentions of a meeting can help align participants' expectations and minimize frustration.&#10;5. Bottom-up project: Although not explicitly linked to addressing frustration, the group mentions a potential project that involves removing high-level constraints and approaching it from a bottom-up perspective. This approach may result in less reliance on extensive data and infrastructure, potentially reducing complexity and confusion for participants.&#10;&#10;In summary, while there are no specific measures discussed to deal with people losing interest or becoming frustrated during the meeting, some related issues are brought up that can contribute to addressing frustration by improving audio quality, promoting clear communication, ensuring privacy, clarifying goals, and simplifying project constraints." />
    <node id=" . That needs to be forthcoming .&#10;Speaker: PhD A&#10;Content: Cuz {disfmarker} OK .&#10;Speaker: Postdoc G&#10;Content: But {disfmarker} but the , uh {disfmarker} I did want to say that it 's hard to follow one channel of a conversation even if you know the people , and if you 're dealing furthermore with highly abstract network concepts you 've never heard of {disfmarker} So , you know , one of these people was {disfmarker} was transcribing the , uh , networks group talk and she said , &quot; I don't really know what a lot of these abbreviations are , &quot; &quot; but I just put them in parentheses cuz that 's the {disfmarker} that 's the convention and I just &quot; {disfmarker} Cuz you know , if you don't know {disfmarker}&#10;Speaker: Grad H&#10;Content: Oh , I 'd be curious to {disfmarker} to look at that .&#10;Speaker: PhD E&#10;Content: Just out of curiosity , I mean {disfmarker}&#10;Speaker: Grad H&#10;Content" />
    <node id="PHP A's concern is that the current method of determining word segments in the transcripts from IBM, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. PHP A is worried that without knowing the exact word segments, it will be difficult to accurately associate each word with its corresponding time boundaries in the original audio, which can negatively impact the accuracy of prosodic feature computation and overall transcription quality.&#10;&#10;To address this issue, PhD E suggests using a forced alignment before chopping up the audio into segments. Forced alignment is a process that creates a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio.&#10;&#10;However, as the discussion continues, the group mentions that IBM might be doing an initial forced alignment and they have to talk to Brian about it. Also, there is a concern about the signal-to-noise ratio being too low which causes problems with getting the right duration pattern or alignment of the words." />
    <node id=" A&#10;Content: If you do a forced alignment on something really {disfmarker}&#10;Speaker: Grad H&#10;Content: Got uh six sixty minutes of {disfmarker}&#10;Speaker: PhD A&#10;Content: well even if you do it on something really long you need to know {disfmarker} you can always chop it up but you need to have a reference of which words went with which , uh , chop .&#10;Speaker: Postdoc G&#10;Content: Now wasn't {disfmarker} I thought that one of the proposals was that IBM was going to do an initial forced alignment ,&#10;Speaker: PhD A&#10;Content: So {disfmarker}&#10;Speaker: Postdoc G&#10;Content: after they {disfmarker}&#10;Speaker: Grad H&#10;Content: Yeah , but {disfmarker}&#10;Speaker: Professor B&#10;Content: I {disfmarker} I think that they are ,&#10;Speaker: Grad H&#10;Content: We 'll have to talk to Brian .&#10;Speaker: Professor B&#10;Content: um , yeah , I 'm sure they will and so we {disfmarker} we have to have a" />
    <node id="The people transcribing the interviews are a group of individuals subcontracted by Brian Kingsbury's team. They are not exactly like court reporters as they do not seem to have special keyboards or specific training for court reporting. Instead, they are more similar to secretarial staff or medical transcriptionists, responsible for manually transcribing audio data into written text. However, they might be using some basic equipment like headphones and a computer for transcription purposes. The level of detail in time information provided by their transcriptions is not as fine-grained as the current transcription methods used.&#10;&#10;It's important to note that organizations such as NIST or LDC are better equipped to handle transcription tasks due to their resources, expertise, and adherence to uniformity and high-quality standards across different projects. The comparison is made between simply placing a microphone to capture audio and the complex process involved in accurate and useful transcriptions." />
    <node id="'t think they have the special keyboards and that {disfmarker} and that type of training .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Postdoc G&#10;Content: I {disfmarker} I get the sense they 're more secretarial . And that , um , uh , what they 're doing is giving them {disfmarker}&#10;Speaker: PhD E&#10;Content: Hmm . Like medical transcriptionist type people {disfmarker}&#10;Speaker: Grad H&#10;Content: Nu - it 's mostly {disfmarker} it 's for their speech recognition products ,&#10;Speaker: PhD E&#10;Content: But aren't {disfmarker} they 're {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Yep .&#10;Speaker: Grad H&#10;Content: that they 've hired these people to do .&#10;Speaker: PhD E&#10;Content: Oh , so they 're hiring them , they 're coming . It 's not a service they send the tapes out to .&#10;Speaker: Grad H&#10;Content: Well they {disfmarker} they do send it out but my" />
    <node id="1. Cross-talk and inconsistent audio quality: Participants mention that people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. This results in inconsistent or inaccurate transcriptions, which oversimplifies the complexity of the task when there is serious interest in capturing dialogue.&#10;&#10;2. Need for uniformity and transcription standards: Discussants emphasize the importance of having uniformity and transcription standards across different projects to ensure accurate and useful results, particularly in meeting recordings where multiple individuals may be speaking simultaneously.&#10;&#10;3. Human subjects concerns: Video recordings can make participants uncomfortable due to human subjects issues, potentially leading to people becoming shy or uninterested. Ensuring privacy and informed consent is crucial for maintaining engagement.&#10;&#10;4. Clarifying goals and intentions: Discussing the primary purpose of using meeting data, which is gathering naturally occurring speech data rather than focusing on individual behaviors like gestures, can help align participants' expectations and minimize frustration.&#10;&#10;While not explicitly mentioned as reasons, it can be inferred that having a serious interest in capturing dialogue or meeting recordings requires careful planning, resource allocation, and expertise to achieve accurate and useful results. Organizations like NIST or LDC are considered better equipped to handle these tasks due to their resources and expertise in ensuring uniformity, transcription standards, and high-quality recordings across different projects." />
    <node id="The issue with having meetings that were exclusively in English, as requested by Jonathan Fiscus, was that it turned out to be a bit of a problem because not many all-English meetings were being held. Postdoc G had to give Fiscus their initial meeting in English because he asked for it, and they don't have a lot of all-English meetings right now. Therefore, providing an exclusively English meeting may have been challenging due to the lack of regular all-English meetings." />
    <node id=" H&#10;Content: Legal .&#10;Speaker: Postdoc G&#10;Content: OK , OK .&#10;Speaker: Professor B&#10;Content: it {disfmarker} it {disfmarker} it {disfmarker} it turned out to be a bit of a problem .&#10;Speaker: PhD A&#10;Content: Or {disfmarker}&#10;Speaker: Postdoc G&#10;Content: And I had one other {disfmarker} one other aspect of this which is , um , uh , uh , Jonathan Fiscus expressed primar uh y a major interest in having meetings which were all English speakers . Now he wasn't trying to shape us in terms of what we gather&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc G&#10;Content: but that 's what he wanted me to show him . So I 'm giving him our , um {disfmarker} our initial meeting because he asked for all English . And I think we don't have a lot of all English meetings right now .&#10;Speaker: Professor B&#10;Content: Of all {disfmarker} all nat all native speakers .&#10;Speaker: PhD E&#10;Content" />
    <node id=" for people with big heads .&#10;Speaker: PhD A&#10;Content: It 's makes our job a lot easier .&#10;Speaker: Professor B&#10;Content: OK . OK .&#10;Speaker: Grad H&#10;Content: And , you know , we 're researchers , so we all have big heads .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . Uh , OK , second item was the , uh , NIST visit , and what 's going on there .&#10;Speaker: Postdoc G&#10;Content: Yeah . OK , so , um , uh , Jonathan Fiscus is coming on the second of February and I 've spoken with , uh , {pause} u u a lot of people here , not everyone . Um , and , um , he expressed an interest in seeing the room and in , um , seeing a demonstration of the modified multitrans , which I 'll mention in a second , and also , um , he was interested in the pre - segmentation and then he 's also interested in the transcription conventions .&#10;Speaker: Grad H&#10;Content: Mm - hmm .&#10;Speaker:" />
    <node id="The transcript does not provide sufficient information to directly answer the question about the average time it takes to transcribe five minutes of real-time audio. The text only mentions that two out of eight transcribers have finished their data sets, but it does not give information on how long those data sets were or how much time the transcribers spent on them. To calculate the average transcription time for a specific duration like five minutes, we would need consistent data points from multiple transcribers working on similar audio segments. Since this information is not available in the text, it's impossible to provide an accurate answer." />
    <node id="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions." />
    <node id=" I bet with the lapel mike there 's plenty , uh , room acoustic&#10;Speaker: PhD A&#10;Content: That {disfmarker} that may be true .&#10;Speaker: Grad H&#10;Content: but I I think the rest is cross - talk .&#10;Speaker: PhD A&#10;Content: But I don't know how good it can get either by those {disfmarker} the {disfmarker} those methods {disfmarker}&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: That 's true .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: Grad H&#10;Content: So I {disfmarker} I think it 's just ,&#10;Speaker: PhD A&#10;Content: Oh , I don't know .&#10;Speaker: Grad H&#10;Content: yeah , what you said , cross - talk .&#10;Speaker: PhD A&#10;Content: All I meant is just that as sort of {disfmarker} as this pipeline of research is going on we 're also experimenting with different ASR , uh , techniques .&#10;Speaker: Grad H&#10;Content: Mm - hmm" />
    <node id="aker: PhD A&#10;Content: It 's like {disfmarker} {comment} {vocalsound} like {disfmarker}&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: And the question is to w to what extent is it getting hurt by , uh {disfmarker} by any room acoustics or is it just {disfmarker} uh , given that it 's close it 's not a problem ?&#10;Speaker: PhD A&#10;Content: It doesn't seem like big room acoustics problems to my ear&#10;Speaker: Professor B&#10;Content: Uh {disfmarker}&#10;Speaker: PhD A&#10;Content: but I 'm not an expert . It seems like a problem with cross - talk .&#10;Speaker: Professor B&#10;Content: OK , so it 's {disfmarker}&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: e I bet with the lapel mike there 's plenty , uh , room acoustic&#10;Speaker: PhD A&#10;Content: That {disfmarker" />
    <node id="&#10;Speaker: Professor B&#10;Content: Um , so , better quality close talking mikes . So the one issue was that the {disfmarker} the , uh , lapel mike , uh , isn't as good as you would like . And so , uh , it {disfmarker} it 'd be better if we had close talking mikes for everybody . Right ?&#10;Speaker: PhD A&#10;Content: Ri - um ,&#10;Speaker: Professor B&#10;Content: Is that {disfmarker} is that basically the point ?&#10;Speaker: PhD A&#10;Content: yeah , the {disfmarker} And actually in addition to that , that the {disfmarker} the close talking mikes are worn in such a way as to best capture the signal . And the reason here is just that for the people doing work not on microphones but on sort of like dialogue and so forth , uh {disfmarker} or and even on prosody , which Don is gonna be working on soon , it adds this extra , you know , vari variable for each speaker to {disfmarker} to deal with when the microphones aren't similar .&#10;Speaker: Professor B&#10;Content:" />
    <node id="1. The suggestion made by PhD A and Postdoc G regarding adding more variation to the beeps in a film is for someone to explore different groups or filmmakers who could bring some color and creativity to the proceedings. They are open to discussing ideas and doing the legwork to contact these potential collaborators but need guidance on which groups might be worth pursuing.&#10;&#10;Based on the transcript, it seems that they want to move away from the monotonous &quot;beeps&quot; and incorporate more varied prosodic contours in the film's audio elements. This would involve finding artists or filmmakers who can contribute creatively to this aspect of the project while considering practical constraints such as commercial value and length of the film." />
    <node id=": Film - maker .&#10;Speaker: Grad H&#10;Content: Of beeps ,&#10;Speaker: PhD C&#10;Content: Yeah . Yeah .&#10;Speaker: Grad H&#10;Content: yeah .&#10;Speaker: PhD A&#10;Content: And I don't mean that they 're angry&#10;Speaker: PhD D&#10;Content: Is {disfmarker}&#10;Speaker: Postdoc G&#10;Content: of {disfmarker} {comment} Yes . Really .&#10;Speaker: PhD A&#10;Content: but just something with some more variation in prosodic contours and so forth would be neat . So if anyone has ideas , I 'm willing to do the leg work to go try to talk to people but I don't really know which groups are worth pursuing .&#10;Speaker: Postdoc G&#10;Content: Well there was this K P F A&#10;Speaker: Grad H&#10;Content: No that 's {disfmarker}&#10;Speaker: Postdoc G&#10;Content: but {disfmarker} OK .&#10;Speaker: Grad H&#10;Content: Legal .&#10;Speaker: Postdoc G&#10;Content: OK , OK .&#10;Speaker: Professor B&#10;Content: it {d" />
    <node id="er} we 've had this discussion many times .&#10;Speaker: Postdoc G&#10;Content: Yeah , we have .&#10;Speaker: Grad H&#10;Content: And the answer is we don't actually know the answer because we haven't tried both ways .&#10;Speaker: Postdoc G&#10;Content: Well , except I can say that my transcribers use the mixed signal mostly&#10;Speaker: Grad H&#10;Content: So . Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Right .&#10;Speaker: Postdoc G&#10;Content: unless there 's a huge disparity in terms of the volume on {disfmarker} on the mix . In which case , you know , they {disfmarker} they wouldn't be able to catch anything except the prominent {comment} channel ,&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: then they 'll switch between .&#10;Speaker: Grad H&#10;Content: Well I think that {disfmarker} that might change if you wanted really fine time markings .&#10;" />
    <node id="1. Positioning between speakers: If individuals are located between two people who are talking, a large peak will appear in the display. This is due to the microphone picking up sound from both sources almost equally.&#10;&#10;2. Speaker location: When someone is speaking on one side or the other of an axis between two individuals, the peak will go the opposite way. The microphone will pick up more sound from the closer speaker and less from the farther speaker, causing a difference in the display's appearance.&#10;&#10;3. Number of speakers: The configuration of speakers also influences the appearance of peaks. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time." />
    <node id="&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: if {disfmarker} if someone who was on the axis between the two is talking , then you {disfmarker} you get a big peak there . And if {disfmarker} if someone 's talking on {disfmarker} on {disfmarker} on , uh , one side or the other , it goes the other way .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And then , uh , it {disfmarker} it {disfmarker} it even looks different if th t if the two {disfmarker} two people on either side are talking than if one in the middle . It {disfmarker} it actually looks somewhat different , so .&#10;Speaker: PhD E&#10;Content: Hmm . Well I was just thinking , you know , as I was sitting here next to Thilo that um , when he 's talking , my mike probably picks it up better than {pause} your guys 's mikes .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content" />
    <node id=": Yeah .&#10;Speaker: PhD E&#10;Content: That would be really neat .&#10;Speaker: Professor B&#10;Content: but they might wanna just , {disfmarker} uh , you know , you could imagine them taking the four signals from these {disfmarker} these table mikes and trying to do something with them {disfmarker} Um , I also had a discussion {disfmarker} So , w uh , we 'll be over {disfmarker} over there talking with him , um , after class on Friday . Um , we 'll let you know what {disfmarker} what goes with that . Also had a completely unrelated thing . I had a , uh , discussion today with , uh , Birger Kollmeier who 's a , uh , a German , uh , scientist who 's got a fair sized group {vocalsound} doing a range of things . It 's sort of auditory related , largely for hearing aids and so on . But {disfmarker} but , uh , he does stuff with auditory models and he 's very interested in directionality , and location , and {disfmarker} and , uh , head models and {" />
    <node id=" , vari variable for each speaker to {disfmarker} to deal with when the microphones aren't similar .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: So {disfmarker} And I also talked to Mari this morning and she also had a strong preference for doing that . And in fact she said that that 's useful for them to know in starting to collect their data too .&#10;Speaker: Professor B&#10;Content: Mm - hmm . Right , so one th&#10;Speaker: Grad H&#10;Content: Well , so {disfmarker}&#10;Speaker: Professor B&#10;Content: uh , well one thing I was gonna say was that , um , i we could get more , uh , of the head mounted microphones even beyond the number of radio channels we have because I think whether it 's radio or wire is probably second - order . And the main thing is having the microphone close to you ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: u although , not too close .&#10;Speaker: Grad H&#10;Content" />
    <node id="The issues discussed regarding signal-processing techniques for detecting energies involve inconsistencies in dialogue transcriptions due to challenges with microphones and cross-talk. The participants mentioned that variability in microphone quality and people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe dialogues. When individuals are not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions.&#10;&#10;It is important for those working on signal-processing techniques for detecting energies to be aware of these issues because improving the quality and consistency of audio recordings will significantly enhance the accuracy and usefulness of transcriptions. By developing better signal-processing techniques, researchers can help minimize cross-talk, distinguish individual speakers, and improve the overall signal-to-noise ratio, ultimately leading to more precise and reliable transcriptions. This is crucial for analyzing dialogue or prosody in meetings and other collaborative settings." />
    <node id="m - hmm .&#10;Speaker: PhD A&#10;Content: cuz what they had done there is align and then chop .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Um , and this problem is a little bit j more global . It 's that there are problems even in inside the alignments , uh , because of the fact that there 's enough acoustic signal there t for the recognizer to {disfmarker} to eat , {vocalsound} as part of a word . And it tends to do that . S So , uh ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: but we probably will have to do something like that in addition . Anyway . So , yeah , bottom {disfmarker} bottom line is just I wanted to make sure I can be aware of whoever 's working on these signal - processing techniques for , uh , detecting energies ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: because that {disfmarker} that 'll really help us .&#10;Speaker: Professor B&#10;Content: O K , uh tea" />
    <node id=" that .&#10;Speaker: PhD A&#10;Content: but that , um , it would be helpful if I can stay in the loop somehow with , um , people who are doing any kind of post - processing , whether it 's to separate speakers or to improve the signal - to - noise ratio , or both , um , that we can sort of try out as we 're running recognition . Um , so , i is that {disfmarker} Who else is work I guess Dan Ellis and you&#10;Speaker: PhD C&#10;Content: Dan , yeah .&#10;Speaker: Professor B&#10;Content: Yeah , and Dave uh {pause} Gel - Gelbart again ,&#10;Speaker: Grad H&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: and Dave .&#10;Speaker: PhD C&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: he 's {disfmarker} he 's interested in {disfmarker} in fact we 're look starting to look at some echo cancellation kind of things .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker:" />
    <node id=" 's just sort of a open world right now of exploring that . So I just wanted to {pause} see , you know , on the transcribing end from here things look good . Uh , the IBM one is more {disfmarker} it 's an open question right now . And then the issue of like global processing of some signal and then , you know , before we chop it up is {disfmarker} is yet another way we can improve things in that .&#10;Speaker: PhD E&#10;Content: What about increasing the flexibility of the alignment ?&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: Do you remember that thing that Michael Finka did ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: that experiment he did a while back ?&#10;Speaker: PhD A&#10;Content: Right . You can , um {disfmarker}  The problem is just that the acoustic {disfmarker} when the signal - to - noise ratio is too low , um , you {disfmarker} you 'll get , a uh {disfmarker} an alignment with the wrong" />
    <node id="1. Postdoc G is describing a prototype that allows for more accurate time marking of overlapping segments in the transcription process. This is achieved through the use of a multi-channel innovation that helps clarify hearings involving overlaps. The current limitations of the original software design for pre-segmentation make it difficult to accurately time-mark overlapping segments, as the system relies on chopping based on speech-silence transitions, which may not provide enough information about word segment boundaries in cases of overlapping speech.&#10;&#10;2. By implementing this prototype, users will be able to more finely time and label overlapping segments using a click-and-drag interface, allowing for more accurate association between words and their corresponding time boundaries in the original audio. This improvement is expected to lead to better prosodic feature computation and overall transcription quality." />
    <node id="Content: So .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Yeah . Well that 's OK , I mean we 'll {disfmarker}&#10;Speaker: Grad H&#10;Content: Sorry .&#10;Speaker: Postdoc G&#10;Content: Yeah , and it 's t and it looks really great . He {disfmarker} he has a prototype . I {disfmarker} I , uh , @ @ {comment} didn't {disfmarker} didn't see it , uh , yesterday but I 'm going to see it today . And , uh , that 's {disfmarker} that will enable us to do {pause} nice um , tight time marking of the beginning and ending of overlapping segments . At present it 's not possible with limitations of {disfmarker} of the , uh , original {pause} design of the software . And um . So , I don't know . In terms of , like , pre - segmentation , that {disfmarker} that continues to be , um , a terrific asset to the {disfmarker} to the transcribers . Do you {disf" />
    <node id=" to {disfmarker} uh , uh , if {disfmarker} if you {disfmarker} wouldn't mind , {comment} {vocalsound} to give us a pre - segmentation .&#10;Speaker: PhD A&#10;Content: y yeah .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Uh , maybe you have one already of that first m of the meeting that uh , the first transcribed meeting , the one that I transcribed .&#10;Speaker: PhD C&#10;Content: Um , I 'm sure I have some&#10;Speaker: Postdoc G&#10;Content: Do you have a {disfmarker} could you generate a pre - segmentation ?&#10;Speaker: Grad H&#10;Content: February sixteenth I think .&#10;Speaker: PhD C&#10;Content: but {disfmarker} but that 's the one where we 're , um , trai training on , so that 's a little bit {disfmarker}&#10;Speaker: Grad H&#10;Content: Oh .&#10;Speaker: Postdoc G&#10;Content: Oh , I see .&#10;Speaker: PhD C&#10;Content: It '" />
    <node id="er} you know , if we 've got two already finished and they 're working on {disfmarker} Uh , right now all eight of them have differe uh , uh , additional data sets . That means potentially as many as ten might be finished by the end of the month .&#10;Speaker: PhD E&#10;Content: Wow .&#10;Speaker: Postdoc G&#10;Content: Hope so . But the pre - segmentation really helps a huge amount .&#10;Speaker: PhD C&#10;Content: OK .&#10;Speaker: Postdoc G&#10;Content: And , uh , also Dan Ellis 's innovation of the , uh {disfmarker} the multi - channel to here really helped a r a lot in terms of clearing {disfmarker} clearing up h hearings that involve overlaps . But , um , just out of curiosity I asked one of them how long {pause} it was taking her , one of these two who has already finished her data set . She said it takes about , uh , sixty minutes transcription for every five minutes of real time . So it 's about twelve to one , which is what we were thinking .&#10;Speaker: Grad H&#10;Content: or Yep .&#10;Speaker: Postdoc" />
    <node id="isfmarker} these people would be very good to shift over to finer grain encoding of overlaps . It 's just a matter of , you know , providing {disfmarker} So if right now you have two overlapping segments in the same time bin , well with {disfmarker} with the improvement in the database {disfmarker} in {disfmarker} in the , uh , sorry , in the interface , it 'd be possible to , um , you know , just do a click and drag thing , and get the {disfmarker} uh , the specific place of each of those , the time tag associated with the beginning and end of {disfmarker} of each segment .&#10;Speaker: Professor B&#10;Content: Right , so I think we talking about three level {disfmarker} three things .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: One {disfmarker} one was uh , we had s had some discussion in the past about some very high level labelings ,&#10;Speaker: Postdoc G&#10;Content: Yeah . The types of overlaps {disfmarker}&#10;Spe" />
    <node id="In the transcript, PhD A is trying to propose the idea of collecting more meetings as a source of naturally occurring speech data for their research. The conversation with Chuck Fillmore, a member who had previously vehemently said no to being recorded, seemed to have a positive influence on PhD A's proposal. Initially, PhD A thought that many people had said &quot;no&quot; to being recorded, but after speaking with Chuck Fillmore, he found that Fillmore was more open to the idea this time. Fillmore even invited Liz (presumably PhD A) to a meeting, which PhD A takes as a sign of potential willingness among other groups to participate in the data collection. This interaction seems to have encouraged PhD A to make a pitch for collecting more meetings as a research resource." />
    <node id=" Mm - hmm .&#10;Speaker: PhD A&#10;Content: OK . I thought that all these people had sort of said &quot; no &quot; twice already .&#10;Speaker: Professor B&#10;Content: Yeah . So the {disfmarker}&#10;Speaker: PhD A&#10;Content: If that 's not the case then {disfmarker}&#10;Speaker: Professor B&#10;Content: No , no . No . So th there was the thing in Fillmore 's group but even there he hadn't {disfmarker} What he 'd said &quot; no &quot; to was for the main meeting . But they have several smaller meetings a week ,&#10;Speaker: Grad H&#10;Content: So .&#10;Speaker: Professor B&#10;Content: and , uh , the notion was raised before that that could happen . And it just , you know {disfmarker} it just didn't come together&#10;Speaker: PhD A&#10;Content: Just {disfmarker} OK .&#10;Speaker: PhD E&#10;Content: Well , and {disfmarker} and the other thing too is when they originally said &quot; no &quot; they didn't know about this post - editing capability thing .&#10;Speaker: Professor" />
    <node id="'t {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: uh , I mean , people have made a lot of use of {disfmarker} of TIMIT and , uh w due to its markings , and then {pause} the Switchboard transcription thing , well I think has been very useful for a lot of people .&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: That 's true .&#10;Speaker: Professor B&#10;Content: So {disfmarker}&#10;Speaker: PhD A&#10;Content: I guess I wanted to , um , sort of make a pitch for trying to collect more meetings .&#10;Speaker: Postdoc G&#10;Content: Cool .&#10;Speaker: PhD A&#10;Content: Um ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: I actually I talked to Chuck Fillmore and I think they 've what , vehemently said no before but this time he wasn't vehement and he said you know , &quot; well , Liz , come to the meeting tomorrow&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A" />
    <node id="Content: Yeah .&#10;Speaker: Grad H&#10;Content: Yeah ? OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Hello .&#10;Speaker: Professor B&#10;Content: OK , agenda item one ,&#10;Speaker: PhD D&#10;Content: We went {disfmarker}&#10;Speaker: Grad F&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: introduce Don . OK , we did that . Uh {disfmarker}&#10;Speaker: PhD A&#10;Content: Well , I had a {disfmarker} just a quick question but I know there was discussion of it at a previous meeting that I missed , but just about the {disfmarker} the wish list item of getting good quality close - talking mikes on every speaker .&#10;Speaker: Professor B&#10;Content: OK , so let 's {disfmarker} let 's {disfmarker} So let 's just do agenda {pause} building right now . OK , so let 's talk about that a bit .&#10;Speaker: PhD A&#10;Content: I mean , that was {disfmarker}&#10;Speaker: Professor B" />
    <node id="1. When people are introduced to the idea of being connected with someone interested in the acoustic side of dialogue recording, they typically ask two main questions:&#10;&#10;- For those interested in general dialogue things, the question is often &quot;are you recording video?&quot; This shows that they consider visual and audio components as a pair, understanding that capturing high-quality video can be crucial for analyzing dialogue effectively.&#10;   (Speaker: Professor B, Content: And just like someone who is interested in the general dialogue thing will always ask 'um, are you recording video?')&#10;&#10;- For those focused on the acoustic side of dialogue recording, a common question is &quot;are you doing array microphones?&quot; This indicates their interest in exploring advanced audio recording techniques and technologies to ensure optimal sound quality for analysis.&#10;   (Speaker: Professor B, Content: And the acoustic people will always say, 'well are you doing, uh, uh, array microphones?')&#10;&#10;These questions reflect individuals' curiosity about different aspects of dialogue recording and their desire to understand the methods and technologies being used in order to ensure high-quality recordings for further analysis." />
    <node id=" and , uh , um {disfmarker} So , yeah , there 's {disfmarker} there 's {disfmarker} there 's a range of things that people do .&#10;Speaker: PhD E&#10;Content: Huh .&#10;Speaker: Professor B&#10;Content: Um , so if we connected up with somebody who was interested in doing that sort of thing that 's {disfmarker} that 's a good thing to do . I mean , whenever I 've described this to other people who are interested on the {disfmarker} with the acoustic side that 's invariably the question they ask . Just like someone who is interested in the general dialogue thing will always ask {vocalsound} &quot; um , are you recording video ? &quot;&#10;Speaker: PhD A&#10;Content: Right ,&#10;Speaker: Professor B&#10;Content: Um , right ?&#10;Speaker: PhD A&#10;Content: right .&#10;Speaker: Professor B&#10;Content: And {disfmarker} and the acoustic people will always say , &quot; well are you doing , uh , uh , array microphones ? &quot; So it 's {disfmarker} it 's a good thing to do ," />
    <node id="Having a chunked audio file before transcription is important for the speech recognition process because it allows for better organization of the audio data and improved accuracy in associating words with their corresponding time boundaries. By dividing the continuous audio stream into smaller segments based on speech-silence transitions, automatic speech recognition (ASR) techniques can more easily identify speakers within these segments.&#10;&#10;When there is a chunked audio file before transcription, it becomes possible to perform forced alignment, which determines as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This helps ensure that each word is accurately associated with its corresponding time boundaries in the original audio, even if words are in different segments or have been inserted or deleted.&#10;&#10;This chunked audio file also facilitates better recognition and feature computation by combining segments from the same speaker to create longer stretches of continuous speech. This consolidation process enables a speaker's unique characteristics and patterns to be analyzed more effectively by the recognizer, ultimately leading to improved accuracy in transcriptions and prosodic feature computation.&#10;&#10;In summary, chunking audio files before transcription is crucial for maintaining consistent time markers, accurately associating words with their time boundaries, and improving overall recognition and feature computation during the speech recognition process." />
    <node id="1. The project that Dan Ellis worked on involves using cross-correlation and beam-forming techniques to construct a map of where people were sitting. This project is related to recording audio in meetings and improving the signal-to-noise ratio for further processing and analysis. Other individuals who may be involved in this project include PhD A, PhD C, Dave Gelbart, and possibly Professor B. This project seems to be focused on removing high-level constraints and approaching it from a bottom-up perspective, using less data and infrastructure than might typically be used for similar projects." />
    <node id=" head worn mikes ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: and that is to try to construct a map of where people were sitting ,&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: uh , based on {disfmarker}&#10;Speaker: Grad H&#10;Content: Well Dan {disfmarker} Dan had worked on that . Dan Ellis ,&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: Oh , did he ? Oh , that 's interesting .&#10;Speaker: Grad H&#10;Content: yeah . So that {disfmarker} that 's the cross - correlation stuff , was {disfmarker} was doing b beam - forming .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: And so you could plot out who was sitting next to who&#10;Speaker: Professor B&#10;Content: A little bit ,&#10;Speaker: PhD E&#10;Content: and {disfmarker}&#10;Speaker: Professor B" />
    <node id=" B&#10;Content: So , um , the , uh {disfmarker} the big arrays , uh , places , uh , like uh , Rutgers , and Brown , and other {disfmarker} other places , uh , they have , uh , big arrays with , I don't know , a hundred {disfmarker} hundred mikes or something .&#10;Speaker: Grad H&#10;Content: Xerox .&#10;Speaker: Professor B&#10;Content: And so there 's a wall of mikes . And you get really , really good beam - forming {comment} with that sort of thing .&#10;Speaker: PhD E&#10;Content: Wow .&#10;Speaker: Professor B&#10;Content: And it 's {disfmarker} and , um , in fact at one point we had a {disfmarker} a proposal in with Rutgers where we were gonna do some of the sort of per channel signal - processing and they were gonna do the multi - channel stuff , but {pause} it d it d we ended up not doing it . But {disfmarker}&#10;Speaker: PhD E&#10;Content: I 've seen demonstrations of the microphone arrays . It 's amazing how {disfmark" />
    <node id="Based on the transcript provided, Postdoc G was thinking about an aspect of PhD D's work that involves taking segments of a particular type (such as speech from one speaker) and putting them together. This is referred to when Postdoc G says &quot;which is that it sounded to me, Liz, as though you also had another aspect of your work that involved taking segments which are of a particular type and putting them together.&quot; This suggestion is then confirmed by PhD D, who says &quot;Yeah.&quot; The ensuing dialogue makes it clear that this process involves cutting out parts of a recording that are not relevant to the speaker in question, combining relevant segments from the same speaker, and running them through a recognizer." />
    <node id=" The {disfmarker} the {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Including LDC .&#10;Speaker: PhD E&#10;Content: Yeah ,&#10;Speaker: Postdoc G&#10;Content: I think so .&#10;Speaker: Grad H&#10;Content: Yep .&#10;Speaker: PhD E&#10;Content: y right , OK .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm . Then there 's their web site that has lots of papers . And I looked through them and they mainly had to do with this , um , this , uh , tree structure , uh , annotated tree diagram thing .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Postdoc G&#10;Content: So , um , um {disfmarker} and , you know , in terms of like the conventions that I 'm a that I 've adopted , it {disfmarker} there {disfmarker} there 's no conflict at all .&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: Postdoc G&#10;Content: And he was , you know , very interested . And , &quot; oh , and how 'd you" />
    <node id="1. This week, Postdoc G and Dave Gelbart will be visiting with John Canny, who is a CS (Computer Science) professor interested in augmented reality (AR) and array microphones.&#10;2. The purpose of their meeting is to explore potential commonalities between their work and John Canny's research on AR and array microphones.&#10;3. One possible outcome could be integrating array microphones into their current work to enhance audio quality during recordings and analysis." />
    <node id="marker} or maybe it 's {disfmarker} No , actually {pause} it 's this week , uh , Dave Gelbart and I will be , uh , visiting with John Canny who i you know , is a CS professor ,&#10;Speaker: Postdoc G&#10;Content: Oh .&#10;Speaker: Professor B&#10;Content: who 's interested in ar in array microphones .&#10;Speaker: Grad H&#10;Content: HCC . Oh , he 's doing array mikes .&#10;Speaker: Professor B&#10;Content: Yeah . And so we wanna see what commonality there is here . You know , maybe they 'd wanna stick an array mike here when we 're doing things&#10;Speaker: PhD E&#10;Content: That would be cool .&#10;Speaker: Grad H&#10;Content: Yeah , that would be neat .&#10;Speaker: Professor B&#10;Content: or {disfmarker} or maybe it 's {disfmarker} it 's not a specific array microphone they want&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: That would be really neat .&#10;Speaker: Professor B&#10;Content: but they might wanna just" />
    <node id="ation and then he 's also interested in the transcription conventions .&#10;Speaker: Grad H&#10;Content: Mm - hmm .&#10;Speaker: Postdoc G&#10;Content: And , um {disfmarker} So , um , it seems to me in terms of like , um , i i it wou You know , OK . So the room , it 's things like the audio and c and audi audio and acoustic {disfmarker} acoustic properties of the room and how it {disfmarker} how the recordings are done , and that kind of thing . And , um . OK , in terms of the multi - trans , well that {disfmarker} that 's being modified by Dave Gelbart to , uh , handle multi - channel recording .&#10;Speaker: Grad H&#10;Content: Oh , I should 've {disfmarker} I was just thinking I should have invited him to this meeting . I forgot to do it .&#10;Speaker: Postdoc G&#10;Content: Yeah , OK .&#10;Speaker: Grad H&#10;Content: So .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Yeah . Well that 's OK" />
    <node id=" see him this Friday what {disfmarker} what kind of level he wants to get involved .&#10;Speaker: Postdoc G&#10;Content: It 's premature . Fine . Good .&#10;Speaker: Professor B&#10;Content: Uh , he might be excited to and it might be very appropriate for him to , uh , or he might have no interest whatsoever . I {disfmarker} I just really don't know .&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: Grad H&#10;Content: Is he involved in {disfmarker} Ach ! {comment} I 'm blanking on the name of the project . NIST has {disfmarker} has done a big meeting room {disfmarker} instrumented meeting room with video and microphone arrays , and very elaborate software . Is {disfmarker} is he the one working on that ?&#10;Speaker: Professor B&#10;Content: Well that 's what they 're starting up .&#10;Speaker: Grad H&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah . No , I mean , that 's what all this is about . They {disfmarker} they haven't done" />
    <node id="1st Issue: Non-speaking individuals picking up signals from other people's conversations: When non-speaking individuals have their microphones active and there is cross-talk or nearby conversations, automatic speech recognition (ASR) techniques may pick up the wrong person's speech. This can lead to inaccurate transcriptions as the ASR system attempts to transcribe the incorrect speaker's words for the corresponding dialogue.&#10;&#10;2nd Issue: Occasional speaking amidst others' speech: When an individual speaks occasionally while others are speaking, the challenge lies in distinguishing their voice from the background noise or other speakers. If the person speaks softly or mumbles, it becomes even harder for ASR systems to identify and transcribe their words accurately. Cross-talk and variability in microphone quality can further exacerbate this issue, leading to inconsistent or inaccurate transcriptions.&#10;&#10;These issues highlight the importance of better signal-processing techniques, such as minimizing cross-talk, distinguishing individual speakers, and improving the overall signal-to-noise ratio. This will help ensure more precise and reliable transcriptions for analyzing dialogue or prosody in meetings and other collaborative settings." />
    <node id="1. Positioning of speakers: Based on the energy levels or loudness on each speaker's microphone, one can determine if individuals are located between two people who are talking (a large peak in the display) or speaking on one side or the other of an axis between two individuals (the peak will go the opposite way, with more sound picked up from the closer speaker and less from the farther speaker).&#10;&#10;2. Volume of speakers: Larger energy levels or loudness on a microphone may indicate that a speaker is speaking more loudly or is closer to the microphone. Conversely, lower energy levels suggest that the speaker is softer or further away. However, it's essential to consider that variations in microphone quality and type can also influence the perceived volume.&#10;&#10;3. Number of speakers: The configuration of speakers affects the appearance of peaks on a display. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time. This information could help determine the number of active speakers in a given situation.&#10;&#10;In summary, analyzing energy levels and loudness on microphones can provide insights into speaker positioning, volume, and the number of speakers in a conversation. However, it is crucial to control for variables such as microphone quality and type to ensure accurate interpretations." />
    <node id="PhD C is currently working on a method to include information about which channel certain speech occurs in, by comparing and normalizing energies of different channels. This approach aims to help transcribers identify the specific channel with speech, adding value to the existing transcription process that primarily focuses on speech-nonspeech detection in mixed files. By distinguishing individual speakers' channels more accurately, this technique may improve overall signal-to-noise ratio and minimize cross-talk issues during dialogues, leading to more precise and reliable transcriptions." />
    <node id="marker} that continues to be , um , a terrific asset to the {disfmarker} to the transcribers . Do you {disfmarker} I know that you 're al also supplementing it further . Do you want to mention something about that c Thilo , or {disfmarker} ?&#10;Speaker: PhD C&#10;Content: Um , yeah . What {disfmarker} what I 'm doing right now is I 'm trying to include some information about which channel , uh , there 's some speech in . But that 's not working at the moment . I 'm just trying to do this by comparing energies , uh {disfmarker} normalizing energies and comparing energies of the different channels .&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: PhD C&#10;Content: And so to {disfmarker} to give the transcribers some information in which channel there 's {disfmarker} there 's speech in addition to {disfmarker} to the thing we {disfmarker} we did now which is just , uh , speech - nonspeech detection on the mixed file . So I 'm {d" />
    <edge source="Based on the discussion, the person being referred to (it's not clear who this is exactly, as there are several people mentioned) is interested in or potentially working on a project that involves removing high-level constraints and approaching it from a bottom-up perspective. This project seems to be related to using less data and infrastructure than might typically be used for similar projects. Professor B also suggests that if someone with a strong interest in this area were to become involved, they could drive the direction of the project. However, there are no specific details about the nature or scope of the project itself." target="Speaker: Professor B&#10;Content: Are we on ? We 're on . OK .&#10;Speaker: PhD E&#10;Content: Is it on ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . OK ,&#10;Speaker: PhD D&#10;Content: One , two {disfmarker} u OK .&#10;Speaker: PhD A&#10;Content: Why is it so cold in here ?&#10;Speaker: Professor B&#10;Content: so , uh , we haven't sent around the agenda . So , i uh , any agenda items anybody has , wants to talk about , what 's going on ?&#10;Speaker: Postdoc G&#10;Content: I c I could talk about the meeting .&#10;Speaker: Grad H&#10;Content: Does everyone {disfmarker} has everyone met Don ?&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: It 's on ?&#10;Speaker: PhD C&#10;Content: Now , yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: Yeah ? OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, the person being referred to (it's not clear who this is exactly, as there are several people mentioned) is interested in or potentially working on a project that involves removing high-level constraints and approaching it from a bottom-up perspective. This project seems to be related to using less data and infrastructure than might typically be used for similar projects. Professor B also suggests that if someone with a strong interest in this area were to become involved, they could drive the direction of the project. However, there are no specific details about the nature or scope of the project itself." target=" .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: So .&#10;Speaker: Grad H&#10;Content: Yeah , and you {disfmarker} and you signed a form .&#10;Speaker: Grad F&#10;Content: Oh , I think so .&#10;Speaker: Postdoc G&#10;Content: Did you sign a form ?&#10;Speaker: Grad F&#10;Content: Did I ? I don't know .&#10;Speaker: Grad H&#10;Content: I 'm pretty sure . Well I 'll {disfmarker} I 'll get another one before the end of the meeting .&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: Thank you .&#10;Speaker: Grad F&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: You don't {disfmarker} you don't have to leave for it .&#10;Speaker: Professor B&#10;Content: Yeah , we {disfmarker} we {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, the person being referred to (it's not clear who this is exactly, as there are several people mentioned) is interested in or potentially working on a project that involves removing high-level constraints and approaching it from a bottom-up perspective. This project seems to be related to using less data and infrastructure than might typically be used for similar projects. Professor B also suggests that if someone with a strong interest in this area were to become involved, they could drive the direction of the project. However, there are no specific details about the nature or scope of the project itself." target=" Professor B&#10;Content: It 's pretty tough , uh , this group . Yeah .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So , uh , what about {disfmarker} what about people who involved in some artistic endeavor ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean , film - making or something like that .&#10;Speaker: PhD A&#10;Content: Exactly , that 's what I was {disfmarker}&#10;Speaker: Professor B&#10;Content: You 'd think like they would be {disfmarker}&#10;Speaker: PhD D&#10;Content: A film - maker .&#10;Speaker: PhD A&#10;Content: something where there {disfmarker} there is actually discussion where there 's no right or wrong answer but {disfmarker} but it 's a matter of opinion kind of thing . Uh , anyway , if you {disfmarker} if you have ideas {disfmarker}&#10;Speaker: Postdoc G&#10;Content: It 's be fun .&#10;Speaker: Grad">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, the person being referred to (it's not clear who this is exactly, as there are several people mentioned) is interested in or potentially working on a project that involves removing high-level constraints and approaching it from a bottom-up perspective. This project seems to be related to using less data and infrastructure than might typically be used for similar projects. Professor B also suggests that if someone with a strong interest in this area were to become involved, they could drive the direction of the project. However, there are no specific details about the nature or scope of the project itself." target=": Grad H&#10;Content: Well , I don't know if they meet regularly or not but they are no longer recording .&#10;Speaker: Professor B&#10;Content: But I mean , ha ha have they said they don't want to anymore or {disfmarker} ?&#10;Speaker: Grad H&#10;Content: Um , ugh , what was his name ?&#10;Speaker: Professor B&#10;Content: Uh , i i&#10;Speaker: Postdoc G&#10;Content: Joe Sokol ?&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: When {disfmarker} with him gone , it sorta trickled off .&#10;Speaker: Professor B&#10;Content: OK , so they 're down to three or four people&#10;Speaker: Grad H&#10;Content: They {disfmarker} and they stopped {disfmarker} Yeah .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: but the thing is three or four people is OK .&#10;Speaker: Grad H&#10;Content: Yep .&#10;Speaker: Postdoc G&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, the person being referred to (it's not clear who this is exactly, as there are several people mentioned) is interested in or potentially working on a project that involves removing high-level constraints and approaching it from a bottom-up perspective. This project seems to be related to using less data and infrastructure than might typically be used for similar projects. Professor B also suggests that if someone with a strong interest in this area were to become involved, they could drive the direction of the project. However, there are no specific details about the nature or scope of the project itself." target="&#10;Speaker: Postdoc G&#10;Content: Mm - hmm , OK .&#10;Speaker: Grad H&#10;Content: Yeah , but {disfmarker} but just saying what the {disfmarker}&#10;Speaker: Professor B&#10;Content: because it 's {disfmarker} you know , there 's this movement from here to here&#10;Speaker: Postdoc G&#10;Content: Yeah , I 'm sure . Uh , yeah , I {disfmarker} I know .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: and {disfmarker} and {disfmarker} and it 's {disfmarker} so I&#10;Speaker: PhD E&#10;Content: You 're saying r sort of remove the high level constraints and go bottom - up .&#10;Speaker: Professor B&#10;Content: Yeah , describe {disfmarker} describe it .&#10;Speaker: PhD E&#10;Content: Then just say {disfmarker}&#10;Speaker: Grad H&#10;Content: Yep , just features .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, the person being referred to (it's not clear who this is exactly, as there are several people mentioned) is interested in or potentially working on a project that involves removing high-level constraints and approaching it from a bottom-up perspective. This project seems to be related to using less data and infrastructure than might typically be used for similar projects. Professor B also suggests that if someone with a strong interest in this area were to become involved, they could drive the direction of the project. However, there are no specific details about the nature or scope of the project itself." target=" not use all the data ?&#10;Speaker: Grad H&#10;Content: It 's {disfmarker} it 's just a lot of infrastructure that for our particular purpose we felt we didn't need to set up .&#10;Speaker: Postdoc G&#10;Content: I see .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Fine .&#10;Speaker: Professor B&#10;Content: Yeah , if ninety - nine percent of what you 're doing is c is shutting off most of the mikes , then going through the {disfmarker}&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: But if you get somebody who 's {disfmarker} who {disfmarker} who has that as a primary interest then that put {disfmarker} then that drives it in that direction .&#10;Speaker: Grad H&#10;Content: That 's right , I mean if someone {disfmarker} if someone came in and said we really want to do it ,&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Grad H&#10;Content: I mean , we don't care">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, the person being referred to (it's not clear who this is exactly, as there are several people mentioned) is interested in or potentially working on a project that involves removing high-level constraints and approaching it from a bottom-up perspective. This project seems to be related to using less data and infrastructure than might typically be used for similar projects. Professor B also suggests that if someone with a strong interest in this area were to become involved, they could drive the direction of the project. However, there are no specific details about the nature or scope of the project itself." target="During the discussion, Professors B and A mentioned some proposed categories that would allow for overlapping and different timing changes. While specific details about these categories were not provided in the transcript, they seem to be related to a project or research endeavor that involves removing high-level constraints and approaching it from a bottom-up perspective. This project may also involve using less data and infrastructure than typical projects in this field. To further explore this topic, Professor B suggests involving John Ohala, who is likely an expert or key figure with valuable insights on the subject matter. However, the transcript does not offer enough context to provide a more precise description of these categories.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, the person being referred to (it's not clear who this is exactly, as there are several people mentioned) is interested in or potentially working on a project that involves removing high-level constraints and approaching it from a bottom-up perspective. This project seems to be related to using less data and infrastructure than might typically be used for similar projects. Professor B also suggests that if someone with a strong interest in this area were to become involved, they could drive the direction of the project. However, there are no specific details about the nature or scope of the project itself." target="Based on the transcript, there are no specific measures explicitly discussed to deal with people losing interest or becoming frustrated during the meeting. However, some related issues are mentioned that can help address the question:&#10;&#10;1. Microphone difficulties: The participants bring up the problem of inconsistent microphone usage leading to cross-talk and inaccurate transcriptions. Improving audio quality can help maintain engagement and make it easier for participants to follow the conversation.&#10;2. Transcription standards: Discussants emphasize the importance of having uniformity and transcription standards, particularly when multiple people are speaking simultaneously or using specialized terminology. Clear communication and understanding can prevent frustration and misunderstandings.&#10;3. Human subjects concerns: The group acknowledges that video recordings may make participants uncomfortable due to human subjects issues, which could contribute to waning interest or frustration. Addressing these concerns by ensuring privacy and informed consent is crucial for maintaining engagement.&#10;4. Meeting data usage: While not directly related to addressing frustration during the meeting itself, it's mentioned that the primary purpose of using meeting data is to gather naturally occurring speech data, rather than focusing on individual behaviors like gestures or comparing it to far-field studies. Clarifying the goals and intentions of a meeting can help align participants' expectations and minimize frustration.&#10;5. Bottom-up project: Although not explicitly linked to addressing frustration, the group mentions a potential project that involves removing high-level constraints and approaching it from a bottom-up perspective. This approach may result in less reliance on extensive data and infrastructure, potentially reducing complexity and confusion for participants.&#10;&#10;In summary, while there are no specific measures discussed to deal with people losing interest or becoming frustrated during the meeting, some related issues are brought up that can contribute to addressing frustration by improving audio quality, promoting clear communication, ensuring privacy, clarifying goals, and simplifying project constraints.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, the person being referred to (it's not clear who this is exactly, as there are several people mentioned) is interested in or potentially working on a project that involves removing high-level constraints and approaching it from a bottom-up perspective. This project seems to be related to using less data and infrastructure than might typically be used for similar projects. Professor B also suggests that if someone with a strong interest in this area were to become involved, they could drive the direction of the project. However, there are no specific details about the nature or scope of the project itself." target="1. The project that Dan Ellis worked on involves using cross-correlation and beam-forming techniques to construct a map of where people were sitting. This project is related to recording audio in meetings and improving the signal-to-noise ratio for further processing and analysis. Other individuals who may be involved in this project include PhD A, PhD C, Dave Gelbart, and possibly Professor B. This project seems to be focused on removing high-level constraints and approaching it from a bottom-up perspective, using less data and infrastructure than might typically be used for similar projects.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Professor B&#10;Content: It 's pretty tough , uh , this group . Yeah .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So , uh , what about {disfmarker} what about people who involved in some artistic endeavor ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean , film - making or something like that .&#10;Speaker: PhD A&#10;Content: Exactly , that 's what I was {disfmarker}&#10;Speaker: Professor B&#10;Content: You 'd think like they would be {disfmarker}&#10;Speaker: PhD D&#10;Content: A film - maker .&#10;Speaker: PhD A&#10;Content: something where there {disfmarker} there is actually discussion where there 's no right or wrong answer but {disfmarker} but it 's a matter of opinion kind of thing . Uh , anyway , if you {disfmarker} if you have ideas {disfmarker}&#10;Speaker: Postdoc G&#10;Content: It 's be fun .&#10;Speaker: Grad" target="1. The speakers seem open to the idea of allowing students or filmmakers to exercise creative control over a potential one-hour film, as they discuss the possibility of having student groups or filmmakers participate in the project. They mention that this could add some color and creativity to the proceedings. (PhD A, Postdoc G, Grad F, Grad H, PhD D, Professor B)&#10;&#10;2. However, there are concerns about the commercial value of such a film. The speakers discuss the possibility that focusing on artistic aspects might detract from the main objectives of the project and reduce its commercial potential. (Professor B, Postdoc G, PhD D)&#10;&#10;3. Another concern is the length of the film. The participants mention that allowing students or filmmakers to create a one-hour film might result in excessive footage, making it difficult to manage and utilize effectively. They discuss the need to strike a balance between artistic freedom and practical constraints. (Postdoc G, PhD D)&#10;&#10;In summary, while the speakers are generally open to the idea of involving students or filmmakers in the project, they have concerns about the commercial value and length of the potential film. They acknowledge the need to balance artistic aspects with practical considerations to ensure the success of the project.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Professor B&#10;Content: It 's pretty tough , uh , this group . Yeah .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So , uh , what about {disfmarker} what about people who involved in some artistic endeavor ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean , film - making or something like that .&#10;Speaker: PhD A&#10;Content: Exactly , that 's what I was {disfmarker}&#10;Speaker: Professor B&#10;Content: You 'd think like they would be {disfmarker}&#10;Speaker: PhD D&#10;Content: A film - maker .&#10;Speaker: PhD A&#10;Content: something where there {disfmarker} there is actually discussion where there 's no right or wrong answer but {disfmarker} but it 's a matter of opinion kind of thing . Uh , anyway , if you {disfmarker} if you have ideas {disfmarker}&#10;Speaker: Postdoc G&#10;Content: It 's be fun .&#10;Speaker: Grad" target="1. The suggestion made by PhD A and Postdoc G regarding adding more variation to the beeps in a film is for someone to explore different groups or filmmakers who could bring some color and creativity to the proceedings. They are open to discussing ideas and doing the legwork to contact these potential collaborators but need guidance on which groups might be worth pursuing.&#10;&#10;Based on the transcript, it seems that they want to move away from the monotonous &quot;beeps&quot; and incorporate more varied prosodic contours in the film's audio elements. This would involve finding artists or filmmakers who can contribute creatively to this aspect of the project while considering practical constraints such as commercial value and length of the film.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Professor B&#10;Content: It 's pretty tough , uh , this group . Yeah .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So , uh , what about {disfmarker} what about people who involved in some artistic endeavor ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: I mean , film - making or something like that .&#10;Speaker: PhD A&#10;Content: Exactly , that 's what I was {disfmarker}&#10;Speaker: Professor B&#10;Content: You 'd think like they would be {disfmarker}&#10;Speaker: PhD D&#10;Content: A film - maker .&#10;Speaker: PhD A&#10;Content: something where there {disfmarker} there is actually discussion where there 's no right or wrong answer but {disfmarker} but it 's a matter of opinion kind of thing . Uh , anyway , if you {disfmarker} if you have ideas {disfmarker}&#10;Speaker: Postdoc G&#10;Content: It 's be fun .&#10;Speaker: Grad" target="Based on the transcript provided, Postdoc G was thinking about an aspect of PhD D's work that involves taking segments of a particular type (such as speech from one speaker) and putting them together. This is referred to when Postdoc G says &quot;which is that it sounded to me, Liz, as though you also had another aspect of your work that involved taking segments which are of a particular type and putting them together.&quot; This suggestion is then confirmed by PhD D, who says &quot;Yeah.&quot; The ensuing dialogue makes it clear that this process involves cutting out parts of a recording that are not relevant to the speaker in question, combining relevant segments from the same speaker, and running them through a recognizer.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="'t know , at least half of them probably {comment} on average are g are ha are {disfmarker} have a lot of cross - ta sorry , some of the segments have a lot of cross - talk . Um , it 's good to get sort of short segments if you 're gonna do recognition , especially forced alignment . So , uh , Don has been taking a first stab actually using Jane 's first {disfmarker} the fir the meeting that Jane transcribed which we did have some problems with , and Thilo , uh , I think told me why this was , but that people were switching microphones around {comment} in the very beginning , so {disfmarker} the SRI re&#10;Speaker: PhD C&#10;Content: No , th Yeah . No . They {disfmarker} they were not switching them but what they were {disfmarker} they were adjusting them ,&#10;Speaker: PhD A&#10;Content: and they {disfmarker} They were not {disfmarker}&#10;Speaker: PhD C&#10;Content: so .&#10;Speaker: Grad F&#10;Content: Mmm .&#10;Speaker: Grad H&#10;Content: Adjusting . Oh .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target=" research is going on we 're also experimenting with different ASR , uh , techniques .&#10;Speaker: Grad H&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: And so it 'd be w good to know about it .&#10;Speaker: PhD E&#10;Content: So the problem is like , uh , on the microphone of somebody who 's not talking they 're picking up signals from other people {comment} and that 's {vocalsound} causing problems ?&#10;Speaker: PhD A&#10;Content: R right , although if they 're not talking , using the {disfmarker} the inhouse transcriptions , were sort of O K because the t no one transcribed any words there and we throw it out .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: But if they 're talking at all and they 're not talking the whole time , so you get some speech and then a &quot; mm - hmm &quot; , and some more speech , so that whole thing is one chunk . And the person in the middle who said only a little bit is picking up the speech around it , that 's where it">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target=" and putting them in that format , and see how that works out . I {disfmarker} I {disfmarker} I explained to him in {disfmarker} in detail the , uh , conventions that we 're using here in this {disfmarker} in this word level transcript . And , um , you know , I {disfmarker} I explained , you know , the reasons that {disfmarker} that we were not coding more elaborately and {disfmarker} and the focus on reliability . He expressed a lot of interest in reliability . It 's like he 's {disfmarker} he 's really up on these things . He 's {disfmarker} he 's very {disfmarker} Um , independently he asked , &quot; well what about reliability ? &quot; So , {vocalsound} he 's interested in the consistency of the encoding and that sort of thing . OK , um {disfmarker}&#10;Speaker: PhD A&#10;Content: Sorry , can you explain what the ATLAS {disfmarker} I 'm not familiar with this ATLAS system .&#10;Speaker: Postdoc G&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target=" what I had read was , uh , they had a uh very large amount of software infrastructure for coordinating all this , both in terms of recording and also live room where you 're interacting {disfmarker} the participants are interacting with the computer , and with the video , and lots of other stuff .&#10;Speaker: Professor B&#10;Content: Well , I 'm {disfmarker} I 'm {disfmarker} I 'm not sure .&#10;Speaker: Grad H&#10;Content: So .&#10;Speaker: Professor B&#10;Content: All {disfmarker} all I know is that they 've been talking to me about a project that they 're going to start up recording people meet in meetings .&#10;Speaker: Grad H&#10;Content: OK . Well {disfmarker}&#10;Speaker: Professor B&#10;Content: And , uh , it is related to ours . They were interested in ours . They wanted to get some uniformity with us , uh , about the transcriptions and so on .&#10;Speaker: Grad H&#10;Content: Alright .&#10;Speaker: Professor B&#10;Content: And one {disfmarker} one notable difference {disfmarker} u u actually">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target=": so {disfmarker} yeah , uniformity would be great .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Is it because {disfmarker} You {disfmarker} you 're saying the {disfmarker} for dialogue purposes , so that means that the transcribers are having trouble with those mikes ? Is that what you mean ?&#10;Speaker: PhD A&#10;Content: Well Jane would know more about the transcribers .&#10;Speaker: PhD E&#10;Content: Or {disfmarker} ?&#10;Speaker: Postdoc G&#10;Content: And that 's true . I mean , I {disfmarker} we did discuss this . Uh , and {disfmarker} and {disfmarker}&#10;Speaker: Grad H&#10;Content: Yep . Couple times .&#10;Speaker: Postdoc G&#10;Content: a couple times , so , um , yeah , the transcribers notice {disfmarker} And in fact there 're some where , um {disfmarker} ugh well , I mean there 's {disfmarker} it 's the double thing">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target=" Professor B&#10;Content: Yeah . No , I mean , that 's what all this is about . They {disfmarker} they haven't done it yet . They wanted to do it {disfmarker}&#10;Speaker: Grad H&#10;Content: OK . I had read some papers that looked like they had already done some work .&#10;Speaker: Professor B&#10;Content: Uh , well I think they 've instrumented a room but I don't {pause} think they {disfmarker} they haven't started recordings yet . They don't have the t the transcription standards . They don't have the {disfmarker}&#10;Speaker: PhD E&#10;Content: Are they going to do video as well ?&#10;Speaker: Grad H&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Yeah . I think .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: I think they are .&#10;Speaker: Grad H&#10;Content: Oh , cuz what {disfmarker} what I had read was , uh , they had a uh very large amount of software infrastructure for coordinating all this , both in terms of recording and also live room">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="1. The speakers seem open to the idea of allowing students or filmmakers to exercise creative control over a potential one-hour film, as they discuss the possibility of having student groups or filmmakers participate in the project. They mention that this could add some color and creativity to the proceedings. (PhD A, Postdoc G, Grad F, Grad H, PhD D, Professor B)&#10;&#10;2. However, there are concerns about the commercial value of such a film. The speakers discuss the possibility that focusing on artistic aspects might detract from the main objectives of the project and reduce its commercial potential. (Professor B, Postdoc G, PhD D)&#10;&#10;3. Another concern is the length of the film. The participants mention that allowing students or filmmakers to create a one-hour film might result in excessive footage, making it difficult to manage and utilize effectively. They discuss the need to strike a balance between artistic freedom and practical constraints. (Postdoc G, PhD D)&#10;&#10;In summary, while the speakers are generally open to the idea of involving students or filmmakers in the project, they have concerns about the commercial value and length of the potential film. They acknowledge the need to balance artistic aspects with practical considerations to ensure the success of the project.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="From the transcript, it is indicated that each transcriber is given their own &quot;meeting&quot; to transcribe. The size of the data set is not explicitly stated in terms of word count or file size. However, it is mentioned that each meeting lasts for an hour to an hour and a half. Therefore, we can infer that the data sets given to the transcribers are roughly an hour to an hour and a half long worth of audio data per meeting. Additionally, there are eight transcribers in total.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="Based on the meeting transcript, the level of detail in the time information provided by Brian Kingsbury's team will not be as fine-grained as the current transcription methods used. The exact difference in the level of detail is not specified in the transcript. However, it is mentioned that the current transcriptions are created by transcribers who are given individual &quot;meetings&quot; to transcribe, with each meeting lasting for an hour to an hour and a half. Therefore, the current transcription methods likely include detailed time information for each speaker in the meeting. In contrast, Brian Kingsbury's team will not provide as much detail in terms of time information, but it is unclear exactly how much less detailed their transcriptions will be.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="PHP A's concern is that the current method of determining word segments in the transcripts, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. A forced alignment can help address this issue by creating a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio. This would improve the accuracy of prosodic feature computation and overall transcription quality.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="During the discussion, Professors B and A mentioned some proposed categories that would allow for overlapping and different timing changes. While specific details about these categories were not provided in the transcript, they seem to be related to a project or research endeavor that involves removing high-level constraints and approaching it from a bottom-up perspective. This project may also involve using less data and infrastructure than typical projects in this field. To further explore this topic, Professor B suggests involving John Ohala, who is likely an expert or key figure with valuable insights on the subject matter. However, the transcript does not offer enough context to provide a more precise description of these categories.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="According to Professor B, the purpose of using meeting data is not specifically for studying gestures or for comparing it to far-field studies. Instead, meetings are seen as a natural way to get people talking and providing a source of naturally occurring speech data. This is inferred from her statements where she mentions viewing meetings as a neat way to get people talking naturally and seeing the value of meeting data as a source of data to help their research. The discussion about gestures was initiated by PhD A, but Professor B does not explicitly connect meeting data to gesture studies in her responses.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="1. Segmentation: The initial step involves segmenting the continuous audio stream based on speech-silence transitions. This results in separate speech segments.&#10;&#10;2. Speaker Identification: Within these segments, speakers are identified using automatic speech recognition (ASR) techniques. Adjustments to microphones and cross-talk can create inconsistencies and errors in transcriptions, so it is crucial to accurately associate each word with its corresponding time boundaries in the original audio.&#10;&#10;3. Forced Alignment: Before dividing the audio into smaller segments, a forced alignment process takes place. This involves determining as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This allows for improved accuracy in associating words with their time boundaries.&#10;&#10;4. Word-level Segmentation: After performing the forced alignment, the audio is divided into smaller segments containing single words or phrases. Each segment is associated with its corresponding transcription and time boundaries.&#10;&#10;5. Combining Segments: The speech segments from the same speaker are combined to create longer stretches of continuous speech for improved accuracy in recognition and feature computation. This consolidation process helps ensure that a speaker's unique characteristics and patterns can be better analyzed by the recognizer.&#10;&#10;6. Running through Recognizer: The combined speech segments from each speaker are then passed through a recognizer, which uses automatic speech recognition techniques to identify phonemes, words, or phrases within the audio. This generates a more accurate transcription and time alignment for further analysis.&#10;&#10;7. Computing Prosodic Features: With the accurately aligned transcriptions and time boundaries obtained from the previous steps, prosodic features such as pitch, intensity, and speaking rate can be computed to better understand the speaker's emotions, fluency, or other characteristics of their speech patterns.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="Based on the transcript, there are no specific measures explicitly discussed to deal with people losing interest or becoming frustrated during the meeting. However, some related issues are mentioned that can help address the question:&#10;&#10;1. Microphone difficulties: The participants bring up the problem of inconsistent microphone usage leading to cross-talk and inaccurate transcriptions. Improving audio quality can help maintain engagement and make it easier for participants to follow the conversation.&#10;2. Transcription standards: Discussants emphasize the importance of having uniformity and transcription standards, particularly when multiple people are speaking simultaneously or using specialized terminology. Clear communication and understanding can prevent frustration and misunderstandings.&#10;3. Human subjects concerns: The group acknowledges that video recordings may make participants uncomfortable due to human subjects issues, which could contribute to waning interest or frustration. Addressing these concerns by ensuring privacy and informed consent is crucial for maintaining engagement.&#10;4. Meeting data usage: While not directly related to addressing frustration during the meeting itself, it's mentioned that the primary purpose of using meeting data is to gather naturally occurring speech data, rather than focusing on individual behaviors like gestures or comparing it to far-field studies. Clarifying the goals and intentions of a meeting can help align participants' expectations and minimize frustration.&#10;5. Bottom-up project: Although not explicitly linked to addressing frustration, the group mentions a potential project that involves removing high-level constraints and approaching it from a bottom-up perspective. This approach may result in less reliance on extensive data and infrastructure, potentially reducing complexity and confusion for participants.&#10;&#10;In summary, while there are no specific measures discussed to deal with people losing interest or becoming frustrated during the meeting, some related issues are brought up that can contribute to addressing frustration by improving audio quality, promoting clear communication, ensuring privacy, clarifying goals, and simplifying project constraints.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="The people transcribing the interviews are a group of individuals subcontracted by Brian Kingsbury's team. They are not exactly like court reporters as they do not seem to have special keyboards or specific training for court reporting. Instead, they are more similar to secretarial staff or medical transcriptionists, responsible for manually transcribing audio data into written text. However, they might be using some basic equipment like headphones and a computer for transcription purposes. The level of detail in time information provided by their transcriptions is not as fine-grained as the current transcription methods used.&#10;&#10;It's important to note that organizations such as NIST or LDC are better equipped to handle transcription tasks due to their resources, expertise, and adherence to uniformity and high-quality standards across different projects. The comparison is made between simply placing a microphone to capture audio and the complex process involved in accurate and useful transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="1. Cross-talk and inconsistent audio quality: Participants mention that people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. This results in inconsistent or inaccurate transcriptions, which oversimplifies the complexity of the task when there is serious interest in capturing dialogue.&#10;&#10;2. Need for uniformity and transcription standards: Discussants emphasize the importance of having uniformity and transcription standards across different projects to ensure accurate and useful results, particularly in meeting recordings where multiple individuals may be speaking simultaneously.&#10;&#10;3. Human subjects concerns: Video recordings can make participants uncomfortable due to human subjects issues, potentially leading to people becoming shy or uninterested. Ensuring privacy and informed consent is crucial for maintaining engagement.&#10;&#10;4. Clarifying goals and intentions: Discussing the primary purpose of using meeting data, which is gathering naturally occurring speech data rather than focusing on individual behaviors like gestures, can help align participants' expectations and minimize frustration.&#10;&#10;While not explicitly mentioned as reasons, it can be inferred that having a serious interest in capturing dialogue or meeting recordings requires careful planning, resource allocation, and expertise to achieve accurate and useful results. Organizations like NIST or LDC are considered better equipped to handle these tasks due to their resources and expertise in ensuring uniformity, transcription standards, and high-quality recordings across different projects.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="The issue with having meetings that were exclusively in English, as requested by Jonathan Fiscus, was that it turned out to be a bit of a problem because not many all-English meetings were being held. Postdoc G had to give Fiscus their initial meeting in English because he asked for it, and they don't have a lot of all-English meetings right now. Therefore, providing an exclusively English meeting may have been challenging due to the lack of regular all-English meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="1. Positioning between speakers: If individuals are located between two people who are talking, a large peak will appear in the display. This is due to the microphone picking up sound from both sources almost equally.&#10;&#10;2. Speaker location: When someone is speaking on one side or the other of an axis between two individuals, the peak will go the opposite way. The microphone will pick up more sound from the closer speaker and less from the farther speaker, causing a difference in the display's appearance.&#10;&#10;3. Number of speakers: The configuration of speakers also influences the appearance of peaks. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="The issues discussed regarding signal-processing techniques for detecting energies involve inconsistencies in dialogue transcriptions due to challenges with microphones and cross-talk. The participants mentioned that variability in microphone quality and people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe dialogues. When individuals are not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions.&#10;&#10;It is important for those working on signal-processing techniques for detecting energies to be aware of these issues because improving the quality and consistency of audio recordings will significantly enhance the accuracy and usefulness of transcriptions. By developing better signal-processing techniques, researchers can help minimize cross-talk, distinguish individual speakers, and improve the overall signal-to-noise ratio, ultimately leading to more precise and reliable transcriptions. This is crucial for analyzing dialogue or prosody in meetings and other collaborative settings.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="1. When people are introduced to the idea of being connected with someone interested in the acoustic side of dialogue recording, they typically ask two main questions:&#10;&#10;- For those interested in general dialogue things, the question is often &quot;are you recording video?&quot; This shows that they consider visual and audio components as a pair, understanding that capturing high-quality video can be crucial for analyzing dialogue effectively.&#10;   (Speaker: Professor B, Content: And just like someone who is interested in the general dialogue thing will always ask 'um, are you recording video?')&#10;&#10;- For those focused on the acoustic side of dialogue recording, a common question is &quot;are you doing array microphones?&quot; This indicates their interest in exploring advanced audio recording techniques and technologies to ensure optimal sound quality for analysis.&#10;   (Speaker: Professor B, Content: And the acoustic people will always say, 'well are you doing, uh, uh, array microphones?')&#10;&#10;These questions reflect individuals' curiosity about different aspects of dialogue recording and their desire to understand the methods and technologies being used in order to ensure high-quality recordings for further analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="Having a chunked audio file before transcription is important for the speech recognition process because it allows for better organization of the audio data and improved accuracy in associating words with their corresponding time boundaries. By dividing the continuous audio stream into smaller segments based on speech-silence transitions, automatic speech recognition (ASR) techniques can more easily identify speakers within these segments.&#10;&#10;When there is a chunked audio file before transcription, it becomes possible to perform forced alignment, which determines as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This helps ensure that each word is accurately associated with its corresponding time boundaries in the original audio, even if words are in different segments or have been inserted or deleted.&#10;&#10;This chunked audio file also facilitates better recognition and feature computation by combining segments from the same speaker to create longer stretches of continuous speech. This consolidation process enables a speaker's unique characteristics and patterns to be analyzed more effectively by the recognizer, ultimately leading to improved accuracy in transcriptions and prosodic feature computation.&#10;&#10;In summary, chunking audio files before transcription is crucial for maintaining consistent time markers, accurately associating words with their time boundaries, and improving overall recognition and feature computation during the speech recognition process.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="1st Issue: Non-speaking individuals picking up signals from other people's conversations: When non-speaking individuals have their microphones active and there is cross-talk or nearby conversations, automatic speech recognition (ASR) techniques may pick up the wrong person's speech. This can lead to inaccurate transcriptions as the ASR system attempts to transcribe the incorrect speaker's words for the corresponding dialogue.&#10;&#10;2nd Issue: Occasional speaking amidst others' speech: When an individual speaks occasionally while others are speaking, the challenge lies in distinguishing their voice from the background noise or other speakers. If the person speaks softly or mumbles, it becomes even harder for ASR systems to identify and transcribe their words accurately. Cross-talk and variability in microphone quality can further exacerbate this issue, leading to inconsistent or inaccurate transcriptions.&#10;&#10;These issues highlight the importance of better signal-processing techniques, such as minimizing cross-talk, distinguishing individual speakers, and improving the overall signal-to-noise ratio. This will help ensure more precise and reliable transcriptions for analyzing dialogue or prosody in meetings and other collaborative settings.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there were issues discussed regarding the uniformity of dialogue transcriptions due to difficulties with microphones. The participants mentioned that some individuals in meetings were adjusting their microphones, which caused cross-talk and made it difficult for the automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. Specifically, if a person was not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions. The group also discussed the need for transcription standards and uniformity across different projects." target="1. Positioning of speakers: Based on the energy levels or loudness on each speaker's microphone, one can determine if individuals are located between two people who are talking (a large peak in the display) or speaking on one side or the other of an axis between two individuals (the peak will go the opposite way, with more sound picked up from the closer speaker and less from the farther speaker).&#10;&#10;2. Volume of speakers: Larger energy levels or loudness on a microphone may indicate that a speaker is speaking more loudly or is closer to the microphone. Conversely, lower energy levels suggest that the speaker is softer or further away. However, it's essential to consider that variations in microphone quality and type can also influence the perceived volume.&#10;&#10;3. Number of speakers: The configuration of speakers affects the appearance of peaks on a display. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time. This information could help determine the number of active speakers in a given situation.&#10;&#10;In summary, analyzing energy levels and loudness on microphones can provide insights into speaker positioning, volume, and the number of speakers in a conversation. However, it is crucial to control for variables such as microphone quality and type to ensure accurate interpretations.">
      <data key="d0">1</data>
    </edge>
    <edge source=" research is going on we 're also experimenting with different ASR , uh , techniques .&#10;Speaker: Grad H&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: And so it 'd be w good to know about it .&#10;Speaker: PhD E&#10;Content: So the problem is like , uh , on the microphone of somebody who 's not talking they 're picking up signals from other people {comment} and that 's {vocalsound} causing problems ?&#10;Speaker: PhD A&#10;Content: R right , although if they 're not talking , using the {disfmarker} the inhouse transcriptions , were sort of O K because the t no one transcribed any words there and we throw it out .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: But if they 're talking at all and they 're not talking the whole time , so you get some speech and then a &quot; mm - hmm &quot; , and some more speech , so that whole thing is one chunk . And the person in the middle who said only a little bit is picking up the speech around it , that 's where it" target="PhD C is currently working on a method to include information about which channel certain speech occurs in, by comparing and normalizing energies of different channels. This approach aims to help transcribers identify the specific channel with speech, adding value to the existing transcription process that primarily focuses on speech-nonspeech detection in mixed files. By distinguishing individual speakers' channels more accurately, this technique may improve overall signal-to-noise ratio and minimize cross-talk issues during dialogues, leading to more precise and reliable transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source=" what I had read was , uh , they had a uh very large amount of software infrastructure for coordinating all this , both in terms of recording and also live room where you 're interacting {disfmarker} the participants are interacting with the computer , and with the video , and lots of other stuff .&#10;Speaker: Professor B&#10;Content: Well , I 'm {disfmarker} I 'm {disfmarker} I 'm not sure .&#10;Speaker: Grad H&#10;Content: So .&#10;Speaker: Professor B&#10;Content: All {disfmarker} all I know is that they 've been talking to me about a project that they 're going to start up recording people meet in meetings .&#10;Speaker: Grad H&#10;Content: OK . Well {disfmarker}&#10;Speaker: Professor B&#10;Content: And , uh , it is related to ours . They were interested in ours . They wanted to get some uniformity with us , uh , about the transcriptions and so on .&#10;Speaker: Grad H&#10;Content: Alright .&#10;Speaker: Professor B&#10;Content: And one {disfmarker} one notable difference {disfmarker} u u actually" target="According to Professor B, the purpose of using meeting data is not specifically for studying gestures or for comparing it to far-field studies. Instead, meetings are seen as a natural way to get people talking and providing a source of naturally occurring speech data. This is inferred from her statements where she mentions viewing meetings as a neat way to get people talking naturally and seeing the value of meeting data as a source of data to help their research. The discussion about gestures was initiated by PhD A, but Professor B does not explicitly connect meeting data to gesture studies in her responses.">
      <data key="d0">1</data>
    </edge>
    <edge source=" what I had read was , uh , they had a uh very large amount of software infrastructure for coordinating all this , both in terms of recording and also live room where you 're interacting {disfmarker} the participants are interacting with the computer , and with the video , and lots of other stuff .&#10;Speaker: Professor B&#10;Content: Well , I 'm {disfmarker} I 'm {disfmarker} I 'm not sure .&#10;Speaker: Grad H&#10;Content: So .&#10;Speaker: Professor B&#10;Content: All {disfmarker} all I know is that they 've been talking to me about a project that they 're going to start up recording people meet in meetings .&#10;Speaker: Grad H&#10;Content: OK . Well {disfmarker}&#10;Speaker: Professor B&#10;Content: And , uh , it is related to ours . They were interested in ours . They wanted to get some uniformity with us , uh , about the transcriptions and so on .&#10;Speaker: Grad H&#10;Content: Alright .&#10;Speaker: Professor B&#10;Content: And one {disfmarker} one notable difference {disfmarker} u u actually" target="The issue with having meetings that were exclusively in English, as requested by Jonathan Fiscus, was that it turned out to be a bit of a problem because not many all-English meetings were being held. Postdoc G had to give Fiscus their initial meeting in English because he asked for it, and they don't have a lot of all-English meetings right now. Therefore, providing an exclusively English meeting may have been challenging due to the lack of regular all-English meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source=" what I had read was , uh , they had a uh very large amount of software infrastructure for coordinating all this , both in terms of recording and also live room where you 're interacting {disfmarker} the participants are interacting with the computer , and with the video , and lots of other stuff .&#10;Speaker: Professor B&#10;Content: Well , I 'm {disfmarker} I 'm {disfmarker} I 'm not sure .&#10;Speaker: Grad H&#10;Content: So .&#10;Speaker: Professor B&#10;Content: All {disfmarker} all I know is that they 've been talking to me about a project that they 're going to start up recording people meet in meetings .&#10;Speaker: Grad H&#10;Content: OK . Well {disfmarker}&#10;Speaker: Professor B&#10;Content: And , uh , it is related to ours . They were interested in ours . They wanted to get some uniformity with us , uh , about the transcriptions and so on .&#10;Speaker: Grad H&#10;Content: Alright .&#10;Speaker: Professor B&#10;Content: And one {disfmarker} one notable difference {disfmarker} u u actually" target="1. The project that Dan Ellis worked on involves using cross-correlation and beam-forming techniques to construct a map of where people were sitting. This project is related to recording audio in meetings and improving the signal-to-noise ratio for further processing and analysis. Other individuals who may be involved in this project include PhD A, PhD C, Dave Gelbart, and possibly Professor B. This project seems to be focused on removing high-level constraints and approaching it from a bottom-up perspective, using less data and infrastructure than might typically be used for similar projects.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers seem open to the idea of allowing students or filmmakers to exercise creative control over a potential one-hour film, as they discuss the possibility of having student groups or filmmakers participate in the project. They mention that this could add some color and creativity to the proceedings. (PhD A, Postdoc G, Grad F, Grad H, PhD D, Professor B)&#10;&#10;2. However, there are concerns about the commercial value of such a film. The speakers discuss the possibility that focusing on artistic aspects might detract from the main objectives of the project and reduce its commercial potential. (Professor B, Postdoc G, PhD D)&#10;&#10;3. Another concern is the length of the film. The participants mention that allowing students or filmmakers to create a one-hour film might result in excessive footage, making it difficult to manage and utilize effectively. They discuss the need to strike a balance between artistic freedom and practical constraints. (Postdoc G, PhD D)&#10;&#10;In summary, while the speakers are generally open to the idea of involving students or filmmakers in the project, they have concerns about the commercial value and length of the potential film. They acknowledge the need to balance artistic aspects with practical considerations to ensure the success of the project." target="aker: PhD A&#10;Content: No , but maybe stu student , uh , groups or , um , film - makers , or som Something a little bit colorful .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Yeah . {vocalsound} Exactly .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . {vocalsound} {vocalsound} {vocalsound} {vocalsound} Yeah , th there 's a problem there in terms of , uh , the um commercial value of {disfmarker} of st uh ,&#10;Speaker: Postdoc G&#10;Content: Yeah , of course there is this problem though , that if we give them the chance to excise later we e {vocalsound} might end up with like five minutes out of a f {comment} {pause} of m one hour&#10;Speaker: PhD D&#10;Content: Film - maker .&#10;Speaker: Grad H&#10;Content: Of beeps ,&#10;Speaker: PhD C&#10;Content: Yeah . Yeah .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers seem open to the idea of allowing students or filmmakers to exercise creative control over a potential one-hour film, as they discuss the possibility of having student groups or filmmakers participate in the project. They mention that this could add some color and creativity to the proceedings. (PhD A, Postdoc G, Grad F, Grad H, PhD D, Professor B)&#10;&#10;2. However, there are concerns about the commercial value of such a film. The speakers discuss the possibility that focusing on artistic aspects might detract from the main objectives of the project and reduce its commercial potential. (Professor B, Postdoc G, PhD D)&#10;&#10;3. Another concern is the length of the film. The participants mention that allowing students or filmmakers to create a one-hour film might result in excessive footage, making it difficult to manage and utilize effectively. They discuss the need to strike a balance between artistic freedom and practical constraints. (Postdoc G, PhD D)&#10;&#10;In summary, while the speakers are generally open to the idea of involving students or filmmakers in the project, they have concerns about the commercial value and length of the potential film. They acknowledge the need to balance artistic aspects with practical considerations to ensure the success of the project." target=" and {disfmarker} or emotion , and things like that . And so I was thinking if there 's any like Berkeley political groups or something . I mean , that 'd be perfect . Some group , &quot; yes , we must {disfmarker} &quot;&#10;Speaker: Grad H&#10;Content: Who 's willing to get recorded and distributed ?&#10;Speaker: PhD A&#10;Content: Well , you know , something {disfmarker}&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Yeah , I don't think the more political argumentative ones would be willing to {disfmarker}&#10;Speaker: PhD A&#10;Content: Um {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , with {disfmarker} with {disfmarker} with potential use from the defense department .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Well , OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: No , but maybe stu student , uh , groups or , um , film - makers , or som Something a little bit">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers seem open to the idea of allowing students or filmmakers to exercise creative control over a potential one-hour film, as they discuss the possibility of having student groups or filmmakers participate in the project. They mention that this could add some color and creativity to the proceedings. (PhD A, Postdoc G, Grad F, Grad H, PhD D, Professor B)&#10;&#10;2. However, there are concerns about the commercial value of such a film. The speakers discuss the possibility that focusing on artistic aspects might detract from the main objectives of the project and reduce its commercial potential. (Professor B, Postdoc G, PhD D)&#10;&#10;3. Another concern is the length of the film. The participants mention that allowing students or filmmakers to create a one-hour film might result in excessive footage, making it difficult to manage and utilize effectively. They discuss the need to strike a balance between artistic freedom and practical constraints. (Postdoc G, PhD D)&#10;&#10;In summary, while the speakers are generally open to the idea of involving students or filmmakers in the project, they have concerns about the commercial value and length of the potential film. They acknowledge the need to balance artistic aspects with practical considerations to ensure the success of the project." target=" I I 've heard comments about this before , &quot; why don't you just put on a video camera ? &quot; But you know , it 's sort of like saying , &quot; uh , well we 're primarily interested in {disfmarker} in some dialogue things , uh , but , uh , why don't we just throw a microphone out there . &quot; I mean , the thing is , once you actually have serious interest in any of these things then you actually have to put a lot of effort in .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: And , uh , you really want to do it right .&#10;Speaker: Grad H&#10;Content: I know . Yep .&#10;Speaker: Professor B&#10;Content: So I think NIST or LDC , or somebody like that I think is much better shape to do all that . We {disfmarker} there will be other meeting recordings . We won't be the only place doing meeting recordings . We are doing what we 're doing .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh , hopefully it 'll be useful .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers seem open to the idea of allowing students or filmmakers to exercise creative control over a potential one-hour film, as they discuss the possibility of having student groups or filmmakers participate in the project. They mention that this could add some color and creativity to the proceedings. (PhD A, Postdoc G, Grad F, Grad H, PhD D, Professor B)&#10;&#10;2. However, there are concerns about the commercial value of such a film. The speakers discuss the possibility that focusing on artistic aspects might detract from the main objectives of the project and reduce its commercial potential. (Professor B, Postdoc G, PhD D)&#10;&#10;3. Another concern is the length of the film. The participants mention that allowing students or filmmakers to create a one-hour film might result in excessive footage, making it difficult to manage and utilize effectively. They discuss the need to strike a balance between artistic freedom and practical constraints. (Postdoc G, PhD D)&#10;&#10;In summary, while the speakers are generally open to the idea of involving students or filmmakers in the project, they have concerns about the commercial value and length of the potential film. They acknowledge the need to balance artistic aspects with practical considerations to ensure the success of the project." target=" a completely different set up than we have ,&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: one that would go up to thirty - two channels or something .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So basically {disfmarker}&#10;Speaker: Grad H&#10;Content: Or a hundred thirty - two .&#10;Speaker: Professor B&#10;Content: or a hun Yeah . So , I 'm kinda skeptical , but um I think that {disfmarker}&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So , uh , I don't think we can share the resource in that way . But what we could do is if there was someone else who 's interested they could have a separate set up which they wouldn't be trying to synch with ours which might be useful for {disfmarker} for them .&#10;Speaker: PhD A&#10;Content: Right , I mean at least they 'd have the data and the transcripts ,&#10;Speaker: Professor B&#10;Content: And then we can offer up the room ,&#10;Speaker: PhD A&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers seem open to the idea of allowing students or filmmakers to exercise creative control over a potential one-hour film, as they discuss the possibility of having student groups or filmmakers participate in the project. They mention that this could add some color and creativity to the proceedings. (PhD A, Postdoc G, Grad F, Grad H, PhD D, Professor B)&#10;&#10;2. However, there are concerns about the commercial value of such a film. The speakers discuss the possibility that focusing on artistic aspects might detract from the main objectives of the project and reduce its commercial potential. (Professor B, Postdoc G, PhD D)&#10;&#10;3. Another concern is the length of the film. The participants mention that allowing students or filmmakers to create a one-hour film might result in excessive footage, making it difficult to manage and utilize effectively. They discuss the need to strike a balance between artistic freedom and practical constraints. (Postdoc G, PhD D)&#10;&#10;In summary, while the speakers are generally open to the idea of involving students or filmmakers in the project, they have concerns about the commercial value and length of the potential film. They acknowledge the need to balance artistic aspects with practical considerations to ensure the success of the project." target="During the discussion, Professors B and A mentioned some proposed categories that would allow for overlapping and different timing changes. While specific details about these categories were not provided in the transcript, they seem to be related to a project or research endeavor that involves removing high-level constraints and approaching it from a bottom-up perspective. This project may also involve using less data and infrastructure than typical projects in this field. To further explore this topic, Professor B suggests involving John Ohala, who is likely an expert or key figure with valuable insights on the subject matter. However, the transcript does not offer enough context to provide a more precise description of these categories.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers seem open to the idea of allowing students or filmmakers to exercise creative control over a potential one-hour film, as they discuss the possibility of having student groups or filmmakers participate in the project. They mention that this could add some color and creativity to the proceedings. (PhD A, Postdoc G, Grad F, Grad H, PhD D, Professor B)&#10;&#10;2. However, there are concerns about the commercial value of such a film. The speakers discuss the possibility that focusing on artistic aspects might detract from the main objectives of the project and reduce its commercial potential. (Professor B, Postdoc G, PhD D)&#10;&#10;3. Another concern is the length of the film. The participants mention that allowing students or filmmakers to create a one-hour film might result in excessive footage, making it difficult to manage and utilize effectively. They discuss the need to strike a balance between artistic freedom and practical constraints. (Postdoc G, PhD D)&#10;&#10;In summary, while the speakers are generally open to the idea of involving students or filmmakers in the project, they have concerns about the commercial value and length of the potential film. They acknowledge the need to balance artistic aspects with practical considerations to ensure the success of the project." target="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers seem open to the idea of allowing students or filmmakers to exercise creative control over a potential one-hour film, as they discuss the possibility of having student groups or filmmakers participate in the project. They mention that this could add some color and creativity to the proceedings. (PhD A, Postdoc G, Grad F, Grad H, PhD D, Professor B)&#10;&#10;2. However, there are concerns about the commercial value of such a film. The speakers discuss the possibility that focusing on artistic aspects might detract from the main objectives of the project and reduce its commercial potential. (Professor B, Postdoc G, PhD D)&#10;&#10;3. Another concern is the length of the film. The participants mention that allowing students or filmmakers to create a one-hour film might result in excessive footage, making it difficult to manage and utilize effectively. They discuss the need to strike a balance between artistic freedom and practical constraints. (Postdoc G, PhD D)&#10;&#10;In summary, while the speakers are generally open to the idea of involving students or filmmakers in the project, they have concerns about the commercial value and length of the potential film. They acknowledge the need to balance artistic aspects with practical considerations to ensure the success of the project." target="1. The suggestion made by PhD A and Postdoc G regarding adding more variation to the beeps in a film is for someone to explore different groups or filmmakers who could bring some color and creativity to the proceedings. They are open to discussing ideas and doing the legwork to contact these potential collaborators but need guidance on which groups might be worth pursuing.&#10;&#10;Based on the transcript, it seems that they want to move away from the monotonous &quot;beeps&quot; and incorporate more varied prosodic contours in the film's audio elements. This would involve finding artists or filmmakers who can contribute creatively to this aspect of the project while considering practical constraints such as commercial value and length of the film.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD A&#10;Content: No , but maybe stu student , uh , groups or , um , film - makers , or som Something a little bit colorful .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Yeah . {vocalsound} Exactly .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . {vocalsound} {vocalsound} {vocalsound} {vocalsound} Yeah , th there 's a problem there in terms of , uh , the um commercial value of {disfmarker} of st uh ,&#10;Speaker: Postdoc G&#10;Content: Yeah , of course there is this problem though , that if we give them the chance to excise later we e {vocalsound} might end up with like five minutes out of a f {comment} {pause} of m one hour&#10;Speaker: PhD D&#10;Content: Film - maker .&#10;Speaker: Grad H&#10;Content: Of beeps ,&#10;Speaker: PhD C&#10;Content: Yeah . Yeah .&#10;" target="1. The suggestion made by PhD A and Postdoc G regarding adding more variation to the beeps in a film is for someone to explore different groups or filmmakers who could bring some color and creativity to the proceedings. They are open to discussing ideas and doing the legwork to contact these potential collaborators but need guidance on which groups might be worth pursuing.&#10;&#10;Based on the transcript, it seems that they want to move away from the monotonous &quot;beeps&quot; and incorporate more varied prosodic contours in the film's audio elements. This would involve finding artists or filmmakers who can contribute creatively to this aspect of the project while considering practical constraints such as commercial value and length of the film.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I I 've heard comments about this before , &quot; why don't you just put on a video camera ? &quot; But you know , it 's sort of like saying , &quot; uh , well we 're primarily interested in {disfmarker} in some dialogue things , uh , but , uh , why don't we just throw a microphone out there . &quot; I mean , the thing is , once you actually have serious interest in any of these things then you actually have to put a lot of effort in .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: And , uh , you really want to do it right .&#10;Speaker: Grad H&#10;Content: I know . Yep .&#10;Speaker: Professor B&#10;Content: So I think NIST or LDC , or somebody like that I think is much better shape to do all that . We {disfmarker} there will be other meeting recordings . We won't be the only place doing meeting recordings . We are doing what we 're doing .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh , hopefully it 'll be useful .&#10;Spe" target="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I I 've heard comments about this before , &quot; why don't you just put on a video camera ? &quot; But you know , it 's sort of like saying , &quot; uh , well we 're primarily interested in {disfmarker} in some dialogue things , uh , but , uh , why don't we just throw a microphone out there . &quot; I mean , the thing is , once you actually have serious interest in any of these things then you actually have to put a lot of effort in .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: And , uh , you really want to do it right .&#10;Speaker: Grad H&#10;Content: I know . Yep .&#10;Speaker: Professor B&#10;Content: So I think NIST or LDC , or somebody like that I think is much better shape to do all that . We {disfmarker} there will be other meeting recordings . We won't be the only place doing meeting recordings . We are doing what we 're doing .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh , hopefully it 'll be useful .&#10;Spe" target="1. Cross-talk and inconsistent audio quality: Participants mention that people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. This results in inconsistent or inaccurate transcriptions, which oversimplifies the complexity of the task when there is serious interest in capturing dialogue.&#10;&#10;2. Need for uniformity and transcription standards: Discussants emphasize the importance of having uniformity and transcription standards across different projects to ensure accurate and useful results, particularly in meeting recordings where multiple individuals may be speaking simultaneously.&#10;&#10;3. Human subjects concerns: Video recordings can make participants uncomfortable due to human subjects issues, potentially leading to people becoming shy or uninterested. Ensuring privacy and informed consent is crucial for maintaining engagement.&#10;&#10;4. Clarifying goals and intentions: Discussing the primary purpose of using meeting data, which is gathering naturally occurring speech data rather than focusing on individual behaviors like gestures, can help align participants' expectations and minimize frustration.&#10;&#10;While not explicitly mentioned as reasons, it can be inferred that having a serious interest in capturing dialogue or meeting recordings requires careful planning, resource allocation, and expertise to achieve accurate and useful results. Organizations like NIST or LDC are considered better equipped to handle these tasks due to their resources and expertise in ensuring uniformity, transcription standards, and high-quality recordings across different projects.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I I 've heard comments about this before , &quot; why don't you just put on a video camera ? &quot; But you know , it 's sort of like saying , &quot; uh , well we 're primarily interested in {disfmarker} in some dialogue things , uh , but , uh , why don't we just throw a microphone out there . &quot; I mean , the thing is , once you actually have serious interest in any of these things then you actually have to put a lot of effort in .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: And , uh , you really want to do it right .&#10;Speaker: Grad H&#10;Content: I know . Yep .&#10;Speaker: Professor B&#10;Content: So I think NIST or LDC , or somebody like that I think is much better shape to do all that . We {disfmarker} there will be other meeting recordings . We won't be the only place doing meeting recordings . We are doing what we 're doing .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh , hopefully it 'll be useful .&#10;Spe" target="1. This week, Postdoc G and Dave Gelbart will be visiting with John Canny, who is a CS (Computer Science) professor interested in augmented reality (AR) and array microphones.&#10;2. The purpose of their meeting is to explore potential commonalities between their work and John Canny's research on AR and array microphones.&#10;3. One possible outcome could be integrating array microphones into their current work to enhance audio quality during recordings and analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="From the transcript, it is indicated that each transcriber is given their own &quot;meeting&quot; to transcribe. The size of the data set is not explicitly stated in terms of word count or file size. However, it is mentioned that each meeting lasts for an hour to an hour and a half. Therefore, we can infer that the data sets given to the transcribers are roughly an hour to an hour and a half long worth of audio data per meeting. Additionally, there are eight transcribers in total." target=" during these meetings ,&#10;Speaker: Grad F&#10;Content: I don't care . You can do whatever you want with it .&#10;Speaker: Professor B&#10;Content: and {disfmarker}&#10;Speaker: PhD E&#10;Content: Usually .&#10;Speaker: Grad F&#10;Content: That 's fine .&#10;Speaker: Professor B&#10;Content: Yeah . OK . Uh , transcriptions .&#10;Speaker: Postdoc G&#10;Content: Transcriptions , OK . Um , I thought about {disfmarker} there are maybe three aspects of this . So first of all , um , I 've got eight transcribers . Uh , seven of them are linguists . One of them is a graduate student in psychology . Um , Each {disfmarker} I gave each of them , uh , their own data set . Two of them have already finished the data sets . And {pause} the meetings run , you know , let 's say an hour . Sometimes as man much as an hour and a half .&#10;Speaker: PhD E&#10;Content: How big is the data set ?&#10;Speaker: Postdoc G&#10;Content: Oh , it 's {disfmarker} what I mean">
      <data key="d0">1</data>
    </edge>
    <edge source="From the transcript, it is indicated that each transcriber is given their own &quot;meeting&quot; to transcribe. The size of the data set is not explicitly stated in terms of word count or file size. However, it is mentioned that each meeting lasts for an hour to an hour and a half. Therefore, we can infer that the data sets given to the transcribers are roughly an hour to an hour and a half long worth of audio data per meeting. Additionally, there are eight transcribers in total." target=": How big is the data set ?&#10;Speaker: Postdoc G&#10;Content: Oh , it 's {disfmarker} what I mean is one meeting .&#10;Speaker: PhD E&#10;Content: Ah , OK .&#10;Speaker: Postdoc G&#10;Content: Each {disfmarker} each person got their own meeting . I didn't want to have any conflicts of , you know , of {disfmarker} of when to stop transcribing this one or {disfmarker} So I wanted to keep it clear whose data were whose , and {disfmarker} and {disfmarker} and so {disfmarker}&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Postdoc G&#10;Content: And , uh , meetings , you know , I think that they 're {disfmarker} they go as long as a {disfmarker} almost two hours in some {disfmarker} in some cases . So , you know , that means {disfmarker} you know , if we 've got two already finished and they 're working on {disfmarker} Uh , right now all eight of">
      <data key="d0">1</data>
    </edge>
    <edge source="From the transcript, it is indicated that each transcriber is given their own &quot;meeting&quot; to transcribe. The size of the data set is not explicitly stated in terms of word count or file size. However, it is mentioned that each meeting lasts for an hour to an hour and a half. Therefore, we can infer that the data sets given to the transcribers are roughly an hour to an hour and a half long worth of audio data per meeting. Additionally, there are eight transcribers in total." target="}&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: I think there 's a second pass and I don't really know what would exist in it . But there 's definitely a second pass worth doing to maybe encode some kinds of , you know , is it a question or not ,&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: or {disfmarker} um , that maybe these transcribers could do . So {disfmarker} Yeah .&#10;Speaker: Postdoc G&#10;Content: They 'd be really good . They 're {disfmarker} they 're very {disfmarker} they 're very consistent .&#10;Speaker: PhD A&#10;Content: That 'd be great .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc G&#10;Content: Uh , I wanted to {disfmarker} whi while we 're {disfmarker} Uh , so , to return just briefly to this question of more meeting data , um {disfmarker} I have two questions . One of">
      <data key="d0">1</data>
    </edge>
    <edge source="From the transcript, it is indicated that each transcriber is given their own &quot;meeting&quot; to transcribe. The size of the data set is not explicitly stated in terms of word count or file size. However, it is mentioned that each meeting lasts for an hour to an hour and a half. Therefore, we can infer that the data sets given to the transcribers are roughly an hour to an hour and a half long worth of audio data per meeting. Additionally, there are eight transcribers in total." target=" that .&#10;Speaker: PhD E&#10;Content: Just out of curiosity , I mean {disfmarker}&#10;Speaker: Grad H&#10;Content: They also all have h heavy accents .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: The networks group meetings are all {disfmarker}&#10;Speaker: PhD E&#10;Content: Given all of the effort that is going on here in transcribing why do we have I B M doing it ? Why not just do it all ourselves ?&#10;Speaker: Professor B&#10;Content: Um , it 's historical . I mean , uh , some point ago we thought that uh , it {disfmarker} &quot; boy , we 'd really have to ramp up to do that &quot; ,&#10;Speaker: PhD C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: No , just {disfmarker}&#10;Speaker: Professor B&#10;Content: you know , like we just did , and , um , here 's , uh , a {disfmarker} a , uh">
      <data key="d0">1</data>
    </edge>
    <edge source="From the transcript, it is indicated that each transcriber is given their own &quot;meeting&quot; to transcribe. The size of the data set is not explicitly stated in terms of word count or file size. However, it is mentioned that each meeting lasts for an hour to an hour and a half. Therefore, we can infer that the data sets given to the transcribers are roughly an hour to an hour and a half long worth of audio data per meeting. Additionally, there are eight transcribers in total." target=" H&#10;Content: Oh .&#10;Speaker: Postdoc G&#10;Content: Oh , I see .&#10;Speaker: PhD C&#10;Content: It 's a little bit at odd to {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Oh , darn . Of course , of course , of course . Yeah , OK .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: And actually as you get transcripts just , um , for new meetings , {comment} um , we can try {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: I mean , the {disfmarker} the more data we have to try the {disfmarker} the alignments on , um , the better . So it 'd be good for {disfmarker} just to know as transcriptions are coming through the pipeline from the transcribers , just to sort of {disfmarker} we 're playing around with sort of uh , parameters f on the recognizer ,&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A">
      <data key="d0">1</data>
    </edge>
    <edge source="From the transcript, it is indicated that each transcriber is given their own &quot;meeting&quot; to transcribe. The size of the data set is not explicitly stated in terms of word count or file size. However, it is mentioned that each meeting lasts for an hour to an hour and a half. Therefore, we can infer that the data sets given to the transcribers are roughly an hour to an hour and a half long worth of audio data per meeting. Additionally, there are eight transcribers in total." target="Based on the meeting transcript, the level of detail in the time information provided by Brian Kingsbury's team will not be as fine-grained as the current transcription methods used. The exact difference in the level of detail is not specified in the transcript. However, it is mentioned that the current transcriptions are created by transcribers who are given individual &quot;meetings&quot; to transcribe, with each meeting lasting for an hour to an hour and a half. Therefore, the current transcription methods likely include detailed time information for each speaker in the meeting. In contrast, Brian Kingsbury's team will not provide as much detail in terms of time information, but it is unclear exactly how much less detailed their transcriptions will be.">
      <data key="d0">1</data>
    </edge>
    <edge source="From the transcript, it is indicated that each transcriber is given their own &quot;meeting&quot; to transcribe. The size of the data set is not explicitly stated in terms of word count or file size. However, it is mentioned that each meeting lasts for an hour to an hour and a half. Therefore, we can infer that the data sets given to the transcribers are roughly an hour to an hour and a half long worth of audio data per meeting. Additionally, there are eight transcribers in total." target="The people transcribing the interviews are a group of individuals subcontracted by Brian Kingsbury's team. They are not exactly like court reporters as they do not seem to have special keyboards or specific training for court reporting. Instead, they are more similar to secretarial staff or medical transcriptionists, responsible for manually transcribing audio data into written text. However, they might be using some basic equipment like headphones and a computer for transcription purposes. The level of detail in time information provided by their transcriptions is not as fine-grained as the current transcription methods used.&#10;&#10;It's important to note that organizations such as NIST or LDC are better equipped to handle transcription tasks due to their resources, expertise, and adherence to uniformity and high-quality standards across different projects. The comparison is made between simply placing a microphone to capture audio and the complex process involved in accurate and useful transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source="From the transcript, it is indicated that each transcriber is given their own &quot;meeting&quot; to transcribe. The size of the data set is not explicitly stated in terms of word count or file size. However, it is mentioned that each meeting lasts for an hour to an hour and a half. Therefore, we can infer that the data sets given to the transcribers are roughly an hour to an hour and a half long worth of audio data per meeting. Additionally, there are eight transcribers in total." target="The transcript does not provide sufficient information to directly answer the question about the average time it takes to transcribe five minutes of real-time audio. The text only mentions that two out of eight transcribers have finished their data sets, but it does not give information on how long those data sets were or how much time the transcribers spent on them. To calculate the average transcription time for a specific duration like five minutes, we would need consistent data points from multiple transcribers working on similar audio segments. Since this information is not available in the text, it's impossible to provide an accurate answer.">
      <data key="d0">1</data>
    </edge>
    <edge source=" during these meetings ,&#10;Speaker: Grad F&#10;Content: I don't care . You can do whatever you want with it .&#10;Speaker: Professor B&#10;Content: and {disfmarker}&#10;Speaker: PhD E&#10;Content: Usually .&#10;Speaker: Grad F&#10;Content: That 's fine .&#10;Speaker: Professor B&#10;Content: Yeah . OK . Uh , transcriptions .&#10;Speaker: Postdoc G&#10;Content: Transcriptions , OK . Um , I thought about {disfmarker} there are maybe three aspects of this . So first of all , um , I 've got eight transcribers . Uh , seven of them are linguists . One of them is a graduate student in psychology . Um , Each {disfmarker} I gave each of them , uh , their own data set . Two of them have already finished the data sets . And {pause} the meetings run , you know , let 's say an hour . Sometimes as man much as an hour and a half .&#10;Speaker: PhD E&#10;Content: How big is the data set ?&#10;Speaker: Postdoc G&#10;Content: Oh , it 's {disfmarker} what I mean" target="The transcript does not provide sufficient information to directly answer the question about the average time it takes to transcribe five minutes of real-time audio. The text only mentions that two out of eight transcribers have finished their data sets, but it does not give information on how long those data sets were or how much time the transcribers spent on them. To calculate the average transcription time for a specific duration like five minutes, we would need consistent data points from multiple transcribers working on similar audio segments. Since this information is not available in the text, it's impossible to provide an accurate answer.">
      <data key="d0">1</data>
    </edge>
    <edge source=": How big is the data set ?&#10;Speaker: Postdoc G&#10;Content: Oh , it 's {disfmarker} what I mean is one meeting .&#10;Speaker: PhD E&#10;Content: Ah , OK .&#10;Speaker: Postdoc G&#10;Content: Each {disfmarker} each person got their own meeting . I didn't want to have any conflicts of , you know , of {disfmarker} of when to stop transcribing this one or {disfmarker} So I wanted to keep it clear whose data were whose , and {disfmarker} and {disfmarker} and so {disfmarker}&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Postdoc G&#10;Content: And , uh , meetings , you know , I think that they 're {disfmarker} they go as long as a {disfmarker} almost two hours in some {disfmarker} in some cases . So , you know , that means {disfmarker} you know , if we 've got two already finished and they 're working on {disfmarker} Uh , right now all eight of" target="The transcript does not provide sufficient information to directly answer the question about the average time it takes to transcribe five minutes of real-time audio. The text only mentions that two out of eight transcribers have finished their data sets, but it does not give information on how long those data sets were or how much time the transcribers spent on them. To calculate the average transcription time for a specific duration like five minutes, we would need consistent data points from multiple transcribers working on similar audio segments. Since this information is not available in the text, it's impossible to provide an accurate answer.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that .&#10;Speaker: PhD E&#10;Content: Just out of curiosity , I mean {disfmarker}&#10;Speaker: Grad H&#10;Content: They also all have h heavy accents .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: The networks group meetings are all {disfmarker}&#10;Speaker: PhD E&#10;Content: Given all of the effort that is going on here in transcribing why do we have I B M doing it ? Why not just do it all ourselves ?&#10;Speaker: Professor B&#10;Content: Um , it 's historical . I mean , uh , some point ago we thought that uh , it {disfmarker} &quot; boy , we 'd really have to ramp up to do that &quot; ,&#10;Speaker: PhD C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: No , just {disfmarker}&#10;Speaker: Professor B&#10;Content: you know , like we just did , and , um , here 's , uh , a {disfmarker} a , uh" target="PHP A's concern is that the current method of determining word segments in the transcripts from IBM, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. PHP A is worried that without knowing the exact word segments, it will be difficult to accurately associate each word with its corresponding time boundaries in the original audio, which can negatively impact the accuracy of prosodic feature computation and overall transcription quality.&#10;&#10;To address this issue, PhD E suggests using a forced alignment before chopping up the audio into segments. Forced alignment is a process that creates a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio.&#10;&#10;However, as the discussion continues, the group mentions that IBM might be doing an initial forced alignment and they have to talk to Brian about it. Also, there is a concern about the signal-to-noise ratio being too low which causes problems with getting the right duration pattern or alignment of the words.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that .&#10;Speaker: PhD E&#10;Content: Just out of curiosity , I mean {disfmarker}&#10;Speaker: Grad H&#10;Content: They also all have h heavy accents .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: The networks group meetings are all {disfmarker}&#10;Speaker: PhD E&#10;Content: Given all of the effort that is going on here in transcribing why do we have I B M doing it ? Why not just do it all ourselves ?&#10;Speaker: Professor B&#10;Content: Um , it 's historical . I mean , uh , some point ago we thought that uh , it {disfmarker} &quot; boy , we 'd really have to ramp up to do that &quot; ,&#10;Speaker: PhD C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: No , just {disfmarker}&#10;Speaker: Professor B&#10;Content: you know , like we just did , and , um , here 's , uh , a {disfmarker} a , uh" target="PhD C is currently working on a method to include information about which channel certain speech occurs in, by comparing and normalizing energies of different channels. This approach aims to help transcribers identify the specific channel with speech, adding value to the existing transcription process that primarily focuses on speech-nonspeech detection in mixed files. By distinguishing individual speakers' channels more accurately, this technique may improve overall signal-to-noise ratio and minimize cross-talk issues during dialogues, leading to more precise and reliable transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source=" H&#10;Content: Oh .&#10;Speaker: Postdoc G&#10;Content: Oh , I see .&#10;Speaker: PhD C&#10;Content: It 's a little bit at odd to {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Oh , darn . Of course , of course , of course . Yeah , OK .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: And actually as you get transcripts just , um , for new meetings , {comment} um , we can try {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: I mean , the {disfmarker} the more data we have to try the {disfmarker} the alignments on , um , the better . So it 'd be good for {disfmarker} just to know as transcriptions are coming through the pipeline from the transcribers , just to sort of {disfmarker} we 're playing around with sort of uh , parameters f on the recognizer ,&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A" target="Based on the meeting transcript, the level of detail in the time information provided by Brian Kingsbury's team will not be as fine-grained as the current transcription methods used. The exact difference in the level of detail is not specified in the transcript. However, it is mentioned that the current transcriptions are created by transcribers who are given individual &quot;meetings&quot; to transcribe, with each meeting lasting for an hour to an hour and a half. Therefore, the current transcription methods likely include detailed time information for each speaker in the meeting. In contrast, Brian Kingsbury's team will not provide as much detail in terms of time information, but it is unclear exactly how much less detailed their transcriptions will be.">
      <data key="d0">1</data>
    </edge>
    <edge source=" H&#10;Content: Oh .&#10;Speaker: Postdoc G&#10;Content: Oh , I see .&#10;Speaker: PhD C&#10;Content: It 's a little bit at odd to {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Oh , darn . Of course , of course , of course . Yeah , OK .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: And actually as you get transcripts just , um , for new meetings , {comment} um , we can try {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: I mean , the {disfmarker} the more data we have to try the {disfmarker} the alignments on , um , the better . So it 'd be good for {disfmarker} just to know as transcriptions are coming through the pipeline from the transcribers , just to sort of {disfmarker} we 're playing around with sort of uh , parameters f on the recognizer ,&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A" target="PHP A's concern is that the current method of determining word segments in the transcripts, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. A forced alignment can help address this issue by creating a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio. This would improve the accuracy of prosodic feature computation and overall transcription quality.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the meeting transcript, the level of detail in the time information provided by Brian Kingsbury's team will not be as fine-grained as the current transcription methods used. The exact difference in the level of detail is not specified in the transcript. However, it is mentioned that the current transcriptions are created by transcribers who are given individual &quot;meetings&quot; to transcribe, with each meeting lasting for an hour to an hour and a half. Therefore, the current transcription methods likely include detailed time information for each speaker in the meeting. In contrast, Brian Kingsbury's team will not provide as much detail in terms of time information, but it is unclear exactly how much less detailed their transcriptions will be." target=" Right .&#10;Speaker: Postdoc G&#10;Content: And he was , you know , very interested . And , &quot; oh , and how 'd you handle this ? &quot; And I said , &quot; well , you know , this way &quot; and {disfmarker} And {disfmarker} and we had a really nice conversation . Um , OK , now I also wanted to say in a different {disfmarker} a different direction is , Brian Kingsbury . So , um , I corresponded briefly with him . I , uh , c I {disfmarker} He still has an account here . I told him he could SSH on and use multi - trans , and have a look at the already done , uh , transcription . And he {disfmarker} and he did . And what he said was that , um , what they 'll be providing is {disfmarker} will not be as fine grained in terms of the time information . And , um , that 's , uh {disfmarker} You know , I need to get back to him and {disfmarker} and , uh , you know , explore that a little bit more and see what they 'll be giving">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the meeting transcript, the level of detail in the time information provided by Brian Kingsbury's team will not be as fine-grained as the current transcription methods used. The exact difference in the level of detail is not specified in the transcript. However, it is mentioned that the current transcriptions are created by transcribers who are given individual &quot;meetings&quot; to transcribe, with each meeting lasting for an hour to an hour and a half. Therefore, the current transcription methods likely include detailed time information for each speaker in the meeting. In contrast, Brian Kingsbury's team will not provide as much detail in terms of time information, but it is unclear exactly how much less detailed their transcriptions will be." target="isfmarker}&#10;Speaker: Grad H&#10;Content: Well we can talk about more details later .&#10;Speaker: PhD A&#10;Content: um , you know , yeah , whether to {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah . Yeah . Yeah , so .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: We 'll see . I mean , I think , th you know , they {disfmarker} they {disfmarker} they 've proceeded along a bit . Let 's see what comes out of it , and {disfmarker} and , uh , you know , have some more discussions with them .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm . It 's very {disfmarker} a real benefit having Brian involved because of his knowledge of what the {disfmarker} how the data need to be used and so what 's useful to have in the format .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Grad H&#10;Content: So , um ,">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the meeting transcript, the level of detail in the time information provided by Brian Kingsbury's team will not be as fine-grained as the current transcription methods used. The exact difference in the level of detail is not specified in the transcript. However, it is mentioned that the current transcriptions are created by transcribers who are given individual &quot;meetings&quot; to transcribe, with each meeting lasting for an hour to an hour and a half. Therefore, the current transcription methods likely include detailed time information for each speaker in the meeting. In contrast, Brian Kingsbury's team will not provide as much detail in terms of time information, but it is unclear exactly how much less detailed their transcriptions will be." target=" get back to him and {disfmarker} and , uh , you know , explore that a little bit more and see what they 'll be giving us in specific ,&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD E&#10;Content: The p the people {disfmarker}&#10;Speaker: Postdoc G&#10;Content: but I just haven't had time yet .&#10;Speaker: PhD E&#10;Content: The {disfmarker} the folks that they 're , uh , subcontracting out the transcription to , are they like court reporters&#10;Speaker: Postdoc G&#10;Content: Sorry , what ? Yes .&#10;Speaker: PhD E&#10;Content: or {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Apparently {disfmarker} Well , I get the sense they 're kind of like that . Like it 's like a pool of {disfmarker} of somewhat uh , secretarial {disfmarker} I don't think that they 're court reporters . I don't think they have the special keyboards and that {disfmarker} and that type of training .&#10;Speaker: PhD E&#10;Content: Mm">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the meeting transcript, the level of detail in the time information provided by Brian Kingsbury's team will not be as fine-grained as the current transcription methods used. The exact difference in the level of detail is not specified in the transcript. However, it is mentioned that the current transcriptions are created by transcribers who are given individual &quot;meetings&quot; to transcribe, with each meeting lasting for an hour to an hour and a half. Therefore, the current transcription methods likely include detailed time information for each speaker in the meeting. In contrast, Brian Kingsbury's team will not provide as much detail in terms of time information, but it is unclear exactly how much less detailed their transcriptions will be." target="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the meeting transcript, the level of detail in the time information provided by Brian Kingsbury's team will not be as fine-grained as the current transcription methods used. The exact difference in the level of detail is not specified in the transcript. However, it is mentioned that the current transcriptions are created by transcribers who are given individual &quot;meetings&quot; to transcribe, with each meeting lasting for an hour to an hour and a half. Therefore, the current transcription methods likely include detailed time information for each speaker in the meeting. In contrast, Brian Kingsbury's team will not provide as much detail in terms of time information, but it is unclear exactly how much less detailed their transcriptions will be." target="The people transcribing the interviews are a group of individuals subcontracted by Brian Kingsbury's team. They are not exactly like court reporters as they do not seem to have special keyboards or specific training for court reporting. Instead, they are more similar to secretarial staff or medical transcriptionists, responsible for manually transcribing audio data into written text. However, they might be using some basic equipment like headphones and a computer for transcription purposes. The level of detail in time information provided by their transcriptions is not as fine-grained as the current transcription methods used.&#10;&#10;It's important to note that organizations such as NIST or LDC are better equipped to handle transcription tasks due to their resources, expertise, and adherence to uniformity and high-quality standards across different projects. The comparison is made between simply placing a microphone to capture audio and the complex process involved in accurate and useful transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the meeting transcript, the level of detail in the time information provided by Brian Kingsbury's team will not be as fine-grained as the current transcription methods used. The exact difference in the level of detail is not specified in the transcript. However, it is mentioned that the current transcriptions are created by transcribers who are given individual &quot;meetings&quot; to transcribe, with each meeting lasting for an hour to an hour and a half. Therefore, the current transcription methods likely include detailed time information for each speaker in the meeting. In contrast, Brian Kingsbury's team will not provide as much detail in terms of time information, but it is unclear exactly how much less detailed their transcriptions will be." target="The issue with having meetings that were exclusively in English, as requested by Jonathan Fiscus, was that it turned out to be a bit of a problem because not many all-English meetings were being held. Postdoc G had to give Fiscus their initial meeting in English because he asked for it, and they don't have a lot of all-English meetings right now. Therefore, providing an exclusively English meeting may have been challenging due to the lack of regular all-English meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the meeting transcript, the level of detail in the time information provided by Brian Kingsbury's team will not be as fine-grained as the current transcription methods used. The exact difference in the level of detail is not specified in the transcript. However, it is mentioned that the current transcriptions are created by transcribers who are given individual &quot;meetings&quot; to transcribe, with each meeting lasting for an hour to an hour and a half. Therefore, the current transcription methods likely include detailed time information for each speaker in the meeting. In contrast, Brian Kingsbury's team will not provide as much detail in terms of time information, but it is unclear exactly how much less detailed their transcriptions will be." target="The transcript does not provide sufficient information to directly answer the question about the average time it takes to transcribe five minutes of real-time audio. The text only mentions that two out of eight transcribers have finished their data sets, but it does not give information on how long those data sets were or how much time the transcribers spent on them. To calculate the average transcription time for a specific duration like five minutes, we would need consistent data points from multiple transcribers working on similar audio segments. Since this information is not available in the text, it's impossible to provide an accurate answer.">
      <data key="d0">1</data>
    </edge>
    <edge source=" get back to him and {disfmarker} and , uh , you know , explore that a little bit more and see what they 'll be giving us in specific ,&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD E&#10;Content: The p the people {disfmarker}&#10;Speaker: Postdoc G&#10;Content: but I just haven't had time yet .&#10;Speaker: PhD E&#10;Content: The {disfmarker} the folks that they 're , uh , subcontracting out the transcription to , are they like court reporters&#10;Speaker: Postdoc G&#10;Content: Sorry , what ? Yes .&#10;Speaker: PhD E&#10;Content: or {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Apparently {disfmarker} Well , I get the sense they 're kind of like that . Like it 's like a pool of {disfmarker} of somewhat uh , secretarial {disfmarker} I don't think that they 're court reporters . I don't think they have the special keyboards and that {disfmarker} and that type of training .&#10;Speaker: PhD E&#10;Content: Mm" target="The people transcribing the interviews are a group of individuals subcontracted by Brian Kingsbury's team. They are not exactly like court reporters as they do not seem to have special keyboards or specific training for court reporting. Instead, they are more similar to secretarial staff or medical transcriptionists, responsible for manually transcribing audio data into written text. However, they might be using some basic equipment like headphones and a computer for transcription purposes. The level of detail in time information provided by their transcriptions is not as fine-grained as the current transcription methods used.&#10;&#10;It's important to note that organizations such as NIST or LDC are better equipped to handle transcription tasks due to their resources, expertise, and adherence to uniformity and high-quality standards across different projects. The comparison is made between simply placing a microphone to capture audio and the complex process involved in accurate and useful transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A's concern is that the current method of determining word segments in the transcripts, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. A forced alignment can help address this issue by creating a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio. This would improve the accuracy of prosodic feature computation and overall transcription quality." target=" it 's all pretty good sized for the recognizer also .&#10;Speaker: PhD A&#10;Content: Right , and it {disfmarker} it helps that it 's made based on sort of heuristics and human ear I think .&#10;Speaker: Postdoc G&#10;Content: Good . Oh good .&#10;Speaker: PhD A&#10;Content: Th - but there 's going to be a real problem , uh , even if we chop up based on speech silence these , uh , the transcripts from I B M , we don't actually know where the words were , which segment they belonged to .&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: So that 's sort of what I 'm {pause} worried about right now .&#10;Speaker: PhD E&#10;Content: Why not do a {disfmarker} a {disfmarker} a forced alignment ?&#10;Speaker: Grad H&#10;Content: That 's what she 's saying , is that you can't .&#10;Speaker: PhD A&#10;Content: If you do a forced alignment on something really {disfmarker}&#10;Speaker: Grad H&#10;Content: Got uh six sixty">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A's concern is that the current method of determining word segments in the transcripts, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. A forced alignment can help address this issue by creating a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio. This would improve the accuracy of prosodic feature computation and overall transcription quality." target=" ratio is too low , um , you {disfmarker} you 'll get , a uh {disfmarker} an alignment with the wrong duration pattern or it {disfmarker}&#10;Speaker: PhD E&#10;Content: Oh , so that 's the problem , is the {disfmarker} the signal - to - noise ratio .&#10;Speaker: PhD A&#10;Content: Yeah . It 's not the fact that you have like {disfmarker} I mean , what he did is allow you to have , uh , words that were in another segment move over to the {disfmarker} at the edges of {disfmarker} of segmentations .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Or even words inserted that weren't {disfmarker} weren't there .&#10;Speaker: PhD A&#10;Content: Right , things {disfmarker} things near the boundaries where if you got your alignment wrong {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: cuz what they had done there is align and then chop .&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A's concern is that the current method of determining word segments in the transcripts, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. A forced alignment can help address this issue by creating a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio. This would improve the accuracy of prosodic feature computation and overall transcription quality." target="marker}&#10;Speaker: PhD E&#10;Content: Well you would need to {disfmarker} like a forced alignment before you did the chopping , right ?&#10;Speaker: PhD A&#10;Content: No , we used the fact that {disfmarker} So when Jane transcribes them the way she has transcribers doing this , whether it 's with the pre - segmentation or not ,&#10;Speaker: Grad H&#10;Content: It 's already chunked .&#10;Speaker: PhD A&#10;Content: they have a chunk and then they transcribes {comment} the words in the chunk . And maybe they choose the chunk or now they use a pre - segmentation and then correct it if necessary . But there 's first a chunk and then a transcription .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Then a chunk , then a transcription . That 's great , cuz the recognizer can {disfmarker}&#10;Speaker: Grad H&#10;Content: Uh , it 's all pretty good sized for the recognizer also .&#10;Speaker: PhD A&#10;Content: Right , and it {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A's concern is that the current method of determining word segments in the transcripts, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. A forced alignment can help address this issue by creating a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio. This would improve the accuracy of prosodic feature computation and overall transcription quality." target=" the analysis that you 're doing involves taking segments which are of a particular type and putting them together .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: And th so if you have like a p a s you know , speech from one speaker , {pause} then you cut out the part that 's not that speaker ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Postdoc G&#10;Content: and you combine segments from {pause} that same speaker to {disfmarker} {comment} and run them through the recognizer . Is that {pause} right ?&#10;Speaker: PhD A&#10;Content: Well we try to find as close of start and end time of {disfmarker} as we can to the speech from an individual speaker ,&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: because then we {disfmarker} we 're more guaranteed that the recognizer will {disfmarker} for the forced alignment which is just to give us the time boundaries , because from those time boundaries then the plan is to compute prosodic features .">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A's concern is that the current method of determining word segments in the transcripts, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. A forced alignment can help address this issue by creating a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio. This would improve the accuracy of prosodic feature computation and overall transcription quality." target="1. Segmentation: The initial step involves segmenting the continuous audio stream based on speech-silence transitions. This results in separate speech segments.&#10;&#10;2. Speaker Identification: Within these segments, speakers are identified using automatic speech recognition (ASR) techniques. Adjustments to microphones and cross-talk can create inconsistencies and errors in transcriptions, so it is crucial to accurately associate each word with its corresponding time boundaries in the original audio.&#10;&#10;3. Forced Alignment: Before dividing the audio into smaller segments, a forced alignment process takes place. This involves determining as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This allows for improved accuracy in associating words with their time boundaries.&#10;&#10;4. Word-level Segmentation: After performing the forced alignment, the audio is divided into smaller segments containing single words or phrases. Each segment is associated with its corresponding transcription and time boundaries.&#10;&#10;5. Combining Segments: The speech segments from the same speaker are combined to create longer stretches of continuous speech for improved accuracy in recognition and feature computation. This consolidation process helps ensure that a speaker's unique characteristics and patterns can be better analyzed by the recognizer.&#10;&#10;6. Running through Recognizer: The combined speech segments from each speaker are then passed through a recognizer, which uses automatic speech recognition techniques to identify phonemes, words, or phrases within the audio. This generates a more accurate transcription and time alignment for further analysis.&#10;&#10;7. Computing Prosodic Features: With the accurately aligned transcriptions and time boundaries obtained from the previous steps, prosodic features such as pitch, intensity, and speaking rate can be computed to better understand the speaker's emotions, fluency, or other characteristics of their speech patterns.">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A's concern is that the current method of determining word segments in the transcripts, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. A forced alignment can help address this issue by creating a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio. This would improve the accuracy of prosodic feature computation and overall transcription quality." target="PHP A's concern is that the current method of determining word segments in the transcripts from IBM, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. PHP A is worried that without knowing the exact word segments, it will be difficult to accurately associate each word with its corresponding time boundaries in the original audio, which can negatively impact the accuracy of prosodic feature computation and overall transcription quality.&#10;&#10;To address this issue, PhD E suggests using a forced alignment before chopping up the audio into segments. Forced alignment is a process that creates a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio.&#10;&#10;However, as the discussion continues, the group mentions that IBM might be doing an initial forced alignment and they have to talk to Brian about it. Also, there is a concern about the signal-to-noise ratio being too low which causes problems with getting the right duration pattern or alignment of the words.">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A's concern is that the current method of determining word segments in the transcripts, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. A forced alignment can help address this issue by creating a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio. This would improve the accuracy of prosodic feature computation and overall transcription quality." target="1. Postdoc G is describing a prototype that allows for more accurate time marking of overlapping segments in the transcription process. This is achieved through the use of a multi-channel innovation that helps clarify hearings involving overlaps. The current limitations of the original software design for pre-segmentation make it difficult to accurately time-mark overlapping segments, as the system relies on chopping based on speech-silence transitions, which may not provide enough information about word segment boundaries in cases of overlapping speech.&#10;&#10;2. By implementing this prototype, users will be able to more finely time and label overlapping segments using a click-and-drag interface, allowing for more accurate association between words and their corresponding time boundaries in the original audio. This improvement is expected to lead to better prosodic feature computation and overall transcription quality.">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A's concern is that the current method of determining word segments in the transcripts, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. A forced alignment can help address this issue by creating a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio. This would improve the accuracy of prosodic feature computation and overall transcription quality." target="Having a chunked audio file before transcription is important for the speech recognition process because it allows for better organization of the audio data and improved accuracy in associating words with their corresponding time boundaries. By dividing the continuous audio stream into smaller segments based on speech-silence transitions, automatic speech recognition (ASR) techniques can more easily identify speakers within these segments.&#10;&#10;When there is a chunked audio file before transcription, it becomes possible to perform forced alignment, which determines as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This helps ensure that each word is accurately associated with its corresponding time boundaries in the original audio, even if words are in different segments or have been inserted or deleted.&#10;&#10;This chunked audio file also facilitates better recognition and feature computation by combining segments from the same speaker to create longer stretches of continuous speech. This consolidation process enables a speaker's unique characteristics and patterns to be analyzed more effectively by the recognizer, ultimately leading to improved accuracy in transcriptions and prosodic feature computation.&#10;&#10;In summary, chunking audio files before transcription is crucial for maintaining consistent time markers, accurately associating words with their time boundaries, and improving overall recognition and feature computation during the speech recognition process.">
      <data key="d0">1</data>
    </edge>
    <edge source=" it 's all pretty good sized for the recognizer also .&#10;Speaker: PhD A&#10;Content: Right , and it {disfmarker} it helps that it 's made based on sort of heuristics and human ear I think .&#10;Speaker: Postdoc G&#10;Content: Good . Oh good .&#10;Speaker: PhD A&#10;Content: Th - but there 's going to be a real problem , uh , even if we chop up based on speech silence these , uh , the transcripts from I B M , we don't actually know where the words were , which segment they belonged to .&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: So that 's sort of what I 'm {pause} worried about right now .&#10;Speaker: PhD E&#10;Content: Why not do a {disfmarker} a {disfmarker} a forced alignment ?&#10;Speaker: Grad H&#10;Content: That 's what she 's saying , is that you can't .&#10;Speaker: PhD A&#10;Content: If you do a forced alignment on something really {disfmarker}&#10;Speaker: Grad H&#10;Content: Got uh six sixty" target="PHP A's concern is that the current method of determining word segments in the transcripts from IBM, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. PHP A is worried that without knowing the exact word segments, it will be difficult to accurately associate each word with its corresponding time boundaries in the original audio, which can negatively impact the accuracy of prosodic feature computation and overall transcription quality.&#10;&#10;To address this issue, PhD E suggests using a forced alignment before chopping up the audio into segments. Forced alignment is a process that creates a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio.&#10;&#10;However, as the discussion continues, the group mentions that IBM might be doing an initial forced alignment and they have to talk to Brian about it. Also, there is a concern about the signal-to-noise ratio being too low which causes problems with getting the right duration pattern or alignment of the words.">
      <data key="d0">1</data>
    </edge>
    <edge source=" ratio is too low , um , you {disfmarker} you 'll get , a uh {disfmarker} an alignment with the wrong duration pattern or it {disfmarker}&#10;Speaker: PhD E&#10;Content: Oh , so that 's the problem , is the {disfmarker} the signal - to - noise ratio .&#10;Speaker: PhD A&#10;Content: Yeah . It 's not the fact that you have like {disfmarker} I mean , what he did is allow you to have , uh , words that were in another segment move over to the {disfmarker} at the edges of {disfmarker} of segmentations .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Or even words inserted that weren't {disfmarker} weren't there .&#10;Speaker: PhD A&#10;Content: Right , things {disfmarker} things near the boundaries where if you got your alignment wrong {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: cuz what they had done there is align and then chop .&#10;Speaker: PhD" target="PHP A's concern is that the current method of determining word segments in the transcripts from IBM, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. PHP A is worried that without knowing the exact word segments, it will be difficult to accurately associate each word with its corresponding time boundaries in the original audio, which can negatively impact the accuracy of prosodic feature computation and overall transcription quality.&#10;&#10;To address this issue, PhD E suggests using a forced alignment before chopping up the audio into segments. Forced alignment is a process that creates a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio.&#10;&#10;However, as the discussion continues, the group mentions that IBM might be doing an initial forced alignment and they have to talk to Brian about it. Also, there is a concern about the signal-to-noise ratio being too low which causes problems with getting the right duration pattern or alignment of the words.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker}&#10;Speaker: PhD E&#10;Content: Well you would need to {disfmarker} like a forced alignment before you did the chopping , right ?&#10;Speaker: PhD A&#10;Content: No , we used the fact that {disfmarker} So when Jane transcribes them the way she has transcribers doing this , whether it 's with the pre - segmentation or not ,&#10;Speaker: Grad H&#10;Content: It 's already chunked .&#10;Speaker: PhD A&#10;Content: they have a chunk and then they transcribes {comment} the words in the chunk . And maybe they choose the chunk or now they use a pre - segmentation and then correct it if necessary . But there 's first a chunk and then a transcription .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Then a chunk , then a transcription . That 's great , cuz the recognizer can {disfmarker}&#10;Speaker: Grad H&#10;Content: Uh , it 's all pretty good sized for the recognizer also .&#10;Speaker: PhD A&#10;Content: Right , and it {disfmarker}" target="PHP A's concern is that the current method of determining word segments in the transcripts from IBM, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. PHP A is worried that without knowing the exact word segments, it will be difficult to accurately associate each word with its corresponding time boundaries in the original audio, which can negatively impact the accuracy of prosodic feature computation and overall transcription quality.&#10;&#10;To address this issue, PhD E suggests using a forced alignment before chopping up the audio into segments. Forced alignment is a process that creates a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio.&#10;&#10;However, as the discussion continues, the group mentions that IBM might be doing an initial forced alignment and they have to talk to Brian about it. Also, there is a concern about the signal-to-noise ratio being too low which causes problems with getting the right duration pattern or alignment of the words.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker}&#10;Speaker: PhD E&#10;Content: Well you would need to {disfmarker} like a forced alignment before you did the chopping , right ?&#10;Speaker: PhD A&#10;Content: No , we used the fact that {disfmarker} So when Jane transcribes them the way she has transcribers doing this , whether it 's with the pre - segmentation or not ,&#10;Speaker: Grad H&#10;Content: It 's already chunked .&#10;Speaker: PhD A&#10;Content: they have a chunk and then they transcribes {comment} the words in the chunk . And maybe they choose the chunk or now they use a pre - segmentation and then correct it if necessary . But there 's first a chunk and then a transcription .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Then a chunk , then a transcription . That 's great , cuz the recognizer can {disfmarker}&#10;Speaker: Grad H&#10;Content: Uh , it 's all pretty good sized for the recognizer also .&#10;Speaker: PhD A&#10;Content: Right , and it {disfmarker}" target="Having a chunked audio file before transcription is important for the speech recognition process because it allows for better organization of the audio data and improved accuracy in associating words with their corresponding time boundaries. By dividing the continuous audio stream into smaller segments based on speech-silence transitions, automatic speech recognition (ASR) techniques can more easily identify speakers within these segments.&#10;&#10;When there is a chunked audio file before transcription, it becomes possible to perform forced alignment, which determines as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This helps ensure that each word is accurately associated with its corresponding time boundaries in the original audio, even if words are in different segments or have been inserted or deleted.&#10;&#10;This chunked audio file also facilitates better recognition and feature computation by combining segments from the same speaker to create longer stretches of continuous speech. This consolidation process enables a speaker's unique characteristics and patterns to be analyzed more effectively by the recognizer, ultimately leading to improved accuracy in transcriptions and prosodic feature computation.&#10;&#10;In summary, chunking audio files before transcription is crucial for maintaining consistent time markers, accurately associating words with their time boundaries, and improving overall recognition and feature computation during the speech recognition process.">
      <data key="d0">1</data>
    </edge>
    <edge source=" the analysis that you 're doing involves taking segments which are of a particular type and putting them together .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: And th so if you have like a p a s you know , speech from one speaker , {pause} then you cut out the part that 's not that speaker ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Postdoc G&#10;Content: and you combine segments from {pause} that same speaker to {disfmarker} {comment} and run them through the recognizer . Is that {pause} right ?&#10;Speaker: PhD A&#10;Content: Well we try to find as close of start and end time of {disfmarker} as we can to the speech from an individual speaker ,&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: because then we {disfmarker} we 're more guaranteed that the recognizer will {disfmarker} for the forced alignment which is just to give us the time boundaries , because from those time boundaries then the plan is to compute prosodic features ." target="1. Segmentation: The initial step involves segmenting the continuous audio stream based on speech-silence transitions. This results in separate speech segments.&#10;&#10;2. Speaker Identification: Within these segments, speakers are identified using automatic speech recognition (ASR) techniques. Adjustments to microphones and cross-talk can create inconsistencies and errors in transcriptions, so it is crucial to accurately associate each word with its corresponding time boundaries in the original audio.&#10;&#10;3. Forced Alignment: Before dividing the audio into smaller segments, a forced alignment process takes place. This involves determining as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This allows for improved accuracy in associating words with their time boundaries.&#10;&#10;4. Word-level Segmentation: After performing the forced alignment, the audio is divided into smaller segments containing single words or phrases. Each segment is associated with its corresponding transcription and time boundaries.&#10;&#10;5. Combining Segments: The speech segments from the same speaker are combined to create longer stretches of continuous speech for improved accuracy in recognition and feature computation. This consolidation process helps ensure that a speaker's unique characteristics and patterns can be better analyzed by the recognizer.&#10;&#10;6. Running through Recognizer: The combined speech segments from each speaker are then passed through a recognizer, which uses automatic speech recognition techniques to identify phonemes, words, or phrases within the audio. This generates a more accurate transcription and time alignment for further analysis.&#10;&#10;7. Computing Prosodic Features: With the accurately aligned transcriptions and time boundaries obtained from the previous steps, prosodic features such as pitch, intensity, and speaking rate can be computed to better understand the speaker's emotions, fluency, or other characteristics of their speech patterns.">
      <data key="d0">1</data>
    </edge>
    <edge source=" the analysis that you 're doing involves taking segments which are of a particular type and putting them together .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: And th so if you have like a p a s you know , speech from one speaker , {pause} then you cut out the part that 's not that speaker ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Postdoc G&#10;Content: and you combine segments from {pause} that same speaker to {disfmarker} {comment} and run them through the recognizer . Is that {pause} right ?&#10;Speaker: PhD A&#10;Content: Well we try to find as close of start and end time of {disfmarker} as we can to the speech from an individual speaker ,&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: because then we {disfmarker} we 're more guaranteed that the recognizer will {disfmarker} for the forced alignment which is just to give us the time boundaries , because from those time boundaries then the plan is to compute prosodic features ." target="Based on the transcript provided, Postdoc G was thinking about an aspect of PhD D's work that involves taking segments of a particular type (such as speech from one speaker) and putting them together. This is referred to when Postdoc G says &quot;which is that it sounded to me, Liz, as though you also had another aspect of your work that involved taking segments which are of a particular type and putting them together.&quot; This suggestion is then confirmed by PhD D, who says &quot;Yeah.&quot; The ensuing dialogue makes it clear that this process involves cutting out parts of a recording that are not relevant to the speaker in question, combining relevant segments from the same speaker, and running them through a recognizer.">
      <data key="d0">1</data>
    </edge>
    <edge source="During the discussion, Professors B and A mentioned some proposed categories that would allow for overlapping and different timing changes. While specific details about these categories were not provided in the transcript, they seem to be related to a project or research endeavor that involves removing high-level constraints and approaching it from a bottom-up perspective. This project may also involve using less data and infrastructure than typical projects in this field. To further explore this topic, Professor B suggests involving John Ohala, who is likely an expert or key figure with valuable insights on the subject matter. However, the transcript does not offer enough context to provide a more precise description of these categories." target="Content: Yeah .&#10;Speaker: Professor B&#10;Content: some sort of categories but {disfmarker} but something that allows for overlapping change of these things and then this would give some more ground work for people who were building statistical models that allowed for overlapping changes , different timing changes as opposed to just &quot; click , you 're now in this state , which corresponds to this speech sound &quot; and so on .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD A&#10;Content: So this is like gestural {disfmarker} uh , these g&#10;Speaker: Professor B&#10;Content: Yeah , something like that .&#10;Speaker: PhD A&#10;Content: Right . OK .&#10;Speaker: Professor B&#10;Content: I mean , actually if we get into that it might be good to , uh , uh , haul John Ohala into this&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: and ask his {disfmarker} his views on it I think .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: But is {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="During the discussion, Professors B and A mentioned some proposed categories that would allow for overlapping and different timing changes. While specific details about these categories were not provided in the transcript, they seem to be related to a project or research endeavor that involves removing high-level constraints and approaching it from a bottom-up perspective. This project may also involve using less data and infrastructure than typical projects in this field. To further explore this topic, Professor B suggests involving John Ohala, who is likely an expert or key figure with valuable insights on the subject matter. However, the transcript does not offer enough context to provide a more precise description of these categories." target="&#10;Content: because that {disfmarker} that 'll really help us .&#10;Speaker: Professor B&#10;Content: O K , uh tea has started out there I suggest we c run through our digits and ,&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Uh , So , OK , we 're done .">
      <data key="d0">1</data>
    </edge>
    <edge source="During the discussion, Professors B and A mentioned some proposed categories that would allow for overlapping and different timing changes. While specific details about these categories were not provided in the transcript, they seem to be related to a project or research endeavor that involves removing high-level constraints and approaching it from a bottom-up perspective. This project may also involve using less data and infrastructure than typical projects in this field. To further explore this topic, Professor B suggests involving John Ohala, who is likely an expert or key figure with valuable insights on the subject matter. However, the transcript does not offer enough context to provide a more precise description of these categories." target=" it 's not finished .&#10;Speaker: Professor B&#10;Content: OK . Alright , that seems like a {disfmarker} a good collection of things . And we 'll undoubtedly think of {pause} other things .&#10;Speaker: Postdoc G&#10;Content: I had thought under my topic that I would mention the , uh , four items that I {disfmarker} I , uh , put out for being on the agenda f on that meeting , which includes like the pre - segmentation and the {disfmarker} and the developments in multitrans .&#10;Speaker: Professor B&#10;Content: Oh , under the NIST meeting .&#10;Speaker: Postdoc G&#10;Content: Yeah , under the NIST thing .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Alright , why don't we start off with this , u u I guess the order we brought them up seems fine .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Um , so , better quality close talking mikes . So the one issue was that the {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="During the discussion, Professors B and A mentioned some proposed categories that would allow for overlapping and different timing changes. While specific details about these categories were not provided in the transcript, they seem to be related to a project or research endeavor that involves removing high-level constraints and approaching it from a bottom-up perspective. This project may also involve using less data and infrastructure than typical projects in this field. To further explore this topic, Professor B suggests involving John Ohala, who is likely an expert or key figure with valuable insights on the subject matter. However, the transcript does not offer enough context to provide a more precise description of these categories." target="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results.">
      <data key="d0">1</data>
    </edge>
    <edge source="During the discussion, Professors B and A mentioned some proposed categories that would allow for overlapping and different timing changes. While specific details about these categories were not provided in the transcript, they seem to be related to a project or research endeavor that involves removing high-level constraints and approaching it from a bottom-up perspective. This project may also involve using less data and infrastructure than typical projects in this field. To further explore this topic, Professor B suggests involving John Ohala, who is likely an expert or key figure with valuable insights on the subject matter. However, the transcript does not offer enough context to provide a more precise description of these categories." target="The issues discussed regarding signal-processing techniques for detecting energies involve inconsistencies in dialogue transcriptions due to challenges with microphones and cross-talk. The participants mentioned that variability in microphone quality and people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe dialogues. When individuals are not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions.&#10;&#10;It is important for those working on signal-processing techniques for detecting energies to be aware of these issues because improving the quality and consistency of audio recordings will significantly enhance the accuracy and usefulness of transcriptions. By developing better signal-processing techniques, researchers can help minimize cross-talk, distinguish individual speakers, and improve the overall signal-to-noise ratio, ultimately leading to more precise and reliable transcriptions. This is crucial for analyzing dialogue or prosody in meetings and other collaborative settings.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Content: because that {disfmarker} that 'll really help us .&#10;Speaker: Professor B&#10;Content: O K , uh tea has started out there I suggest we c run through our digits and ,&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Uh , So , OK , we 're done ." target="According to Professor B, the purpose of using meeting data is not specifically for studying gestures or for comparing it to far-field studies. Instead, meetings are seen as a natural way to get people talking and providing a source of naturally occurring speech data. This is inferred from her statements where she mentions viewing meetings as a neat way to get people talking naturally and seeing the value of meeting data as a source of data to help their research. The discussion about gestures was initiated by PhD A, but Professor B does not explicitly connect meeting data to gesture studies in her responses.">
      <data key="d0">1</data>
    </edge>
    <edge source="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results." target=" 's not enough interest to overcome all of {disfmarker}&#10;Speaker: Grad H&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Right . Internally , but I know there is interest from other places that are interested in looking at meeting data and having the video . So it 's just {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Yeah , w although I {disfmarker} m {vocalsound} I {disfmarker} I have to u u mention the human subjects problems , {pause} that i increase with video .&#10;Speaker: PhD A&#10;Content: Right , that 's true .&#10;Speaker: Professor B&#10;Content: Yeah , so it 's , uh , people {disfmarker} people getting shy about it .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: There 's this human subjects problem . There 's the fact that then um , if {disfmarker} i I I 've heard comments about this before , &quot; why don't you just put on a video camera ? &quot; But you know , it 's sort of">
      <data key="d0">1</data>
    </edge>
    <edge source="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results." target="Based on the transcript, there are no specific measures explicitly discussed to deal with people losing interest or becoming frustrated during the meeting. However, some related issues are mentioned that can help address the question:&#10;&#10;1. Microphone difficulties: The participants bring up the problem of inconsistent microphone usage leading to cross-talk and inaccurate transcriptions. Improving audio quality can help maintain engagement and make it easier for participants to follow the conversation.&#10;2. Transcription standards: Discussants emphasize the importance of having uniformity and transcription standards, particularly when multiple people are speaking simultaneously or using specialized terminology. Clear communication and understanding can prevent frustration and misunderstandings.&#10;3. Human subjects concerns: The group acknowledges that video recordings may make participants uncomfortable due to human subjects issues, which could contribute to waning interest or frustration. Addressing these concerns by ensuring privacy and informed consent is crucial for maintaining engagement.&#10;4. Meeting data usage: While not directly related to addressing frustration during the meeting itself, it's mentioned that the primary purpose of using meeting data is to gather naturally occurring speech data, rather than focusing on individual behaviors like gestures or comparing it to far-field studies. Clarifying the goals and intentions of a meeting can help align participants' expectations and minimize frustration.&#10;5. Bottom-up project: Although not explicitly linked to addressing frustration, the group mentions a potential project that involves removing high-level constraints and approaching it from a bottom-up perspective. This approach may result in less reliance on extensive data and infrastructure, potentially reducing complexity and confusion for participants.&#10;&#10;In summary, while there are no specific measures discussed to deal with people losing interest or becoming frustrated during the meeting, some related issues are brought up that can contribute to addressing frustration by improving audio quality, promoting clear communication, ensuring privacy, clarifying goals, and simplifying project constraints.">
      <data key="d0">1</data>
    </edge>
    <edge source="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results." target="The people transcribing the interviews are a group of individuals subcontracted by Brian Kingsbury's team. They are not exactly like court reporters as they do not seem to have special keyboards or specific training for court reporting. Instead, they are more similar to secretarial staff or medical transcriptionists, responsible for manually transcribing audio data into written text. However, they might be using some basic equipment like headphones and a computer for transcription purposes. The level of detail in time information provided by their transcriptions is not as fine-grained as the current transcription methods used.&#10;&#10;It's important to note that organizations such as NIST or LDC are better equipped to handle transcription tasks due to their resources, expertise, and adherence to uniformity and high-quality standards across different projects. The comparison is made between simply placing a microphone to capture audio and the complex process involved in accurate and useful transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results." target="1. Cross-talk and inconsistent audio quality: Participants mention that people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. This results in inconsistent or inaccurate transcriptions, which oversimplifies the complexity of the task when there is serious interest in capturing dialogue.&#10;&#10;2. Need for uniformity and transcription standards: Discussants emphasize the importance of having uniformity and transcription standards across different projects to ensure accurate and useful results, particularly in meeting recordings where multiple individuals may be speaking simultaneously.&#10;&#10;3. Human subjects concerns: Video recordings can make participants uncomfortable due to human subjects issues, potentially leading to people becoming shy or uninterested. Ensuring privacy and informed consent is crucial for maintaining engagement.&#10;&#10;4. Clarifying goals and intentions: Discussing the primary purpose of using meeting data, which is gathering naturally occurring speech data rather than focusing on individual behaviors like gestures, can help align participants' expectations and minimize frustration.&#10;&#10;While not explicitly mentioned as reasons, it can be inferred that having a serious interest in capturing dialogue or meeting recordings requires careful planning, resource allocation, and expertise to achieve accurate and useful results. Organizations like NIST or LDC are considered better equipped to handle these tasks due to their resources and expertise in ensuring uniformity, transcription standards, and high-quality recordings across different projects.">
      <data key="d0">1</data>
    </edge>
    <edge source="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results." target="The transcript does not provide sufficient information to directly answer the question about the average time it takes to transcribe five minutes of real-time audio. The text only mentions that two out of eight transcribers have finished their data sets, but it does not give information on how long those data sets were or how much time the transcribers spent on them. To calculate the average transcription time for a specific duration like five minutes, we would need consistent data points from multiple transcribers working on similar audio segments. Since this information is not available in the text, it's impossible to provide an accurate answer.">
      <data key="d0">1</data>
    </edge>
    <edge source="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results." target="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source="The participants discuss the idea that having serious interest in capturing dialogue or meeting recordings requires a significant effort beyond simply using a video camera or microphone. They compare this to saying &quot;why don't we just throw a microphone out there&quot; when one is primarily interested in dialogue things, implying that it oversimplifies the complexity of the task. Organizations like NIST or LDC are considered better equipped to handle these tasks because they have the necessary resources and expertise to ensure uniformity, transcription standards, and high-quality recordings across different projects. This is particularly important for meeting recordings where multiple individuals may be speaking simultaneously, causing cross-talk and making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. The group emphasizes that once there is serious interest in such tasks, a lot of effort needs to be invested to achieve accurate and useful results." target="1. When people are introduced to the idea of being connected with someone interested in the acoustic side of dialogue recording, they typically ask two main questions:&#10;&#10;- For those interested in general dialogue things, the question is often &quot;are you recording video?&quot; This shows that they consider visual and audio components as a pair, understanding that capturing high-quality video can be crucial for analyzing dialogue effectively.&#10;   (Speaker: Professor B, Content: And just like someone who is interested in the general dialogue thing will always ask 'um, are you recording video?')&#10;&#10;- For those focused on the acoustic side of dialogue recording, a common question is &quot;are you doing array microphones?&quot; This indicates their interest in exploring advanced audio recording techniques and technologies to ensure optimal sound quality for analysis.&#10;   (Speaker: Professor B, Content: And the acoustic people will always say, 'well are you doing, uh, uh, array microphones?')&#10;&#10;These questions reflect individuals' curiosity about different aspects of dialogue recording and their desire to understand the methods and technologies being used in order to ensure high-quality recordings for further analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source=" 's not enough interest to overcome all of {disfmarker}&#10;Speaker: Grad H&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Right . Internally , but I know there is interest from other places that are interested in looking at meeting data and having the video . So it 's just {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Yeah , w although I {disfmarker} m {vocalsound} I {disfmarker} I have to u u mention the human subjects problems , {pause} that i increase with video .&#10;Speaker: PhD A&#10;Content: Right , that 's true .&#10;Speaker: Professor B&#10;Content: Yeah , so it 's , uh , people {disfmarker} people getting shy about it .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: There 's this human subjects problem . There 's the fact that then um , if {disfmarker} i I I 've heard comments about this before , &quot; why don't you just put on a video camera ? &quot; But you know , it 's sort of" target="Based on the transcript, there are no specific measures explicitly discussed to deal with people losing interest or becoming frustrated during the meeting. However, some related issues are mentioned that can help address the question:&#10;&#10;1. Microphone difficulties: The participants bring up the problem of inconsistent microphone usage leading to cross-talk and inaccurate transcriptions. Improving audio quality can help maintain engagement and make it easier for participants to follow the conversation.&#10;2. Transcription standards: Discussants emphasize the importance of having uniformity and transcription standards, particularly when multiple people are speaking simultaneously or using specialized terminology. Clear communication and understanding can prevent frustration and misunderstandings.&#10;3. Human subjects concerns: The group acknowledges that video recordings may make participants uncomfortable due to human subjects issues, which could contribute to waning interest or frustration. Addressing these concerns by ensuring privacy and informed consent is crucial for maintaining engagement.&#10;4. Meeting data usage: While not directly related to addressing frustration during the meeting itself, it's mentioned that the primary purpose of using meeting data is to gather naturally occurring speech data, rather than focusing on individual behaviors like gestures or comparing it to far-field studies. Clarifying the goals and intentions of a meeting can help align participants' expectations and minimize frustration.&#10;5. Bottom-up project: Although not explicitly linked to addressing frustration, the group mentions a potential project that involves removing high-level constraints and approaching it from a bottom-up perspective. This approach may result in less reliance on extensive data and infrastructure, potentially reducing complexity and confusion for participants.&#10;&#10;In summary, while there are no specific measures discussed to deal with people losing interest or becoming frustrated during the meeting, some related issues are brought up that can contribute to addressing frustration by improving audio quality, promoting clear communication, ensuring privacy, clarifying goals, and simplifying project constraints.">
      <data key="d0">1</data>
    </edge>
    <edge source=" 's not enough interest to overcome all of {disfmarker}&#10;Speaker: Grad H&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Right . Internally , but I know there is interest from other places that are interested in looking at meeting data and having the video . So it 's just {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Yeah , w although I {disfmarker} m {vocalsound} I {disfmarker} I have to u u mention the human subjects problems , {pause} that i increase with video .&#10;Speaker: PhD A&#10;Content: Right , that 's true .&#10;Speaker: Professor B&#10;Content: Yeah , so it 's , uh , people {disfmarker} people getting shy about it .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: There 's this human subjects problem . There 's the fact that then um , if {disfmarker} i I I 've heard comments about this before , &quot; why don't you just put on a video camera ? &quot; But you know , it 's sort of" target="1. Cross-talk and inconsistent audio quality: Participants mention that people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. This results in inconsistent or inaccurate transcriptions, which oversimplifies the complexity of the task when there is serious interest in capturing dialogue.&#10;&#10;2. Need for uniformity and transcription standards: Discussants emphasize the importance of having uniformity and transcription standards across different projects to ensure accurate and useful results, particularly in meeting recordings where multiple individuals may be speaking simultaneously.&#10;&#10;3. Human subjects concerns: Video recordings can make participants uncomfortable due to human subjects issues, potentially leading to people becoming shy or uninterested. Ensuring privacy and informed consent is crucial for maintaining engagement.&#10;&#10;4. Clarifying goals and intentions: Discussing the primary purpose of using meeting data, which is gathering naturally occurring speech data rather than focusing on individual behaviors like gestures, can help align participants' expectations and minimize frustration.&#10;&#10;While not explicitly mentioned as reasons, it can be inferred that having a serious interest in capturing dialogue or meeting recordings requires careful planning, resource allocation, and expertise to achieve accurate and useful results. Organizations like NIST or LDC are considered better equipped to handle these tasks due to their resources and expertise in ensuring uniformity, transcription standards, and high-quality recordings across different projects.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to Professor B, the purpose of using meeting data is not specifically for studying gestures or for comparing it to far-field studies. Instead, meetings are seen as a natural way to get people talking and providing a source of naturally occurring speech data. This is inferred from her statements where she mentions viewing meetings as a neat way to get people talking naturally and seeing the value of meeting data as a source of data to help their research. The discussion about gestures was initiated by PhD A, but Professor B does not explicitly connect meeting data to gesture studies in her responses." target=" on it I think .&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: But is {disfmarker} is the goal there to have this on meeting data ,&#10;Speaker: Postdoc G&#10;Content: Excellent .&#10;Speaker: PhD A&#10;Content: like so that you can do far field studies {comment} of those gestures or {disfmarker} um , or is it because you think there 's a different kind of actual production in meetings {comment} that people use ? Or {disfmarker} ?&#10;Speaker: Professor B&#10;Content: No , I think {disfmarker} I think it 's {disfmarker} for {disfmarker} for {disfmarker} for that purpose I 'm just viewing meetings as being a {disfmarker} a neat way to get people talking naturally . And then you have i and then {disfmarker} and then it 's natural in all senses ,&#10;Speaker: PhD E&#10;Content: Just a source of data ?&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: in the sense that you">
      <data key="d0">1</data>
    </edge>
    <edge source="According to Professor B, the purpose of using meeting data is not specifically for studying gestures or for comparing it to far-field studies. Instead, meetings are seen as a natural way to get people talking and providing a source of naturally occurring speech data. This is inferred from her statements where she mentions viewing meetings as a neat way to get people talking naturally and seeing the value of meeting data as a source of data to help their research. The discussion about gestures was initiated by PhD A, but Professor B does not explicitly connect meeting data to gesture studies in her responses." target=" with that is right now , um , some of the Jimlets aren't working . The little {disfmarker} the boxes under the table .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: And so , w Uh , I 've only been able to find three jacks that are working .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Can we get these , wireless ?&#10;Speaker: Grad H&#10;Content: So {disfmarker}&#10;Speaker: Professor B&#10;Content: No , but my point is {disfmarker}&#10;Speaker: PhD A&#10;Content: But y we could just record these signals separately and time align them with the start of the meeting .&#10;Speaker: Professor B&#10;Content: R r right {disfmarker}&#10;Speaker: Grad H&#10;Content: I {disfmarker} I 'm not sure I 'm follow . Say that again ?&#10;Speaker: Professor B&#10;Content: Right now , we 've got , uh , two microphones in the room , that are not quote - unquote standard . So why don't we replace those {d">
      <data key="d0">1</data>
    </edge>
    <edge source="According to Professor B, the purpose of using meeting data is not specifically for studying gestures or for comparing it to far-field studies. Instead, meetings are seen as a natural way to get people talking and providing a source of naturally occurring speech data. This is inferred from her statements where she mentions viewing meetings as a neat way to get people talking naturally and seeing the value of meeting data as a source of data to help their research. The discussion about gestures was initiated by PhD A, but Professor B does not explicitly connect meeting data to gesture studies in her responses." target="er} Uh , so , to return just briefly to this question of more meeting data , um {disfmarker} I have two questions . One of them is , um , Jerry Feldman 's group , they {disfmarker} they , uh , are they {disfmarker} I know that they recorded one meeting . Are they willing ?&#10;Speaker: Professor B&#10;Content: I think they 're open to it . I think , you know , all these things are {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , yeah .&#10;Speaker: Professor B&#10;Content: I think there 's {disfmarker} we should go beyond , uh , ICSI but , I mean , there 's a lot of stuff happening at ICSI that we 're not getting now that we could .&#10;Speaker: PhD A&#10;Content: Oh , that we could .&#10;Speaker: Professor B&#10;Content: So it 's just {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: OK . I thought that all these people had sort of said &quot; no &quot; twice already">
      <data key="d0">1</data>
    </edge>
    <edge source="According to Professor B, the purpose of using meeting data is not specifically for studying gestures or for comparing it to far-field studies. Instead, meetings are seen as a natural way to get people talking and providing a source of naturally occurring speech data. This is inferred from her statements where she mentions viewing meetings as a neat way to get people talking naturally and seeing the value of meeting data as a source of data to help their research. The discussion about gestures was initiated by PhD A, but Professor B does not explicitly connect meeting data to gesture studies in her responses." target="Based on the transcript, there are no specific measures explicitly discussed to deal with people losing interest or becoming frustrated during the meeting. However, some related issues are mentioned that can help address the question:&#10;&#10;1. Microphone difficulties: The participants bring up the problem of inconsistent microphone usage leading to cross-talk and inaccurate transcriptions. Improving audio quality can help maintain engagement and make it easier for participants to follow the conversation.&#10;2. Transcription standards: Discussants emphasize the importance of having uniformity and transcription standards, particularly when multiple people are speaking simultaneously or using specialized terminology. Clear communication and understanding can prevent frustration and misunderstandings.&#10;3. Human subjects concerns: The group acknowledges that video recordings may make participants uncomfortable due to human subjects issues, which could contribute to waning interest or frustration. Addressing these concerns by ensuring privacy and informed consent is crucial for maintaining engagement.&#10;4. Meeting data usage: While not directly related to addressing frustration during the meeting itself, it's mentioned that the primary purpose of using meeting data is to gather naturally occurring speech data, rather than focusing on individual behaviors like gestures or comparing it to far-field studies. Clarifying the goals and intentions of a meeting can help align participants' expectations and minimize frustration.&#10;5. Bottom-up project: Although not explicitly linked to addressing frustration, the group mentions a potential project that involves removing high-level constraints and approaching it from a bottom-up perspective. This approach may result in less reliance on extensive data and infrastructure, potentially reducing complexity and confusion for participants.&#10;&#10;In summary, while there are no specific measures discussed to deal with people losing interest or becoming frustrated during the meeting, some related issues are brought up that can contribute to addressing frustration by improving audio quality, promoting clear communication, ensuring privacy, clarifying goals, and simplifying project constraints.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to Professor B, the purpose of using meeting data is not specifically for studying gestures or for comparing it to far-field studies. Instead, meetings are seen as a natural way to get people talking and providing a source of naturally occurring speech data. This is inferred from her statements where she mentions viewing meetings as a neat way to get people talking naturally and seeing the value of meeting data as a source of data to help their research. The discussion about gestures was initiated by PhD A, but Professor B does not explicitly connect meeting data to gesture studies in her responses." target="1. Cross-talk and inconsistent audio quality: Participants mention that people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. This results in inconsistent or inaccurate transcriptions, which oversimplifies the complexity of the task when there is serious interest in capturing dialogue.&#10;&#10;2. Need for uniformity and transcription standards: Discussants emphasize the importance of having uniformity and transcription standards across different projects to ensure accurate and useful results, particularly in meeting recordings where multiple individuals may be speaking simultaneously.&#10;&#10;3. Human subjects concerns: Video recordings can make participants uncomfortable due to human subjects issues, potentially leading to people becoming shy or uninterested. Ensuring privacy and informed consent is crucial for maintaining engagement.&#10;&#10;4. Clarifying goals and intentions: Discussing the primary purpose of using meeting data, which is gathering naturally occurring speech data rather than focusing on individual behaviors like gestures, can help align participants' expectations and minimize frustration.&#10;&#10;While not explicitly mentioned as reasons, it can be inferred that having a serious interest in capturing dialogue or meeting recordings requires careful planning, resource allocation, and expertise to achieve accurate and useful results. Organizations like NIST or LDC are considered better equipped to handle these tasks due to their resources and expertise in ensuring uniformity, transcription standards, and high-quality recordings across different projects.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to Professor B, the purpose of using meeting data is not specifically for studying gestures or for comparing it to far-field studies. Instead, meetings are seen as a natural way to get people talking and providing a source of naturally occurring speech data. This is inferred from her statements where she mentions viewing meetings as a neat way to get people talking naturally and seeing the value of meeting data as a source of data to help their research. The discussion about gestures was initiated by PhD A, but Professor B does not explicitly connect meeting data to gesture studies in her responses." target="The issue with having meetings that were exclusively in English, as requested by Jonathan Fiscus, was that it turned out to be a bit of a problem because not many all-English meetings were being held. Postdoc G had to give Fiscus their initial meeting in English because he asked for it, and they don't have a lot of all-English meetings right now. Therefore, providing an exclusively English meeting may have been challenging due to the lack of regular all-English meetings.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to Professor B, the purpose of using meeting data is not specifically for studying gestures or for comparing it to far-field studies. Instead, meetings are seen as a natural way to get people talking and providing a source of naturally occurring speech data. This is inferred from her statements where she mentions viewing meetings as a neat way to get people talking naturally and seeing the value of meeting data as a source of data to help their research. The discussion about gestures was initiated by PhD A, but Professor B does not explicitly connect meeting data to gesture studies in her responses." target="1. Positioning between speakers: If individuals are located between two people who are talking, a large peak will appear in the display. This is due to the microphone picking up sound from both sources almost equally.&#10;&#10;2. Speaker location: When someone is speaking on one side or the other of an axis between two individuals, the peak will go the opposite way. The microphone will pick up more sound from the closer speaker and less from the farther speaker, causing a difference in the display's appearance.&#10;&#10;3. Number of speakers: The configuration of speakers also influences the appearance of peaks. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to Professor B, the purpose of using meeting data is not specifically for studying gestures or for comparing it to far-field studies. Instead, meetings are seen as a natural way to get people talking and providing a source of naturally occurring speech data. This is inferred from her statements where she mentions viewing meetings as a neat way to get people talking naturally and seeing the value of meeting data as a source of data to help their research. The discussion about gestures was initiated by PhD A, but Professor B does not explicitly connect meeting data to gesture studies in her responses." target="In the transcript, PhD A is trying to propose the idea of collecting more meetings as a source of naturally occurring speech data for their research. The conversation with Chuck Fillmore, a member who had previously vehemently said no to being recorded, seemed to have a positive influence on PhD A's proposal. Initially, PhD A thought that many people had said &quot;no&quot; to being recorded, but after speaking with Chuck Fillmore, he found that Fillmore was more open to the idea this time. Fillmore even invited Liz (presumably PhD A) to a meeting, which PhD A takes as a sign of potential willingness among other groups to participate in the data collection. This interaction seems to have encouraged PhD A to make a pitch for collecting more meetings as a research resource.">
      <data key="d0">1</data>
    </edge>
    <edge source="er} Uh , so , to return just briefly to this question of more meeting data , um {disfmarker} I have two questions . One of them is , um , Jerry Feldman 's group , they {disfmarker} they , uh , are they {disfmarker} I know that they recorded one meeting . Are they willing ?&#10;Speaker: Professor B&#10;Content: I think they 're open to it . I think , you know , all these things are {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , yeah .&#10;Speaker: Professor B&#10;Content: I think there 's {disfmarker} we should go beyond , uh , ICSI but , I mean , there 's a lot of stuff happening at ICSI that we 're not getting now that we could .&#10;Speaker: PhD A&#10;Content: Oh , that we could .&#10;Speaker: Professor B&#10;Content: So it 's just {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: OK . I thought that all these people had sort of said &quot; no &quot; twice already" target="In the transcript, PhD A is trying to propose the idea of collecting more meetings as a source of naturally occurring speech data for their research. The conversation with Chuck Fillmore, a member who had previously vehemently said no to being recorded, seemed to have a positive influence on PhD A's proposal. Initially, PhD A thought that many people had said &quot;no&quot; to being recorded, but after speaking with Chuck Fillmore, he found that Fillmore was more open to the idea this time. Fillmore even invited Liz (presumably PhD A) to a meeting, which PhD A takes as a sign of potential willingness among other groups to participate in the data collection. This interaction seems to have encouraged PhD A to make a pitch for collecting more meetings as a research resource.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Segmentation: The initial step involves segmenting the continuous audio stream based on speech-silence transitions. This results in separate speech segments.&#10;&#10;2. Speaker Identification: Within these segments, speakers are identified using automatic speech recognition (ASR) techniques. Adjustments to microphones and cross-talk can create inconsistencies and errors in transcriptions, so it is crucial to accurately associate each word with its corresponding time boundaries in the original audio.&#10;&#10;3. Forced Alignment: Before dividing the audio into smaller segments, a forced alignment process takes place. This involves determining as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This allows for improved accuracy in associating words with their time boundaries.&#10;&#10;4. Word-level Segmentation: After performing the forced alignment, the audio is divided into smaller segments containing single words or phrases. Each segment is associated with its corresponding transcription and time boundaries.&#10;&#10;5. Combining Segments: The speech segments from the same speaker are combined to create longer stretches of continuous speech for improved accuracy in recognition and feature computation. This consolidation process helps ensure that a speaker's unique characteristics and patterns can be better analyzed by the recognizer.&#10;&#10;6. Running through Recognizer: The combined speech segments from each speaker are then passed through a recognizer, which uses automatic speech recognition techniques to identify phonemes, words, or phrases within the audio. This generates a more accurate transcription and time alignment for further analysis.&#10;&#10;7. Computing Prosodic Features: With the accurately aligned transcriptions and time boundaries obtained from the previous steps, prosodic features such as pitch, intensity, and speaking rate can be computed to better understand the speaker's emotions, fluency, or other characteristics of their speech patterns." target=": You know , just sort of say , OK , it 's voiced from here to here , there 's {disfmarker} it 's nasal from here to here , and so forth . Um , as opposed to doing phonetic {disfmarker} uh , you know , phonemic and the phonetic analysis ,&#10;Speaker: Grad H&#10;Content: &#10;Speaker: Professor B&#10;Content: and , uh , assuming , uh , articulatory feature values for those {disfmarker} those things . Um , obviously that 's extremely time - consuming . Uh {disfmarker}&#10;Speaker: PhD E&#10;Content: That would be really valuable I think .&#10;Speaker: Professor B&#10;Content: but , uh , we could do it on some small subset .&#10;Speaker: Postdoc G&#10;Content: Also if you 're dealing with consonants that would be easier than vowels , wouldn't it ? I mean , I would think that {disfmarker} that , uh , being able to code that there 's a {disfmarker} a fricative extending from here to here would be a lot easier than classifying precisely which vowel that was .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Segmentation: The initial step involves segmenting the continuous audio stream based on speech-silence transitions. This results in separate speech segments.&#10;&#10;2. Speaker Identification: Within these segments, speakers are identified using automatic speech recognition (ASR) techniques. Adjustments to microphones and cross-talk can create inconsistencies and errors in transcriptions, so it is crucial to accurately associate each word with its corresponding time boundaries in the original audio.&#10;&#10;3. Forced Alignment: Before dividing the audio into smaller segments, a forced alignment process takes place. This involves determining as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This allows for improved accuracy in associating words with their time boundaries.&#10;&#10;4. Word-level Segmentation: After performing the forced alignment, the audio is divided into smaller segments containing single words or phrases. Each segment is associated with its corresponding transcription and time boundaries.&#10;&#10;5. Combining Segments: The speech segments from the same speaker are combined to create longer stretches of continuous speech for improved accuracy in recognition and feature computation. This consolidation process helps ensure that a speaker's unique characteristics and patterns can be better analyzed by the recognizer.&#10;&#10;6. Running through Recognizer: The combined speech segments from each speaker are then passed through a recognizer, which uses automatic speech recognition techniques to identify phonemes, words, or phrases within the audio. This generates a more accurate transcription and time alignment for further analysis.&#10;&#10;7. Computing Prosodic Features: With the accurately aligned transcriptions and time boundaries obtained from the previous steps, prosodic features such as pitch, intensity, and speaking rate can be computed to better understand the speaker's emotions, fluency, or other characteristics of their speech patterns." target="doc G&#10;Content: Or some other kind of thing ?&#10;Speaker: PhD D&#10;Content: No ? To {disfmarker} to mark {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Well , I wouldn't say {vocalsound} symbols so much . The {disfmarker} the main change that I {disfmarker} that I see in the interface is {disfmarker} is just that we 'll be able to more finely c uh , time things .&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Postdoc G&#10;Content: But I {disfmarker} I also st there was another aspect of your work that I was thinking about when I was talking to you&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Postdoc G&#10;Content: which is that it sounded to me , Liz , as though you {disfmarker} and , uh , maybe I didn't q understand this , but it sounded to me as though part of the analysis that you 're doing involves taking segments which are of a particular type and putting them together .&#10;Speaker: PhD D&#10;Content: Yeah .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Segmentation: The initial step involves segmenting the continuous audio stream based on speech-silence transitions. This results in separate speech segments.&#10;&#10;2. Speaker Identification: Within these segments, speakers are identified using automatic speech recognition (ASR) techniques. Adjustments to microphones and cross-talk can create inconsistencies and errors in transcriptions, so it is crucial to accurately associate each word with its corresponding time boundaries in the original audio.&#10;&#10;3. Forced Alignment: Before dividing the audio into smaller segments, a forced alignment process takes place. This involves determining as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This allows for improved accuracy in associating words with their time boundaries.&#10;&#10;4. Word-level Segmentation: After performing the forced alignment, the audio is divided into smaller segments containing single words or phrases. Each segment is associated with its corresponding transcription and time boundaries.&#10;&#10;5. Combining Segments: The speech segments from the same speaker are combined to create longer stretches of continuous speech for improved accuracy in recognition and feature computation. This consolidation process helps ensure that a speaker's unique characteristics and patterns can be better analyzed by the recognizer.&#10;&#10;6. Running through Recognizer: The combined speech segments from each speaker are then passed through a recognizer, which uses automatic speech recognition techniques to identify phonemes, words, or phrases within the audio. This generates a more accurate transcription and time alignment for further analysis.&#10;&#10;7. Computing Prosodic Features: With the accurately aligned transcriptions and time boundaries obtained from the previous steps, prosodic features such as pitch, intensity, and speaking rate can be computed to better understand the speaker's emotions, fluency, or other characteristics of their speech patterns." target=" some very high level labelings ,&#10;Speaker: Postdoc G&#10;Content: Yeah . The types of overlaps {disfmarker}&#10;Speaker: Professor B&#10;Content: types of overlaps , and so forth that {disfmarker} that someone could do . Second was , uh , somewhat lower level&#10;Speaker: Postdoc G&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: just doing these more precise timings . And the third one is {disfmarker} is , uh , just a completely wild hair brained idea that I have which is that , um , if , uh {disfmarker} if we have time and people are able to do it , to take some subset of the data and do some very fine grained analysis of the speech . For instance , uh , marking in some overlapping {disfmarker} potentially overlapping fashion , uh , the value of , uh , ar articulatory features .&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: You know , just sort of say , OK , it 's voiced from here to here , there 's {disfmarker} it 's">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Segmentation: The initial step involves segmenting the continuous audio stream based on speech-silence transitions. This results in separate speech segments.&#10;&#10;2. Speaker Identification: Within these segments, speakers are identified using automatic speech recognition (ASR) techniques. Adjustments to microphones and cross-talk can create inconsistencies and errors in transcriptions, so it is crucial to accurately associate each word with its corresponding time boundaries in the original audio.&#10;&#10;3. Forced Alignment: Before dividing the audio into smaller segments, a forced alignment process takes place. This involves determining as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This allows for improved accuracy in associating words with their time boundaries.&#10;&#10;4. Word-level Segmentation: After performing the forced alignment, the audio is divided into smaller segments containing single words or phrases. Each segment is associated with its corresponding transcription and time boundaries.&#10;&#10;5. Combining Segments: The speech segments from the same speaker are combined to create longer stretches of continuous speech for improved accuracy in recognition and feature computation. This consolidation process helps ensure that a speaker's unique characteristics and patterns can be better analyzed by the recognizer.&#10;&#10;6. Running through Recognizer: The combined speech segments from each speaker are then passed through a recognizer, which uses automatic speech recognition techniques to identify phonemes, words, or phrases within the audio. This generates a more accurate transcription and time alignment for further analysis.&#10;&#10;7. Computing Prosodic Features: With the accurately aligned transcriptions and time boundaries obtained from the previous steps, prosodic features such as pitch, intensity, and speaking rate can be computed to better understand the speaker's emotions, fluency, or other characteristics of their speech patterns." target="Having a chunked audio file before transcription is important for the speech recognition process because it allows for better organization of the audio data and improved accuracy in associating words with their corresponding time boundaries. By dividing the continuous audio stream into smaller segments based on speech-silence transitions, automatic speech recognition (ASR) techniques can more easily identify speakers within these segments.&#10;&#10;When there is a chunked audio file before transcription, it becomes possible to perform forced alignment, which determines as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This helps ensure that each word is accurately associated with its corresponding time boundaries in the original audio, even if words are in different segments or have been inserted or deleted.&#10;&#10;This chunked audio file also facilitates better recognition and feature computation by combining segments from the same speaker to create longer stretches of continuous speech. This consolidation process enables a speaker's unique characteristics and patterns to be analyzed more effectively by the recognizer, ultimately leading to improved accuracy in transcriptions and prosodic feature computation.&#10;&#10;In summary, chunking audio files before transcription is crucial for maintaining consistent time markers, accurately associating words with their time boundaries, and improving overall recognition and feature computation during the speech recognition process.">
      <data key="d0">1</data>
    </edge>
    <edge source="doc G&#10;Content: Or some other kind of thing ?&#10;Speaker: PhD D&#10;Content: No ? To {disfmarker} to mark {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Well , I wouldn't say {vocalsound} symbols so much . The {disfmarker} the main change that I {disfmarker} that I see in the interface is {disfmarker} is just that we 'll be able to more finely c uh , time things .&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Postdoc G&#10;Content: But I {disfmarker} I also st there was another aspect of your work that I was thinking about when I was talking to you&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Postdoc G&#10;Content: which is that it sounded to me , Liz , as though you {disfmarker} and , uh , maybe I didn't q understand this , but it sounded to me as though part of the analysis that you 're doing involves taking segments which are of a particular type and putting them together .&#10;Speaker: PhD D&#10;Content: Yeah ." target="1. Postdoc G is describing a prototype that allows for more accurate time marking of overlapping segments in the transcription process. This is achieved through the use of a multi-channel innovation that helps clarify hearings involving overlaps. The current limitations of the original software design for pre-segmentation make it difficult to accurately time-mark overlapping segments, as the system relies on chopping based on speech-silence transitions, which may not provide enough information about word segment boundaries in cases of overlapping speech.&#10;&#10;2. By implementing this prototype, users will be able to more finely time and label overlapping segments using a click-and-drag interface, allowing for more accurate association between words and their corresponding time boundaries in the original audio. This improvement is expected to lead to better prosodic feature computation and overall transcription quality.">
      <data key="d0">1</data>
    </edge>
    <edge source="doc G&#10;Content: Or some other kind of thing ?&#10;Speaker: PhD D&#10;Content: No ? To {disfmarker} to mark {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Well , I wouldn't say {vocalsound} symbols so much . The {disfmarker} the main change that I {disfmarker} that I see in the interface is {disfmarker} is just that we 'll be able to more finely c uh , time things .&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Postdoc G&#10;Content: But I {disfmarker} I also st there was another aspect of your work that I was thinking about when I was talking to you&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Postdoc G&#10;Content: which is that it sounded to me , Liz , as though you {disfmarker} and , uh , maybe I didn't q understand this , but it sounded to me as though part of the analysis that you 're doing involves taking segments which are of a particular type and putting them together .&#10;Speaker: PhD D&#10;Content: Yeah ." target="Based on the transcript provided, Postdoc G was thinking about an aspect of PhD D's work that involves taking segments of a particular type (such as speech from one speaker) and putting them together. This is referred to when Postdoc G says &quot;which is that it sounded to me, Liz, as though you also had another aspect of your work that involved taking segments which are of a particular type and putting them together.&quot; This suggestion is then confirmed by PhD D, who says &quot;Yeah.&quot; The ensuing dialogue makes it clear that this process involves cutting out parts of a recording that are not relevant to the speaker in question, combining relevant segments from the same speaker, and running them through a recognizer.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there are no specific measures explicitly discussed to deal with people losing interest or becoming frustrated during the meeting. However, some related issues are mentioned that can help address the question:&#10;&#10;1. Microphone difficulties: The participants bring up the problem of inconsistent microphone usage leading to cross-talk and inaccurate transcriptions. Improving audio quality can help maintain engagement and make it easier for participants to follow the conversation.&#10;2. Transcription standards: Discussants emphasize the importance of having uniformity and transcription standards, particularly when multiple people are speaking simultaneously or using specialized terminology. Clear communication and understanding can prevent frustration and misunderstandings.&#10;3. Human subjects concerns: The group acknowledges that video recordings may make participants uncomfortable due to human subjects issues, which could contribute to waning interest or frustration. Addressing these concerns by ensuring privacy and informed consent is crucial for maintaining engagement.&#10;4. Meeting data usage: While not directly related to addressing frustration during the meeting itself, it's mentioned that the primary purpose of using meeting data is to gather naturally occurring speech data, rather than focusing on individual behaviors like gestures or comparing it to far-field studies. Clarifying the goals and intentions of a meeting can help align participants' expectations and minimize frustration.&#10;5. Bottom-up project: Although not explicitly linked to addressing frustration, the group mentions a potential project that involves removing high-level constraints and approaching it from a bottom-up perspective. This approach may result in less reliance on extensive data and infrastructure, potentially reducing complexity and confusion for participants.&#10;&#10;In summary, while there are no specific measures discussed to deal with people losing interest or becoming frustrated during the meeting, some related issues are brought up that can contribute to addressing frustration by improving audio quality, promoting clear communication, ensuring privacy, clarifying goals, and simplifying project constraints." target=" . That needs to be forthcoming .&#10;Speaker: PhD A&#10;Content: Cuz {disfmarker} OK .&#10;Speaker: Postdoc G&#10;Content: But {disfmarker} but the , uh {disfmarker} I did want to say that it 's hard to follow one channel of a conversation even if you know the people , and if you 're dealing furthermore with highly abstract network concepts you 've never heard of {disfmarker} So , you know , one of these people was {disfmarker} was transcribing the , uh , networks group talk and she said , &quot; I don't really know what a lot of these abbreviations are , &quot; &quot; but I just put them in parentheses cuz that 's the {disfmarker} that 's the convention and I just &quot; {disfmarker} Cuz you know , if you don't know {disfmarker}&#10;Speaker: Grad H&#10;Content: Oh , I 'd be curious to {disfmarker} to look at that .&#10;Speaker: PhD E&#10;Content: Just out of curiosity , I mean {disfmarker}&#10;Speaker: Grad H&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there are no specific measures explicitly discussed to deal with people losing interest or becoming frustrated during the meeting. However, some related issues are mentioned that can help address the question:&#10;&#10;1. Microphone difficulties: The participants bring up the problem of inconsistent microphone usage leading to cross-talk and inaccurate transcriptions. Improving audio quality can help maintain engagement and make it easier for participants to follow the conversation.&#10;2. Transcription standards: Discussants emphasize the importance of having uniformity and transcription standards, particularly when multiple people are speaking simultaneously or using specialized terminology. Clear communication and understanding can prevent frustration and misunderstandings.&#10;3. Human subjects concerns: The group acknowledges that video recordings may make participants uncomfortable due to human subjects issues, which could contribute to waning interest or frustration. Addressing these concerns by ensuring privacy and informed consent is crucial for maintaining engagement.&#10;4. Meeting data usage: While not directly related to addressing frustration during the meeting itself, it's mentioned that the primary purpose of using meeting data is to gather naturally occurring speech data, rather than focusing on individual behaviors like gestures or comparing it to far-field studies. Clarifying the goals and intentions of a meeting can help align participants' expectations and minimize frustration.&#10;5. Bottom-up project: Although not explicitly linked to addressing frustration, the group mentions a potential project that involves removing high-level constraints and approaching it from a bottom-up perspective. This approach may result in less reliance on extensive data and infrastructure, potentially reducing complexity and confusion for participants.&#10;&#10;In summary, while there are no specific measures discussed to deal with people losing interest or becoming frustrated during the meeting, some related issues are brought up that can contribute to addressing frustration by improving audio quality, promoting clear communication, ensuring privacy, clarifying goals, and simplifying project constraints." target="1. Cross-talk and inconsistent audio quality: Participants mention that people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. This results in inconsistent or inaccurate transcriptions, which oversimplifies the complexity of the task when there is serious interest in capturing dialogue.&#10;&#10;2. Need for uniformity and transcription standards: Discussants emphasize the importance of having uniformity and transcription standards across different projects to ensure accurate and useful results, particularly in meeting recordings where multiple individuals may be speaking simultaneously.&#10;&#10;3. Human subjects concerns: Video recordings can make participants uncomfortable due to human subjects issues, potentially leading to people becoming shy or uninterested. Ensuring privacy and informed consent is crucial for maintaining engagement.&#10;&#10;4. Clarifying goals and intentions: Discussing the primary purpose of using meeting data, which is gathering naturally occurring speech data rather than focusing on individual behaviors like gestures, can help align participants' expectations and minimize frustration.&#10;&#10;While not explicitly mentioned as reasons, it can be inferred that having a serious interest in capturing dialogue or meeting recordings requires careful planning, resource allocation, and expertise to achieve accurate and useful results. Organizations like NIST or LDC are considered better equipped to handle these tasks due to their resources and expertise in ensuring uniformity, transcription standards, and high-quality recordings across different projects.">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP A's concern is that the current method of determining word segments in the transcripts from IBM, which involves chopping based on speech-silence transitions, does not provide enough information about which words belong to which segments. This becomes problematic when trying to align transcriptions with the original audio and compute prosodic features. PHP A is worried that without knowing the exact word segments, it will be difficult to accurately associate each word with its corresponding time boundaries in the original audio, which can negatively impact the accuracy of prosodic feature computation and overall transcription quality.&#10;&#10;To address this issue, PhD E suggests using a forced alignment before chopping up the audio into segments. Forced alignment is a process that creates a more accurate alignment of words to the audio, even if words are in different segments or have been inserted or deleted. By performing a forced alignment before chopping up the audio into segments, it would be possible to ensure that each word is accurately associated with its corresponding time boundaries in the original audio.&#10;&#10;However, as the discussion continues, the group mentions that IBM might be doing an initial forced alignment and they have to talk to Brian about it. Also, there is a concern about the signal-to-noise ratio being too low which causes problems with getting the right duration pattern or alignment of the words." target=" A&#10;Content: If you do a forced alignment on something really {disfmarker}&#10;Speaker: Grad H&#10;Content: Got uh six sixty minutes of {disfmarker}&#10;Speaker: PhD A&#10;Content: well even if you do it on something really long you need to know {disfmarker} you can always chop it up but you need to have a reference of which words went with which , uh , chop .&#10;Speaker: Postdoc G&#10;Content: Now wasn't {disfmarker} I thought that one of the proposals was that IBM was going to do an initial forced alignment ,&#10;Speaker: PhD A&#10;Content: So {disfmarker}&#10;Speaker: Postdoc G&#10;Content: after they {disfmarker}&#10;Speaker: Grad H&#10;Content: Yeah , but {disfmarker}&#10;Speaker: Professor B&#10;Content: I {disfmarker} I think that they are ,&#10;Speaker: Grad H&#10;Content: We 'll have to talk to Brian .&#10;Speaker: Professor B&#10;Content: um , yeah , I 'm sure they will and so we {disfmarker} we have to have a">
      <data key="d0">1</data>
    </edge>
    <edge source="The people transcribing the interviews are a group of individuals subcontracted by Brian Kingsbury's team. They are not exactly like court reporters as they do not seem to have special keyboards or specific training for court reporting. Instead, they are more similar to secretarial staff or medical transcriptionists, responsible for manually transcribing audio data into written text. However, they might be using some basic equipment like headphones and a computer for transcription purposes. The level of detail in time information provided by their transcriptions is not as fine-grained as the current transcription methods used.&#10;&#10;It's important to note that organizations such as NIST or LDC are better equipped to handle transcription tasks due to their resources, expertise, and adherence to uniformity and high-quality standards across different projects. The comparison is made between simply placing a microphone to capture audio and the complex process involved in accurate and useful transcriptions." target="'t think they have the special keyboards and that {disfmarker} and that type of training .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Postdoc G&#10;Content: I {disfmarker} I get the sense they 're more secretarial . And that , um , uh , what they 're doing is giving them {disfmarker}&#10;Speaker: PhD E&#10;Content: Hmm . Like medical transcriptionist type people {disfmarker}&#10;Speaker: Grad H&#10;Content: Nu - it 's mostly {disfmarker} it 's for their speech recognition products ,&#10;Speaker: PhD E&#10;Content: But aren't {disfmarker} they 're {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Yep .&#10;Speaker: Grad H&#10;Content: that they 've hired these people to do .&#10;Speaker: PhD E&#10;Content: Oh , so they 're hiring them , they 're coming . It 's not a service they send the tapes out to .&#10;Speaker: Grad H&#10;Content: Well they {disfmarker} they do send it out but my">
      <data key="d0">1</data>
    </edge>
    <edge source="The people transcribing the interviews are a group of individuals subcontracted by Brian Kingsbury's team. They are not exactly like court reporters as they do not seem to have special keyboards or specific training for court reporting. Instead, they are more similar to secretarial staff or medical transcriptionists, responsible for manually transcribing audio data into written text. However, they might be using some basic equipment like headphones and a computer for transcription purposes. The level of detail in time information provided by their transcriptions is not as fine-grained as the current transcription methods used.&#10;&#10;It's important to note that organizations such as NIST or LDC are better equipped to handle transcription tasks due to their resources, expertise, and adherence to uniformity and high-quality standards across different projects. The comparison is made between simply placing a microphone to capture audio and the complex process involved in accurate and useful transcriptions." target="The transcript does not provide sufficient information to directly answer the question about the average time it takes to transcribe five minutes of real-time audio. The text only mentions that two out of eight transcribers have finished their data sets, but it does not give information on how long those data sets were or how much time the transcribers spent on them. To calculate the average transcription time for a specific duration like five minutes, we would need consistent data points from multiple transcribers working on similar audio segments. Since this information is not available in the text, it's impossible to provide an accurate answer.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Cross-talk and inconsistent audio quality: Participants mention that people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe the dialogue. This results in inconsistent or inaccurate transcriptions, which oversimplifies the complexity of the task when there is serious interest in capturing dialogue.&#10;&#10;2. Need for uniformity and transcription standards: Discussants emphasize the importance of having uniformity and transcription standards across different projects to ensure accurate and useful results, particularly in meeting recordings where multiple individuals may be speaking simultaneously.&#10;&#10;3. Human subjects concerns: Video recordings can make participants uncomfortable due to human subjects issues, potentially leading to people becoming shy or uninterested. Ensuring privacy and informed consent is crucial for maintaining engagement.&#10;&#10;4. Clarifying goals and intentions: Discussing the primary purpose of using meeting data, which is gathering naturally occurring speech data rather than focusing on individual behaviors like gestures, can help align participants' expectations and minimize frustration.&#10;&#10;While not explicitly mentioned as reasons, it can be inferred that having a serious interest in capturing dialogue or meeting recordings requires careful planning, resource allocation, and expertise to achieve accurate and useful results. Organizations like NIST or LDC are considered better equipped to handle these tasks due to their resources and expertise in ensuring uniformity, transcription standards, and high-quality recordings across different projects." target="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source="The issue with having meetings that were exclusively in English, as requested by Jonathan Fiscus, was that it turned out to be a bit of a problem because not many all-English meetings were being held. Postdoc G had to give Fiscus their initial meeting in English because he asked for it, and they don't have a lot of all-English meetings right now. Therefore, providing an exclusively English meeting may have been challenging due to the lack of regular all-English meetings." target=" H&#10;Content: Legal .&#10;Speaker: Postdoc G&#10;Content: OK , OK .&#10;Speaker: Professor B&#10;Content: it {disfmarker} it {disfmarker} it {disfmarker} it turned out to be a bit of a problem .&#10;Speaker: PhD A&#10;Content: Or {disfmarker}&#10;Speaker: Postdoc G&#10;Content: And I had one other {disfmarker} one other aspect of this which is , um , uh , uh , Jonathan Fiscus expressed primar uh y a major interest in having meetings which were all English speakers . Now he wasn't trying to shape us in terms of what we gather&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc G&#10;Content: but that 's what he wanted me to show him . So I 'm giving him our , um {disfmarker} our initial meeting because he asked for all English . And I think we don't have a lot of all English meetings right now .&#10;Speaker: Professor B&#10;Content: Of all {disfmarker} all nat all native speakers .&#10;Speaker: PhD E&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="The issue with having meetings that were exclusively in English, as requested by Jonathan Fiscus, was that it turned out to be a bit of a problem because not many all-English meetings were being held. Postdoc G had to give Fiscus their initial meeting in English because he asked for it, and they don't have a lot of all-English meetings right now. Therefore, providing an exclusively English meeting may have been challenging due to the lack of regular all-English meetings." target=" for people with big heads .&#10;Speaker: PhD A&#10;Content: It 's makes our job a lot easier .&#10;Speaker: Professor B&#10;Content: OK . OK .&#10;Speaker: Grad H&#10;Content: And , you know , we 're researchers , so we all have big heads .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . Uh , OK , second item was the , uh , NIST visit , and what 's going on there .&#10;Speaker: Postdoc G&#10;Content: Yeah . OK , so , um , uh , Jonathan Fiscus is coming on the second of February and I 've spoken with , uh , {pause} u u a lot of people here , not everyone . Um , and , um , he expressed an interest in seeing the room and in , um , seeing a demonstration of the modified multitrans , which I 'll mention in a second , and also , um , he was interested in the pre - segmentation and then he 's also interested in the transcription conventions .&#10;Speaker: Grad H&#10;Content: Mm - hmm .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions." target=" I bet with the lapel mike there 's plenty , uh , room acoustic&#10;Speaker: PhD A&#10;Content: That {disfmarker} that may be true .&#10;Speaker: Grad H&#10;Content: but I I think the rest is cross - talk .&#10;Speaker: PhD A&#10;Content: But I don't know how good it can get either by those {disfmarker} the {disfmarker} those methods {disfmarker}&#10;Speaker: Grad H&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: That 's true .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: Grad H&#10;Content: So I {disfmarker} I think it 's just ,&#10;Speaker: PhD A&#10;Content: Oh , I don't know .&#10;Speaker: Grad H&#10;Content: yeah , what you said , cross - talk .&#10;Speaker: PhD A&#10;Content: All I meant is just that as sort of {disfmarker} as this pipeline of research is going on we 're also experimenting with different ASR , uh , techniques .&#10;Speaker: Grad H&#10;Content: Mm - hmm">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions." target="aker: PhD A&#10;Content: It 's like {disfmarker} {comment} {vocalsound} like {disfmarker}&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: And the question is to w to what extent is it getting hurt by , uh {disfmarker} by any room acoustics or is it just {disfmarker} uh , given that it 's close it 's not a problem ?&#10;Speaker: PhD A&#10;Content: It doesn't seem like big room acoustics problems to my ear&#10;Speaker: Professor B&#10;Content: Uh {disfmarker}&#10;Speaker: PhD A&#10;Content: but I 'm not an expert . It seems like a problem with cross - talk .&#10;Speaker: Professor B&#10;Content: OK , so it 's {disfmarker}&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Grad H&#10;Content: e I bet with the lapel mike there 's plenty , uh , room acoustic&#10;Speaker: PhD A&#10;Content: That {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions." target="&#10;Speaker: Professor B&#10;Content: Um , so , better quality close talking mikes . So the one issue was that the {disfmarker} the , uh , lapel mike , uh , isn't as good as you would like . And so , uh , it {disfmarker} it 'd be better if we had close talking mikes for everybody . Right ?&#10;Speaker: PhD A&#10;Content: Ri - um ,&#10;Speaker: Professor B&#10;Content: Is that {disfmarker} is that basically the point ?&#10;Speaker: PhD A&#10;Content: yeah , the {disfmarker} And actually in addition to that , that the {disfmarker} the close talking mikes are worn in such a way as to best capture the signal . And the reason here is just that for the people doing work not on microphones but on sort of like dialogue and so forth , uh {disfmarker} or and even on prosody , which Don is gonna be working on soon , it adds this extra , you know , vari variable for each speaker to {disfmarker} to deal with when the microphones aren't similar .&#10;Speaker: Professor B&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions." target="1. The suggestion made by PhD A and Postdoc G regarding adding more variation to the beeps in a film is for someone to explore different groups or filmmakers who could bring some color and creativity to the proceedings. They are open to discussing ideas and doing the legwork to contact these potential collaborators but need guidance on which groups might be worth pursuing.&#10;&#10;Based on the transcript, it seems that they want to move away from the monotonous &quot;beeps&quot; and incorporate more varied prosodic contours in the film's audio elements. This would involve finding artists or filmmakers who can contribute creatively to this aspect of the project while considering practical constraints such as commercial value and length of the film.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions." target="1. Positioning between speakers: If individuals are located between two people who are talking, a large peak will appear in the display. This is due to the microphone picking up sound from both sources almost equally.&#10;&#10;2. Speaker location: When someone is speaking on one side or the other of an axis between two individuals, the peak will go the opposite way. The microphone will pick up more sound from the closer speaker and less from the farther speaker, causing a difference in the display's appearance.&#10;&#10;3. Number of speakers: The configuration of speakers also influences the appearance of peaks. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions." target="The issues discussed regarding signal-processing techniques for detecting energies involve inconsistencies in dialogue transcriptions due to challenges with microphones and cross-talk. The participants mentioned that variability in microphone quality and people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe dialogues. When individuals are not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions.&#10;&#10;It is important for those working on signal-processing techniques for detecting energies to be aware of these issues because improving the quality and consistency of audio recordings will significantly enhance the accuracy and usefulness of transcriptions. By developing better signal-processing techniques, researchers can help minimize cross-talk, distinguish individual speakers, and improve the overall signal-to-noise ratio, ultimately leading to more precise and reliable transcriptions. This is crucial for analyzing dialogue or prosody in meetings and other collaborative settings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions." target="1. When people are introduced to the idea of being connected with someone interested in the acoustic side of dialogue recording, they typically ask two main questions:&#10;&#10;- For those interested in general dialogue things, the question is often &quot;are you recording video?&quot; This shows that they consider visual and audio components as a pair, understanding that capturing high-quality video can be crucial for analyzing dialogue effectively.&#10;   (Speaker: Professor B, Content: And just like someone who is interested in the general dialogue thing will always ask 'um, are you recording video?')&#10;&#10;- For those focused on the acoustic side of dialogue recording, a common question is &quot;are you doing array microphones?&quot; This indicates their interest in exploring advanced audio recording techniques and technologies to ensure optimal sound quality for analysis.&#10;   (Speaker: Professor B, Content: And the acoustic people will always say, 'well are you doing, uh, uh, array microphones?')&#10;&#10;These questions reflect individuals' curiosity about different aspects of dialogue recording and their desire to understand the methods and technologies being used in order to ensure high-quality recordings for further analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions." target="1st Issue: Non-speaking individuals picking up signals from other people's conversations: When non-speaking individuals have their microphones active and there is cross-talk or nearby conversations, automatic speech recognition (ASR) techniques may pick up the wrong person's speech. This can lead to inaccurate transcriptions as the ASR system attempts to transcribe the incorrect speaker's words for the corresponding dialogue.&#10;&#10;2nd Issue: Occasional speaking amidst others' speech: When an individual speaks occasionally while others are speaking, the challenge lies in distinguishing their voice from the background noise or other speakers. If the person speaks softly or mumbles, it becomes even harder for ASR systems to identify and transcribe their words accurately. Cross-talk and variability in microphone quality can further exacerbate this issue, leading to inconsistent or inaccurate transcriptions.&#10;&#10;These issues highlight the importance of better signal-processing techniques, such as minimizing cross-talk, distinguishing individual speakers, and improving the overall signal-to-noise ratio. This will help ensure more precise and reliable transcriptions for analyzing dialogue or prosody in meetings and other collaborative settings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions." target="1. Positioning of speakers: Based on the energy levels or loudness on each speaker's microphone, one can determine if individuals are located between two people who are talking (a large peak in the display) or speaking on one side or the other of an axis between two individuals (the peak will go the opposite way, with more sound picked up from the closer speaker and less from the farther speaker).&#10;&#10;2. Volume of speakers: Larger energy levels or loudness on a microphone may indicate that a speaker is speaking more loudly or is closer to the microphone. Conversely, lower energy levels suggest that the speaker is softer or further away. However, it's essential to consider that variations in microphone quality and type can also influence the perceived volume.&#10;&#10;3. Number of speakers: The configuration of speakers affects the appearance of peaks on a display. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time. This information could help determine the number of active speakers in a given situation.&#10;&#10;In summary, analyzing energy levels and loudness on microphones can provide insights into speaker positioning, volume, and the number of speakers in a conversation. However, it is crucial to control for variables such as microphone quality and type to ensure accurate interpretations.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Limitations of lapel microphones: The participants discuss how the quality of lapel microphones is not as good as desired, and they would prefer close-talking microphones for each person involved in the recording instead. They mention that variability in microphone quality adds an extra challenge when analyzing dialogue or prosody.&#10;&#10;2. Cross-talk issues: The group highlights cross-talk as a significant issue affecting audio recordings, making it difficult to accurately transcribe dialogues using automatic speech recognition (ASR) techniques.&#10;&#10;3. Ongoing experiments with ASR techniques: The speakers mention their ongoing efforts in experimenting with different ASR techniques within the context of their research pipeline, aiming to improve the accuracy and usefulness of the transcriptions." target="PhD C is currently working on a method to include information about which channel certain speech occurs in, by comparing and normalizing energies of different channels. This approach aims to help transcribers identify the specific channel with speech, adding value to the existing transcription process that primarily focuses on speech-nonspeech detection in mixed files. By distinguishing individual speakers' channels more accurately, this technique may improve overall signal-to-noise ratio and minimize cross-talk issues during dialogues, leading to more precise and reliable transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The suggestion made by PhD A and Postdoc G regarding adding more variation to the beeps in a film is for someone to explore different groups or filmmakers who could bring some color and creativity to the proceedings. They are open to discussing ideas and doing the legwork to contact these potential collaborators but need guidance on which groups might be worth pursuing.&#10;&#10;Based on the transcript, it seems that they want to move away from the monotonous &quot;beeps&quot; and incorporate more varied prosodic contours in the film's audio elements. This would involve finding artists or filmmakers who can contribute creatively to this aspect of the project while considering practical constraints such as commercial value and length of the film." target=": Film - maker .&#10;Speaker: Grad H&#10;Content: Of beeps ,&#10;Speaker: PhD C&#10;Content: Yeah . Yeah .&#10;Speaker: Grad H&#10;Content: yeah .&#10;Speaker: PhD A&#10;Content: And I don't mean that they 're angry&#10;Speaker: PhD D&#10;Content: Is {disfmarker}&#10;Speaker: Postdoc G&#10;Content: of {disfmarker} {comment} Yes . Really .&#10;Speaker: PhD A&#10;Content: but just something with some more variation in prosodic contours and so forth would be neat . So if anyone has ideas , I 'm willing to do the leg work to go try to talk to people but I don't really know which groups are worth pursuing .&#10;Speaker: Postdoc G&#10;Content: Well there was this K P F A&#10;Speaker: Grad H&#10;Content: No that 's {disfmarker}&#10;Speaker: Postdoc G&#10;Content: but {disfmarker} OK .&#10;Speaker: Grad H&#10;Content: Legal .&#10;Speaker: Postdoc G&#10;Content: OK , OK .&#10;Speaker: Professor B&#10;Content: it {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The suggestion made by PhD A and Postdoc G regarding adding more variation to the beeps in a film is for someone to explore different groups or filmmakers who could bring some color and creativity to the proceedings. They are open to discussing ideas and doing the legwork to contact these potential collaborators but need guidance on which groups might be worth pursuing.&#10;&#10;Based on the transcript, it seems that they want to move away from the monotonous &quot;beeps&quot; and incorporate more varied prosodic contours in the film's audio elements. This would involve finding artists or filmmakers who can contribute creatively to this aspect of the project while considering practical constraints such as commercial value and length of the film." target="er} we 've had this discussion many times .&#10;Speaker: Postdoc G&#10;Content: Yeah , we have .&#10;Speaker: Grad H&#10;Content: And the answer is we don't actually know the answer because we haven't tried both ways .&#10;Speaker: Postdoc G&#10;Content: Well , except I can say that my transcribers use the mixed signal mostly&#10;Speaker: Grad H&#10;Content: So . Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Right .&#10;Speaker: Postdoc G&#10;Content: unless there 's a huge disparity in terms of the volume on {disfmarker} on the mix . In which case , you know , they {disfmarker} they wouldn't be able to catch anything except the prominent {comment} channel ,&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: then they 'll switch between .&#10;Speaker: Grad H&#10;Content: Well I think that {disfmarker} that might change if you wanted really fine time markings .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The suggestion made by PhD A and Postdoc G regarding adding more variation to the beeps in a film is for someone to explore different groups or filmmakers who could bring some color and creativity to the proceedings. They are open to discussing ideas and doing the legwork to contact these potential collaborators but need guidance on which groups might be worth pursuing.&#10;&#10;Based on the transcript, it seems that they want to move away from the monotonous &quot;beeps&quot; and incorporate more varied prosodic contours in the film's audio elements. This would involve finding artists or filmmakers who can contribute creatively to this aspect of the project while considering practical constraints such as commercial value and length of the film." target="1. When people are introduced to the idea of being connected with someone interested in the acoustic side of dialogue recording, they typically ask two main questions:&#10;&#10;- For those interested in general dialogue things, the question is often &quot;are you recording video?&quot; This shows that they consider visual and audio components as a pair, understanding that capturing high-quality video can be crucial for analyzing dialogue effectively.&#10;   (Speaker: Professor B, Content: And just like someone who is interested in the general dialogue thing will always ask 'um, are you recording video?')&#10;&#10;- For those focused on the acoustic side of dialogue recording, a common question is &quot;are you doing array microphones?&quot; This indicates their interest in exploring advanced audio recording techniques and technologies to ensure optimal sound quality for analysis.&#10;   (Speaker: Professor B, Content: And the acoustic people will always say, 'well are you doing, uh, uh, array microphones?')&#10;&#10;These questions reflect individuals' curiosity about different aspects of dialogue recording and their desire to understand the methods and technologies being used in order to ensure high-quality recordings for further analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Positioning between speakers: If individuals are located between two people who are talking, a large peak will appear in the display. This is due to the microphone picking up sound from both sources almost equally.&#10;&#10;2. Speaker location: When someone is speaking on one side or the other of an axis between two individuals, the peak will go the opposite way. The microphone will pick up more sound from the closer speaker and less from the farther speaker, causing a difference in the display's appearance.&#10;&#10;3. Number of speakers: The configuration of speakers also influences the appearance of peaks. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time." target="&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: if {disfmarker} if someone who was on the axis between the two is talking , then you {disfmarker} you get a big peak there . And if {disfmarker} if someone 's talking on {disfmarker} on {disfmarker} on , uh , one side or the other , it goes the other way .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And then , uh , it {disfmarker} it {disfmarker} it even looks different if th t if the two {disfmarker} two people on either side are talking than if one in the middle . It {disfmarker} it actually looks somewhat different , so .&#10;Speaker: PhD E&#10;Content: Hmm . Well I was just thinking , you know , as I was sitting here next to Thilo that um , when he 's talking , my mike probably picks it up better than {pause} your guys 's mikes .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Positioning between speakers: If individuals are located between two people who are talking, a large peak will appear in the display. This is due to the microphone picking up sound from both sources almost equally.&#10;&#10;2. Speaker location: When someone is speaking on one side or the other of an axis between two individuals, the peak will go the opposite way. The microphone will pick up more sound from the closer speaker and less from the farther speaker, causing a difference in the display's appearance.&#10;&#10;3. Number of speakers: The configuration of speakers also influences the appearance of peaks. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time." target=": Yeah .&#10;Speaker: PhD E&#10;Content: That would be really neat .&#10;Speaker: Professor B&#10;Content: but they might wanna just , {disfmarker} uh , you know , you could imagine them taking the four signals from these {disfmarker} these table mikes and trying to do something with them {disfmarker} Um , I also had a discussion {disfmarker} So , w uh , we 'll be over {disfmarker} over there talking with him , um , after class on Friday . Um , we 'll let you know what {disfmarker} what goes with that . Also had a completely unrelated thing . I had a , uh , discussion today with , uh , Birger Kollmeier who 's a , uh , a German , uh , scientist who 's got a fair sized group {vocalsound} doing a range of things . It 's sort of auditory related , largely for hearing aids and so on . But {disfmarker} but , uh , he does stuff with auditory models and he 's very interested in directionality , and location , and {disfmarker} and , uh , head models and {">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Positioning between speakers: If individuals are located between two people who are talking, a large peak will appear in the display. This is due to the microphone picking up sound from both sources almost equally.&#10;&#10;2. Speaker location: When someone is speaking on one side or the other of an axis between two individuals, the peak will go the opposite way. The microphone will pick up more sound from the closer speaker and less from the farther speaker, causing a difference in the display's appearance.&#10;&#10;3. Number of speakers: The configuration of speakers also influences the appearance of peaks. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time." target=" , vari variable for each speaker to {disfmarker} to deal with when the microphones aren't similar .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: So {disfmarker} And I also talked to Mari this morning and she also had a strong preference for doing that . And in fact she said that that 's useful for them to know in starting to collect their data too .&#10;Speaker: Professor B&#10;Content: Mm - hmm . Right , so one th&#10;Speaker: Grad H&#10;Content: Well , so {disfmarker}&#10;Speaker: Professor B&#10;Content: uh , well one thing I was gonna say was that , um , i we could get more , uh , of the head mounted microphones even beyond the number of radio channels we have because I think whether it 's radio or wire is probably second - order . And the main thing is having the microphone close to you ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: u although , not too close .&#10;Speaker: Grad H&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Positioning between speakers: If individuals are located between two people who are talking, a large peak will appear in the display. This is due to the microphone picking up sound from both sources almost equally.&#10;&#10;2. Speaker location: When someone is speaking on one side or the other of an axis between two individuals, the peak will go the opposite way. The microphone will pick up more sound from the closer speaker and less from the farther speaker, causing a difference in the display's appearance.&#10;&#10;3. Number of speakers: The configuration of speakers also influences the appearance of peaks. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time." target="1st Issue: Non-speaking individuals picking up signals from other people's conversations: When non-speaking individuals have their microphones active and there is cross-talk or nearby conversations, automatic speech recognition (ASR) techniques may pick up the wrong person's speech. This can lead to inaccurate transcriptions as the ASR system attempts to transcribe the incorrect speaker's words for the corresponding dialogue.&#10;&#10;2nd Issue: Occasional speaking amidst others' speech: When an individual speaks occasionally while others are speaking, the challenge lies in distinguishing their voice from the background noise or other speakers. If the person speaks softly or mumbles, it becomes even harder for ASR systems to identify and transcribe their words accurately. Cross-talk and variability in microphone quality can further exacerbate this issue, leading to inconsistent or inaccurate transcriptions.&#10;&#10;These issues highlight the importance of better signal-processing techniques, such as minimizing cross-talk, distinguishing individual speakers, and improving the overall signal-to-noise ratio. This will help ensure more precise and reliable transcriptions for analyzing dialogue or prosody in meetings and other collaborative settings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Positioning between speakers: If individuals are located between two people who are talking, a large peak will appear in the display. This is due to the microphone picking up sound from both sources almost equally.&#10;&#10;2. Speaker location: When someone is speaking on one side or the other of an axis between two individuals, the peak will go the opposite way. The microphone will pick up more sound from the closer speaker and less from the farther speaker, causing a difference in the display's appearance.&#10;&#10;3. Number of speakers: The configuration of speakers also influences the appearance of peaks. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time." target="1. Positioning of speakers: Based on the energy levels or loudness on each speaker's microphone, one can determine if individuals are located between two people who are talking (a large peak in the display) or speaking on one side or the other of an axis between two individuals (the peak will go the opposite way, with more sound picked up from the closer speaker and less from the farther speaker).&#10;&#10;2. Volume of speakers: Larger energy levels or loudness on a microphone may indicate that a speaker is speaking more loudly or is closer to the microphone. Conversely, lower energy levels suggest that the speaker is softer or further away. However, it's essential to consider that variations in microphone quality and type can also influence the perceived volume.&#10;&#10;3. Number of speakers: The configuration of speakers affects the appearance of peaks on a display. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time. This information could help determine the number of active speakers in a given situation.&#10;&#10;In summary, analyzing energy levels and loudness on microphones can provide insights into speaker positioning, volume, and the number of speakers in a conversation. However, it is crucial to control for variables such as microphone quality and type to ensure accurate interpretations.">
      <data key="d0">1</data>
    </edge>
    <edge source="The issues discussed regarding signal-processing techniques for detecting energies involve inconsistencies in dialogue transcriptions due to challenges with microphones and cross-talk. The participants mentioned that variability in microphone quality and people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe dialogues. When individuals are not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions.&#10;&#10;It is important for those working on signal-processing techniques for detecting energies to be aware of these issues because improving the quality and consistency of audio recordings will significantly enhance the accuracy and usefulness of transcriptions. By developing better signal-processing techniques, researchers can help minimize cross-talk, distinguish individual speakers, and improve the overall signal-to-noise ratio, ultimately leading to more precise and reliable transcriptions. This is crucial for analyzing dialogue or prosody in meetings and other collaborative settings." target="m - hmm .&#10;Speaker: PhD A&#10;Content: cuz what they had done there is align and then chop .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Um , and this problem is a little bit j more global . It 's that there are problems even in inside the alignments , uh , because of the fact that there 's enough acoustic signal there t for the recognizer to {disfmarker} to eat , {vocalsound} as part of a word . And it tends to do that . S So , uh ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: but we probably will have to do something like that in addition . Anyway . So , yeah , bottom {disfmarker} bottom line is just I wanted to make sure I can be aware of whoever 's working on these signal - processing techniques for , uh , detecting energies ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: because that {disfmarker} that 'll really help us .&#10;Speaker: Professor B&#10;Content: O K , uh tea">
      <data key="d0">1</data>
    </edge>
    <edge source="The issues discussed regarding signal-processing techniques for detecting energies involve inconsistencies in dialogue transcriptions due to challenges with microphones and cross-talk. The participants mentioned that variability in microphone quality and people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe dialogues. When individuals are not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions.&#10;&#10;It is important for those working on signal-processing techniques for detecting energies to be aware of these issues because improving the quality and consistency of audio recordings will significantly enhance the accuracy and usefulness of transcriptions. By developing better signal-processing techniques, researchers can help minimize cross-talk, distinguish individual speakers, and improve the overall signal-to-noise ratio, ultimately leading to more precise and reliable transcriptions. This is crucial for analyzing dialogue or prosody in meetings and other collaborative settings." target=" that .&#10;Speaker: PhD A&#10;Content: but that , um , it would be helpful if I can stay in the loop somehow with , um , people who are doing any kind of post - processing , whether it 's to separate speakers or to improve the signal - to - noise ratio , or both , um , that we can sort of try out as we 're running recognition . Um , so , i is that {disfmarker} Who else is work I guess Dan Ellis and you&#10;Speaker: PhD C&#10;Content: Dan , yeah .&#10;Speaker: Professor B&#10;Content: Yeah , and Dave uh {pause} Gel - Gelbart again ,&#10;Speaker: Grad H&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: and Dave .&#10;Speaker: PhD C&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: he 's {disfmarker} he 's interested in {disfmarker} in fact we 're look starting to look at some echo cancellation kind of things .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="The issues discussed regarding signal-processing techniques for detecting energies involve inconsistencies in dialogue transcriptions due to challenges with microphones and cross-talk. The participants mentioned that variability in microphone quality and people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe dialogues. When individuals are not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions.&#10;&#10;It is important for those working on signal-processing techniques for detecting energies to be aware of these issues because improving the quality and consistency of audio recordings will significantly enhance the accuracy and usefulness of transcriptions. By developing better signal-processing techniques, researchers can help minimize cross-talk, distinguish individual speakers, and improve the overall signal-to-noise ratio, ultimately leading to more precise and reliable transcriptions. This is crucial for analyzing dialogue or prosody in meetings and other collaborative settings." target=" 's just sort of a open world right now of exploring that . So I just wanted to {pause} see , you know , on the transcribing end from here things look good . Uh , the IBM one is more {disfmarker} it 's an open question right now . And then the issue of like global processing of some signal and then , you know , before we chop it up is {disfmarker} is yet another way we can improve things in that .&#10;Speaker: PhD E&#10;Content: What about increasing the flexibility of the alignment ?&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: Do you remember that thing that Michael Finka did ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: that experiment he did a while back ?&#10;Speaker: PhD A&#10;Content: Right . You can , um {disfmarker}  The problem is just that the acoustic {disfmarker} when the signal - to - noise ratio is too low , um , you {disfmarker} you 'll get , a uh {disfmarker} an alignment with the wrong">
      <data key="d0">1</data>
    </edge>
    <edge source="The issues discussed regarding signal-processing techniques for detecting energies involve inconsistencies in dialogue transcriptions due to challenges with microphones and cross-talk. The participants mentioned that variability in microphone quality and people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe dialogues. When individuals are not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions.&#10;&#10;It is important for those working on signal-processing techniques for detecting energies to be aware of these issues because improving the quality and consistency of audio recordings will significantly enhance the accuracy and usefulness of transcriptions. By developing better signal-processing techniques, researchers can help minimize cross-talk, distinguish individual speakers, and improve the overall signal-to-noise ratio, ultimately leading to more precise and reliable transcriptions. This is crucial for analyzing dialogue or prosody in meetings and other collaborative settings." target="1. When people are introduced to the idea of being connected with someone interested in the acoustic side of dialogue recording, they typically ask two main questions:&#10;&#10;- For those interested in general dialogue things, the question is often &quot;are you recording video?&quot; This shows that they consider visual and audio components as a pair, understanding that capturing high-quality video can be crucial for analyzing dialogue effectively.&#10;   (Speaker: Professor B, Content: And just like someone who is interested in the general dialogue thing will always ask 'um, are you recording video?')&#10;&#10;- For those focused on the acoustic side of dialogue recording, a common question is &quot;are you doing array microphones?&quot; This indicates their interest in exploring advanced audio recording techniques and technologies to ensure optimal sound quality for analysis.&#10;   (Speaker: Professor B, Content: And the acoustic people will always say, 'well are you doing, uh, uh, array microphones?')&#10;&#10;These questions reflect individuals' curiosity about different aspects of dialogue recording and their desire to understand the methods and technologies being used in order to ensure high-quality recordings for further analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="The issues discussed regarding signal-processing techniques for detecting energies involve inconsistencies in dialogue transcriptions due to challenges with microphones and cross-talk. The participants mentioned that variability in microphone quality and people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe dialogues. When individuals are not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions.&#10;&#10;It is important for those working on signal-processing techniques for detecting energies to be aware of these issues because improving the quality and consistency of audio recordings will significantly enhance the accuracy and usefulness of transcriptions. By developing better signal-processing techniques, researchers can help minimize cross-talk, distinguish individual speakers, and improve the overall signal-to-noise ratio, ultimately leading to more precise and reliable transcriptions. This is crucial for analyzing dialogue or prosody in meetings and other collaborative settings." target="Having a chunked audio file before transcription is important for the speech recognition process because it allows for better organization of the audio data and improved accuracy in associating words with their corresponding time boundaries. By dividing the continuous audio stream into smaller segments based on speech-silence transitions, automatic speech recognition (ASR) techniques can more easily identify speakers within these segments.&#10;&#10;When there is a chunked audio file before transcription, it becomes possible to perform forced alignment, which determines as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This helps ensure that each word is accurately associated with its corresponding time boundaries in the original audio, even if words are in different segments or have been inserted or deleted.&#10;&#10;This chunked audio file also facilitates better recognition and feature computation by combining segments from the same speaker to create longer stretches of continuous speech. This consolidation process enables a speaker's unique characteristics and patterns to be analyzed more effectively by the recognizer, ultimately leading to improved accuracy in transcriptions and prosodic feature computation.&#10;&#10;In summary, chunking audio files before transcription is crucial for maintaining consistent time markers, accurately associating words with their time boundaries, and improving overall recognition and feature computation during the speech recognition process.">
      <data key="d0">1</data>
    </edge>
    <edge source="The issues discussed regarding signal-processing techniques for detecting energies involve inconsistencies in dialogue transcriptions due to challenges with microphones and cross-talk. The participants mentioned that variability in microphone quality and people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe dialogues. When individuals are not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions.&#10;&#10;It is important for those working on signal-processing techniques for detecting energies to be aware of these issues because improving the quality and consistency of audio recordings will significantly enhance the accuracy and usefulness of transcriptions. By developing better signal-processing techniques, researchers can help minimize cross-talk, distinguish individual speakers, and improve the overall signal-to-noise ratio, ultimately leading to more precise and reliable transcriptions. This is crucial for analyzing dialogue or prosody in meetings and other collaborative settings." target="1st Issue: Non-speaking individuals picking up signals from other people's conversations: When non-speaking individuals have their microphones active and there is cross-talk or nearby conversations, automatic speech recognition (ASR) techniques may pick up the wrong person's speech. This can lead to inaccurate transcriptions as the ASR system attempts to transcribe the incorrect speaker's words for the corresponding dialogue.&#10;&#10;2nd Issue: Occasional speaking amidst others' speech: When an individual speaks occasionally while others are speaking, the challenge lies in distinguishing their voice from the background noise or other speakers. If the person speaks softly or mumbles, it becomes even harder for ASR systems to identify and transcribe their words accurately. Cross-talk and variability in microphone quality can further exacerbate this issue, leading to inconsistent or inaccurate transcriptions.&#10;&#10;These issues highlight the importance of better signal-processing techniques, such as minimizing cross-talk, distinguishing individual speakers, and improving the overall signal-to-noise ratio. This will help ensure more precise and reliable transcriptions for analyzing dialogue or prosody in meetings and other collaborative settings.">
      <data key="d0">1</data>
    </edge>
    <edge source="The issues discussed regarding signal-processing techniques for detecting energies involve inconsistencies in dialogue transcriptions due to challenges with microphones and cross-talk. The participants mentioned that variability in microphone quality and people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe dialogues. When individuals are not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions.&#10;&#10;It is important for those working on signal-processing techniques for detecting energies to be aware of these issues because improving the quality and consistency of audio recordings will significantly enhance the accuracy and usefulness of transcriptions. By developing better signal-processing techniques, researchers can help minimize cross-talk, distinguish individual speakers, and improve the overall signal-to-noise ratio, ultimately leading to more precise and reliable transcriptions. This is crucial for analyzing dialogue or prosody in meetings and other collaborative settings." target="1. Positioning of speakers: Based on the energy levels or loudness on each speaker's microphone, one can determine if individuals are located between two people who are talking (a large peak in the display) or speaking on one side or the other of an axis between two individuals (the peak will go the opposite way, with more sound picked up from the closer speaker and less from the farther speaker).&#10;&#10;2. Volume of speakers: Larger energy levels or loudness on a microphone may indicate that a speaker is speaking more loudly or is closer to the microphone. Conversely, lower energy levels suggest that the speaker is softer or further away. However, it's essential to consider that variations in microphone quality and type can also influence the perceived volume.&#10;&#10;3. Number of speakers: The configuration of speakers affects the appearance of peaks on a display. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time. This information could help determine the number of active speakers in a given situation.&#10;&#10;In summary, analyzing energy levels and loudness on microphones can provide insights into speaker positioning, volume, and the number of speakers in a conversation. However, it is crucial to control for variables such as microphone quality and type to ensure accurate interpretations.">
      <data key="d0">1</data>
    </edge>
    <edge source="The issues discussed regarding signal-processing techniques for detecting energies involve inconsistencies in dialogue transcriptions due to challenges with microphones and cross-talk. The participants mentioned that variability in microphone quality and people adjusting their microphones during meetings can cause cross-talk, making it difficult for automatic speech recognition (ASR) techniques to accurately transcribe dialogues. When individuals are not speaking consistently, the ASR might pick up speech from others around them, resulting in inconsistent or inaccurate transcriptions.&#10;&#10;It is important for those working on signal-processing techniques for detecting energies to be aware of these issues because improving the quality and consistency of audio recordings will significantly enhance the accuracy and usefulness of transcriptions. By developing better signal-processing techniques, researchers can help minimize cross-talk, distinguish individual speakers, and improve the overall signal-to-noise ratio, ultimately leading to more precise and reliable transcriptions. This is crucial for analyzing dialogue or prosody in meetings and other collaborative settings." target="PhD C is currently working on a method to include information about which channel certain speech occurs in, by comparing and normalizing energies of different channels. This approach aims to help transcribers identify the specific channel with speech, adding value to the existing transcription process that primarily focuses on speech-nonspeech detection in mixed files. By distinguishing individual speakers' channels more accurately, this technique may improve overall signal-to-noise ratio and minimize cross-talk issues during dialogues, leading to more precise and reliable transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source="m - hmm .&#10;Speaker: PhD A&#10;Content: cuz what they had done there is align and then chop .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Um , and this problem is a little bit j more global . It 's that there are problems even in inside the alignments , uh , because of the fact that there 's enough acoustic signal there t for the recognizer to {disfmarker} to eat , {vocalsound} as part of a word . And it tends to do that . S So , uh ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: but we probably will have to do something like that in addition . Anyway . So , yeah , bottom {disfmarker} bottom line is just I wanted to make sure I can be aware of whoever 's working on these signal - processing techniques for , uh , detecting energies ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: because that {disfmarker} that 'll really help us .&#10;Speaker: Professor B&#10;Content: O K , uh tea" target="PhD C is currently working on a method to include information about which channel certain speech occurs in, by comparing and normalizing energies of different channels. This approach aims to help transcribers identify the specific channel with speech, adding value to the existing transcription process that primarily focuses on speech-nonspeech detection in mixed files. By distinguishing individual speakers' channels more accurately, this technique may improve overall signal-to-noise ratio and minimize cross-talk issues during dialogues, leading to more precise and reliable transcriptions.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that .&#10;Speaker: PhD A&#10;Content: but that , um , it would be helpful if I can stay in the loop somehow with , um , people who are doing any kind of post - processing , whether it 's to separate speakers or to improve the signal - to - noise ratio , or both , um , that we can sort of try out as we 're running recognition . Um , so , i is that {disfmarker} Who else is work I guess Dan Ellis and you&#10;Speaker: PhD C&#10;Content: Dan , yeah .&#10;Speaker: Professor B&#10;Content: Yeah , and Dave uh {pause} Gel - Gelbart again ,&#10;Speaker: Grad H&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: and Dave .&#10;Speaker: PhD C&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: he 's {disfmarker} he 's interested in {disfmarker} in fact we 're look starting to look at some echo cancellation kind of things .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker:" target="In the transcript, PhD A is trying to propose the idea of collecting more meetings as a source of naturally occurring speech data for their research. The conversation with Chuck Fillmore, a member who had previously vehemently said no to being recorded, seemed to have a positive influence on PhD A's proposal. Initially, PhD A thought that many people had said &quot;no&quot; to being recorded, but after speaking with Chuck Fillmore, he found that Fillmore was more open to the idea this time. Fillmore even invited Liz (presumably PhD A) to a meeting, which PhD A takes as a sign of potential willingness among other groups to participate in the data collection. This interaction seems to have encouraged PhD A to make a pitch for collecting more meetings as a research resource.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that .&#10;Speaker: PhD A&#10;Content: but that , um , it would be helpful if I can stay in the loop somehow with , um , people who are doing any kind of post - processing , whether it 's to separate speakers or to improve the signal - to - noise ratio , or both , um , that we can sort of try out as we 're running recognition . Um , so , i is that {disfmarker} Who else is work I guess Dan Ellis and you&#10;Speaker: PhD C&#10;Content: Dan , yeah .&#10;Speaker: Professor B&#10;Content: Yeah , and Dave uh {pause} Gel - Gelbart again ,&#10;Speaker: Grad H&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: and Dave .&#10;Speaker: PhD C&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: he 's {disfmarker} he 's interested in {disfmarker} in fact we 're look starting to look at some echo cancellation kind of things .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker:" target="1. The project that Dan Ellis worked on involves using cross-correlation and beam-forming techniques to construct a map of where people were sitting. This project is related to recording audio in meetings and improving the signal-to-noise ratio for further processing and analysis. Other individuals who may be involved in this project include PhD A, PhD C, Dave Gelbart, and possibly Professor B. This project seems to be focused on removing high-level constraints and approaching it from a bottom-up perspective, using less data and infrastructure than might typically be used for similar projects.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that .&#10;Speaker: PhD A&#10;Content: but that , um , it would be helpful if I can stay in the loop somehow with , um , people who are doing any kind of post - processing , whether it 's to separate speakers or to improve the signal - to - noise ratio , or both , um , that we can sort of try out as we 're running recognition . Um , so , i is that {disfmarker} Who else is work I guess Dan Ellis and you&#10;Speaker: PhD C&#10;Content: Dan , yeah .&#10;Speaker: Professor B&#10;Content: Yeah , and Dave uh {pause} Gel - Gelbart again ,&#10;Speaker: Grad H&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: and Dave .&#10;Speaker: PhD C&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: he 's {disfmarker} he 's interested in {disfmarker} in fact we 're look starting to look at some echo cancellation kind of things .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker:" target="1. This week, Postdoc G and Dave Gelbart will be visiting with John Canny, who is a CS (Computer Science) professor interested in augmented reality (AR) and array microphones.&#10;2. The purpose of their meeting is to explore potential commonalities between their work and John Canny's research on AR and array microphones.&#10;3. One possible outcome could be integrating array microphones into their current work to enhance audio quality during recordings and analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Postdoc G is describing a prototype that allows for more accurate time marking of overlapping segments in the transcription process. This is achieved through the use of a multi-channel innovation that helps clarify hearings involving overlaps. The current limitations of the original software design for pre-segmentation make it difficult to accurately time-mark overlapping segments, as the system relies on chopping based on speech-silence transitions, which may not provide enough information about word segment boundaries in cases of overlapping speech.&#10;&#10;2. By implementing this prototype, users will be able to more finely time and label overlapping segments using a click-and-drag interface, allowing for more accurate association between words and their corresponding time boundaries in the original audio. This improvement is expected to lead to better prosodic feature computation and overall transcription quality." target="Content: So .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Yeah . Well that 's OK , I mean we 'll {disfmarker}&#10;Speaker: Grad H&#10;Content: Sorry .&#10;Speaker: Postdoc G&#10;Content: Yeah , and it 's t and it looks really great . He {disfmarker} he has a prototype . I {disfmarker} I , uh , @ @ {comment} didn't {disfmarker} didn't see it , uh , yesterday but I 'm going to see it today . And , uh , that 's {disfmarker} that will enable us to do {pause} nice um , tight time marking of the beginning and ending of overlapping segments . At present it 's not possible with limitations of {disfmarker} of the , uh , original {pause} design of the software . And um . So , I don't know . In terms of , like , pre - segmentation , that {disfmarker} that continues to be , um , a terrific asset to the {disfmarker} to the transcribers . Do you {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Postdoc G is describing a prototype that allows for more accurate time marking of overlapping segments in the transcription process. This is achieved through the use of a multi-channel innovation that helps clarify hearings involving overlaps. The current limitations of the original software design for pre-segmentation make it difficult to accurately time-mark overlapping segments, as the system relies on chopping based on speech-silence transitions, which may not provide enough information about word segment boundaries in cases of overlapping speech.&#10;&#10;2. By implementing this prototype, users will be able to more finely time and label overlapping segments using a click-and-drag interface, allowing for more accurate association between words and their corresponding time boundaries in the original audio. This improvement is expected to lead to better prosodic feature computation and overall transcription quality." target=" to {disfmarker} uh , uh , if {disfmarker} if you {disfmarker} wouldn't mind , {comment} {vocalsound} to give us a pre - segmentation .&#10;Speaker: PhD A&#10;Content: y yeah .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Uh , maybe you have one already of that first m of the meeting that uh , the first transcribed meeting , the one that I transcribed .&#10;Speaker: PhD C&#10;Content: Um , I 'm sure I have some&#10;Speaker: Postdoc G&#10;Content: Do you have a {disfmarker} could you generate a pre - segmentation ?&#10;Speaker: Grad H&#10;Content: February sixteenth I think .&#10;Speaker: PhD C&#10;Content: but {disfmarker} but that 's the one where we 're , um , trai training on , so that 's a little bit {disfmarker}&#10;Speaker: Grad H&#10;Content: Oh .&#10;Speaker: Postdoc G&#10;Content: Oh , I see .&#10;Speaker: PhD C&#10;Content: It '">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Postdoc G is describing a prototype that allows for more accurate time marking of overlapping segments in the transcription process. This is achieved through the use of a multi-channel innovation that helps clarify hearings involving overlaps. The current limitations of the original software design for pre-segmentation make it difficult to accurately time-mark overlapping segments, as the system relies on chopping based on speech-silence transitions, which may not provide enough information about word segment boundaries in cases of overlapping speech.&#10;&#10;2. By implementing this prototype, users will be able to more finely time and label overlapping segments using a click-and-drag interface, allowing for more accurate association between words and their corresponding time boundaries in the original audio. This improvement is expected to lead to better prosodic feature computation and overall transcription quality." target="er} you know , if we 've got two already finished and they 're working on {disfmarker} Uh , right now all eight of them have differe uh , uh , additional data sets . That means potentially as many as ten might be finished by the end of the month .&#10;Speaker: PhD E&#10;Content: Wow .&#10;Speaker: Postdoc G&#10;Content: Hope so . But the pre - segmentation really helps a huge amount .&#10;Speaker: PhD C&#10;Content: OK .&#10;Speaker: Postdoc G&#10;Content: And , uh , also Dan Ellis 's innovation of the , uh {disfmarker} the multi - channel to here really helped a r a lot in terms of clearing {disfmarker} clearing up h hearings that involve overlaps . But , um , just out of curiosity I asked one of them how long {pause} it was taking her , one of these two who has already finished her data set . She said it takes about , uh , sixty minutes transcription for every five minutes of real time . So it 's about twelve to one , which is what we were thinking .&#10;Speaker: Grad H&#10;Content: or Yep .&#10;Speaker: Postdoc">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Postdoc G is describing a prototype that allows for more accurate time marking of overlapping segments in the transcription process. This is achieved through the use of a multi-channel innovation that helps clarify hearings involving overlaps. The current limitations of the original software design for pre-segmentation make it difficult to accurately time-mark overlapping segments, as the system relies on chopping based on speech-silence transitions, which may not provide enough information about word segment boundaries in cases of overlapping speech.&#10;&#10;2. By implementing this prototype, users will be able to more finely time and label overlapping segments using a click-and-drag interface, allowing for more accurate association between words and their corresponding time boundaries in the original audio. This improvement is expected to lead to better prosodic feature computation and overall transcription quality." target="isfmarker} these people would be very good to shift over to finer grain encoding of overlaps . It 's just a matter of , you know , providing {disfmarker} So if right now you have two overlapping segments in the same time bin , well with {disfmarker} with the improvement in the database {disfmarker} in {disfmarker} in the , uh , sorry , in the interface , it 'd be possible to , um , you know , just do a click and drag thing , and get the {disfmarker} uh , the specific place of each of those , the time tag associated with the beginning and end of {disfmarker} of each segment .&#10;Speaker: Professor B&#10;Content: Right , so I think we talking about three level {disfmarker} three things .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: One {disfmarker} one was uh , we had s had some discussion in the past about some very high level labelings ,&#10;Speaker: Postdoc G&#10;Content: Yeah . The types of overlaps {disfmarker}&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Postdoc G is describing a prototype that allows for more accurate time marking of overlapping segments in the transcription process. This is achieved through the use of a multi-channel innovation that helps clarify hearings involving overlaps. The current limitations of the original software design for pre-segmentation make it difficult to accurately time-mark overlapping segments, as the system relies on chopping based on speech-silence transitions, which may not provide enough information about word segment boundaries in cases of overlapping speech.&#10;&#10;2. By implementing this prototype, users will be able to more finely time and label overlapping segments using a click-and-drag interface, allowing for more accurate association between words and their corresponding time boundaries in the original audio. This improvement is expected to lead to better prosodic feature computation and overall transcription quality." target="Having a chunked audio file before transcription is important for the speech recognition process because it allows for better organization of the audio data and improved accuracy in associating words with their corresponding time boundaries. By dividing the continuous audio stream into smaller segments based on speech-silence transitions, automatic speech recognition (ASR) techniques can more easily identify speakers within these segments.&#10;&#10;When there is a chunked audio file before transcription, it becomes possible to perform forced alignment, which determines as precisely as possible the start and end times of an individual speaker's speech within the continuous audio stream. This helps ensure that each word is accurately associated with its corresponding time boundaries in the original audio, even if words are in different segments or have been inserted or deleted.&#10;&#10;This chunked audio file also facilitates better recognition and feature computation by combining segments from the same speaker to create longer stretches of continuous speech. This consolidation process enables a speaker's unique characteristics and patterns to be analyzed more effectively by the recognizer, ultimately leading to improved accuracy in transcriptions and prosodic feature computation.&#10;&#10;In summary, chunking audio files before transcription is crucial for maintaining consistent time markers, accurately associating words with their time boundaries, and improving overall recognition and feature computation during the speech recognition process.">
      <data key="d0">1</data>
    </edge>
    <edge source=" to {disfmarker} uh , uh , if {disfmarker} if you {disfmarker} wouldn't mind , {comment} {vocalsound} to give us a pre - segmentation .&#10;Speaker: PhD A&#10;Content: y yeah .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Uh , maybe you have one already of that first m of the meeting that uh , the first transcribed meeting , the one that I transcribed .&#10;Speaker: PhD C&#10;Content: Um , I 'm sure I have some&#10;Speaker: Postdoc G&#10;Content: Do you have a {disfmarker} could you generate a pre - segmentation ?&#10;Speaker: Grad H&#10;Content: February sixteenth I think .&#10;Speaker: PhD C&#10;Content: but {disfmarker} but that 's the one where we 're , um , trai training on , so that 's a little bit {disfmarker}&#10;Speaker: Grad H&#10;Content: Oh .&#10;Speaker: Postdoc G&#10;Content: Oh , I see .&#10;Speaker: PhD C&#10;Content: It '" target="Based on the transcript provided, Postdoc G was thinking about an aspect of PhD D's work that involves taking segments of a particular type (such as speech from one speaker) and putting them together. This is referred to when Postdoc G says &quot;which is that it sounded to me, Liz, as though you also had another aspect of your work that involved taking segments which are of a particular type and putting them together.&quot; This suggestion is then confirmed by PhD D, who says &quot;Yeah.&quot; The ensuing dialogue makes it clear that this process involves cutting out parts of a recording that are not relevant to the speaker in question, combining relevant segments from the same speaker, and running them through a recognizer.">
      <data key="d0">1</data>
    </edge>
    <edge source="er} you know , if we 've got two already finished and they 're working on {disfmarker} Uh , right now all eight of them have differe uh , uh , additional data sets . That means potentially as many as ten might be finished by the end of the month .&#10;Speaker: PhD E&#10;Content: Wow .&#10;Speaker: Postdoc G&#10;Content: Hope so . But the pre - segmentation really helps a huge amount .&#10;Speaker: PhD C&#10;Content: OK .&#10;Speaker: Postdoc G&#10;Content: And , uh , also Dan Ellis 's innovation of the , uh {disfmarker} the multi - channel to here really helped a r a lot in terms of clearing {disfmarker} clearing up h hearings that involve overlaps . But , um , just out of curiosity I asked one of them how long {pause} it was taking her , one of these two who has already finished her data set . She said it takes about , uh , sixty minutes transcription for every five minutes of real time . So it 's about twelve to one , which is what we were thinking .&#10;Speaker: Grad H&#10;Content: or Yep .&#10;Speaker: Postdoc" target="Based on the transcript provided, Postdoc G was thinking about an aspect of PhD D's work that involves taking segments of a particular type (such as speech from one speaker) and putting them together. This is referred to when Postdoc G says &quot;which is that it sounded to me, Liz, as though you also had another aspect of your work that involved taking segments which are of a particular type and putting them together.&quot; This suggestion is then confirmed by PhD D, who says &quot;Yeah.&quot; The ensuing dialogue makes it clear that this process involves cutting out parts of a recording that are not relevant to the speaker in question, combining relevant segments from the same speaker, and running them through a recognizer.">
      <data key="d0">1</data>
    </edge>
    <edge source="In the transcript, PhD A is trying to propose the idea of collecting more meetings as a source of naturally occurring speech data for their research. The conversation with Chuck Fillmore, a member who had previously vehemently said no to being recorded, seemed to have a positive influence on PhD A's proposal. Initially, PhD A thought that many people had said &quot;no&quot; to being recorded, but after speaking with Chuck Fillmore, he found that Fillmore was more open to the idea this time. Fillmore even invited Liz (presumably PhD A) to a meeting, which PhD A takes as a sign of potential willingness among other groups to participate in the data collection. This interaction seems to have encouraged PhD A to make a pitch for collecting more meetings as a research resource." target=" Mm - hmm .&#10;Speaker: PhD A&#10;Content: OK . I thought that all these people had sort of said &quot; no &quot; twice already .&#10;Speaker: Professor B&#10;Content: Yeah . So the {disfmarker}&#10;Speaker: PhD A&#10;Content: If that 's not the case then {disfmarker}&#10;Speaker: Professor B&#10;Content: No , no . No . So th there was the thing in Fillmore 's group but even there he hadn't {disfmarker} What he 'd said &quot; no &quot; to was for the main meeting . But they have several smaller meetings a week ,&#10;Speaker: Grad H&#10;Content: So .&#10;Speaker: Professor B&#10;Content: and , uh , the notion was raised before that that could happen . And it just , you know {disfmarker} it just didn't come together&#10;Speaker: PhD A&#10;Content: Just {disfmarker} OK .&#10;Speaker: PhD E&#10;Content: Well , and {disfmarker} and the other thing too is when they originally said &quot; no &quot; they didn't know about this post - editing capability thing .&#10;Speaker: Professor">
      <data key="d0">1</data>
    </edge>
    <edge source="In the transcript, PhD A is trying to propose the idea of collecting more meetings as a source of naturally occurring speech data for their research. The conversation with Chuck Fillmore, a member who had previously vehemently said no to being recorded, seemed to have a positive influence on PhD A's proposal. Initially, PhD A thought that many people had said &quot;no&quot; to being recorded, but after speaking with Chuck Fillmore, he found that Fillmore was more open to the idea this time. Fillmore even invited Liz (presumably PhD A) to a meeting, which PhD A takes as a sign of potential willingness among other groups to participate in the data collection. This interaction seems to have encouraged PhD A to make a pitch for collecting more meetings as a research resource." target="'t {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: uh , I mean , people have made a lot of use of {disfmarker} of TIMIT and , uh w due to its markings , and then {pause} the Switchboard transcription thing , well I think has been very useful for a lot of people .&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: That 's true .&#10;Speaker: Professor B&#10;Content: So {disfmarker}&#10;Speaker: PhD A&#10;Content: I guess I wanted to , um , sort of make a pitch for trying to collect more meetings .&#10;Speaker: Postdoc G&#10;Content: Cool .&#10;Speaker: PhD A&#10;Content: Um ,&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: I actually I talked to Chuck Fillmore and I think they 've what , vehemently said no before but this time he wasn't vehement and he said you know , &quot; well , Liz , come to the meeting tomorrow&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A">
      <data key="d0">1</data>
    </edge>
    <edge source="In the transcript, PhD A is trying to propose the idea of collecting more meetings as a source of naturally occurring speech data for their research. The conversation with Chuck Fillmore, a member who had previously vehemently said no to being recorded, seemed to have a positive influence on PhD A's proposal. Initially, PhD A thought that many people had said &quot;no&quot; to being recorded, but after speaking with Chuck Fillmore, he found that Fillmore was more open to the idea this time. Fillmore even invited Liz (presumably PhD A) to a meeting, which PhD A takes as a sign of potential willingness among other groups to participate in the data collection. This interaction seems to have encouraged PhD A to make a pitch for collecting more meetings as a research resource." target="Content: Yeah .&#10;Speaker: Grad H&#10;Content: Yeah ? OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Hello .&#10;Speaker: Professor B&#10;Content: OK , agenda item one ,&#10;Speaker: PhD D&#10;Content: We went {disfmarker}&#10;Speaker: Grad F&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: introduce Don . OK , we did that . Uh {disfmarker}&#10;Speaker: PhD A&#10;Content: Well , I had a {disfmarker} just a quick question but I know there was discussion of it at a previous meeting that I missed , but just about the {disfmarker} the wish list item of getting good quality close - talking mikes on every speaker .&#10;Speaker: Professor B&#10;Content: OK , so let 's {disfmarker} let 's {disfmarker} So let 's just do agenda {pause} building right now . OK , so let 's talk about that a bit .&#10;Speaker: PhD A&#10;Content: I mean , that was {disfmarker}&#10;Speaker: Professor B">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When people are introduced to the idea of being connected with someone interested in the acoustic side of dialogue recording, they typically ask two main questions:&#10;&#10;- For those interested in general dialogue things, the question is often &quot;are you recording video?&quot; This shows that they consider visual and audio components as a pair, understanding that capturing high-quality video can be crucial for analyzing dialogue effectively.&#10;   (Speaker: Professor B, Content: And just like someone who is interested in the general dialogue thing will always ask 'um, are you recording video?')&#10;&#10;- For those focused on the acoustic side of dialogue recording, a common question is &quot;are you doing array microphones?&quot; This indicates their interest in exploring advanced audio recording techniques and technologies to ensure optimal sound quality for analysis.&#10;   (Speaker: Professor B, Content: And the acoustic people will always say, 'well are you doing, uh, uh, array microphones?')&#10;&#10;These questions reflect individuals' curiosity about different aspects of dialogue recording and their desire to understand the methods and technologies being used in order to ensure high-quality recordings for further analysis." target=" and , uh , um {disfmarker} So , yeah , there 's {disfmarker} there 's {disfmarker} there 's a range of things that people do .&#10;Speaker: PhD E&#10;Content: Huh .&#10;Speaker: Professor B&#10;Content: Um , so if we connected up with somebody who was interested in doing that sort of thing that 's {disfmarker} that 's a good thing to do . I mean , whenever I 've described this to other people who are interested on the {disfmarker} with the acoustic side that 's invariably the question they ask . Just like someone who is interested in the general dialogue thing will always ask {vocalsound} &quot; um , are you recording video ? &quot;&#10;Speaker: PhD A&#10;Content: Right ,&#10;Speaker: Professor B&#10;Content: Um , right ?&#10;Speaker: PhD A&#10;Content: right .&#10;Speaker: Professor B&#10;Content: And {disfmarker} and the acoustic people will always say , &quot; well are you doing , uh , uh , array microphones ? &quot; So it 's {disfmarker} it 's a good thing to do ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When people are introduced to the idea of being connected with someone interested in the acoustic side of dialogue recording, they typically ask two main questions:&#10;&#10;- For those interested in general dialogue things, the question is often &quot;are you recording video?&quot; This shows that they consider visual and audio components as a pair, understanding that capturing high-quality video can be crucial for analyzing dialogue effectively.&#10;   (Speaker: Professor B, Content: And just like someone who is interested in the general dialogue thing will always ask 'um, are you recording video?')&#10;&#10;- For those focused on the acoustic side of dialogue recording, a common question is &quot;are you doing array microphones?&quot; This indicates their interest in exploring advanced audio recording techniques and technologies to ensure optimal sound quality for analysis.&#10;   (Speaker: Professor B, Content: And the acoustic people will always say, 'well are you doing, uh, uh, array microphones?')&#10;&#10;These questions reflect individuals' curiosity about different aspects of dialogue recording and their desire to understand the methods and technologies being used in order to ensure high-quality recordings for further analysis." target="1. The project that Dan Ellis worked on involves using cross-correlation and beam-forming techniques to construct a map of where people were sitting. This project is related to recording audio in meetings and improving the signal-to-noise ratio for further processing and analysis. Other individuals who may be involved in this project include PhD A, PhD C, Dave Gelbart, and possibly Professor B. This project seems to be focused on removing high-level constraints and approaching it from a bottom-up perspective, using less data and infrastructure than might typically be used for similar projects.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When people are introduced to the idea of being connected with someone interested in the acoustic side of dialogue recording, they typically ask two main questions:&#10;&#10;- For those interested in general dialogue things, the question is often &quot;are you recording video?&quot; This shows that they consider visual and audio components as a pair, understanding that capturing high-quality video can be crucial for analyzing dialogue effectively.&#10;   (Speaker: Professor B, Content: And just like someone who is interested in the general dialogue thing will always ask 'um, are you recording video?')&#10;&#10;- For those focused on the acoustic side of dialogue recording, a common question is &quot;are you doing array microphones?&quot; This indicates their interest in exploring advanced audio recording techniques and technologies to ensure optimal sound quality for analysis.&#10;   (Speaker: Professor B, Content: And the acoustic people will always say, 'well are you doing, uh, uh, array microphones?')&#10;&#10;These questions reflect individuals' curiosity about different aspects of dialogue recording and their desire to understand the methods and technologies being used in order to ensure high-quality recordings for further analysis." target="1. This week, Postdoc G and Dave Gelbart will be visiting with John Canny, who is a CS (Computer Science) professor interested in augmented reality (AR) and array microphones.&#10;2. The purpose of their meeting is to explore potential commonalities between their work and John Canny's research on AR and array microphones.&#10;3. One possible outcome could be integrating array microphones into their current work to enhance audio quality during recordings and analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When people are introduced to the idea of being connected with someone interested in the acoustic side of dialogue recording, they typically ask two main questions:&#10;&#10;- For those interested in general dialogue things, the question is often &quot;are you recording video?&quot; This shows that they consider visual and audio components as a pair, understanding that capturing high-quality video can be crucial for analyzing dialogue effectively.&#10;   (Speaker: Professor B, Content: And just like someone who is interested in the general dialogue thing will always ask 'um, are you recording video?')&#10;&#10;- For those focused on the acoustic side of dialogue recording, a common question is &quot;are you doing array microphones?&quot; This indicates their interest in exploring advanced audio recording techniques and technologies to ensure optimal sound quality for analysis.&#10;   (Speaker: Professor B, Content: And the acoustic people will always say, 'well are you doing, uh, uh, array microphones?')&#10;&#10;These questions reflect individuals' curiosity about different aspects of dialogue recording and their desire to understand the methods and technologies being used in order to ensure high-quality recordings for further analysis." target="1st Issue: Non-speaking individuals picking up signals from other people's conversations: When non-speaking individuals have their microphones active and there is cross-talk or nearby conversations, automatic speech recognition (ASR) techniques may pick up the wrong person's speech. This can lead to inaccurate transcriptions as the ASR system attempts to transcribe the incorrect speaker's words for the corresponding dialogue.&#10;&#10;2nd Issue: Occasional speaking amidst others' speech: When an individual speaks occasionally while others are speaking, the challenge lies in distinguishing their voice from the background noise or other speakers. If the person speaks softly or mumbles, it becomes even harder for ASR systems to identify and transcribe their words accurately. Cross-talk and variability in microphone quality can further exacerbate this issue, leading to inconsistent or inaccurate transcriptions.&#10;&#10;These issues highlight the importance of better signal-processing techniques, such as minimizing cross-talk, distinguishing individual speakers, and improving the overall signal-to-noise ratio. This will help ensure more precise and reliable transcriptions for analyzing dialogue or prosody in meetings and other collaborative settings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When people are introduced to the idea of being connected with someone interested in the acoustic side of dialogue recording, they typically ask two main questions:&#10;&#10;- For those interested in general dialogue things, the question is often &quot;are you recording video?&quot; This shows that they consider visual and audio components as a pair, understanding that capturing high-quality video can be crucial for analyzing dialogue effectively.&#10;   (Speaker: Professor B, Content: And just like someone who is interested in the general dialogue thing will always ask 'um, are you recording video?')&#10;&#10;- For those focused on the acoustic side of dialogue recording, a common question is &quot;are you doing array microphones?&quot; This indicates their interest in exploring advanced audio recording techniques and technologies to ensure optimal sound quality for analysis.&#10;   (Speaker: Professor B, Content: And the acoustic people will always say, 'well are you doing, uh, uh, array microphones?')&#10;&#10;These questions reflect individuals' curiosity about different aspects of dialogue recording and their desire to understand the methods and technologies being used in order to ensure high-quality recordings for further analysis." target="1. Positioning of speakers: Based on the energy levels or loudness on each speaker's microphone, one can determine if individuals are located between two people who are talking (a large peak in the display) or speaking on one side or the other of an axis between two individuals (the peak will go the opposite way, with more sound picked up from the closer speaker and less from the farther speaker).&#10;&#10;2. Volume of speakers: Larger energy levels or loudness on a microphone may indicate that a speaker is speaking more loudly or is closer to the microphone. Conversely, lower energy levels suggest that the speaker is softer or further away. However, it's essential to consider that variations in microphone quality and type can also influence the perceived volume.&#10;&#10;3. Number of speakers: The configuration of speakers affects the appearance of peaks on a display. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time. This information could help determine the number of active speakers in a given situation.&#10;&#10;In summary, analyzing energy levels and loudness on microphones can provide insights into speaker positioning, volume, and the number of speakers in a conversation. However, it is crucial to control for variables such as microphone quality and type to ensure accurate interpretations.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The project that Dan Ellis worked on involves using cross-correlation and beam-forming techniques to construct a map of where people were sitting. This project is related to recording audio in meetings and improving the signal-to-noise ratio for further processing and analysis. Other individuals who may be involved in this project include PhD A, PhD C, Dave Gelbart, and possibly Professor B. This project seems to be focused on removing high-level constraints and approaching it from a bottom-up perspective, using less data and infrastructure than might typically be used for similar projects." target=" head worn mikes ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: and that is to try to construct a map of where people were sitting ,&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: uh , based on {disfmarker}&#10;Speaker: Grad H&#10;Content: Well Dan {disfmarker} Dan had worked on that . Dan Ellis ,&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: Oh , did he ? Oh , that 's interesting .&#10;Speaker: Grad H&#10;Content: yeah . So that {disfmarker} that 's the cross - correlation stuff , was {disfmarker} was doing b beam - forming .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: And so you could plot out who was sitting next to who&#10;Speaker: Professor B&#10;Content: A little bit ,&#10;Speaker: PhD E&#10;Content: and {disfmarker}&#10;Speaker: Professor B">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The project that Dan Ellis worked on involves using cross-correlation and beam-forming techniques to construct a map of where people were sitting. This project is related to recording audio in meetings and improving the signal-to-noise ratio for further processing and analysis. Other individuals who may be involved in this project include PhD A, PhD C, Dave Gelbart, and possibly Professor B. This project seems to be focused on removing high-level constraints and approaching it from a bottom-up perspective, using less data and infrastructure than might typically be used for similar projects." target=" B&#10;Content: So , um , the , uh {disfmarker} the big arrays , uh , places , uh , like uh , Rutgers , and Brown , and other {disfmarker} other places , uh , they have , uh , big arrays with , I don't know , a hundred {disfmarker} hundred mikes or something .&#10;Speaker: Grad H&#10;Content: Xerox .&#10;Speaker: Professor B&#10;Content: And so there 's a wall of mikes . And you get really , really good beam - forming {comment} with that sort of thing .&#10;Speaker: PhD E&#10;Content: Wow .&#10;Speaker: Professor B&#10;Content: And it 's {disfmarker} and , um , in fact at one point we had a {disfmarker} a proposal in with Rutgers where we were gonna do some of the sort of per channel signal - processing and they were gonna do the multi - channel stuff , but {pause} it d it d we ended up not doing it . But {disfmarker}&#10;Speaker: PhD E&#10;Content: I 've seen demonstrations of the microphone arrays . It 's amazing how {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Postdoc G was thinking about an aspect of PhD D's work that involves taking segments of a particular type (such as speech from one speaker) and putting them together. This is referred to when Postdoc G says &quot;which is that it sounded to me, Liz, as though you also had another aspect of your work that involved taking segments which are of a particular type and putting them together.&quot; This suggestion is then confirmed by PhD D, who says &quot;Yeah.&quot; The ensuing dialogue makes it clear that this process involves cutting out parts of a recording that are not relevant to the speaker in question, combining relevant segments from the same speaker, and running them through a recognizer." target=" The {disfmarker} the {disfmarker}&#10;Speaker: Postdoc G&#10;Content: Including LDC .&#10;Speaker: PhD E&#10;Content: Yeah ,&#10;Speaker: Postdoc G&#10;Content: I think so .&#10;Speaker: Grad H&#10;Content: Yep .&#10;Speaker: PhD E&#10;Content: y right , OK .&#10;Speaker: Postdoc G&#10;Content: Mm - hmm . Then there 's their web site that has lots of papers . And I looked through them and they mainly had to do with this , um , this , uh , tree structure , uh , annotated tree diagram thing .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Postdoc G&#10;Content: So , um , um {disfmarker} and , you know , in terms of like the conventions that I 'm a that I 've adopted , it {disfmarker} there {disfmarker} there 's no conflict at all .&#10;Speaker: Grad H&#10;Content: Right .&#10;Speaker: Postdoc G&#10;Content: And he was , you know , very interested . And , &quot; oh , and how 'd you">
      <data key="d0">1</data>
    </edge>
    <edge source="1. This week, Postdoc G and Dave Gelbart will be visiting with John Canny, who is a CS (Computer Science) professor interested in augmented reality (AR) and array microphones.&#10;2. The purpose of their meeting is to explore potential commonalities between their work and John Canny's research on AR and array microphones.&#10;3. One possible outcome could be integrating array microphones into their current work to enhance audio quality during recordings and analysis." target="marker} or maybe it 's {disfmarker} No , actually {pause} it 's this week , uh , Dave Gelbart and I will be , uh , visiting with John Canny who i you know , is a CS professor ,&#10;Speaker: Postdoc G&#10;Content: Oh .&#10;Speaker: Professor B&#10;Content: who 's interested in ar in array microphones .&#10;Speaker: Grad H&#10;Content: HCC . Oh , he 's doing array mikes .&#10;Speaker: Professor B&#10;Content: Yeah . And so we wanna see what commonality there is here . You know , maybe they 'd wanna stick an array mike here when we 're doing things&#10;Speaker: PhD E&#10;Content: That would be cool .&#10;Speaker: Grad H&#10;Content: Yeah , that would be neat .&#10;Speaker: Professor B&#10;Content: or {disfmarker} or maybe it 's {disfmarker} it 's not a specific array microphone they want&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: That would be really neat .&#10;Speaker: Professor B&#10;Content: but they might wanna just">
      <data key="d0">1</data>
    </edge>
    <edge source="1. This week, Postdoc G and Dave Gelbart will be visiting with John Canny, who is a CS (Computer Science) professor interested in augmented reality (AR) and array microphones.&#10;2. The purpose of their meeting is to explore potential commonalities between their work and John Canny's research on AR and array microphones.&#10;3. One possible outcome could be integrating array microphones into their current work to enhance audio quality during recordings and analysis." target="ation and then he 's also interested in the transcription conventions .&#10;Speaker: Grad H&#10;Content: Mm - hmm .&#10;Speaker: Postdoc G&#10;Content: And , um {disfmarker} So , um , it seems to me in terms of like , um , i i it wou You know , OK . So the room , it 's things like the audio and c and audi audio and acoustic {disfmarker} acoustic properties of the room and how it {disfmarker} how the recordings are done , and that kind of thing . And , um . OK , in terms of the multi - trans , well that {disfmarker} that 's being modified by Dave Gelbart to , uh , handle multi - channel recording .&#10;Speaker: Grad H&#10;Content: Oh , I should 've {disfmarker} I was just thinking I should have invited him to this meeting . I forgot to do it .&#10;Speaker: Postdoc G&#10;Content: Yeah , OK .&#10;Speaker: Grad H&#10;Content: So .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc G&#10;Content: Yeah . Well that 's OK">
      <data key="d0">1</data>
    </edge>
    <edge source="1. This week, Postdoc G and Dave Gelbart will be visiting with John Canny, who is a CS (Computer Science) professor interested in augmented reality (AR) and array microphones.&#10;2. The purpose of their meeting is to explore potential commonalities between their work and John Canny's research on AR and array microphones.&#10;3. One possible outcome could be integrating array microphones into their current work to enhance audio quality during recordings and analysis." target=" see him this Friday what {disfmarker} what kind of level he wants to get involved .&#10;Speaker: Postdoc G&#10;Content: It 's premature . Fine . Good .&#10;Speaker: Professor B&#10;Content: Uh , he might be excited to and it might be very appropriate for him to , uh , or he might have no interest whatsoever . I {disfmarker} I just really don't know .&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: Grad H&#10;Content: Is he involved in {disfmarker} Ach ! {comment} I 'm blanking on the name of the project . NIST has {disfmarker} has done a big meeting room {disfmarker} instrumented meeting room with video and microphone arrays , and very elaborate software . Is {disfmarker} is he the one working on that ?&#10;Speaker: Professor B&#10;Content: Well that 's what they 're starting up .&#10;Speaker: Grad H&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah . No , I mean , that 's what all this is about . They {disfmarker} they haven't done">
      <data key="d0">1</data>
    </edge>
    <edge source="1. This week, Postdoc G and Dave Gelbart will be visiting with John Canny, who is a CS (Computer Science) professor interested in augmented reality (AR) and array microphones.&#10;2. The purpose of their meeting is to explore potential commonalities between their work and John Canny's research on AR and array microphones.&#10;3. One possible outcome could be integrating array microphones into their current work to enhance audio quality during recordings and analysis." target="1st Issue: Non-speaking individuals picking up signals from other people's conversations: When non-speaking individuals have their microphones active and there is cross-talk or nearby conversations, automatic speech recognition (ASR) techniques may pick up the wrong person's speech. This can lead to inaccurate transcriptions as the ASR system attempts to transcribe the incorrect speaker's words for the corresponding dialogue.&#10;&#10;2nd Issue: Occasional speaking amidst others' speech: When an individual speaks occasionally while others are speaking, the challenge lies in distinguishing their voice from the background noise or other speakers. If the person speaks softly or mumbles, it becomes even harder for ASR systems to identify and transcribe their words accurately. Cross-talk and variability in microphone quality can further exacerbate this issue, leading to inconsistent or inaccurate transcriptions.&#10;&#10;These issues highlight the importance of better signal-processing techniques, such as minimizing cross-talk, distinguishing individual speakers, and improving the overall signal-to-noise ratio. This will help ensure more precise and reliable transcriptions for analyzing dialogue or prosody in meetings and other collaborative settings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. This week, Postdoc G and Dave Gelbart will be visiting with John Canny, who is a CS (Computer Science) professor interested in augmented reality (AR) and array microphones.&#10;2. The purpose of their meeting is to explore potential commonalities between their work and John Canny's research on AR and array microphones.&#10;3. One possible outcome could be integrating array microphones into their current work to enhance audio quality during recordings and analysis." target="1. Positioning of speakers: Based on the energy levels or loudness on each speaker's microphone, one can determine if individuals are located between two people who are talking (a large peak in the display) or speaking on one side or the other of an axis between two individuals (the peak will go the opposite way, with more sound picked up from the closer speaker and less from the farther speaker).&#10;&#10;2. Volume of speakers: Larger energy levels or loudness on a microphone may indicate that a speaker is speaking more loudly or is closer to the microphone. Conversely, lower energy levels suggest that the speaker is softer or further away. However, it's essential to consider that variations in microphone quality and type can also influence the perceived volume.&#10;&#10;3. Number of speakers: The configuration of speakers affects the appearance of peaks on a display. If there are two people speaking on either side of a third person, the display will look different compared to having only one person speaking at a time. This information could help determine the number of active speakers in a given situation.&#10;&#10;In summary, analyzing energy levels and loudness on microphones can provide insights into speaker positioning, volume, and the number of speakers in a conversation. However, it is crucial to control for variables such as microphone quality and type to ensure accurate interpretations.">
      <data key="d0">1</data>
    </edge>
    <edge source="PhD C is currently working on a method to include information about which channel certain speech occurs in, by comparing and normalizing energies of different channels. This approach aims to help transcribers identify the specific channel with speech, adding value to the existing transcription process that primarily focuses on speech-nonspeech detection in mixed files. By distinguishing individual speakers' channels more accurately, this technique may improve overall signal-to-noise ratio and minimize cross-talk issues during dialogues, leading to more precise and reliable transcriptions." target="marker} that continues to be , um , a terrific asset to the {disfmarker} to the transcribers . Do you {disfmarker} I know that you 're al also supplementing it further . Do you want to mention something about that c Thilo , or {disfmarker} ?&#10;Speaker: PhD C&#10;Content: Um , yeah . What {disfmarker} what I 'm doing right now is I 'm trying to include some information about which channel , uh , there 's some speech in . But that 's not working at the moment . I 'm just trying to do this by comparing energies , uh {disfmarker} normalizing energies and comparing energies of the different channels .&#10;Speaker: Postdoc G&#10;Content: OK .&#10;Speaker: PhD C&#10;Content: And so to {disfmarker} to give the transcribers some information in which channel there 's {disfmarker} there 's speech in addition to {disfmarker} to the thing we {disfmarker} we did now which is just , uh , speech - nonspeech detection on the mixed file . So I 'm {d">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
