<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." />
    <node id="Content: And {disfmarker} Right .&#10;Speaker: PhD F&#10;Content: Cuz {vocalsound} once we start , sort of , doing this I don't {disfmarker} we don't actually have enough time to probably have to rehash it out again&#10;Speaker: Grad C&#10;Content: The {disfmarker} Yep . The other thing {disfmarker} the other way that I sort of established this was as easy translation to and from the Transcriber format .&#10;Speaker: PhD F&#10;Content: and {disfmarker} s Right .&#10;Speaker: Grad C&#10;Content: Um ,&#10;Speaker: PhD F&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: but {disfmarker}&#10;Speaker: PhD F&#10;Content: I mean , I like this . This is sort of intuitively easy to actually r read ,&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD F&#10;Content: as easy it could {disfmarker} as it could be . But , I suppose that {pause} as long as they have a type here that specifies &quot; utt &quot; , um ,&#10;" />
    <node id="} as it could be . But , I suppose that {pause} as long as they have a type here that specifies &quot; utt &quot; , um ,&#10;Speaker: Grad C&#10;Content: It 's almost the same .&#10;Speaker: PhD F&#10;Content: it 's {disfmarker} yeah , close enough that {disfmarker}&#10;Speaker: Grad C&#10;Content: The {disfmarker} the {disfmarker} the {disfmarker} the point is {disfmarker} with this , though , is that you can't really add any supplementary information . Right ? So if you suddenly decide that you want {disfmarker}&#10;Speaker: PhD F&#10;Content: You have to make a different type .&#10;Speaker: Grad C&#10;Content: Yeah . You 'd have to make a different type .&#10;Speaker: PhD F&#10;Content: So {disfmarker} Well , if you look at it and {disfmarker} Um , I guess in my mind I don't know enough {disfmarker} Jane would know better , {comment} about the {pause} types of annotations and {disfmarker}" />
    <node id="er}&#10;Speaker: Postdoc E&#10;Content: It {disfmarker} it seems to me that , um , we will have o an official version of the corpus , which will be only one {disfmarker} one version in terms of the words {disfmarker} where the words are concerned . We 'd still have the {disfmarker} the merging issue maybe if coding were done independently of the {disfmarker}&#10;Speaker: PhD A&#10;Content: And you 're gonna get that&#10;Speaker: Postdoc E&#10;Content: But {disfmarker} but {disfmarker}&#10;Speaker: PhD A&#10;Content: because if the data gets out , people will do all kinds of things to it . And , uh , s you know , several years from now you might want to look into , um , the prosody of referring expressions . And someone at the university of who knows where has annotated the referring expressions . So you want to get that annotation and bring it back in line with your data .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: OK ?&#10;Speaker: Grad C&#10;Content: But" />
    <node id=" , we 'd just have to translate everything .&#10;Speaker: Grad C&#10;Content: Write a translator . But it se Since they are developing a big {disfmarker}&#10;Speaker: PhD F&#10;Content: But it {disfmarker} but that sounds {disfmarker}&#10;Speaker: PhD D&#10;Content: But that 's {disfmarker} I don't think that 's a big deal .&#10;Speaker: PhD F&#10;Content: As long as it is {disfmarker}&#10;Speaker: Grad C&#10;Content: they 're developing a big infrastructure . And so it seems to me that if {disfmarker} if we want to use that , we might as well go directly to what they 're doing , rather than {disfmarker}&#10;Speaker: PhD A&#10;Content: If we want to {disfmarker} Do they already have something that 's {disfmarker} that would be useful for us in place ?&#10;Speaker: PhD D&#10;Content: Yeah . See , that 's the question . I mean , how stable is their {disfmarker} Are they ready to go ,&#10;Speaker" />
    <node id=" C&#10;Content: You 'd have another tag which says this is of type &quot; sentence &quot; .&#10;Speaker: PhD F&#10;Content: OK . OK .&#10;Speaker: Grad C&#10;Content: And , what {disfmarker}&#10;Speaker: PhD F&#10;Content: But it 's just not overtly in the {disfmarker}&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Um , cuz this is exactly the kind of {disfmarker}&#10;Speaker: PhD A&#10;Content: So {disfmarker}&#10;Speaker: PhD F&#10;Content: I think that should be {pause} possible as long as the {disfmarker} But , uh , what I don't understand is where the {disfmarker} where in this type of file {pause} that would be expressed .&#10;Speaker: Grad C&#10;Content: Right . You would have another tag somewhere . It 's {disfmarker} well , there 're two ways of doing it .&#10;Speaker: PhD F&#10;Content: S so it would just be floating before the sentence or floating after the sentence without a time - mark .&#10;" />
    <node id=" .&#10;Speaker: PhD F&#10;Content: S so it would just be floating before the sentence or floating after the sentence without a time - mark .&#10;Speaker: Grad C&#10;Content: You could have some sort of link type {disfmarker} type equals &quot; sentence &quot; , and ID is &quot; S - whatever &quot; . And then lower down you could have an utterance . So the type is &quot; utterance &quot; {disfmarker} equals &quot; utt &quot; . And you could either say that {disfmarker} No . I don't know {disfmarker}&#10;Speaker: PhD A&#10;Content: So here 's the thing .&#10;Speaker: Grad C&#10;Content: I take that back .&#10;Speaker: PhD A&#10;Content: Um {disfmarker}&#10;Speaker: Grad C&#10;Content: Can you {disfmarker} can you say that this is part of this ,&#10;Speaker: PhD F&#10;Content: See , cuz it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Hhh .&#10;Speaker: PhD F&#10;Content: it 's {disfmarker}&#10;Speaker: PhD D&#10;" />
    <node id="1. When there are new recognition outputs or changes in forced alignment, the time boundaries of words and phrases may also change. To handle this, the team discusses using a flexible database format that can easily accommodate these modifications.&#10;2. The proposed solution involves creating an elegant and easy-to-use database system that allows for recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed. This ensures that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;3. One suggestion is to use a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers." />
    <node id="marker} the utterance that it 's referring to could be U - seventeen or something like that .&#10;Speaker: PhD F&#10;Content: OK . So , I mean , that seems {disfmarker} that seems g great for all of the encoding of things with time and ,&#10;Speaker: Grad C&#10;Content: Oh , well .&#10;Speaker: PhD F&#10;Content: um {disfmarker} I {disfmarker} I guess my question is more , uh , what d what do you do with , say , a forced alignment ?&#10;Speaker: PhD A&#10;Content: How - how&#10;Speaker: PhD F&#10;Content: I mean you 've got all these phone labels , and what do you do if you {disfmarker} just conceptually , if you get , um , transcriptions where the words are staying but the time boundaries are changing , cuz you 've got a new recognition output , or s sort of {disfmarker} what 's the , um , sequence of going from the waveforms that stay the same , the transcripts that may or may not change , and then the utterance which {disfmarker} where the time boundaries that may" />
    <node id=" recently or something ?&#10;Speaker: PhD F&#10;Content: Sorry . Exactly . It 's very disconcerting . OK . So , um ,&#10;Speaker: Grad C&#10;Content: &#10;Speaker: PhD F&#10;Content: I was gonna try to get out of here , like , in half an hour , um , cuz I really appreciate people coming , and {vocalsound} the main thing that I was gonna ask people to help with today is {pause} to give input on what kinds of database format we should {pause} use in starting to link up things like word transcripts and annotations of word transcripts , so anything that transcribers or discourse coders or whatever put in the signal , {vocalsound} with time - marks for , like , words and phone boundaries and all the stuff we get out of the forced alignments and the recognizer . So , we have this , um {disfmarker} I think a starting point is clearly the {disfmarker} the channelized {pause} output of Dave Gelbart 's program , which Don brought a copy of ,&#10;Speaker: Grad C&#10;Content: Yeah . Yeah , I 'm {disfmarker} I 'm familiar" />
    <node id=" was trying to figure out what 's the best format for this representation .&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD F&#10;Content: And it 's still gonna be {disfmarker}&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD F&#10;Content: it 's still gonna be , uh , not direct .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: You know , it {disfmarker} Or another example was , you know , uh , where in the language {disfmarker} where in the word sequence are people interrupting ? So , I guess that one 's actually easier .&#10;Speaker: PhD D&#10;Content: What about {disfmarker} what about , um , the idea of using a relational database to , uh , store the information from the XML ? So you would have {disfmarker} XML basically would {disfmarker} Uh , you {disfmarker} you could use the XML to put the data in , and then when you get data out , you put it back in XML . So use XML as sort of the {disfmark" />
    <node id=" what to do .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: And especially for the prosody work , what {disfmarker} what it ends up being is you get features from the signal , and of course those change every time your alignments change . So you re - run a recognizer , you want to recompute your features , um , and then keep the database up to date .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Or you change a word , or you change a {vocalsound} utterance boundary segment , which is gonna happen a lot . And so I wanted something where {pause} all of this can be done in a elegant way and that if somebody wants to try something or compute something else , that it can be done flexibly . Um , it doesn't have to be pretty , it just has to be , you know , easy to use , and {disfmarker}&#10;Speaker: Grad C&#10;Content: Yeah , the other thing {disfmarker} We should look at ATLAS , the NIST thing ,&#10;Speaker: PhD F&#10;Content: Oh .&#10;Speaker:" />
    <node id=" that stay the same , the transcripts that may or may not change , and then the utterance which {disfmarker} where the time boundaries that may or may not change {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Oh , that 's {disfmarker} That 's actually very nicely handled here because you could {disfmarker} you could {disfmarker} all you 'd have to change is the , {vocalsound} um , time - stamps in the time - line without {disfmarker} without , uh , changing the I Ds .&#10;Speaker: PhD F&#10;Content: Um . And you 'd be able to propagate all of the {disfmarker} the information ?&#10;Speaker: Grad C&#10;Content: Right . That 's , the who that 's why you do that extra level of indirection . So that you can just change the time - line .&#10;Speaker: PhD A&#10;Content: Except the time - line is gonna be huge . If you say {disfmarker}&#10;Speaker: Grad C&#10;Content: Yes .&#10;Speaker: PhD F&#10;Content: Yeah ,&#10;Speaker:" />
    <node id="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data." />
    <node id=" {disfmarker}&#10;Speaker: Grad C&#10;Content: Yes .&#10;Speaker: PhD F&#10;Content: Yeah ,&#10;Speaker: PhD A&#10;Content: suppose you have a phone - level alignment .&#10;Speaker: PhD F&#10;Content: yeah , especially at the phone - level .&#10;Speaker: PhD A&#10;Content: You 'd have {disfmarker} you 'd have {disfmarker}&#10;Speaker: PhD F&#10;Content: The {disfmarker} we {disfmarker} we have phone - level backtraces .&#10;Speaker: Grad C&#10;Content: Yeah , this {disfmarker} I don't think I would do this for phone - level . I think for phone - level you want to use some sort of binary representation&#10;Speaker: PhD F&#10;Content: Um {disfmarker}&#10;Speaker: Grad C&#10;Content: because it 'll be too dense otherwise .&#10;Speaker: PhD F&#10;Content: OK . So , if you were doing that and you had this sort of companion , uh , thing that gets called up for phone - level , uh , what would that look like ?&#10;Speaker: PhD" />
    <node id="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." />
    <node id=" um {disfmarker} Yeah . You just have to know wha what to tie it to .&#10;Speaker: Grad C&#10;Content: Yeah , exactly . The problem is saying &quot; what are the semantics ,&#10;Speaker: PhD F&#10;Content: And {disfmarker}&#10;Speaker: Grad C&#10;Content: what do you mean by &quot; merge &quot; ? &quot;&#10;Speaker: PhD F&#10;Content: Right , right .&#10;Speaker: PhD A&#10;Content: Right . So {disfmarker} so just to let you know what we {disfmarker} where we kluged it by , uh , doing {disfmarker} uh , by doing {disfmarker} Hhh .&#10;Speaker: Grad C&#10;Content: So .&#10;Speaker: PhD A&#10;Content: Both were based on words , so , bo we have two versions of the same words intersp you know , sprinkled with {disfmarker} with different tags for annotations .&#10;Speaker: Grad C&#10;Content: And then you did diff .&#10;Speaker: PhD A&#10;Content: And we did diff . Exactly !&#10;Speaker: Grad C&#10;Content: Yeah , that '" />
    <node id=" Postdoc E&#10;Content: we sh change the boundaries of the units , it 's still unique and {disfmarker} and , uh , fits with the format ,&#10;Speaker: PhD F&#10;Content: Right .&#10;Speaker: Postdoc E&#10;Content: flexible , all that .&#10;Speaker: PhD A&#10;Content: Um , it would be nice {disfmarker} um , eh , gr this is sort of r regarding {disfmarker} uh , uh it 's related but not directly germane to the topic of discussion , but , when it comes to annotations , um , you often find yourself in the situation where you have {pause} different annotations {pause} of the same , say , word sequence . OK ?&#10;Speaker: Postdoc E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: And sometimes the word sequences even differ slightly because they were edited s at one place but not the other .&#10;Speaker: Postdoc E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So , once this data gets out there , some people might start annotating this for , I don't know , dialogue acts or , um , you know , topics or what" />
    <node id=" as , uh , you know , um , what was it ? uh ,&#10;Speaker: PhD F&#10;Content: Well , all the Switchboard in it .&#10;Speaker: PhD A&#10;Content: utterance types . There 's , uh , automatic , uh , punctuation and stuff like that .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Because we had one set of {pause} annotations that were based on , uh , one version of the transcripts with a particular segmentation , and then we had another version that was based on , uh , a different s slightly edited version of the transcripts with a different segmentation . So , {vocalsound} we had these two different versions which were {disfmarker} you know , you could tell they were from the same source but they weren't identical . So it was extremely hard {vocalsound} to reliably merge these two back together to correlate the information from the different annotations .&#10;Speaker: Grad C&#10;Content: Yep . I {disfmarker} I don't see any way that file formats are gonna help us with that .&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Grad C&#10;" />
    <node id=" - hmm .&#10;Speaker: PhD A&#10;Content: I mean , most people aren't as sophisticated as {disfmarker} as we are here with , you know , uh , time alignments and stuff . So {disfmarker} So the {disfmarker} the {disfmarker} the point is {disfmarker}&#10;Speaker: Grad C&#10;Content: Should {disfmarker} should we mention some names on the people who are n ?&#10;Speaker: PhD A&#10;Content: Right . So , um , the p my point is that {pause} you 're gonna end up with , uh , word sequences that are differently annotated . And {pause} you want some tool , uh , that is able to sort of merge these different annotations back into a single , uh , version . OK ? Um , and we had this problem very massively , uh , at SRI when we worked , uh , a while back on , {vocalsound} uh {disfmarker} well , on dialogue acts as well as , uh , you know , um , what was it ? uh ,&#10;Speaker: PhD F&#10;Content: Well , all the Switchboard in it" />
    <node id=" Exactly .&#10;Speaker: Grad C&#10;Content: Sure . But what if {disfmarker} what if they change the words ?&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Postdoc E&#10;Content: Not {disfmarker} Well , but you 'd have some anchoring point . He couldn't have changed all the words .&#10;Speaker: PhD D&#10;Content: But can they change the words without changing the time of the word ?&#10;Speaker: Grad C&#10;Content: Sure . But they could have changed it a little . The {disfmarker} the point is , that {disfmarker} that they may have annotated it off a word transcript that isn't the same as our word transcript , so how do you merge it back in ? I understand what you 're saying .&#10;Speaker: PhD A&#10;Content: Mmm . Mm - hmm .&#10;Speaker: Grad C&#10;Content: And I {disfmarker} I guess the answer is , um , it 's gonna be different every time . It 's j it 's just gonna be {disfmarker}&#10;Speaker: Postdoc E&#10;Content: Yeah" />
    <node id="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." />
    <node id="isfmarker} some function that merges two {disfmarker} two versions .&#10;Speaker: Grad C&#10;Content: Yeah , I think it 's gonna be very hard . Any sort of structured anything when you try to merge is really , really hard&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: because you ha i The hard part isn't the file format . The hard part is specifying what you mean by &quot; merge &quot; .&#10;Speaker: PhD A&#10;Content: Is {disfmarker} Exactly .&#10;Speaker: Grad C&#10;Content: And that 's very difficult .&#10;Speaker: PhD F&#10;Content: But the one thing that would work here actually for i that is more reliable than the utterances is the {disfmarker} the speaker ons and offs . So if you have a good ,&#10;Speaker: Grad C&#10;Content: But this is exactly what I mean , is that {disfmarker} that the problem i&#10;Speaker: PhD F&#10;Content: um {disfmarker} Yeah . You just have to know wha what to tie it to .&#10;Speaker: Grad C&#10;Content: Yeah" />
    <node id=" Grad C&#10;Content: that would make it very difficult to translate from one to the other .&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: I {disfmarker} I think if it 's conceptually close , and they already have or will have tools that everybody else will be using , I mean , {vocalsound} it would be crazy to do something s you know , separate that {disfmarker}&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Grad C&#10;Content: Yeah , we might as well . Yep .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: So I 'll {disfmarker} I 'll take a closer look at it .&#10;Speaker: PhD F&#10;Content: Actually , so it 's {disfmarker} that {disfmarker} that would really be the question , is just what you would feel is in the long run the best thing .&#10;Speaker: Grad C&#10;Content: And {disfmarker} Right .&#10;Speaker: PhD F&#10;Content: Cuz {vocalsound} once we start , sort" />
    <node id="isfmarker} two diffs f {comment} based on the same original .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Postdoc E&#10;Content: Is it S - diff ?&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: Postdoc E&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: Something like that , um , but operating on these lattices that are really what 's behind this {disfmarker} uh , this annotation format .&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: So {disfmarker}&#10;Speaker: Grad C&#10;Content: There 's actually a diff library you can use {pause} to do things like that that {disfmarker} so you have different formats .&#10;Speaker: PhD F&#10;Content: You could definitely do that with the {disfmarker}&#10;Speaker: PhD A&#10;Content: So somewhere in the API you would like to have like a merge or some {disfmarker} some function that merges two {disfmarker} two versions .&#10;Speaker: Grad C&#10;Content: Yeah , I think" />
    <node id="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." />
    <node id=" {disfmarker} other tag later in the file that would be something like , um , oh , I don't know , {comment} uh , {nonvocalsound} &quot; noise - type equals {nonvocalsound} door - slam &quot; . You know ? And then , uh , {nonvocalsound} you could either say &quot; time equals a particular time - mark &quot; or you could do other sorts of references . So {disfmarker} or {disfmarker} or you might have a prosody {disfmarker} &quot; Prosody &quot; right ? D ? T ? D ? T ? T ?&#10;Speaker: PhD F&#10;Content: It 's an O instead of an I , but the D is good .&#10;Speaker: Grad C&#10;Content: You like the D ? That 's a good D .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Um , you know , so you could have some sort of type here , and then you could have , um {disfmarker} the utterance that it 's referring to could be U - seventeen or something like that .&#10;Speaker: PhD F&#10;Content: OK" />
    <node id=" what they mean yourself .&#10;Speaker: PhD F&#10;Content: And why did you not choose that type of approach ?&#10;Speaker: Grad C&#10;Content: Uh , because I knew that we were doing speech , and I thought it was better if you 're looking at a raw file to be {disfmarker} t for the tags to say &quot; it 's an utterance &quot; , as opposed to the tag to say &quot; it 's a link &quot; .&#10;Speaker: PhD F&#10;Content: OK . OK .&#10;Speaker: Grad C&#10;Content: So , but {disfmarker}&#10;Speaker: PhD F&#10;Content: But other than that , are they compatible ? I mean , you could sort of {disfmarker}&#10;Speaker: Grad C&#10;Content: Yeah , they 're reasonably compatible .&#10;Speaker: PhD F&#10;Content: I mean , you {disfmarker} you could {disfmarker}&#10;Speaker: PhD D&#10;Content: You could probably translate between them .&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD F&#10;Content: Yeah , that 's w So ,&#10;Speaker: Grad C&#10;Content" />
    <node id=" T - eighteen &quot; . So what that 's saying is , we know it starts at this particular time . We don't know when it ends .&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Grad C&#10;Content: Right ? But it ends at this T - eighteen , which may be somewhere else . We say there 's another utterance . We don't know what the t time actually is but we know that it 's the same time as this end time .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Grad C&#10;Content: You know , thirty - eight , whatever you want .&#10;Speaker: PhD A&#10;Content: So you 're essentially defining a lattice .&#10;Speaker: Grad C&#10;Content: OK . Yes , exactly .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: And then , uh {disfmarker} and then these also have I Ds . Right ? So you could {disfmarker} you could have some sort of other {disfmarker} other tag later in the file that would be something like , um , oh , I don't know , {comment} uh ," />
    <node id="1. Relational database format: The team can consider storing the time-marked data, word transcripts, and annotations in a relational database instead of the P-files. This would allow for greater flexibility in computations and modifications by researchers.&#10;2. XML files: Since the team has been working with XML files, they could continue using this format as an alternative to the P-files, especially if there is existing software or tools that can handle XML data.&#10;3. Other tag or marker formats: The team can explore other tag or marker formats used in forced alignment programs to identify utterances and prosody markers. By evaluating their compatibility with the current system, they might find a more suitable alternative that still maintains the benefits of P-files, such as ease of use and familiarity.&#10;4. IP-API: If an API (Application Programming Interface) is available for the P-files, it could be used to facilitate communication between different systems or software components, making it easier to switch from P-files to another format if necessary.&#10;5. Utilities and libraries: The team should also consider any libraries or utilities available for other tag or marker formats, as these can help streamline the transition process and minimize the learning curve associated with adopting a new system." />
    <node id=" worked with P - files .&#10;Speaker: PhD F&#10;Content: Yeah ?&#10;Speaker: PhD D&#10;Content: I don't remember what the &quot; P &quot; is , though .&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Grad C&#10;Content: But there are ni they 're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P - files ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: so it works pretty well .&#10;Speaker: PhD A&#10;Content: &#10;Speaker: PhD F&#10;Content: And that isn't really , I guess , as important as the {disfmarker} the main {disfmarker} I don't know what you call it , the {disfmarker} the main sort of word - level {disfmarker}&#10;Speaker: Grad C&#10;Content: Neither do I .&#10;Speaker: PhD D&#10;Content: Probably stands for &quot; Phil &quot; . Phil Kohn .&#10;Speaker: Grad C&#10;Content: It 's a Phil file ?&#10;Speaker: PhD D&#10;Content: Yeah . That 's" />
    <node id=": So , I guess , yeah , if {disfmarker} if you and Don can {disfmarker} if you can show him the P - file stuff and see .&#10;Speaker: Grad C&#10;Content: Sure .&#10;Speaker: PhD F&#10;Content: So this would be like for the F - zero {disfmarker}&#10;Speaker: Grad B&#10;Content: True .&#10;Speaker: Grad C&#10;Content: I mean , if you do &quot; man P - file &quot; or &quot; apropos P - file &quot; , you 'll see a lot .&#10;Speaker: Grad B&#10;Content: I 've used the P - file , I think . I 've looked at it at least , briefly , I think when we were doing s something .&#10;Speaker: PhD A&#10;Content: What does the P stand for anyway ?&#10;Speaker: Grad C&#10;Content: I have no idea .&#10;Speaker: Grad B&#10;Content: Oh , in there .&#10;Speaker: Grad C&#10;Content: I didn't de I didn't develop it . You know , it was {disfmarker} I think it was Dave Johnson . So it 's all part of the Quick" />
    <node id="&#10;Speaker: Grad B&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Sure .&#10;Speaker: PhD F&#10;Content: um {disfmarker} I think we 're {disfmarker} we 're {disfmarker} {vocalsound} we 're actually just {disfmarker}&#10;Speaker: Grad C&#10;Content: We 're about done .&#10;Speaker: PhD F&#10;Content: yeah ,&#10;Speaker: Grad B&#10;Content: Hmm .&#10;Speaker: PhD F&#10;Content: wrapping up , but , um {disfmarker} Yeah , sorry , it 's a uh short meeting , but , um {disfmarker} Well , I don't know . Is there anything else , like {disfmarker} I mean that helps me a lot ,&#10;Speaker: Grad C&#10;Content: Well , I think the other thing we might want to look at is alternatives to P - file .&#10;Speaker: PhD F&#10;Content: but {disfmarker}&#10;Speaker: Grad C&#10;Content: I mean , th the reason I like P - file is I 'm already familiar with it , we have" />
    <node id="}&#10;Speaker: Grad C&#10;Content: I mean , th the reason I like P - file is I 'm already familiar with it , we have expertise here , and so if we pick something else , there 's the learning - curve problem . But , I mean , it is just something we developed at ICSI .&#10;Speaker: PhD A&#10;Content: Is there an {disfmarker} is there an IP - API ?&#10;Speaker: Grad C&#10;Content: And so {disfmarker} Yeah .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Grad C&#10;Content: There 's an API for it . And , uh ,&#10;Speaker: PhD A&#10;Content: There used to be a problem that they get too large ,&#10;Speaker: Grad C&#10;Content: a bunch of libraries , P - file utilities .&#10;Speaker: PhD A&#10;Content: and so {pause} basically the {disfmarker} uh the filesystem wouldn't {disfmarker}&#10;Speaker: Grad C&#10;Content: Well , that 's gonna be a problem no matter what . You have the two - gigabyte limit on the filesystem size . And we definitely hit" />
    <node id=" didn't develop it . You know , it was {disfmarker} I think it was Dave Johnson . So it 's all part of the Quicknet library . It has all the utilities for it .&#10;Speaker: PhD A&#10;Content: No , P - files were around way before Quicknet . P - files were {disfmarker} were around when {disfmarker} w with , um , {vocalsound} RAP .&#10;Speaker: Grad C&#10;Content: Oh , were they ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Right ?&#10;Speaker: PhD F&#10;Content: It 's like the history of ICSI .&#10;Speaker: PhD A&#10;Content: You worked with P - files .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Like {disfmarker}&#10;Speaker: PhD D&#10;Content: No .&#10;Speaker: PhD A&#10;Content: I worked with P - files .&#10;Speaker: PhD F&#10;Content: Yeah ?&#10;Speaker: PhD D&#10;Content: I don't remember what the" />
    <node id="1. The concept being discussed refers to the creation of a single time-line with unique identifiers (IDs) attached to different sections in a transcription file. This approach allows for consistent annotation and easy reference between various sections within the transcription. By starting the timeline tag with a consistent marker, such as 'utt' for utterances, the team can maintain clarity and consistency in their annotation process. This concept is significant because it enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database." />
    <node id="er}&#10;Speaker: Grad C&#10;Content: You mean , this {disfmarker} I guess I am gonna be standing up and drawing on the board .&#10;Speaker: PhD F&#10;Content: OK , yeah . So you should , definitely .&#10;Speaker: Grad C&#10;Content: Um , so {disfmarker} so it definitely had that as a concept . So tha it has a single time - line ,&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: and then you can have lots of different sections , each of which have I Ds attached to it , and then you can refer from other sections to those I Ds , if you want to . So that , um {disfmarker} so that you start with {disfmarker} with a time - line tag . &quot; Time - line &quot; . And then you have a bunch of times . I don't e I don't remember exactly what my notation was ,&#10;Speaker: PhD A&#10;Content: Oh , I remember seeing an example of this .&#10;Speaker: Grad C&#10;Content: but it {disfmarker}&#10;Speaker: PhD F&#10;Content" />
    <node id="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments." />
    <node id=" nature . They 'll be looking at either a word - level prosodic , uh , an {disfmarker} a value ,&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: like a continuous value , like the slope of something . But you know , we 'll do something where we {disfmarker} some kind of data reduction where the prosodic features are sort o uh , either at the word - level or at the segment - level ,&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: or {disfmarker} or something like that . They 're not gonna be at the phone - level and they 're no not gonna be at the frame - level when we get done with sort of giving them simpler shapes and things . And so the main thing is just being able {disfmarker} Well , I guess , the two goals . Um , one that Chuck mentioned is starting out with something that we don't have to start over , that we don't have to throw away if other people want to extend it for other kinds of questions ,&#10;Speaker: Grad C&#10;Content: Right .&#10;Spe" />
    <node id="1. The abbreviation &quot;P&quot; in &quot;P-files&quot; most likely stands for &quot;Phil,&quot; referring to Phil Kohn. This conclusion can be drawn from the comment made by PhD D: &quot;Probably stands for 'Phil.' Phil Kohn.&quot; (Speaker: PhD D, Content: Probably stands for &quot;Phil&quot;. Phil Kohn.)&#10;&#10;2. The significance of P-files in this context is their role as a file format used for storing time-marked data, word transcripts, and annotations in speech research. As mentioned by Grad C, the Quicknet library has utilities to handle these files efficiently. (Speaker: Grad C, Content: But there are nice they're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P-files.)" />
    <node id="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data." />
    <node id="1. The process of handling forced alignment when time boundaries of words in transcriptions change due to a new recognition output involves using a flexible database format, such as a relational database that stores information from XML files. This allows for easy input and output of data while maintaining flexibility for further computations or modifications by researchers.&#10;2. When alignments, word transcripts, or utterance boundaries are changed, the system enables recomputing features and updating entries. The time-stamps in the time-line can be adjusted without altering the IDs of the entries, preserving the original time-stamps and IDs.&#10;3. A consistent marker, like 'utt' for utterances, is used to start the timeline tag, ensuring clarity and consistency in the annotation process. This concept enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;4. When different annotations of the same word sequences emerge as the data gets shared and used by others, a tool or method to reliably merge these different annotations into a single version is needed. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." />
    <node id="1. Frames: The basic unit of data storage in a P-file is the frame, which likely corresponds to a small segment of audio or speech.&#10;2. Utterances and Sentences: Frames are grouped together into higher-level structures such as utterances and sentences. These groupings provide a more meaningful organization for time-marked data, word transcripts, and annotations.&#10;3. Arbitrary Vector of Values: Each P-file can contain an arbitrary vector of values, which may include different types such as floats, integers, or doubles. This vector could be used to store various features or attributes related to the frames, utterances, or sentences within the file.&#10;4. Header Format: The P-file has a header format that describes some aspects of its contents. Specifically, it stores information about utterance numbers and frame numbers, allowing for easy reference and organization of the data within the file.&#10;5. Cepstral or PLP Values: In the context of speech research, each P-file might store a vector of cepstral or Perceptual Linear Prediction (PLP) values for each frame, which are commonly used features in speech processing and analysis." />
    <node id="aker: PhD D&#10;Content: right ? Right .&#10;Speaker: Grad C&#10;Content: So that {disfmarker} what 's nice about the P - file {disfmarker} It {disfmarker} i Built into it is the concept of {pause} frames , utterances , sentences , that sort of thing , that structure . And then also attached to it is an arbitrary vector of values . And it can take different types .&#10;Speaker: PhD F&#10;Content: Oh .&#10;Speaker: Grad C&#10;Content: So it {disfmarker} th they don't all have to be floats . You know , you can have integers and you can have doubles , and all that sort of stuff .&#10;Speaker: PhD F&#10;Content: So that {disfmarker} that sounds {disfmarker} that sounds about what I w&#10;Speaker: Grad C&#10;Content: Um . Right ? And it has a header {disfmarker} it has a header format that {pause} describes it {pause} to some extent . So , the only problem with it is it 's actually storing the {pause} utterance numbers and the {pause} frame numbers in the" />
    <node id="Content: OK . So {disfmarker}&#10;Speaker: Grad C&#10;Content: I would use something tighter than P - files .&#10;Speaker: PhD F&#10;Content: Do you {disfmarker} Are you familiar with it ?&#10;Speaker: Grad C&#10;Content: So .&#10;Speaker: PhD F&#10;Content: I haven't seen this particular format ,&#10;Speaker: PhD A&#10;Content: I mean , I 've {disfmarker} I 've used them .&#10;Speaker: PhD F&#10;Content: but {disfmarker}&#10;Speaker: PhD A&#10;Content: I don't know what their structure is .&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: I 've forgot what the str&#10;Speaker: PhD D&#10;Content: But , wait a minute , P - file for each frame is storing a vector of cepstral or PLP values ,&#10;Speaker: Grad C&#10;Content: It 's whatever you want , actually .&#10;Speaker: PhD D&#10;Content: right ? Right .&#10;Speaker: Grad C&#10;Content: So that {disfmarker} what 's nice" />
    <node id=" values&#10;Speaker: Grad C&#10;Content: Yeah , I mean , for something like that I would use P - file&#10;Speaker: PhD F&#10;Content: depending on {disfmarker}&#10;Speaker: Grad C&#10;Content: or {disfmarker} or any frame - level stuff I would use P - file .&#10;Speaker: PhD F&#10;Content: Meaning {disfmarker} ?&#10;Speaker: Grad C&#10;Content: Uh , that 's a {disfmarker} well , or something like it . It 's ICS uh , ICSI has a format for frame - level representation of features . Um .&#10;Speaker: PhD F&#10;Content: OK . That you could call {disfmarker} that you would tie into this representation with like an ID .&#10;Speaker: Grad C&#10;Content: Right . Right . Or {disfmarker} or there 's a {disfmarker} there 's a particular way in XML to refer to external resources .&#10;Speaker: PhD F&#10;Content: And {disfmarker} OK .&#10;Speaker: Grad C&#10;Content: So you would say &quot; refer to this external file &quot; ." />
    <node id="1. To efficiently extract and distinguish words within a prosodic boundary and those within a &quot;disfluency marker,&quot; the researchers can utilize their proposed relational database format to store this information from XML files. This would allow for flexibility in computations and modifications by researchers. They can create specific tags or identifiers for prosodic boundaries, disfluency markers, and individual words. By querying the database with these specific tags, they can extract all the words associated with a particular prosodic boundary or disfluency marker.&#10;2. For example, they can use a tag like 'utt' to identify utterances (units of speech starting with a pause and ending with a pause) and 'disfmarker' for disfluency markers. When a word is associated with both a prosodic boundary and a disfluency marker, it would have both tags. By using a relational database, they can efficiently query the data to extract words based on these tags and distinguish between words within prosodic boundaries and those within disfluency markers.&#10;&#10;In summary, by implementing a relational database system with specific tags for prosodic boundaries, sentences, and disfluency markers, the researchers can efficiently extract and distinguish words associated with each category. This system allows for easier computation and analysis of speech data while maintaining clarity and consistency in the annotation process." />
    <node id=" Grad C&#10;Content: And then lower down you would say &quot; here 's a prosodic boundary and it has these words in it &quot; . And lower down you 'd have &quot; here 's a sentence ,&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: An - Right .&#10;Speaker: Grad C&#10;Content: and it has these words in it &quot; .&#10;Speaker: PhD F&#10;Content: So you would be able to go in and say , you know , &quot; give me all the words in the bound in the prosodic phrase&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD F&#10;Content: and give me all the words in the {disfmarker} &quot; Yeah .&#10;Speaker: Grad C&#10;Content: So I think that 's {disfmarker} that would wor&#10;Speaker: PhD F&#10;Content: Um , OK .&#10;Speaker: Grad C&#10;Content: Let me look at it again .&#10;Speaker: PhD A&#10;Content: Mm - hmm . The {disfmarker} the o the other issue that you had was , how do you actually efficiently extract , um {disfmark" />
    <node id=" say this {disfmarker} this word is part of that sentence and this prosodic phrase .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: But the phrase is not part of the sentence&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and neither is the sentence part of the phrase .&#10;Speaker: PhD F&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: I I 'm pretty sure that you can do that , but I 'm forgetting the exact level of nesting .&#10;Speaker: PhD A&#10;Content: So , you would have to have {vocalsound} two different pointers from the word up {disfmarker} one level up , one to the sent&#10;Speaker: Grad C&#10;Content: So {disfmarker} so what you would end up having is a tag saying &quot; here 's a word , and it starts here and it ends here &quot; .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: And then lower down you would say &quot; here 's a prosodic boundary and it has these words in it &quot; . And lower down" />
    <node id="The sharing aspect is important in the context of storing feature vectors on a frame-by-frame basis for two main reasons: adaptability and collaboration. By making the data more accessible and usable, other researchers can adopt and build upon this work more easily.&#10;&#10;1. Adaptability: Creating a system where prosodic features are represented as continuous values at the word or segment level allows for greater flexibility in analyzing speech data. This approach makes it easier to adapt the system to various research questions and domains, as researchers can work with simpler and more meaningful structures (words or segments) instead of being constrained by specific phones or frames.&#10;2. Collaboration: By sharing the feature vectors and related data, researchers enable better collaboration within their field. Other researchers can use this data for their own studies, validate findings, or develop new methods based on the existing work. This not only fosters a sense of community but also helps to advance the field more rapidly by building upon previous research efforts.&#10;&#10;In the context of ten-minute Switchboard conversations, sharing feature vectors on a frame-by-frame basis allows researchers to create a robust and adaptable system for prosodic analysis. The shared data can help address the challenges associated with managing large files containing numerous frames while maintaining flexibility for further computations or modifications by researchers. Additionally, having easily accessible and usable data promotes collaboration and knowledge sharing within the research community, which is essential for advancing our understanding of speech processing and analysis." />
    <node id=" ten - minute Switchboard conversations ,&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: and {disfmarker} So it 's doable , it 's just that you can only store a feature vector at frame - by - frame and it doesn't have any kind of ,&#10;Speaker: PhD D&#10;Content: Is {disfmarker} is the sharing part of this a pretty important {pause} consideration&#10;Speaker: PhD F&#10;Content: um {disfmarker}&#10;Speaker: PhD D&#10;Content: or does that just sort of , uh {disfmarker} a nice thing to have ?&#10;Speaker: PhD F&#10;Content: I {disfmarker} I don't know enough about what we 're gonna do with the data . But I thought it would be good to get something that we can {disfmarker} that other people can use or adopt for their own kinds of encoding . And just , I mean we have to use some we have to make some decision about what to do .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: And especially for the prosody work ," />
    <node id=" of frame - by - frame .&#10;Speaker: Grad C&#10;Content: Ah , right .&#10;Speaker: PhD F&#10;Content: Which is fine , but it gets unwieldy to go in and {disfmarker} and query these files with really huge files .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: I mean , we could do it . I was just thinking if there 's something that {disfmarker} where all the frame values are {disfmarker}&#10;Speaker: Grad C&#10;Content: And a and again , if you have a {disfmarker} if you have a two - hour - long meeting , that 's gonna {disfmarker}&#10;Speaker: PhD F&#10;Content: Hmm ? They 're {disfmarker} they 're fair they 're quite large .&#10;Speaker: Grad C&#10;Content: Yeah , I mean , they 'd be emo enormous .&#10;Speaker: PhD F&#10;Content: And these are for ten - minute Switchboard conversations ,&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: and {disfmark" />
    <node id="Based on the discussion, the restriction for adding supplementary information to a type specified as 'utt' is that it cannot be directly added to the existing 'utt' type. This is because 'utt' is used to identify an utterance in the transcription file, and changing this type might affect its consistency across different annotation files.&#10;&#10;If the team wants to add supplementary information, they would need to create a new type, which would require defining the criteria and rules for that specific type. This process involves careful consideration of how the new type will interact with existing types and the overall structure of the database. It may also involve additional work in modifying annotation scripts or tools to accommodate the new type.&#10;&#10;To make a different type, the team would need to:&#10;1. Determine the purpose and criteria for the new type&#10;2. Define how it will interact with other types in the database&#10;3. Modify annotation scripts or tools to recognize and process the new type, if necessary&#10;4. Test the new type to ensure compatibility with existing annotations and the forced alignment program." />
    <node id="Grad C is referring to the possibility of an additional level of complexity in the segmentation of a word sequence. In this conversation, two levels of segmentation have already been mentioned: sentences and prosodic phrases. Grad C suggests that there might be another level, as yet unconsidered, which adds further structure to the word sequence." />
    <node id="aker: PhD F&#10;Content: You have to have another type then , I guess .&#10;Speaker: PhD A&#10;Content: s Um , well , s let 's {disfmarker} let 's ta so let 's {disfmarker}&#10;Speaker: Grad C&#10;Content: Well , I think I 'm {disfmarker} I think w I had better look at it again&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: so {disfmarker}&#10;Speaker: Grad C&#10;Content: because I {disfmarker} I 'm {disfmarker}&#10;Speaker: PhD F&#10;Content: OK . OK .&#10;Speaker: PhD A&#10;Content: y So for instance @ @ {comment} sup&#10;Speaker: Grad C&#10;Content: There 's one level {disfmarker} there 's one more level of indirection that I 'm forgetting .&#10;Speaker: PhD A&#10;Content: Suppose you have a word sequence and you have two different segmentations of that same word sequence . f Say , one segmentation is in terms of , um , you know , uh , sentences ." />
    <node id=" you have two different segmentations of that same word sequence . f Say , one segmentation is in terms of , um , you know , uh , sentences . And another segmentation is in terms of , um , {vocalsound} I don't know , {comment} prosodic phrases . And let 's say that they don't {pause} nest . So , you know , a prosodic phrase may cross two sentences or something .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: I don't know if that 's true or not but {vocalsound} let 's as&#10;Speaker: PhD F&#10;Content: Well , it 's definitely true with the segment .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: That 's what I {disfmarker} exactly what I meant by the utterances versus the sentence could be sort of {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah . So , you want to be s you want to say this {disfmarker} this word is part of that sentence and this prosodic phrase .&#10;Speaker: PhD F&#10;Content: Yeah ." />
    <node id=" C&#10;Content: So {disfmarker} so imagine {disfmarker} I think his {disfmarker} his example is a good one . Imagine that this person who developed the corpus of the referring expressions didn't include time .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Grad C&#10;Content: He included references to words .&#10;Speaker: Postdoc E&#10;Content: Ach !&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: He said that at this word is when {disfmarker} when it happened .&#10;Speaker: Postdoc E&#10;Content: Well , then {disfmarker}&#10;Speaker: PhD A&#10;Content: Or she .&#10;Speaker: Grad C&#10;Content: Or she .&#10;Speaker: Postdoc E&#10;Content: But then couldn't you just indirectly figure out the time {pause} tied to the word ?&#10;Speaker: PhD F&#10;Content: But still they {disfmarker} Exactly .&#10;Speaker: Grad C&#10;Content: Sure . But what if {disfmarker} what if they change the words ?&#10;Speaker" />
    <node id="Content: Well , channel or speaker or whatever .&#10;Speaker: PhD F&#10;Content: I mean , w yeah , channel is what the channelized output out&#10;Speaker: PhD A&#10;Content: It doesn't {disfmarker}&#10;Speaker: Grad C&#10;Content: This isn't quite right .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: I have to look at it again .&#10;Speaker: PhD F&#10;Content: Yeah , but {disfmarker}&#10;Speaker: PhD A&#10;Content: But {disfmarker} but {disfmarker} so how in the NIST format do we express {vocalsound} a hierarchical relationship between , um , say , an utterance and the words within it ? So how do you {pause} tell {pause} that {pause} these are the words that belong to that utterance ?&#10;Speaker: Grad C&#10;Content: Um , you would have another structure lower down than this that would be saying they 're all belonging to this ID .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: So each thing refers to" />
    <node id=" this ID .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: So each thing refers to the {pause} utterance that it belongs to .&#10;Speaker: Grad C&#10;Content: Right . And then each utterance could refer to a turn ,&#10;Speaker: PhD D&#10;Content: So it 's {disfmarker} it 's not hi it 's sort of bottom - up .&#10;Speaker: Grad C&#10;Content: and each turn could refer to something higher up .&#10;Speaker: PhD F&#10;Content: And what if you actually have {disfmarker} So right now what you have as utterance , um , the closest thing that comes out of the channelized is the stuff between the segment boundaries that the transcribers put in or that Thilo put in , which may or may not actually be , like , a s it 's usually not {disfmarker} um , the beginning and end of a sentence , say .&#10;Speaker: Grad C&#10;Content: Well , that 's why I didn't call it &quot; sentence &quot; .&#10;Speaker: PhD F&#10;Content: So , right . Um , so" />
    <node id="1. The limitation of frame-level analysis for phone-level speech processing, as discussed by the Ph.D. speakers, is that most frames are not actual speech, but silent or transitional periods between sounds. This makes frame-level analysis less reliable for phone-level speech processing since phones can undergo significant modification during merging processes, and analyzing at this level increases computational burden without adding meaningful value to the analysis of prosodic features.&#10;   &#10;2. The average number of phones in an English word is not explicitly stated in the transcript, but it is mentioned that there are approximately five frames per phone. This suggests that the average number of phones in an English word could be around five or more, given that some words might have additional phones due to affixes or other phonetic processes." />
    <node id=" limit on most O Ss .&#10;Speaker: PhD A&#10;Content: Right , OK . I would say {disfmarker} OK , so frame - level is probably not a good idea . But for phone - level stuff it 's perfectly {disfmarker}&#10;Speaker: PhD F&#10;Content: And th it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Like phones , or syllables , or anything like that .&#10;Speaker: PhD F&#10;Content: Phones are every five frames though , so . Or something like that .&#10;Speaker: PhD A&#10;Content: But {disfmarker} but {disfmarker} but most of the frames are actually not speech . So , you know , people don't {disfmarker} v Look at it , words times the average {disfmarker} The average number of phones in an English word is , I don't know , {comment} five maybe ?&#10;Speaker: PhD F&#10;Content: Yeah , but we actually {disfmarker}&#10;Speaker: PhD A&#10;Content: So , look at it , t number of words times five . That 's not {disf" />
    <node id="marker}&#10;Speaker: PhD A&#10;Content: So , look at it , t number of words times five . That 's not {disfmarker} that not {disfmarker}&#10;Speaker: PhD F&#10;Content: Oh , so you mean pause phones take up a lot of the {disfmarker} long pause phones .&#10;Speaker: PhD A&#10;Content: Exactly .&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Yeah . OK . That 's true . But you do have to keep them in there . Y yeah .&#10;Speaker: Grad C&#10;Content: So I think it {disfmarker} it 's debatable whether you want to do phone - level in the same thing .&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Grad C&#10;Content: But I think , a anything at frame - level , even P - file , is too verbose .&#10;Speaker: PhD F&#10;Content: OK . So {disfmarker}&#10;Speaker: Grad C&#10;Content: I would use something tighter than P - files .&#10;Spe" />
    <node id="The issue at hand is the difficulty in reliably merging different versions of word transcripts, especially when those changes are tied to acoustic segments. The speakers express concern that crucial information may be lost during the merge process, making it challenging to combine different annotations or coding methods. They discuss the need for a stable reference point, such as speaker ons and offs or acoustic segments with time-marks, to reduce ambiguity in the merging process. However, they acknowledge that this information might not always be available or correctly preserved during merges. The aim is to create a consistent and clear annotation system that can handle changes in word transcripts while maintaining the original time-stamps and IDs of entries in a relational database." />
    <node id=" so {disfmarker}&#10;Speaker: PhD F&#10;Content: But , d isn't that something where whoever {disfmarker} if {vocalsound} {disfmarker} if the people who are making changes , say in the transcripts , cuz this all happened when the transcripts were different {disfmarker} ye um , if they tie it to something , like if they tied it to the acoustic segment {disfmarker} if they {disfmarker} You know what I mean ? Then {disfmarker} Or if they tied it to an acoustic segment and we had the time - marks , that would help .&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD F&#10;Content: But the problem is exactly as Adam said , that you get , you know , y you don't have that information or it 's lost in the merge somehow ,&#10;Speaker: Postdoc E&#10;Content: Well , can I ask one question ?&#10;Speaker: PhD F&#10;Content: so {disfmarker}&#10;Speaker: Postdoc E&#10;Content: It {disfmarker} it seems to me that , um , we will have o an" />
    <node id="The goals of the project involve creating a system where prosodic features are represented as continuous values at the word or segment level, rather than being tied to specific phones or frames. This approach allows for more flexibility and easier computation in understanding and analyzing speech data. The researchers prefer simpler shapes at the word or segment level because it provides a good balance between granularity and complexity, reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;&#10;The methods to achieve these goals include using a relational database format to store word transcripts, discourse codings, and time-marked data from XML files. This approach provides flexibility for further computations or modifications by researchers. The team is also considering using consistent markers (e.g., 'utt' for utterances) to identify units of speech and potentially prosody markers to denote aspects related to pitch, stress, or intonation in speech data. Additionally, the project aims to define clear merging criteria and rules while utilizing speaker ons and offs as reliable reference points for tying other annotations together, thereby reducing ambiguity in the merging process." />
    <node id="Based on the discussion transcript, the team is considering using a relational database in a format that stores information from XML files to express the type of a sentence in a file. This would allow flexibility for further computations or modifications by researchers. The exact tag or marker for this information is not explicitly stated in the transcript. However, they discuss the importance of maintaining consistency and clarity in the annotation process, as well as ensuring that any new types created can interact properly with existing types in the database. Therefore, careful consideration and definition of the criteria and rules for the new type would be required before implementation." />
    <node id="1. The recommended way to handle data that has been edited and distributed among multiple parties for accurate merging involves the use of a reliable merging tool or method that can incorporate different annotations or coding methods while maintaining unique word boundaries in a consistent manner.&#10;2. Utilizing a stable reference point, such as speaker onsets and offsets or acoustic segments with time-marks, can help reduce ambiguity during the merging process.&#10;3. Carefully defining the criteria and rules for merging is crucial to specify what is meant by &quot;merging&quot; different annotations or coding methods.&#10;4. Implementing a flexible database system, like a relational database that stores information from XML files, can accommodate modifications in word transcripts and time boundaries due to new recognition outputs or changes in forced alignment. This system allows for recomputing features and updating entries while preserving original time-stamps and IDs of the entries.&#10;5. Ensuring consistency and clarity in the annotation process by using clear markers, like 'utt' for utterances, can help manage changes in word transcripts or utterance boundaries, ultimately facilitating accurate merging of edited data from multiple parties." />
    <node id="The speakers discuss an efficient way to extract and find information in a complex structure by using a relational database that stores information from XML files. This approach allows flexibility for further computations or modifications by researchers, ensuring consistency and clarity in the annotation process. They also mention the importance of maintaining unique word boundaries while incorporating different annotations or coding methods, acknowledging that changes in word transcripts might create challenges during merges. However, they do not explicitly provide a solution for efficiently extracting and finding information in complex structures in this transcript.&#10;&#10;To efficiently extract and find information in a complex structure like an XML file or relational database, you can use query languages such as XQuery for XML files or SQL (Structured Query Language) for relational databases. These query languages allow users to search, filter, and manipulate data based on specific criteria, making it easier to locate the desired information within large and complex datasets. To implement this solution, further details about the structure of the data and the desired information would be required." />
    <node id=" hmm . The {disfmarker} the o the other issue that you had was , how do you actually efficiently extract , um {disfmarker} find and extract information in a structure of this type ?&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Grad C&#10;Content: So .&#10;Speaker: PhD F&#10;Content: That 's good .&#10;Speaker: PhD A&#10;Content: So you gave some examples like {disfmarker}&#10;Speaker: PhD F&#10;Content: Well , uh , and , I mean , you guys might {disfmarker} I don't know if this is premature because I suppose once you get the representation you can do this , but the kinds of things I was worried about is ,&#10;Speaker: PhD A&#10;Content: No , that 's not clear .&#10;Speaker: PhD F&#10;Content: uh {disfmarker}&#10;Speaker: PhD A&#10;Content: I mean , yeah , you c sure you can do it ,&#10;Speaker: PhD F&#10;Content: Well , OK . So i if it {disfmarker}&#10;Speaker: PhD A&#10;Content: but can you do it sort of l" />
    <node id="Speaker PhD A expresses skepticism about being able to perform meaningful actions or recover information from a text representation that does not reflect the structure of the original words and annotations. They argue that the text representation will not contain sufficient information to accurately reconstruct the original annotations, as it does not capture the structural relationships between words and annotations.&#10;&#10;Speaker Grad C disagrees with this assessment, arguing that it is still possible to recover the original structure from the text representation. They suggest that by tagging certain features of the text (such as sentence boundaries), it may be possible to reconstruct the original annotations with a high degree of accuracy.&#10;&#10;Overall, the disagreement between Speaker PhD A and Speaker Grad C centers on whether it is possible to accurately recover the original structure of words and annotations from a text representation that does not explicitly encode this information. While Speaker PhD A is skeptical of this approach, Speaker Grad C believes that it may be possible with the use of additional tags or markers." />
    <node id=" ?&#10;Speaker: PhD A&#10;Content: But this is {disfmarker}&#10;Speaker: Grad C&#10;Content: So , as opposed to {disfmarker}&#10;Speaker: PhD A&#10;Content: I {disfmarker} I 'm still , um , {vocalsound} not convinced that you can do much at all on the text {disfmarker} on the flat file that {disfmarker} that {disfmarker} you know , the text representation . e Because the text representation is gonna be , uh , not reflecting the structure of {disfmarker} of your words and annotations . It 's just {disfmarker} it 's {disfmarker}&#10;Speaker: Grad C&#10;Content: Well , if it 's not representing it , then how do you recover it ? Of course it 's representing it .&#10;Speaker: PhD A&#10;Content: No . You {disfmarker} you have to {disfmarker} what you have to do is you have to basically {disfmarker}&#10;Speaker: Grad C&#10;Content: That 's the whole point .&#10;Speaker: PhD A" />
    <node id=": Well , that 's why I didn't call it &quot; sentence &quot; .&#10;Speaker: PhD F&#10;Content: So , right . Um , so it 's like a segment or something .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: So , I mean , I assume this is possible , that if you have {disfmarker} someone annotates the punctuation or whatever when they transcribe , you can say , you know , from {disfmarker} for {disfmarker} from the c beginning of the sentence to the end of the sentence , from the annotations , this is a unit , even though it never actually {disfmarker} i It 's only a unit by virtue of the annotations {pause} at the word - level .&#10;Speaker: Grad C&#10;Content: Sure . I mean , so you would {disfmarker} you would have yet another tag .&#10;Speaker: PhD F&#10;Content: And then that would get a tag somehow .&#10;Speaker: Grad C&#10;Content: You 'd have another tag which says this is of type &quot; sentence &quot; .&#10;Speaker: PhD F&#10;Content: OK . OK" />
    <node id="1. Potential uses of a system for extracting phone-level information from XML files include:&#10;   - Prosodic feature analysis at a fine-grained level (although the speakers express concerns about reliability and increased computational burden)&#10;   - Facilitating time-aligned annotation of phones, which could be useful for specific research questions or applications requiring detailed phonetic information." />
    <node id="Yes, it is possible for a prosodic phrase to span across two sentences in the scenario described. This possibility is acknowledged by the researchers when they discuss different segmentation levels that do not nest, meaning they may not fit neatly within each other. In this case, a prosodic phrase might start in one sentence and end in another without being contained entirely within either sentence." />
    <node id="1. The group is discussing the possibility of an additional level of complexity in segmenting a word sequence, beyond sentences and prosodic phrases (as suggested by Grad C). This third level would add further structure to the word sequence.&#10;2. It is acknowledged that prosodic phrases can span across two sentences, not being contained entirely within either sentence (this is accepted when they discuss different segmentation levels that do not nest).&#10;3. The idea of overlapping codes or concepts is mentioned, but it is decided that they don't need to delve into the specifics of the coding system since it's not the main topic of the meeting." />
    <node id=" A&#10;Content: Hhh .&#10;Speaker: PhD F&#10;Content: it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: You would just have a r&#10;Speaker: PhD F&#10;Content: S&#10;Speaker: Grad C&#10;Content: or do you say this is part of this ? I think {disfmarker}&#10;Speaker: PhD D&#10;Content: You would refer up to the sentence .&#10;Speaker: PhD F&#10;Content: But they 're {disfmarker}&#10;Speaker: PhD A&#10;Content: Well , the thing {disfmarker}&#10;Speaker: PhD F&#10;Content: they 're actually overlapping each other , sort of .&#10;Speaker: Grad C&#10;Content: So {disfmarker}&#10;Speaker: PhD A&#10;Content: the thing is that some something may be a part of one thing for one purpose and another thing of another purpose .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: So f&#10;Speaker: PhD F&#10;Content: You have to have another type then , I guess .&#10;Speaker: PhD A&#10;Content: s Um , well ," />
    <node id=": PhD D&#10;Content: Uh .&#10;Speaker: Postdoc E&#10;Content: What , the codes themselves ?&#10;Speaker: PhD D&#10;Content: Well , th overlap codes .&#10;Speaker: Postdoc E&#10;Content: Or the {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I 'm not sure what that @ @ {disfmarker}&#10;Speaker: Grad C&#10;Content: Well , I mean , is that {disfmarker}&#10;Speaker: PhD D&#10;Content: It probably doesn't matter .&#10;Speaker: Postdoc E&#10;Content: Well , we don't have to go into the codes .&#10;Speaker: Grad C&#10;Content: I mean , it doesn't .&#10;Speaker: PhD D&#10;Content: No , I d&#10;Speaker: Postdoc E&#10;Content: We don't have to go into the codes .&#10;Speaker: Grad C&#10;Content: I mean , that {disfmarker} not for the topic of this meeting .&#10;Speaker: Postdoc E&#10;Content: But let me just {disfmarker} No . W the idea is just to have a separate green ribbon , you know , and {" />
    <node id="1. More compact data storage: By referring to an external file, the size of the primary file or format can be significantly reduced, making it more efficient and easier to manage during long meetings with multiple frames. This is especially beneficial when dealing with large datasets.&#10;2. Reduced computational burden: Storing information in an external file may decrease the computational load on the system since only the essential data needs to be loaded into memory for processing during the meeting.&#10;3. Easier organization and maintenance: External files can help keep related but separate information organized, making it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files can facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components." />
    <node id="Content: And {disfmarker} OK .&#10;Speaker: Grad C&#10;Content: So you would say &quot; refer to this external file &quot; . Um , so that external file wouldn't be in {disfmarker}&#10;Speaker: PhD F&#10;Content: So that might {disfmarker} that might work .&#10;Speaker: PhD D&#10;Content: But what {disfmarker} what 's the advantage of doing that versus just putting it into this format ?&#10;Speaker: Grad C&#10;Content: More compact , which I think is {disfmarker} is better .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Grad C&#10;Content: I mean , if you did it at this {disfmarker}&#10;Speaker: PhD F&#10;Content: I mean these are long meetings and with {disfmarker} for every frame ,&#10;Speaker: Grad C&#10;Content: You don't want to do it with that {disfmarker} Anything at frame - level you had better encode binary&#10;Speaker: PhD F&#10;Content: um {disfmarker}&#10;Speaker: Grad C&#10;Content: or it 's gonna" />
    <node id="1. More compact data storage: By storing additional or detailed information in an external file, the primary file remains smaller and more manageable during long meetings with multiple frames. This is particularly useful when dealing with large datasets.&#10;2. Reduced computational burden: Loading only essential data into memory for processing can decrease the computational load on the system, making it run more efficiently.&#10;3. Easier organization and maintenance: External files help keep related but separate information organized and make it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components." />
    <node id="1. Based on Grad C's observations a year ago, the annotated transcription graph system was not very complete, lacking external file format representation at that time. The only existing component was a conceptual node annotated transcription graph, which Grad C found appealing. Since then, there have been new developments, such as the creation of their own external file format and the development of numerous tools, although Grad C has not yet reviewed these updates.&#10;2. The team is considering using a relational database to express sentence types from XML files, which would allow flexibility for further computations or modifications by researchers. No specific tag or marker for this information has been determined in the transcript. However, the team emphasizes the importance of maintaining consistency and clarity in the annotation process and ensuring that any new types created can interact properly with existing types in the database.&#10;3. The use of a single timeline with unique identifiers (IDs) attached to different sections in a transcription file ensures consistent annotation and easy reference between various sections within the transcription. This concept, which starts the timeline tag with a consistent marker like 'utt' for utterances, enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;&#10;In summary, the annotated transcription graph system has made progress since Grad C's observations a year ago, with new developments such as an external file format and various tools. However, these updates have not been reviewed by Grad C yet. The team is considering using a relational database for flexibility in expressing sentence types from XML files while maintaining consistency and clarity in the annotation process. The single timeline concept allows for consistent annotation and reference between sections within the transcription." />
    <node id=" Yeah . See , that 's the question . I mean , how stable is their {disfmarker} Are they ready to go ,&#10;Speaker: Grad C&#10;Content: The {disfmarker} I looked at it {disfmarker}&#10;Speaker: PhD D&#10;Content: or {disfmarker} ?&#10;Speaker: Grad C&#10;Content: The last time I looked at it was a while ago , probably a year ago , uh , when we first started talking about this .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Grad C&#10;Content: And at that time at least {vocalsound} it was still not very {pause} complete . And so , specifically they didn't have any external format representation at that time . They just had the sort of conceptual {pause} node {disfmarker} uh , annotated transcription graph , which I really liked . And that 's exactly what this stuff is based on . Since then , they 've developed their own external file format , which is , uh , you know , this sort of s this sort of thing . Um , and apparently they 've also developed a lot of tools , but I haven't looked at" />
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="Content: And {disfmarker} Right .&#10;Speaker: PhD F&#10;Content: Cuz {vocalsound} once we start , sort of , doing this I don't {disfmarker} we don't actually have enough time to probably have to rehash it out again&#10;Speaker: Grad C&#10;Content: The {disfmarker} Yep . The other thing {disfmarker} the other way that I sort of established this was as easy translation to and from the Transcriber format .&#10;Speaker: PhD F&#10;Content: and {disfmarker} s Right .&#10;Speaker: Grad C&#10;Content: Um ,&#10;Speaker: PhD F&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: but {disfmarker}&#10;Speaker: PhD F&#10;Content: I mean , I like this . This is sort of intuitively easy to actually r read ,&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD F&#10;Content: as easy it could {disfmarker} as it could be . But , I suppose that {pause} as long as they have a type here that specifies &quot; utt &quot; , um ,&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="} as it could be . But , I suppose that {pause} as long as they have a type here that specifies &quot; utt &quot; , um ,&#10;Speaker: Grad C&#10;Content: It 's almost the same .&#10;Speaker: PhD F&#10;Content: it 's {disfmarker} yeah , close enough that {disfmarker}&#10;Speaker: Grad C&#10;Content: The {disfmarker} the {disfmarker} the {disfmarker} the point is {disfmarker} with this , though , is that you can't really add any supplementary information . Right ? So if you suddenly decide that you want {disfmarker}&#10;Speaker: PhD F&#10;Content: You have to make a different type .&#10;Speaker: Grad C&#10;Content: Yeah . You 'd have to make a different type .&#10;Speaker: PhD F&#10;Content: So {disfmarker} Well , if you look at it and {disfmarker} Um , I guess in my mind I don't know enough {disfmarker} Jane would know better , {comment} about the {pause} types of annotations and {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="er}&#10;Speaker: Postdoc E&#10;Content: It {disfmarker} it seems to me that , um , we will have o an official version of the corpus , which will be only one {disfmarker} one version in terms of the words {disfmarker} where the words are concerned . We 'd still have the {disfmarker} the merging issue maybe if coding were done independently of the {disfmarker}&#10;Speaker: PhD A&#10;Content: And you 're gonna get that&#10;Speaker: Postdoc E&#10;Content: But {disfmarker} but {disfmarker}&#10;Speaker: PhD A&#10;Content: because if the data gets out , people will do all kinds of things to it . And , uh , s you know , several years from now you might want to look into , um , the prosody of referring expressions . And someone at the university of who knows where has annotated the referring expressions . So you want to get that annotation and bring it back in line with your data .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: OK ?&#10;Speaker: Grad C&#10;Content: But">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target=" , we 'd just have to translate everything .&#10;Speaker: Grad C&#10;Content: Write a translator . But it se Since they are developing a big {disfmarker}&#10;Speaker: PhD F&#10;Content: But it {disfmarker} but that sounds {disfmarker}&#10;Speaker: PhD D&#10;Content: But that 's {disfmarker} I don't think that 's a big deal .&#10;Speaker: PhD F&#10;Content: As long as it is {disfmarker}&#10;Speaker: Grad C&#10;Content: they 're developing a big infrastructure . And so it seems to me that if {disfmarker} if we want to use that , we might as well go directly to what they 're doing , rather than {disfmarker}&#10;Speaker: PhD A&#10;Content: If we want to {disfmarker} Do they already have something that 's {disfmarker} that would be useful for us in place ?&#10;Speaker: PhD D&#10;Content: Yeah . See , that 's the question . I mean , how stable is their {disfmarker} Are they ready to go ,&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target=" C&#10;Content: You 'd have another tag which says this is of type &quot; sentence &quot; .&#10;Speaker: PhD F&#10;Content: OK . OK .&#10;Speaker: Grad C&#10;Content: And , what {disfmarker}&#10;Speaker: PhD F&#10;Content: But it 's just not overtly in the {disfmarker}&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Um , cuz this is exactly the kind of {disfmarker}&#10;Speaker: PhD A&#10;Content: So {disfmarker}&#10;Speaker: PhD F&#10;Content: I think that should be {pause} possible as long as the {disfmarker} But , uh , what I don't understand is where the {disfmarker} where in this type of file {pause} that would be expressed .&#10;Speaker: Grad C&#10;Content: Right . You would have another tag somewhere . It 's {disfmarker} well , there 're two ways of doing it .&#10;Speaker: PhD F&#10;Content: S so it would just be floating before the sentence or floating after the sentence without a time - mark .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target=" .&#10;Speaker: PhD F&#10;Content: S so it would just be floating before the sentence or floating after the sentence without a time - mark .&#10;Speaker: Grad C&#10;Content: You could have some sort of link type {disfmarker} type equals &quot; sentence &quot; , and ID is &quot; S - whatever &quot; . And then lower down you could have an utterance . So the type is &quot; utterance &quot; {disfmarker} equals &quot; utt &quot; . And you could either say that {disfmarker} No . I don't know {disfmarker}&#10;Speaker: PhD A&#10;Content: So here 's the thing .&#10;Speaker: Grad C&#10;Content: I take that back .&#10;Speaker: PhD A&#10;Content: Um {disfmarker}&#10;Speaker: Grad C&#10;Content: Can you {disfmarker} can you say that this is part of this ,&#10;Speaker: PhD F&#10;Content: See , cuz it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Hhh .&#10;Speaker: PhD F&#10;Content: it 's {disfmarker}&#10;Speaker: PhD D&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="1. When there are new recognition outputs or changes in forced alignment, the time boundaries of words and phrases may also change. To handle this, the team discusses using a flexible database format that can easily accommodate these modifications.&#10;2. The proposed solution involves creating an elegant and easy-to-use database system that allows for recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed. This ensures that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;3. One suggestion is to use a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="1. The concept being discussed refers to the creation of a single time-line with unique identifiers (IDs) attached to different sections in a transcription file. This approach allows for consistent annotation and easy reference between various sections within the transcription. By starting the timeline tag with a consistent marker, such as 'utt' for utterances, the team can maintain clarity and consistency in their annotation process. This concept is significant because it enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="1. The abbreviation &quot;P&quot; in &quot;P-files&quot; most likely stands for &quot;Phil,&quot; referring to Phil Kohn. This conclusion can be drawn from the comment made by PhD D: &quot;Probably stands for 'Phil.' Phil Kohn.&quot; (Speaker: PhD D, Content: Probably stands for &quot;Phil&quot;. Phil Kohn.)&#10;&#10;2. The significance of P-files in this context is their role as a file format used for storing time-marked data, word transcripts, and annotations in speech research. As mentioned by Grad C, the Quicknet library has utilities to handle these files efficiently. (Speaker: Grad C, Content: But there are nice they're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P-files.)">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="1. The process of handling forced alignment when time boundaries of words in transcriptions change due to a new recognition output involves using a flexible database format, such as a relational database that stores information from XML files. This allows for easy input and output of data while maintaining flexibility for further computations or modifications by researchers.&#10;2. When alignments, word transcripts, or utterance boundaries are changed, the system enables recomputing features and updating entries. The time-stamps in the time-line can be adjusted without altering the IDs of the entries, preserving the original time-stamps and IDs.&#10;3. A consistent marker, like 'utt' for utterances, is used to start the timeline tag, ensuring clarity and consistency in the annotation process. This concept enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;4. When different annotations of the same word sequences emerge as the data gets shared and used by others, a tool or method to reliably merge these different annotations into a single version is needed. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="Based on the discussion, the restriction for adding supplementary information to a type specified as 'utt' is that it cannot be directly added to the existing 'utt' type. This is because 'utt' is used to identify an utterance in the transcription file, and changing this type might affect its consistency across different annotation files.&#10;&#10;If the team wants to add supplementary information, they would need to create a new type, which would require defining the criteria and rules for that specific type. This process involves careful consideration of how the new type will interact with existing types and the overall structure of the database. It may also involve additional work in modifying annotation scripts or tools to accommodate the new type.&#10;&#10;To make a different type, the team would need to:&#10;1. Determine the purpose and criteria for the new type&#10;2. Define how it will interact with other types in the database&#10;3. Modify annotation scripts or tools to recognize and process the new type, if necessary&#10;4. Test the new type to ensure compatibility with existing annotations and the forced alignment program.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="Based on the discussion transcript, the team is considering using a relational database in a format that stores information from XML files to express the type of a sentence in a file. This would allow flexibility for further computations or modifications by researchers. The exact tag or marker for this information is not explicitly stated in the transcript. However, they discuss the importance of maintaining consistency and clarity in the annotation process, as well as ensuring that any new types created can interact properly with existing types in the database. Therefore, careful consideration and definition of the criteria and rules for the new type would be required before implementation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="1. More compact data storage: By referring to an external file, the size of the primary file or format can be significantly reduced, making it more efficient and easier to manage during long meetings with multiple frames. This is especially beneficial when dealing with large datasets.&#10;2. Reduced computational burden: Storing information in an external file may decrease the computational load on the system since only the essential data needs to be loaded into memory for processing during the meeting.&#10;3. Easier organization and maintenance: External files can help keep related but separate information organized, making it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files can facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution for ensuring there's enough time to discuss transcriptions is not explicitly stated in the transcript. However, PhD F mentions that they don't have enough time to rehash out the discussion if they start doing something else. It can be inferred that a suggested solution might be setting aside adequate time for the transcription discussion or preparing materials beforehand to optimize time management.&#10;&#10;2. The significance of having a type specified as 'utt' is that it helps identify an utterance (a unit of speech starting with a pause and ending with a pause) in the transcription file. By having this specification, the team can maintain consistency in their annotation process. As Grad C mentions, &quot;It's almost the same&quot; when other types are used, but if they want to add supplementary information, they would have to create a different type. This emphasizes the importance of using 'utt' as a consistent marker for utterances." target="1. More compact data storage: By storing additional or detailed information in an external file, the primary file remains smaller and more manageable during long meetings with multiple frames. This is particularly useful when dealing with large datasets.&#10;2. Reduced computational burden: Loading only essential data into memory for processing can decrease the computational load on the system, making it run more efficiently.&#10;3. Easier organization and maintenance: External files help keep related but separate information organized and make it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components.">
      <data key="d0">1</data>
    </edge>
    <edge source="} as it could be . But , I suppose that {pause} as long as they have a type here that specifies &quot; utt &quot; , um ,&#10;Speaker: Grad C&#10;Content: It 's almost the same .&#10;Speaker: PhD F&#10;Content: it 's {disfmarker} yeah , close enough that {disfmarker}&#10;Speaker: Grad C&#10;Content: The {disfmarker} the {disfmarker} the {disfmarker} the point is {disfmarker} with this , though , is that you can't really add any supplementary information . Right ? So if you suddenly decide that you want {disfmarker}&#10;Speaker: PhD F&#10;Content: You have to make a different type .&#10;Speaker: Grad C&#10;Content: Yeah . You 'd have to make a different type .&#10;Speaker: PhD F&#10;Content: So {disfmarker} Well , if you look at it and {disfmarker} Um , I guess in my mind I don't know enough {disfmarker} Jane would know better , {comment} about the {pause} types of annotations and {disfmarker}" target="Based on the discussion, the restriction for adding supplementary information to a type specified as 'utt' is that it cannot be directly added to the existing 'utt' type. This is because 'utt' is used to identify an utterance in the transcription file, and changing this type might affect its consistency across different annotation files.&#10;&#10;If the team wants to add supplementary information, they would need to create a new type, which would require defining the criteria and rules for that specific type. This process involves careful consideration of how the new type will interact with existing types and the overall structure of the database. It may also involve additional work in modifying annotation scripts or tools to accommodate the new type.&#10;&#10;To make a different type, the team would need to:&#10;1. Determine the purpose and criteria for the new type&#10;2. Define how it will interact with other types in the database&#10;3. Modify annotation scripts or tools to recognize and process the new type, if necessary&#10;4. Test the new type to ensure compatibility with existing annotations and the forced alignment program.">
      <data key="d0">1</data>
    </edge>
    <edge source="er}&#10;Speaker: Postdoc E&#10;Content: It {disfmarker} it seems to me that , um , we will have o an official version of the corpus , which will be only one {disfmarker} one version in terms of the words {disfmarker} where the words are concerned . We 'd still have the {disfmarker} the merging issue maybe if coding were done independently of the {disfmarker}&#10;Speaker: PhD A&#10;Content: And you 're gonna get that&#10;Speaker: Postdoc E&#10;Content: But {disfmarker} but {disfmarker}&#10;Speaker: PhD A&#10;Content: because if the data gets out , people will do all kinds of things to it . And , uh , s you know , several years from now you might want to look into , um , the prosody of referring expressions . And someone at the university of who knows where has annotated the referring expressions . So you want to get that annotation and bring it back in line with your data .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: OK ?&#10;Speaker: Grad C&#10;Content: But" target="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When there are new recognition outputs or changes in forced alignment, the time boundaries of words and phrases may also change. To handle this, the team discusses using a flexible database format that can easily accommodate these modifications.&#10;2. The proposed solution involves creating an elegant and easy-to-use database system that allows for recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed. This ensures that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;3. One suggestion is to use a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers." target="marker} the utterance that it 's referring to could be U - seventeen or something like that .&#10;Speaker: PhD F&#10;Content: OK . So , I mean , that seems {disfmarker} that seems g great for all of the encoding of things with time and ,&#10;Speaker: Grad C&#10;Content: Oh , well .&#10;Speaker: PhD F&#10;Content: um {disfmarker} I {disfmarker} I guess my question is more , uh , what d what do you do with , say , a forced alignment ?&#10;Speaker: PhD A&#10;Content: How - how&#10;Speaker: PhD F&#10;Content: I mean you 've got all these phone labels , and what do you do if you {disfmarker} just conceptually , if you get , um , transcriptions where the words are staying but the time boundaries are changing , cuz you 've got a new recognition output , or s sort of {disfmarker} what 's the , um , sequence of going from the waveforms that stay the same , the transcripts that may or may not change , and then the utterance which {disfmarker} where the time boundaries that may">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When there are new recognition outputs or changes in forced alignment, the time boundaries of words and phrases may also change. To handle this, the team discusses using a flexible database format that can easily accommodate these modifications.&#10;2. The proposed solution involves creating an elegant and easy-to-use database system that allows for recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed. This ensures that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;3. One suggestion is to use a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers." target=" recently or something ?&#10;Speaker: PhD F&#10;Content: Sorry . Exactly . It 's very disconcerting . OK . So , um ,&#10;Speaker: Grad C&#10;Content: &#10;Speaker: PhD F&#10;Content: I was gonna try to get out of here , like , in half an hour , um , cuz I really appreciate people coming , and {vocalsound} the main thing that I was gonna ask people to help with today is {pause} to give input on what kinds of database format we should {pause} use in starting to link up things like word transcripts and annotations of word transcripts , so anything that transcribers or discourse coders or whatever put in the signal , {vocalsound} with time - marks for , like , words and phone boundaries and all the stuff we get out of the forced alignments and the recognizer . So , we have this , um {disfmarker} I think a starting point is clearly the {disfmarker} the channelized {pause} output of Dave Gelbart 's program , which Don brought a copy of ,&#10;Speaker: Grad C&#10;Content: Yeah . Yeah , I 'm {disfmarker} I 'm familiar">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When there are new recognition outputs or changes in forced alignment, the time boundaries of words and phrases may also change. To handle this, the team discusses using a flexible database format that can easily accommodate these modifications.&#10;2. The proposed solution involves creating an elegant and easy-to-use database system that allows for recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed. This ensures that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;3. One suggestion is to use a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers." target=" was trying to figure out what 's the best format for this representation .&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD F&#10;Content: And it 's still gonna be {disfmarker}&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD F&#10;Content: it 's still gonna be , uh , not direct .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: You know , it {disfmarker} Or another example was , you know , uh , where in the language {disfmarker} where in the word sequence are people interrupting ? So , I guess that one 's actually easier .&#10;Speaker: PhD D&#10;Content: What about {disfmarker} what about , um , the idea of using a relational database to , uh , store the information from the XML ? So you would have {disfmarker} XML basically would {disfmarker} Uh , you {disfmarker} you could use the XML to put the data in , and then when you get data out , you put it back in XML . So use XML as sort of the {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When there are new recognition outputs or changes in forced alignment, the time boundaries of words and phrases may also change. To handle this, the team discusses using a flexible database format that can easily accommodate these modifications.&#10;2. The proposed solution involves creating an elegant and easy-to-use database system that allows for recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed. This ensures that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;3. One suggestion is to use a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers." target=" what to do .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: And especially for the prosody work , what {disfmarker} what it ends up being is you get features from the signal , and of course those change every time your alignments change . So you re - run a recognizer , you want to recompute your features , um , and then keep the database up to date .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: Or you change a word , or you change a {vocalsound} utterance boundary segment , which is gonna happen a lot . And so I wanted something where {pause} all of this can be done in a elegant way and that if somebody wants to try something or compute something else , that it can be done flexibly . Um , it doesn't have to be pretty , it just has to be , you know , easy to use , and {disfmarker}&#10;Speaker: Grad C&#10;Content: Yeah , the other thing {disfmarker} We should look at ATLAS , the NIST thing ,&#10;Speaker: PhD F&#10;Content: Oh .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When there are new recognition outputs or changes in forced alignment, the time boundaries of words and phrases may also change. To handle this, the team discusses using a flexible database format that can easily accommodate these modifications.&#10;2. The proposed solution involves creating an elegant and easy-to-use database system that allows for recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed. This ensures that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;3. One suggestion is to use a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers." target=" that stay the same , the transcripts that may or may not change , and then the utterance which {disfmarker} where the time boundaries that may or may not change {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Oh , that 's {disfmarker} That 's actually very nicely handled here because you could {disfmarker} you could {disfmarker} all you 'd have to change is the , {vocalsound} um , time - stamps in the time - line without {disfmarker} without , uh , changing the I Ds .&#10;Speaker: PhD F&#10;Content: Um . And you 'd be able to propagate all of the {disfmarker} the information ?&#10;Speaker: Grad C&#10;Content: Right . That 's , the who that 's why you do that extra level of indirection . So that you can just change the time - line .&#10;Speaker: PhD A&#10;Content: Except the time - line is gonna be huge . If you say {disfmarker}&#10;Speaker: Grad C&#10;Content: Yes .&#10;Speaker: PhD F&#10;Content: Yeah ,&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When there are new recognition outputs or changes in forced alignment, the time boundaries of words and phrases may also change. To handle this, the team discusses using a flexible database format that can easily accommodate these modifications.&#10;2. The proposed solution involves creating an elegant and easy-to-use database system that allows for recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed. This ensures that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;3. One suggestion is to use a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers." target="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When there are new recognition outputs or changes in forced alignment, the time boundaries of words and phrases may also change. To handle this, the team discusses using a flexible database format that can easily accommodate these modifications.&#10;2. The proposed solution involves creating an elegant and easy-to-use database system that allows for recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed. This ensures that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;3. One suggestion is to use a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers." target="1. The process of handling forced alignment when time boundaries of words in transcriptions change due to a new recognition output involves using a flexible database format, such as a relational database that stores information from XML files. This allows for easy input and output of data while maintaining flexibility for further computations or modifications by researchers.&#10;2. When alignments, word transcripts, or utterance boundaries are changed, the system enables recomputing features and updating entries. The time-stamps in the time-line can be adjusted without altering the IDs of the entries, preserving the original time-stamps and IDs.&#10;3. A consistent marker, like 'utt' for utterances, is used to start the timeline tag, ensuring clarity and consistency in the annotation process. This concept enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;4. When different annotations of the same word sequences emerge as the data gets shared and used by others, a tool or method to reliably merge these different annotations into a single version is needed. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When there are new recognition outputs or changes in forced alignment, the time boundaries of words and phrases may also change. To handle this, the team discusses using a flexible database format that can easily accommodate these modifications.&#10;2. The proposed solution involves creating an elegant and easy-to-use database system that allows for recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed. This ensures that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;3. One suggestion is to use a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers." target="Based on the discussion transcript, the team is considering using a relational database in a format that stores information from XML files to express the type of a sentence in a file. This would allow flexibility for further computations or modifications by researchers. The exact tag or marker for this information is not explicitly stated in the transcript. However, they discuss the importance of maintaining consistency and clarity in the annotation process, as well as ensuring that any new types created can interact properly with existing types in the database. Therefore, careful consideration and definition of the criteria and rules for the new type would be required before implementation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When there are new recognition outputs or changes in forced alignment, the time boundaries of words and phrases may also change. To handle this, the team discusses using a flexible database format that can easily accommodate these modifications.&#10;2. The proposed solution involves creating an elegant and easy-to-use database system that allows for recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed. This ensures that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;3. One suggestion is to use a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers." target="1. The recommended way to handle data that has been edited and distributed among multiple parties for accurate merging involves the use of a reliable merging tool or method that can incorporate different annotations or coding methods while maintaining unique word boundaries in a consistent manner.&#10;2. Utilizing a stable reference point, such as speaker onsets and offsets or acoustic segments with time-marks, can help reduce ambiguity during the merging process.&#10;3. Carefully defining the criteria and rules for merging is crucial to specify what is meant by &quot;merging&quot; different annotations or coding methods.&#10;4. Implementing a flexible database system, like a relational database that stores information from XML files, can accommodate modifications in word transcripts and time boundaries due to new recognition outputs or changes in forced alignment. This system allows for recomputing features and updating entries while preserving original time-stamps and IDs of the entries.&#10;5. Ensuring consistency and clarity in the annotation process by using clear markers, like 'utt' for utterances, can help manage changes in word transcripts or utterance boundaries, ultimately facilitating accurate merging of edited data from multiple parties.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker} the utterance that it 's referring to could be U - seventeen or something like that .&#10;Speaker: PhD F&#10;Content: OK . So , I mean , that seems {disfmarker} that seems g great for all of the encoding of things with time and ,&#10;Speaker: Grad C&#10;Content: Oh , well .&#10;Speaker: PhD F&#10;Content: um {disfmarker} I {disfmarker} I guess my question is more , uh , what d what do you do with , say , a forced alignment ?&#10;Speaker: PhD A&#10;Content: How - how&#10;Speaker: PhD F&#10;Content: I mean you 've got all these phone labels , and what do you do if you {disfmarker} just conceptually , if you get , um , transcriptions where the words are staying but the time boundaries are changing , cuz you 've got a new recognition output , or s sort of {disfmarker} what 's the , um , sequence of going from the waveforms that stay the same , the transcripts that may or may not change , and then the utterance which {disfmarker} where the time boundaries that may" target="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker} the utterance that it 's referring to could be U - seventeen or something like that .&#10;Speaker: PhD F&#10;Content: OK . So , I mean , that seems {disfmarker} that seems g great for all of the encoding of things with time and ,&#10;Speaker: Grad C&#10;Content: Oh , well .&#10;Speaker: PhD F&#10;Content: um {disfmarker} I {disfmarker} I guess my question is more , uh , what d what do you do with , say , a forced alignment ?&#10;Speaker: PhD A&#10;Content: How - how&#10;Speaker: PhD F&#10;Content: I mean you 've got all these phone labels , and what do you do if you {disfmarker} just conceptually , if you get , um , transcriptions where the words are staying but the time boundaries are changing , cuz you 've got a new recognition output , or s sort of {disfmarker} what 's the , um , sequence of going from the waveforms that stay the same , the transcripts that may or may not change , and then the utterance which {disfmarker} where the time boundaries that may" target="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker} the utterance that it 's referring to could be U - seventeen or something like that .&#10;Speaker: PhD F&#10;Content: OK . So , I mean , that seems {disfmarker} that seems g great for all of the encoding of things with time and ,&#10;Speaker: Grad C&#10;Content: Oh , well .&#10;Speaker: PhD F&#10;Content: um {disfmarker} I {disfmarker} I guess my question is more , uh , what d what do you do with , say , a forced alignment ?&#10;Speaker: PhD A&#10;Content: How - how&#10;Speaker: PhD F&#10;Content: I mean you 've got all these phone labels , and what do you do if you {disfmarker} just conceptually , if you get , um , transcriptions where the words are staying but the time boundaries are changing , cuz you 've got a new recognition output , or s sort of {disfmarker} what 's the , um , sequence of going from the waveforms that stay the same , the transcripts that may or may not change , and then the utterance which {disfmarker} where the time boundaries that may" target="1. The process of handling forced alignment when time boundaries of words in transcriptions change due to a new recognition output involves using a flexible database format, such as a relational database that stores information from XML files. This allows for easy input and output of data while maintaining flexibility for further computations or modifications by researchers.&#10;2. When alignments, word transcripts, or utterance boundaries are changed, the system enables recomputing features and updating entries. The time-stamps in the time-line can be adjusted without altering the IDs of the entries, preserving the original time-stamps and IDs.&#10;3. A consistent marker, like 'utt' for utterances, is used to start the timeline tag, ensuring clarity and consistency in the annotation process. This concept enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;4. When different annotations of the same word sequences emerge as the data gets shared and used by others, a tool or method to reliably merge these different annotations into a single version is needed. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases.">
      <data key="d0">1</data>
    </edge>
    <edge source=" recently or something ?&#10;Speaker: PhD F&#10;Content: Sorry . Exactly . It 's very disconcerting . OK . So , um ,&#10;Speaker: Grad C&#10;Content: &#10;Speaker: PhD F&#10;Content: I was gonna try to get out of here , like , in half an hour , um , cuz I really appreciate people coming , and {vocalsound} the main thing that I was gonna ask people to help with today is {pause} to give input on what kinds of database format we should {pause} use in starting to link up things like word transcripts and annotations of word transcripts , so anything that transcribers or discourse coders or whatever put in the signal , {vocalsound} with time - marks for , like , words and phone boundaries and all the stuff we get out of the forced alignments and the recognizer . So , we have this , um {disfmarker} I think a starting point is clearly the {disfmarker} the channelized {pause} output of Dave Gelbart 's program , which Don brought a copy of ,&#10;Speaker: Grad C&#10;Content: Yeah . Yeah , I 'm {disfmarker} I 'm familiar" target="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data.">
      <data key="d0">1</data>
    </edge>
    <edge source=" was trying to figure out what 's the best format for this representation .&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD F&#10;Content: And it 's still gonna be {disfmarker}&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD F&#10;Content: it 's still gonna be , uh , not direct .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: You know , it {disfmarker} Or another example was , you know , uh , where in the language {disfmarker} where in the word sequence are people interrupting ? So , I guess that one 's actually easier .&#10;Speaker: PhD D&#10;Content: What about {disfmarker} what about , um , the idea of using a relational database to , uh , store the information from the XML ? So you would have {disfmarker} XML basically would {disfmarker} Uh , you {disfmarker} you could use the XML to put the data in , and then when you get data out , you put it back in XML . So use XML as sort of the {disfmark" target="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data." target=" {disfmarker}&#10;Speaker: Grad C&#10;Content: Yes .&#10;Speaker: PhD F&#10;Content: Yeah ,&#10;Speaker: PhD A&#10;Content: suppose you have a phone - level alignment .&#10;Speaker: PhD F&#10;Content: yeah , especially at the phone - level .&#10;Speaker: PhD A&#10;Content: You 'd have {disfmarker} you 'd have {disfmarker}&#10;Speaker: PhD F&#10;Content: The {disfmarker} we {disfmarker} we have phone - level backtraces .&#10;Speaker: Grad C&#10;Content: Yeah , this {disfmarker} I don't think I would do this for phone - level . I think for phone - level you want to use some sort of binary representation&#10;Speaker: PhD F&#10;Content: Um {disfmarker}&#10;Speaker: Grad C&#10;Content: because it 'll be too dense otherwise .&#10;Speaker: PhD F&#10;Content: OK . So , if you were doing that and you had this sort of companion , uh , thing that gets called up for phone - level , uh , what would that look like ?&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data." target="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data." target="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data." target="1. The concept being discussed refers to the creation of a single time-line with unique identifiers (IDs) attached to different sections in a transcription file. This approach allows for consistent annotation and easy reference between various sections within the transcription. By starting the timeline tag with a consistent marker, such as 'utt' for utterances, the team can maintain clarity and consistency in their annotation process. This concept is significant because it enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data." target="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data." target="1. The process of handling forced alignment when time boundaries of words in transcriptions change due to a new recognition output involves using a flexible database format, such as a relational database that stores information from XML files. This allows for easy input and output of data while maintaining flexibility for further computations or modifications by researchers.&#10;2. When alignments, word transcripts, or utterance boundaries are changed, the system enables recomputing features and updating entries. The time-stamps in the time-line can be adjusted without altering the IDs of the entries, preserving the original time-stamps and IDs.&#10;3. A consistent marker, like 'utt' for utterances, is used to start the timeline tag, ensuring clarity and consistency in the annotation process. This concept enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;4. When different annotations of the same word sequences emerge as the data gets shared and used by others, a tool or method to reliably merge these different annotations into a single version is needed. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data." target="Based on the discussion, the restriction for adding supplementary information to a type specified as 'utt' is that it cannot be directly added to the existing 'utt' type. This is because 'utt' is used to identify an utterance in the transcription file, and changing this type might affect its consistency across different annotation files.&#10;&#10;If the team wants to add supplementary information, they would need to create a new type, which would require defining the criteria and rules for that specific type. This process involves careful consideration of how the new type will interact with existing types and the overall structure of the database. It may also involve additional work in modifying annotation scripts or tools to accommodate the new type.&#10;&#10;To make a different type, the team would need to:&#10;1. Determine the purpose and criteria for the new type&#10;2. Define how it will interact with other types in the database&#10;3. Modify annotation scripts or tools to recognize and process the new type, if necessary&#10;4. Test the new type to ensure compatibility with existing annotations and the forced alignment program.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data." target="1. Potential uses of a system for extracting phone-level information from XML files include:&#10;   - Prosodic feature analysis at a fine-grained level (although the speakers express concerns about reliability and increased computational burden)&#10;   - Facilitating time-aligned annotation of phones, which could be useful for specific research questions or applications requiring detailed phonetic information.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The team is discussing using a flexible database format to link up word transcripts, annotations, and time-marked data from transcribers and discourse coders in a forced alignment program. They are considering a relational database that stores information from XML files, allowing for easy input and output of data while maintaining flexibility for further computations or modifications by researchers. This approach would enable recomputing features and updating entries when alignments, word transcripts, or utterance boundaries are changed, ensuring that the time-stamps in the time-line can be adjusted without altering the IDs of the entries.&#10;&#10;2. The speakers have also considered using the channelized output of Dave Gelbart's program as a starting point for this database format since it already provides a foundation for organizing and storing transcription data." target="1. More compact data storage: By referring to an external file, the size of the primary file or format can be significantly reduced, making it more efficient and easier to manage during long meetings with multiple frames. This is especially beneficial when dealing with large datasets.&#10;2. Reduced computational burden: Storing information in an external file may decrease the computational load on the system since only the essential data needs to be loaded into memory for processing during the meeting.&#10;3. Easier organization and maintenance: External files can help keep related but separate information organized, making it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files can facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker}&#10;Speaker: Grad C&#10;Content: Yes .&#10;Speaker: PhD F&#10;Content: Yeah ,&#10;Speaker: PhD A&#10;Content: suppose you have a phone - level alignment .&#10;Speaker: PhD F&#10;Content: yeah , especially at the phone - level .&#10;Speaker: PhD A&#10;Content: You 'd have {disfmarker} you 'd have {disfmarker}&#10;Speaker: PhD F&#10;Content: The {disfmarker} we {disfmarker} we have phone - level backtraces .&#10;Speaker: Grad C&#10;Content: Yeah , this {disfmarker} I don't think I would do this for phone - level . I think for phone - level you want to use some sort of binary representation&#10;Speaker: PhD F&#10;Content: Um {disfmarker}&#10;Speaker: Grad C&#10;Content: because it 'll be too dense otherwise .&#10;Speaker: PhD F&#10;Content: OK . So , if you were doing that and you had this sort of companion , uh , thing that gets called up for phone - level , uh , what would that look like ?&#10;Speaker: PhD" target="1. The limitation of frame-level analysis for phone-level speech processing, as discussed by the Ph.D. speakers, is that most frames are not actual speech, but silent or transitional periods between sounds. This makes frame-level analysis less reliable for phone-level speech processing since phones can undergo significant modification during merging processes, and analyzing at this level increases computational burden without adding meaningful value to the analysis of prosodic features.&#10;   &#10;2. The average number of phones in an English word is not explicitly stated in the transcript, but it is mentioned that there are approximately five frames per phone. This suggests that the average number of phones in an English word could be around five or more, given that some words might have additional phones due to affixes or other phonetic processes.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target=" um {disfmarker} Yeah . You just have to know wha what to tie it to .&#10;Speaker: Grad C&#10;Content: Yeah , exactly . The problem is saying &quot; what are the semantics ,&#10;Speaker: PhD F&#10;Content: And {disfmarker}&#10;Speaker: Grad C&#10;Content: what do you mean by &quot; merge &quot; ? &quot;&#10;Speaker: PhD F&#10;Content: Right , right .&#10;Speaker: PhD A&#10;Content: Right . So {disfmarker} so just to let you know what we {disfmarker} where we kluged it by , uh , doing {disfmarker} uh , by doing {disfmarker} Hhh .&#10;Speaker: Grad C&#10;Content: So .&#10;Speaker: PhD A&#10;Content: Both were based on words , so , bo we have two versions of the same words intersp you know , sprinkled with {disfmarker} with different tags for annotations .&#10;Speaker: Grad C&#10;Content: And then you did diff .&#10;Speaker: PhD A&#10;Content: And we did diff . Exactly !&#10;Speaker: Grad C&#10;Content: Yeah , that '">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target=" Postdoc E&#10;Content: we sh change the boundaries of the units , it 's still unique and {disfmarker} and , uh , fits with the format ,&#10;Speaker: PhD F&#10;Content: Right .&#10;Speaker: Postdoc E&#10;Content: flexible , all that .&#10;Speaker: PhD A&#10;Content: Um , it would be nice {disfmarker} um , eh , gr this is sort of r regarding {disfmarker} uh , uh it 's related but not directly germane to the topic of discussion , but , when it comes to annotations , um , you often find yourself in the situation where you have {pause} different annotations {pause} of the same , say , word sequence . OK ?&#10;Speaker: Postdoc E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: And sometimes the word sequences even differ slightly because they were edited s at one place but not the other .&#10;Speaker: Postdoc E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So , once this data gets out there , some people might start annotating this for , I don't know , dialogue acts or , um , you know , topics or what">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target=" as , uh , you know , um , what was it ? uh ,&#10;Speaker: PhD F&#10;Content: Well , all the Switchboard in it .&#10;Speaker: PhD A&#10;Content: utterance types . There 's , uh , automatic , uh , punctuation and stuff like that .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Because we had one set of {pause} annotations that were based on , uh , one version of the transcripts with a particular segmentation , and then we had another version that was based on , uh , a different s slightly edited version of the transcripts with a different segmentation . So , {vocalsound} we had these two different versions which were {disfmarker} you know , you could tell they were from the same source but they weren't identical . So it was extremely hard {vocalsound} to reliably merge these two back together to correlate the information from the different annotations .&#10;Speaker: Grad C&#10;Content: Yep . I {disfmarker} I don't see any way that file formats are gonna help us with that .&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Grad C&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target=" - hmm .&#10;Speaker: PhD A&#10;Content: I mean , most people aren't as sophisticated as {disfmarker} as we are here with , you know , uh , time alignments and stuff . So {disfmarker} So the {disfmarker} the {disfmarker} the point is {disfmarker}&#10;Speaker: Grad C&#10;Content: Should {disfmarker} should we mention some names on the people who are n ?&#10;Speaker: PhD A&#10;Content: Right . So , um , the p my point is that {pause} you 're gonna end up with , uh , word sequences that are differently annotated . And {pause} you want some tool , uh , that is able to sort of merge these different annotations back into a single , uh , version . OK ? Um , and we had this problem very massively , uh , at SRI when we worked , uh , a while back on , {vocalsound} uh {disfmarker} well , on dialogue acts as well as , uh , you know , um , what was it ? uh ,&#10;Speaker: PhD F&#10;Content: Well , all the Switchboard in it">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target=" Exactly .&#10;Speaker: Grad C&#10;Content: Sure . But what if {disfmarker} what if they change the words ?&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Postdoc E&#10;Content: Not {disfmarker} Well , but you 'd have some anchoring point . He couldn't have changed all the words .&#10;Speaker: PhD D&#10;Content: But can they change the words without changing the time of the word ?&#10;Speaker: Grad C&#10;Content: Sure . But they could have changed it a little . The {disfmarker} the point is , that {disfmarker} that they may have annotated it off a word transcript that isn't the same as our word transcript , so how do you merge it back in ? I understand what you 're saying .&#10;Speaker: PhD A&#10;Content: Mmm . Mm - hmm .&#10;Speaker: Grad C&#10;Content: And I {disfmarker} I guess the answer is , um , it 's gonna be different every time . It 's j it 's just gonna be {disfmarker}&#10;Speaker: Postdoc E&#10;Content: Yeah">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="1. The concept being discussed refers to the creation of a single time-line with unique identifiers (IDs) attached to different sections in a transcription file. This approach allows for consistent annotation and easy reference between various sections within the transcription. By starting the timeline tag with a consistent marker, such as 'utt' for utterances, the team can maintain clarity and consistency in their annotation process. This concept is significant because it enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="1. The process of handling forced alignment when time boundaries of words in transcriptions change due to a new recognition output involves using a flexible database format, such as a relational database that stores information from XML files. This allows for easy input and output of data while maintaining flexibility for further computations or modifications by researchers.&#10;2. When alignments, word transcripts, or utterance boundaries are changed, the system enables recomputing features and updating entries. The time-stamps in the time-line can be adjusted without altering the IDs of the entries, preserving the original time-stamps and IDs.&#10;3. A consistent marker, like 'utt' for utterances, is used to start the timeline tag, ensuring clarity and consistency in the annotation process. This concept enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;4. When different annotations of the same word sequences emerge as the data gets shared and used by others, a tool or method to reliably merge these different annotations into a single version is needed. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="Based on the discussion, the restriction for adding supplementary information to a type specified as 'utt' is that it cannot be directly added to the existing 'utt' type. This is because 'utt' is used to identify an utterance in the transcription file, and changing this type might affect its consistency across different annotation files.&#10;&#10;If the team wants to add supplementary information, they would need to create a new type, which would require defining the criteria and rules for that specific type. This process involves careful consideration of how the new type will interact with existing types and the overall structure of the database. It may also involve additional work in modifying annotation scripts or tools to accommodate the new type.&#10;&#10;To make a different type, the team would need to:&#10;1. Determine the purpose and criteria for the new type&#10;2. Define how it will interact with other types in the database&#10;3. Modify annotation scripts or tools to recognize and process the new type, if necessary&#10;4. Test the new type to ensure compatibility with existing annotations and the forced alignment program.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="The issue at hand is the difficulty in reliably merging different versions of word transcripts, especially when those changes are tied to acoustic segments. The speakers express concern that crucial information may be lost during the merge process, making it challenging to combine different annotations or coding methods. They discuss the need for a stable reference point, such as speaker ons and offs or acoustic segments with time-marks, to reduce ambiguity in the merging process. However, they acknowledge that this information might not always be available or correctly preserved during merges. The aim is to create a consistent and clear annotation system that can handle changes in word transcripts while maintaining the original time-stamps and IDs of entries in a relational database.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="Based on the discussion transcript, the team is considering using a relational database in a format that stores information from XML files to express the type of a sentence in a file. This would allow flexibility for further computations or modifications by researchers. The exact tag or marker for this information is not explicitly stated in the transcript. However, they discuss the importance of maintaining consistency and clarity in the annotation process, as well as ensuring that any new types created can interact properly with existing types in the database. Therefore, careful consideration and definition of the criteria and rules for the new type would be required before implementation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="1. The recommended way to handle data that has been edited and distributed among multiple parties for accurate merging involves the use of a reliable merging tool or method that can incorporate different annotations or coding methods while maintaining unique word boundaries in a consistent manner.&#10;2. Utilizing a stable reference point, such as speaker onsets and offsets or acoustic segments with time-marks, can help reduce ambiguity during the merging process.&#10;3. Carefully defining the criteria and rules for merging is crucial to specify what is meant by &quot;merging&quot; different annotations or coding methods.&#10;4. Implementing a flexible database system, like a relational database that stores information from XML files, can accommodate modifications in word transcripts and time boundaries due to new recognition outputs or changes in forced alignment. This system allows for recomputing features and updating entries while preserving original time-stamps and IDs of the entries.&#10;5. Ensuring consistency and clarity in the annotation process by using clear markers, like 'utt' for utterances, can help manage changes in word transcripts or utterance boundaries, ultimately facilitating accurate merging of edited data from multiple parties.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="The speakers discuss an efficient way to extract and find information in a complex structure by using a relational database that stores information from XML files. This approach allows flexibility for further computations or modifications by researchers, ensuring consistency and clarity in the annotation process. They also mention the importance of maintaining unique word boundaries while incorporating different annotations or coding methods, acknowledging that changes in word transcripts might create challenges during merges. However, they do not explicitly provide a solution for efficiently extracting and finding information in complex structures in this transcript.&#10;&#10;To efficiently extract and find information in a complex structure like an XML file or relational database, you can use query languages such as XQuery for XML files or SQL (Structured Query Language) for relational databases. These query languages allow users to search, filter, and manipulate data based on specific criteria, making it easier to locate the desired information within large and complex datasets. To implement this solution, further details about the structure of the data and the desired information would be required.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Two versions of the same words, interspersed with different tags for annotations, were identified.&#10;2. A diff operation was performed to compare and contrast the differences between these two versions.&#10;3. The group discussed the need to have an official version of the corpus that would have unique word boundaries, but still be flexible enough to incorporate different annotations or coding methods.&#10;4. It was acknowledged that as the data gets shared and used by others, different annotations of the same word sequences might emerge, making it difficult to merge them back together.&#10;5. A need for a tool or method to reliably merge these different annotations into a single version was identified.&#10;6. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="1. The group is discussing the possibility of an additional level of complexity in segmenting a word sequence, beyond sentences and prosodic phrases (as suggested by Grad C). This third level would add further structure to the word sequence.&#10;2. It is acknowledged that prosodic phrases can span across two sentences, not being contained entirely within either sentence (this is accepted when they discuss different segmentation levels that do not nest).&#10;3. The idea of overlapping codes or concepts is mentioned, but it is decided that they don't need to delve into the specifics of the coding system since it's not the main topic of the meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source=" um {disfmarker} Yeah . You just have to know wha what to tie it to .&#10;Speaker: Grad C&#10;Content: Yeah , exactly . The problem is saying &quot; what are the semantics ,&#10;Speaker: PhD F&#10;Content: And {disfmarker}&#10;Speaker: Grad C&#10;Content: what do you mean by &quot; merge &quot; ? &quot;&#10;Speaker: PhD F&#10;Content: Right , right .&#10;Speaker: PhD A&#10;Content: Right . So {disfmarker} so just to let you know what we {disfmarker} where we kluged it by , uh , doing {disfmarker} uh , by doing {disfmarker} Hhh .&#10;Speaker: Grad C&#10;Content: So .&#10;Speaker: PhD A&#10;Content: Both were based on words , so , bo we have two versions of the same words intersp you know , sprinkled with {disfmarker} with different tags for annotations .&#10;Speaker: Grad C&#10;Content: And then you did diff .&#10;Speaker: PhD A&#10;Content: And we did diff . Exactly !&#10;Speaker: Grad C&#10;Content: Yeah , that '" target="Speaker PhD A expresses skepticism about being able to perform meaningful actions or recover information from a text representation that does not reflect the structure of the original words and annotations. They argue that the text representation will not contain sufficient information to accurately reconstruct the original annotations, as it does not capture the structural relationships between words and annotations.&#10;&#10;Speaker Grad C disagrees with this assessment, arguing that it is still possible to recover the original structure from the text representation. They suggest that by tagging certain features of the text (such as sentence boundaries), it may be possible to reconstruct the original annotations with a high degree of accuracy.&#10;&#10;Overall, the disagreement between Speaker PhD A and Speaker Grad C centers on whether it is possible to accurately recover the original structure of words and annotations from a text representation that does not explicitly encode this information. While Speaker PhD A is skeptical of this approach, Speaker Grad C believes that it may be possible with the use of additional tags or markers.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Postdoc E&#10;Content: we sh change the boundaries of the units , it 's still unique and {disfmarker} and , uh , fits with the format ,&#10;Speaker: PhD F&#10;Content: Right .&#10;Speaker: Postdoc E&#10;Content: flexible , all that .&#10;Speaker: PhD A&#10;Content: Um , it would be nice {disfmarker} um , eh , gr this is sort of r regarding {disfmarker} uh , uh it 's related but not directly germane to the topic of discussion , but , when it comes to annotations , um , you often find yourself in the situation where you have {pause} different annotations {pause} of the same , say , word sequence . OK ?&#10;Speaker: Postdoc E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: And sometimes the word sequences even differ slightly because they were edited s at one place but not the other .&#10;Speaker: Postdoc E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So , once this data gets out there , some people might start annotating this for , I don't know , dialogue acts or , um , you know , topics or what" target="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Postdoc E&#10;Content: we sh change the boundaries of the units , it 's still unique and {disfmarker} and , uh , fits with the format ,&#10;Speaker: PhD F&#10;Content: Right .&#10;Speaker: Postdoc E&#10;Content: flexible , all that .&#10;Speaker: PhD A&#10;Content: Um , it would be nice {disfmarker} um , eh , gr this is sort of r regarding {disfmarker} uh , uh it 's related but not directly germane to the topic of discussion , but , when it comes to annotations , um , you often find yourself in the situation where you have {pause} different annotations {pause} of the same , say , word sequence . OK ?&#10;Speaker: Postdoc E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: And sometimes the word sequences even differ slightly because they were edited s at one place but not the other .&#10;Speaker: Postdoc E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So , once this data gets out there , some people might start annotating this for , I don't know , dialogue acts or , um , you know , topics or what" target="Speaker PhD A expresses skepticism about being able to perform meaningful actions or recover information from a text representation that does not reflect the structure of the original words and annotations. They argue that the text representation will not contain sufficient information to accurately reconstruct the original annotations, as it does not capture the structural relationships between words and annotations.&#10;&#10;Speaker Grad C disagrees with this assessment, arguing that it is still possible to recover the original structure from the text representation. They suggest that by tagging certain features of the text (such as sentence boundaries), it may be possible to reconstruct the original annotations with a high degree of accuracy.&#10;&#10;Overall, the disagreement between Speaker PhD A and Speaker Grad C centers on whether it is possible to accurately recover the original structure of words and annotations from a text representation that does not explicitly encode this information. While Speaker PhD A is skeptical of this approach, Speaker Grad C believes that it may be possible with the use of additional tags or markers.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Postdoc E&#10;Content: we sh change the boundaries of the units , it 's still unique and {disfmarker} and , uh , fits with the format ,&#10;Speaker: PhD F&#10;Content: Right .&#10;Speaker: Postdoc E&#10;Content: flexible , all that .&#10;Speaker: PhD A&#10;Content: Um , it would be nice {disfmarker} um , eh , gr this is sort of r regarding {disfmarker} uh , uh it 's related but not directly germane to the topic of discussion , but , when it comes to annotations , um , you often find yourself in the situation where you have {pause} different annotations {pause} of the same , say , word sequence . OK ?&#10;Speaker: Postdoc E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: And sometimes the word sequences even differ slightly because they were edited s at one place but not the other .&#10;Speaker: Postdoc E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So , once this data gets out there , some people might start annotating this for , I don't know , dialogue acts or , um , you know , topics or what" target="1. The group is discussing the possibility of an additional level of complexity in segmenting a word sequence, beyond sentences and prosodic phrases (as suggested by Grad C). This third level would add further structure to the word sequence.&#10;2. It is acknowledged that prosodic phrases can span across two sentences, not being contained entirely within either sentence (this is accepted when they discuss different segmentation levels that do not nest).&#10;3. The idea of overlapping codes or concepts is mentioned, but it is decided that they don't need to delve into the specifics of the coding system since it's not the main topic of the meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source=" as , uh , you know , um , what was it ? uh ,&#10;Speaker: PhD F&#10;Content: Well , all the Switchboard in it .&#10;Speaker: PhD A&#10;Content: utterance types . There 's , uh , automatic , uh , punctuation and stuff like that .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Because we had one set of {pause} annotations that were based on , uh , one version of the transcripts with a particular segmentation , and then we had another version that was based on , uh , a different s slightly edited version of the transcripts with a different segmentation . So , {vocalsound} we had these two different versions which were {disfmarker} you know , you could tell they were from the same source but they weren't identical . So it was extremely hard {vocalsound} to reliably merge these two back together to correlate the information from the different annotations .&#10;Speaker: Grad C&#10;Content: Yep . I {disfmarker} I don't see any way that file formats are gonna help us with that .&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Grad C&#10;" target="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target="isfmarker} some function that merges two {disfmarker} two versions .&#10;Speaker: Grad C&#10;Content: Yeah , I think it 's gonna be very hard . Any sort of structured anything when you try to merge is really , really hard&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: because you ha i The hard part isn't the file format . The hard part is specifying what you mean by &quot; merge &quot; .&#10;Speaker: PhD A&#10;Content: Is {disfmarker} Exactly .&#10;Speaker: Grad C&#10;Content: And that 's very difficult .&#10;Speaker: PhD F&#10;Content: But the one thing that would work here actually for i that is more reliable than the utterances is the {disfmarker} the speaker ons and offs . So if you have a good ,&#10;Speaker: Grad C&#10;Content: But this is exactly what I mean , is that {disfmarker} that the problem i&#10;Speaker: PhD F&#10;Content: um {disfmarker} Yeah . You just have to know wha what to tie it to .&#10;Speaker: Grad C&#10;Content: Yeah">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target=" Grad C&#10;Content: that would make it very difficult to translate from one to the other .&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: I {disfmarker} I think if it 's conceptually close , and they already have or will have tools that everybody else will be using , I mean , {vocalsound} it would be crazy to do something s you know , separate that {disfmarker}&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Grad C&#10;Content: Yeah , we might as well . Yep .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: So I 'll {disfmarker} I 'll take a closer look at it .&#10;Speaker: PhD F&#10;Content: Actually , so it 's {disfmarker} that {disfmarker} that would really be the question , is just what you would feel is in the long run the best thing .&#10;Speaker: Grad C&#10;Content: And {disfmarker} Right .&#10;Speaker: PhD F&#10;Content: Cuz {vocalsound} once we start , sort">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target="isfmarker} two diffs f {comment} based on the same original .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Postdoc E&#10;Content: Is it S - diff ?&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: Postdoc E&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: Something like that , um , but operating on these lattices that are really what 's behind this {disfmarker} uh , this annotation format .&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: So {disfmarker}&#10;Speaker: Grad C&#10;Content: There 's actually a diff library you can use {pause} to do things like that that {disfmarker} so you have different formats .&#10;Speaker: PhD F&#10;Content: You could definitely do that with the {disfmarker}&#10;Speaker: PhD A&#10;Content: So somewhere in the API you would like to have like a merge or some {disfmarker} some function that merges two {disfmarker} two versions .&#10;Speaker: Grad C&#10;Content: Yeah , I think">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target="1. The concept being discussed refers to the creation of a single time-line with unique identifiers (IDs) attached to different sections in a transcription file. This approach allows for consistent annotation and easy reference between various sections within the transcription. By starting the timeline tag with a consistent marker, such as 'utt' for utterances, the team can maintain clarity and consistency in their annotation process. This concept is significant because it enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target="The sharing aspect is important in the context of storing feature vectors on a frame-by-frame basis for two main reasons: adaptability and collaboration. By making the data more accessible and usable, other researchers can adopt and build upon this work more easily.&#10;&#10;1. Adaptability: Creating a system where prosodic features are represented as continuous values at the word or segment level allows for greater flexibility in analyzing speech data. This approach makes it easier to adapt the system to various research questions and domains, as researchers can work with simpler and more meaningful structures (words or segments) instead of being constrained by specific phones or frames.&#10;2. Collaboration: By sharing the feature vectors and related data, researchers enable better collaboration within their field. Other researchers can use this data for their own studies, validate findings, or develop new methods based on the existing work. This not only fosters a sense of community but also helps to advance the field more rapidly by building upon previous research efforts.&#10;&#10;In the context of ten-minute Switchboard conversations, sharing feature vectors on a frame-by-frame basis allows researchers to create a robust and adaptable system for prosodic analysis. The shared data can help address the challenges associated with managing large files containing numerous frames while maintaining flexibility for further computations or modifications by researchers. Additionally, having easily accessible and usable data promotes collaboration and knowledge sharing within the research community, which is essential for advancing our understanding of speech processing and analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target="Based on the discussion, the restriction for adding supplementary information to a type specified as 'utt' is that it cannot be directly added to the existing 'utt' type. This is because 'utt' is used to identify an utterance in the transcription file, and changing this type might affect its consistency across different annotation files.&#10;&#10;If the team wants to add supplementary information, they would need to create a new type, which would require defining the criteria and rules for that specific type. This process involves careful consideration of how the new type will interact with existing types and the overall structure of the database. It may also involve additional work in modifying annotation scripts or tools to accommodate the new type.&#10;&#10;To make a different type, the team would need to:&#10;1. Determine the purpose and criteria for the new type&#10;2. Define how it will interact with other types in the database&#10;3. Modify annotation scripts or tools to recognize and process the new type, if necessary&#10;4. Test the new type to ensure compatibility with existing annotations and the forced alignment program.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target="The issue at hand is the difficulty in reliably merging different versions of word transcripts, especially when those changes are tied to acoustic segments. The speakers express concern that crucial information may be lost during the merge process, making it challenging to combine different annotations or coding methods. They discuss the need for a stable reference point, such as speaker ons and offs or acoustic segments with time-marks, to reduce ambiguity in the merging process. However, they acknowledge that this information might not always be available or correctly preserved during merges. The aim is to create a consistent and clear annotation system that can handle changes in word transcripts while maintaining the original time-stamps and IDs of entries in a relational database.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target="The goals of the project involve creating a system where prosodic features are represented as continuous values at the word or segment level, rather than being tied to specific phones or frames. This approach allows for more flexibility and easier computation in understanding and analyzing speech data. The researchers prefer simpler shapes at the word or segment level because it provides a good balance between granularity and complexity, reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;&#10;The methods to achieve these goals include using a relational database format to store word transcripts, discourse codings, and time-marked data from XML files. This approach provides flexibility for further computations or modifications by researchers. The team is also considering using consistent markers (e.g., 'utt' for utterances) to identify units of speech and potentially prosody markers to denote aspects related to pitch, stress, or intonation in speech data. Additionally, the project aims to define clear merging criteria and rules while utilizing speaker ons and offs as reliable reference points for tying other annotations together, thereby reducing ambiguity in the merging process.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target="1. The recommended way to handle data that has been edited and distributed among multiple parties for accurate merging involves the use of a reliable merging tool or method that can incorporate different annotations or coding methods while maintaining unique word boundaries in a consistent manner.&#10;2. Utilizing a stable reference point, such as speaker onsets and offsets or acoustic segments with time-marks, can help reduce ambiguity during the merging process.&#10;3. Carefully defining the criteria and rules for merging is crucial to specify what is meant by &quot;merging&quot; different annotations or coding methods.&#10;4. Implementing a flexible database system, like a relational database that stores information from XML files, can accommodate modifications in word transcripts and time boundaries due to new recognition outputs or changes in forced alignment. This system allows for recomputing features and updating entries while preserving original time-stamps and IDs of the entries.&#10;5. Ensuring consistency and clarity in the annotation process by using clear markers, like 'utt' for utterances, can help manage changes in word transcripts or utterance boundaries, ultimately facilitating accurate merging of edited data from multiple parties.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target="The speakers discuss an efficient way to extract and find information in a complex structure by using a relational database that stores information from XML files. This approach allows flexibility for further computations or modifications by researchers, ensuring consistency and clarity in the annotation process. They also mention the importance of maintaining unique word boundaries while incorporating different annotations or coding methods, acknowledging that changes in word transcripts might create challenges during merges. However, they do not explicitly provide a solution for efficiently extracting and finding information in complex structures in this transcript.&#10;&#10;To efficiently extract and find information in a complex structure like an XML file or relational database, you can use query languages such as XQuery for XML files or SQL (Structured Query Language) for relational databases. These query languages allow users to search, filter, and manipulate data based on specific criteria, making it easier to locate the desired information within large and complex datasets. To implement this solution, further details about the structure of the data and the desired information would be required.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main challenge in merging two versions of something, as discussed by the team, is specifying what is meant by &quot;merging&quot; - i.e., determining the criteria and rules for combining different annotations or coding methods. In their specific context, they are dealing with word transcripts and discourse codings that need to be linked in a relational database while maintaining flexibility for further computations or modifications by researchers.&#10;&#10;The tracking of speaker ons and offs is considered more reliable because, unlike utterances, speaker changes are less likely to undergo significant modification during the merging process. This information can serve as a stable reference point for tying other annotations together, thereby reducing ambiguity in the merging process.&#10;&#10;In summary, the main challenge lies in defining the merging criteria and rules, while speaker ons and offs provide a more reliable reference point for merging different annotations or coding methods." target="Speaker PhD A expresses skepticism about being able to perform meaningful actions or recover information from a text representation that does not reflect the structure of the original words and annotations. They argue that the text representation will not contain sufficient information to accurately reconstruct the original annotations, as it does not capture the structural relationships between words and annotations.&#10;&#10;Speaker Grad C disagrees with this assessment, arguing that it is still possible to recover the original structure from the text representation. They suggest that by tagging certain features of the text (such as sentence boundaries), it may be possible to reconstruct the original annotations with a high degree of accuracy.&#10;&#10;Overall, the disagreement between Speaker PhD A and Speaker Grad C centers on whether it is possible to accurately recover the original structure of words and annotations from a text representation that does not explicitly encode this information. While Speaker PhD A is skeptical of this approach, Speaker Grad C believes that it may be possible with the use of additional tags or markers.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." target=" {disfmarker} other tag later in the file that would be something like , um , oh , I don't know , {comment} uh , {nonvocalsound} &quot; noise - type equals {nonvocalsound} door - slam &quot; . You know ? And then , uh , {nonvocalsound} you could either say &quot; time equals a particular time - mark &quot; or you could do other sorts of references . So {disfmarker} or {disfmarker} or you might have a prosody {disfmarker} &quot; Prosody &quot; right ? D ? T ? D ? T ? T ?&#10;Speaker: PhD F&#10;Content: It 's an O instead of an I , but the D is good .&#10;Speaker: Grad C&#10;Content: You like the D ? That 's a good D .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Um , you know , so you could have some sort of type here , and then you could have , um {disfmarker} the utterance that it 's referring to could be U - seventeen or something like that .&#10;Speaker: PhD F&#10;Content: OK">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." target=" what they mean yourself .&#10;Speaker: PhD F&#10;Content: And why did you not choose that type of approach ?&#10;Speaker: Grad C&#10;Content: Uh , because I knew that we were doing speech , and I thought it was better if you 're looking at a raw file to be {disfmarker} t for the tags to say &quot; it 's an utterance &quot; , as opposed to the tag to say &quot; it 's a link &quot; .&#10;Speaker: PhD F&#10;Content: OK . OK .&#10;Speaker: Grad C&#10;Content: So , but {disfmarker}&#10;Speaker: PhD F&#10;Content: But other than that , are they compatible ? I mean , you could sort of {disfmarker}&#10;Speaker: Grad C&#10;Content: Yeah , they 're reasonably compatible .&#10;Speaker: PhD F&#10;Content: I mean , you {disfmarker} you could {disfmarker}&#10;Speaker: PhD D&#10;Content: You could probably translate between them .&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD F&#10;Content: Yeah , that 's w So ,&#10;Speaker: Grad C&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." target=" T - eighteen &quot; . So what that 's saying is , we know it starts at this particular time . We don't know when it ends .&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Grad C&#10;Content: Right ? But it ends at this T - eighteen , which may be somewhere else . We say there 's another utterance . We don't know what the t time actually is but we know that it 's the same time as this end time .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Grad C&#10;Content: You know , thirty - eight , whatever you want .&#10;Speaker: PhD A&#10;Content: So you 're essentially defining a lattice .&#10;Speaker: Grad C&#10;Content: OK . Yes , exactly .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: And then , uh {disfmarker} and then these also have I Ds . Right ? So you could {disfmarker} you could have some sort of other {disfmarker} other tag later in the file that would be something like , um , oh , I don't know , {comment} uh ,">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." target="1. Relational database format: The team can consider storing the time-marked data, word transcripts, and annotations in a relational database instead of the P-files. This would allow for greater flexibility in computations and modifications by researchers.&#10;2. XML files: Since the team has been working with XML files, they could continue using this format as an alternative to the P-files, especially if there is existing software or tools that can handle XML data.&#10;3. Other tag or marker formats: The team can explore other tag or marker formats used in forced alignment programs to identify utterances and prosody markers. By evaluating their compatibility with the current system, they might find a more suitable alternative that still maintains the benefits of P-files, such as ease of use and familiarity.&#10;4. IP-API: If an API (Application Programming Interface) is available for the P-files, it could be used to facilitate communication between different systems or software components, making it easier to switch from P-files to another format if necessary.&#10;5. Utilities and libraries: The team should also consider any libraries or utilities available for other tag or marker formats, as these can help streamline the transition process and minimize the learning curve associated with adopting a new system.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." target="1. The concept being discussed refers to the creation of a single time-line with unique identifiers (IDs) attached to different sections in a transcription file. This approach allows for consistent annotation and easy reference between various sections within the transcription. By starting the timeline tag with a consistent marker, such as 'utt' for utterances, the team can maintain clarity and consistency in their annotation process. This concept is significant because it enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." target="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." target="1. The abbreviation &quot;P&quot; in &quot;P-files&quot; most likely stands for &quot;Phil,&quot; referring to Phil Kohn. This conclusion can be drawn from the comment made by PhD D: &quot;Probably stands for 'Phil.' Phil Kohn.&quot; (Speaker: PhD D, Content: Probably stands for &quot;Phil&quot;. Phil Kohn.)&#10;&#10;2. The significance of P-files in this context is their role as a file format used for storing time-marked data, word transcripts, and annotations in speech research. As mentioned by Grad C, the Quicknet library has utilities to handle these files efficiently. (Speaker: Grad C, Content: But there are nice they're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P-files.)">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." target="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." target="1. To efficiently extract and distinguish words within a prosodic boundary and those within a &quot;disfluency marker,&quot; the researchers can utilize their proposed relational database format to store this information from XML files. This would allow for flexibility in computations and modifications by researchers. They can create specific tags or identifiers for prosodic boundaries, disfluency markers, and individual words. By querying the database with these specific tags, they can extract all the words associated with a particular prosodic boundary or disfluency marker.&#10;2. For example, they can use a tag like 'utt' to identify utterances (units of speech starting with a pause and ending with a pause) and 'disfmarker' for disfluency markers. When a word is associated with both a prosodic boundary and a disfluency marker, it would have both tags. By using a relational database, they can efficiently query the data to extract words based on these tags and distinguish between words within prosodic boundaries and those within disfluency markers.&#10;&#10;In summary, by implementing a relational database system with specific tags for prosodic boundaries, sentences, and disfluency markers, the researchers can efficiently extract and distinguish words associated with each category. This system allows for easier computation and analysis of speech data while maintaining clarity and consistency in the annotation process.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." target="Based on the discussion, the restriction for adding supplementary information to a type specified as 'utt' is that it cannot be directly added to the existing 'utt' type. This is because 'utt' is used to identify an utterance in the transcription file, and changing this type might affect its consistency across different annotation files.&#10;&#10;If the team wants to add supplementary information, they would need to create a new type, which would require defining the criteria and rules for that specific type. This process involves careful consideration of how the new type will interact with existing types and the overall structure of the database. It may also involve additional work in modifying annotation scripts or tools to accommodate the new type.&#10;&#10;To make a different type, the team would need to:&#10;1. Determine the purpose and criteria for the new type&#10;2. Define how it will interact with other types in the database&#10;3. Modify annotation scripts or tools to recognize and process the new type, if necessary&#10;4. Test the new type to ensure compatibility with existing annotations and the forced alignment program.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." target="The goals of the project involve creating a system where prosodic features are represented as continuous values at the word or segment level, rather than being tied to specific phones or frames. This approach allows for more flexibility and easier computation in understanding and analyzing speech data. The researchers prefer simpler shapes at the word or segment level because it provides a good balance between granularity and complexity, reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;&#10;The methods to achieve these goals include using a relational database format to store word transcripts, discourse codings, and time-marked data from XML files. This approach provides flexibility for further computations or modifications by researchers. The team is also considering using consistent markers (e.g., 'utt' for utterances) to identify units of speech and potentially prosody markers to denote aspects related to pitch, stress, or intonation in speech data. Additionally, the project aims to define clear merging criteria and rules while utilizing speaker ons and offs as reliable reference points for tying other annotations together, thereby reducing ambiguity in the merging process.">
      <data key="d0">1</data>
    </edge>
    <edge source="The discussion revolves around the use of tags or markers in speech data, specifically for a forced alignment program that links word transcripts, annotations, and time-marked data. The team is considering using a relational database format to store this information from XML files, allowing flexibility for further computations or modifications by researchers. They discuss the compatibility between different types of tags and how they could translate between them.&#10;&#10;An example of an utterance referred to as &quot;U-seventeen&quot; is mentioned in the context of using a specific type (e.g., 'utt') to identify utterances, which are units of speech starting with a pause and ending with a pause. Using this consistent marker helps maintain clarity and consistency in the annotation process. The team also briefly mentions the concept of prosody markers, which could be used to denote aspects related to pitch, stress, or intonation in speech data." target="Based on the discussion transcript, the team is considering using a relational database in a format that stores information from XML files to express the type of a sentence in a file. This would allow flexibility for further computations or modifications by researchers. The exact tag or marker for this information is not explicitly stated in the transcript. However, they discuss the importance of maintaining consistency and clarity in the annotation process, as well as ensuring that any new types created can interact properly with existing types in the database. Therefore, careful consideration and definition of the criteria and rules for the new type would be required before implementation.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker} other tag later in the file that would be something like , um , oh , I don't know , {comment} uh , {nonvocalsound} &quot; noise - type equals {nonvocalsound} door - slam &quot; . You know ? And then , uh , {nonvocalsound} you could either say &quot; time equals a particular time - mark &quot; or you could do other sorts of references . So {disfmarker} or {disfmarker} or you might have a prosody {disfmarker} &quot; Prosody &quot; right ? D ? T ? D ? T ? T ?&#10;Speaker: PhD F&#10;Content: It 's an O instead of an I , but the D is good .&#10;Speaker: Grad C&#10;Content: You like the D ? That 's a good D .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Um , you know , so you could have some sort of type here , and then you could have , um {disfmarker} the utterance that it 's referring to could be U - seventeen or something like that .&#10;Speaker: PhD F&#10;Content: OK" target="1. To efficiently extract and distinguish words within a prosodic boundary and those within a &quot;disfluency marker,&quot; the researchers can utilize their proposed relational database format to store this information from XML files. This would allow for flexibility in computations and modifications by researchers. They can create specific tags or identifiers for prosodic boundaries, disfluency markers, and individual words. By querying the database with these specific tags, they can extract all the words associated with a particular prosodic boundary or disfluency marker.&#10;2. For example, they can use a tag like 'utt' to identify utterances (units of speech starting with a pause and ending with a pause) and 'disfmarker' for disfluency markers. When a word is associated with both a prosodic boundary and a disfluency marker, it would have both tags. By using a relational database, they can efficiently query the data to extract words based on these tags and distinguish between words within prosodic boundaries and those within disfluency markers.&#10;&#10;In summary, by implementing a relational database system with specific tags for prosodic boundaries, sentences, and disfluency markers, the researchers can efficiently extract and distinguish words associated with each category. This system allows for easier computation and analysis of speech data while maintaining clarity and consistency in the annotation process.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Relational database format: The team can consider storing the time-marked data, word transcripts, and annotations in a relational database instead of the P-files. This would allow for greater flexibility in computations and modifications by researchers.&#10;2. XML files: Since the team has been working with XML files, they could continue using this format as an alternative to the P-files, especially if there is existing software or tools that can handle XML data.&#10;3. Other tag or marker formats: The team can explore other tag or marker formats used in forced alignment programs to identify utterances and prosody markers. By evaluating their compatibility with the current system, they might find a more suitable alternative that still maintains the benefits of P-files, such as ease of use and familiarity.&#10;4. IP-API: If an API (Application Programming Interface) is available for the P-files, it could be used to facilitate communication between different systems or software components, making it easier to switch from P-files to another format if necessary.&#10;5. Utilities and libraries: The team should also consider any libraries or utilities available for other tag or marker formats, as these can help streamline the transition process and minimize the learning curve associated with adopting a new system." target=" worked with P - files .&#10;Speaker: PhD F&#10;Content: Yeah ?&#10;Speaker: PhD D&#10;Content: I don't remember what the &quot; P &quot; is , though .&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Grad C&#10;Content: But there are ni they 're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P - files ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: so it works pretty well .&#10;Speaker: PhD A&#10;Content: &#10;Speaker: PhD F&#10;Content: And that isn't really , I guess , as important as the {disfmarker} the main {disfmarker} I don't know what you call it , the {disfmarker} the main sort of word - level {disfmarker}&#10;Speaker: Grad C&#10;Content: Neither do I .&#10;Speaker: PhD D&#10;Content: Probably stands for &quot; Phil &quot; . Phil Kohn .&#10;Speaker: Grad C&#10;Content: It 's a Phil file ?&#10;Speaker: PhD D&#10;Content: Yeah . That 's">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Relational database format: The team can consider storing the time-marked data, word transcripts, and annotations in a relational database instead of the P-files. This would allow for greater flexibility in computations and modifications by researchers.&#10;2. XML files: Since the team has been working with XML files, they could continue using this format as an alternative to the P-files, especially if there is existing software or tools that can handle XML data.&#10;3. Other tag or marker formats: The team can explore other tag or marker formats used in forced alignment programs to identify utterances and prosody markers. By evaluating their compatibility with the current system, they might find a more suitable alternative that still maintains the benefits of P-files, such as ease of use and familiarity.&#10;4. IP-API: If an API (Application Programming Interface) is available for the P-files, it could be used to facilitate communication between different systems or software components, making it easier to switch from P-files to another format if necessary.&#10;5. Utilities and libraries: The team should also consider any libraries or utilities available for other tag or marker formats, as these can help streamline the transition process and minimize the learning curve associated with adopting a new system." target=": So , I guess , yeah , if {disfmarker} if you and Don can {disfmarker} if you can show him the P - file stuff and see .&#10;Speaker: Grad C&#10;Content: Sure .&#10;Speaker: PhD F&#10;Content: So this would be like for the F - zero {disfmarker}&#10;Speaker: Grad B&#10;Content: True .&#10;Speaker: Grad C&#10;Content: I mean , if you do &quot; man P - file &quot; or &quot; apropos P - file &quot; , you 'll see a lot .&#10;Speaker: Grad B&#10;Content: I 've used the P - file , I think . I 've looked at it at least , briefly , I think when we were doing s something .&#10;Speaker: PhD A&#10;Content: What does the P stand for anyway ?&#10;Speaker: Grad C&#10;Content: I have no idea .&#10;Speaker: Grad B&#10;Content: Oh , in there .&#10;Speaker: Grad C&#10;Content: I didn't de I didn't develop it . You know , it was {disfmarker} I think it was Dave Johnson . So it 's all part of the Quick">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Relational database format: The team can consider storing the time-marked data, word transcripts, and annotations in a relational database instead of the P-files. This would allow for greater flexibility in computations and modifications by researchers.&#10;2. XML files: Since the team has been working with XML files, they could continue using this format as an alternative to the P-files, especially if there is existing software or tools that can handle XML data.&#10;3. Other tag or marker formats: The team can explore other tag or marker formats used in forced alignment programs to identify utterances and prosody markers. By evaluating their compatibility with the current system, they might find a more suitable alternative that still maintains the benefits of P-files, such as ease of use and familiarity.&#10;4. IP-API: If an API (Application Programming Interface) is available for the P-files, it could be used to facilitate communication between different systems or software components, making it easier to switch from P-files to another format if necessary.&#10;5. Utilities and libraries: The team should also consider any libraries or utilities available for other tag or marker formats, as these can help streamline the transition process and minimize the learning curve associated with adopting a new system." target="&#10;Speaker: Grad B&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Sure .&#10;Speaker: PhD F&#10;Content: um {disfmarker} I think we 're {disfmarker} we 're {disfmarker} {vocalsound} we 're actually just {disfmarker}&#10;Speaker: Grad C&#10;Content: We 're about done .&#10;Speaker: PhD F&#10;Content: yeah ,&#10;Speaker: Grad B&#10;Content: Hmm .&#10;Speaker: PhD F&#10;Content: wrapping up , but , um {disfmarker} Yeah , sorry , it 's a uh short meeting , but , um {disfmarker} Well , I don't know . Is there anything else , like {disfmarker} I mean that helps me a lot ,&#10;Speaker: Grad C&#10;Content: Well , I think the other thing we might want to look at is alternatives to P - file .&#10;Speaker: PhD F&#10;Content: but {disfmarker}&#10;Speaker: Grad C&#10;Content: I mean , th the reason I like P - file is I 'm already familiar with it , we have">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Relational database format: The team can consider storing the time-marked data, word transcripts, and annotations in a relational database instead of the P-files. This would allow for greater flexibility in computations and modifications by researchers.&#10;2. XML files: Since the team has been working with XML files, they could continue using this format as an alternative to the P-files, especially if there is existing software or tools that can handle XML data.&#10;3. Other tag or marker formats: The team can explore other tag or marker formats used in forced alignment programs to identify utterances and prosody markers. By evaluating their compatibility with the current system, they might find a more suitable alternative that still maintains the benefits of P-files, such as ease of use and familiarity.&#10;4. IP-API: If an API (Application Programming Interface) is available for the P-files, it could be used to facilitate communication between different systems or software components, making it easier to switch from P-files to another format if necessary.&#10;5. Utilities and libraries: The team should also consider any libraries or utilities available for other tag or marker formats, as these can help streamline the transition process and minimize the learning curve associated with adopting a new system." target="}&#10;Speaker: Grad C&#10;Content: I mean , th the reason I like P - file is I 'm already familiar with it , we have expertise here , and so if we pick something else , there 's the learning - curve problem . But , I mean , it is just something we developed at ICSI .&#10;Speaker: PhD A&#10;Content: Is there an {disfmarker} is there an IP - API ?&#10;Speaker: Grad C&#10;Content: And so {disfmarker} Yeah .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Grad C&#10;Content: There 's an API for it . And , uh ,&#10;Speaker: PhD A&#10;Content: There used to be a problem that they get too large ,&#10;Speaker: Grad C&#10;Content: a bunch of libraries , P - file utilities .&#10;Speaker: PhD A&#10;Content: and so {pause} basically the {disfmarker} uh the filesystem wouldn't {disfmarker}&#10;Speaker: Grad C&#10;Content: Well , that 's gonna be a problem no matter what . You have the two - gigabyte limit on the filesystem size . And we definitely hit">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Relational database format: The team can consider storing the time-marked data, word transcripts, and annotations in a relational database instead of the P-files. This would allow for greater flexibility in computations and modifications by researchers.&#10;2. XML files: Since the team has been working with XML files, they could continue using this format as an alternative to the P-files, especially if there is existing software or tools that can handle XML data.&#10;3. Other tag or marker formats: The team can explore other tag or marker formats used in forced alignment programs to identify utterances and prosody markers. By evaluating their compatibility with the current system, they might find a more suitable alternative that still maintains the benefits of P-files, such as ease of use and familiarity.&#10;4. IP-API: If an API (Application Programming Interface) is available for the P-files, it could be used to facilitate communication between different systems or software components, making it easier to switch from P-files to another format if necessary.&#10;5. Utilities and libraries: The team should also consider any libraries or utilities available for other tag or marker formats, as these can help streamline the transition process and minimize the learning curve associated with adopting a new system." target=" didn't develop it . You know , it was {disfmarker} I think it was Dave Johnson . So it 's all part of the Quicknet library . It has all the utilities for it .&#10;Speaker: PhD A&#10;Content: No , P - files were around way before Quicknet . P - files were {disfmarker} were around when {disfmarker} w with , um , {vocalsound} RAP .&#10;Speaker: Grad C&#10;Content: Oh , were they ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Right ?&#10;Speaker: PhD F&#10;Content: It 's like the history of ICSI .&#10;Speaker: PhD A&#10;Content: You worked with P - files .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Like {disfmarker}&#10;Speaker: PhD D&#10;Content: No .&#10;Speaker: PhD A&#10;Content: I worked with P - files .&#10;Speaker: PhD F&#10;Content: Yeah ?&#10;Speaker: PhD D&#10;Content: I don't remember what the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Relational database format: The team can consider storing the time-marked data, word transcripts, and annotations in a relational database instead of the P-files. This would allow for greater flexibility in computations and modifications by researchers.&#10;2. XML files: Since the team has been working with XML files, they could continue using this format as an alternative to the P-files, especially if there is existing software or tools that can handle XML data.&#10;3. Other tag or marker formats: The team can explore other tag or marker formats used in forced alignment programs to identify utterances and prosody markers. By evaluating their compatibility with the current system, they might find a more suitable alternative that still maintains the benefits of P-files, such as ease of use and familiarity.&#10;4. IP-API: If an API (Application Programming Interface) is available for the P-files, it could be used to facilitate communication between different systems or software components, making it easier to switch from P-files to another format if necessary.&#10;5. Utilities and libraries: The team should also consider any libraries or utilities available for other tag or marker formats, as these can help streamline the transition process and minimize the learning curve associated with adopting a new system." target="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Relational database format: The team can consider storing the time-marked data, word transcripts, and annotations in a relational database instead of the P-files. This would allow for greater flexibility in computations and modifications by researchers.&#10;2. XML files: Since the team has been working with XML files, they could continue using this format as an alternative to the P-files, especially if there is existing software or tools that can handle XML data.&#10;3. Other tag or marker formats: The team can explore other tag or marker formats used in forced alignment programs to identify utterances and prosody markers. By evaluating their compatibility with the current system, they might find a more suitable alternative that still maintains the benefits of P-files, such as ease of use and familiarity.&#10;4. IP-API: If an API (Application Programming Interface) is available for the P-files, it could be used to facilitate communication between different systems or software components, making it easier to switch from P-files to another format if necessary.&#10;5. Utilities and libraries: The team should also consider any libraries or utilities available for other tag or marker formats, as these can help streamline the transition process and minimize the learning curve associated with adopting a new system." target="1. The abbreviation &quot;P&quot; in &quot;P-files&quot; most likely stands for &quot;Phil,&quot; referring to Phil Kohn. This conclusion can be drawn from the comment made by PhD D: &quot;Probably stands for 'Phil.' Phil Kohn.&quot; (Speaker: PhD D, Content: Probably stands for &quot;Phil&quot;. Phil Kohn.)&#10;&#10;2. The significance of P-files in this context is their role as a file format used for storing time-marked data, word transcripts, and annotations in speech research. As mentioned by Grad C, the Quicknet library has utilities to handle these files efficiently. (Speaker: Grad C, Content: But there are nice they're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P-files.)">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Relational database format: The team can consider storing the time-marked data, word transcripts, and annotations in a relational database instead of the P-files. This would allow for greater flexibility in computations and modifications by researchers.&#10;2. XML files: Since the team has been working with XML files, they could continue using this format as an alternative to the P-files, especially if there is existing software or tools that can handle XML data.&#10;3. Other tag or marker formats: The team can explore other tag or marker formats used in forced alignment programs to identify utterances and prosody markers. By evaluating their compatibility with the current system, they might find a more suitable alternative that still maintains the benefits of P-files, such as ease of use and familiarity.&#10;4. IP-API: If an API (Application Programming Interface) is available for the P-files, it could be used to facilitate communication between different systems or software components, making it easier to switch from P-files to another format if necessary.&#10;5. Utilities and libraries: The team should also consider any libraries or utilities available for other tag or marker formats, as these can help streamline the transition process and minimize the learning curve associated with adopting a new system." target="1. Frames: The basic unit of data storage in a P-file is the frame, which likely corresponds to a small segment of audio or speech.&#10;2. Utterances and Sentences: Frames are grouped together into higher-level structures such as utterances and sentences. These groupings provide a more meaningful organization for time-marked data, word transcripts, and annotations.&#10;3. Arbitrary Vector of Values: Each P-file can contain an arbitrary vector of values, which may include different types such as floats, integers, or doubles. This vector could be used to store various features or attributes related to the frames, utterances, or sentences within the file.&#10;4. Header Format: The P-file has a header format that describes some aspects of its contents. Specifically, it stores information about utterance numbers and frame numbers, allowing for easy reference and organization of the data within the file.&#10;5. Cepstral or PLP Values: In the context of speech research, each P-file might store a vector of cepstral or Perceptual Linear Prediction (PLP) values for each frame, which are commonly used features in speech processing and analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source=" worked with P - files .&#10;Speaker: PhD F&#10;Content: Yeah ?&#10;Speaker: PhD D&#10;Content: I don't remember what the &quot; P &quot; is , though .&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Grad C&#10;Content: But there are ni they 're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P - files ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: so it works pretty well .&#10;Speaker: PhD A&#10;Content: &#10;Speaker: PhD F&#10;Content: And that isn't really , I guess , as important as the {disfmarker} the main {disfmarker} I don't know what you call it , the {disfmarker} the main sort of word - level {disfmarker}&#10;Speaker: Grad C&#10;Content: Neither do I .&#10;Speaker: PhD D&#10;Content: Probably stands for &quot; Phil &quot; . Phil Kohn .&#10;Speaker: Grad C&#10;Content: It 's a Phil file ?&#10;Speaker: PhD D&#10;Content: Yeah . That 's" target="1. The abbreviation &quot;P&quot; in &quot;P-files&quot; most likely stands for &quot;Phil,&quot; referring to Phil Kohn. This conclusion can be drawn from the comment made by PhD D: &quot;Probably stands for 'Phil.' Phil Kohn.&quot; (Speaker: PhD D, Content: Probably stands for &quot;Phil&quot;. Phil Kohn.)&#10;&#10;2. The significance of P-files in this context is their role as a file format used for storing time-marked data, word transcripts, and annotations in speech research. As mentioned by Grad C, the Quicknet library has utilities to handle these files efficiently. (Speaker: Grad C, Content: But there are nice they're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P-files.)">
      <data key="d0">1</data>
    </edge>
    <edge source=" worked with P - files .&#10;Speaker: PhD F&#10;Content: Yeah ?&#10;Speaker: PhD D&#10;Content: I don't remember what the &quot; P &quot; is , though .&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Grad C&#10;Content: But there are ni they 're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P - files ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: so it works pretty well .&#10;Speaker: PhD A&#10;Content: &#10;Speaker: PhD F&#10;Content: And that isn't really , I guess , as important as the {disfmarker} the main {disfmarker} I don't know what you call it , the {disfmarker} the main sort of word - level {disfmarker}&#10;Speaker: Grad C&#10;Content: Neither do I .&#10;Speaker: PhD D&#10;Content: Probably stands for &quot; Phil &quot; . Phil Kohn .&#10;Speaker: Grad C&#10;Content: It 's a Phil file ?&#10;Speaker: PhD D&#10;Content: Yeah . That 's" target="1. Frames: The basic unit of data storage in a P-file is the frame, which likely corresponds to a small segment of audio or speech.&#10;2. Utterances and Sentences: Frames are grouped together into higher-level structures such as utterances and sentences. These groupings provide a more meaningful organization for time-marked data, word transcripts, and annotations.&#10;3. Arbitrary Vector of Values: Each P-file can contain an arbitrary vector of values, which may include different types such as floats, integers, or doubles. This vector could be used to store various features or attributes related to the frames, utterances, or sentences within the file.&#10;4. Header Format: The P-file has a header format that describes some aspects of its contents. Specifically, it stores information about utterance numbers and frame numbers, allowing for easy reference and organization of the data within the file.&#10;5. Cepstral or PLP Values: In the context of speech research, each P-file might store a vector of cepstral or Perceptual Linear Prediction (PLP) values for each frame, which are commonly used features in speech processing and analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source=": So , I guess , yeah , if {disfmarker} if you and Don can {disfmarker} if you can show him the P - file stuff and see .&#10;Speaker: Grad C&#10;Content: Sure .&#10;Speaker: PhD F&#10;Content: So this would be like for the F - zero {disfmarker}&#10;Speaker: Grad B&#10;Content: True .&#10;Speaker: Grad C&#10;Content: I mean , if you do &quot; man P - file &quot; or &quot; apropos P - file &quot; , you 'll see a lot .&#10;Speaker: Grad B&#10;Content: I 've used the P - file , I think . I 've looked at it at least , briefly , I think when we were doing s something .&#10;Speaker: PhD A&#10;Content: What does the P stand for anyway ?&#10;Speaker: Grad C&#10;Content: I have no idea .&#10;Speaker: Grad B&#10;Content: Oh , in there .&#10;Speaker: Grad C&#10;Content: I didn't de I didn't develop it . You know , it was {disfmarker} I think it was Dave Johnson . So it 's all part of the Quick" target="1. The abbreviation &quot;P&quot; in &quot;P-files&quot; most likely stands for &quot;Phil,&quot; referring to Phil Kohn. This conclusion can be drawn from the comment made by PhD D: &quot;Probably stands for 'Phil.' Phil Kohn.&quot; (Speaker: PhD D, Content: Probably stands for &quot;Phil&quot;. Phil Kohn.)&#10;&#10;2. The significance of P-files in this context is their role as a file format used for storing time-marked data, word transcripts, and annotations in speech research. As mentioned by Grad C, the Quicknet library has utilities to handle these files efficiently. (Speaker: Grad C, Content: But there are nice they're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P-files.)">
      <data key="d0">1</data>
    </edge>
    <edge source=" didn't develop it . You know , it was {disfmarker} I think it was Dave Johnson . So it 's all part of the Quicknet library . It has all the utilities for it .&#10;Speaker: PhD A&#10;Content: No , P - files were around way before Quicknet . P - files were {disfmarker} were around when {disfmarker} w with , um , {vocalsound} RAP .&#10;Speaker: Grad C&#10;Content: Oh , were they ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Right ?&#10;Speaker: PhD F&#10;Content: It 's like the history of ICSI .&#10;Speaker: PhD A&#10;Content: You worked with P - files .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: Like {disfmarker}&#10;Speaker: PhD D&#10;Content: No .&#10;Speaker: PhD A&#10;Content: I worked with P - files .&#10;Speaker: PhD F&#10;Content: Yeah ?&#10;Speaker: PhD D&#10;Content: I don't remember what the" target="1. The abbreviation &quot;P&quot; in &quot;P-files&quot; most likely stands for &quot;Phil,&quot; referring to Phil Kohn. This conclusion can be drawn from the comment made by PhD D: &quot;Probably stands for 'Phil.' Phil Kohn.&quot; (Speaker: PhD D, Content: Probably stands for &quot;Phil&quot;. Phil Kohn.)&#10;&#10;2. The significance of P-files in this context is their role as a file format used for storing time-marked data, word transcripts, and annotations in speech research. As mentioned by Grad C, the Quicknet library has utilities to handle these files efficiently. (Speaker: Grad C, Content: But there are nice they're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P-files.)">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The concept being discussed refers to the creation of a single time-line with unique identifiers (IDs) attached to different sections in a transcription file. This approach allows for consistent annotation and easy reference between various sections within the transcription. By starting the timeline tag with a consistent marker, such as 'utt' for utterances, the team can maintain clarity and consistency in their annotation process. This concept is significant because it enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database." target="er}&#10;Speaker: Grad C&#10;Content: You mean , this {disfmarker} I guess I am gonna be standing up and drawing on the board .&#10;Speaker: PhD F&#10;Content: OK , yeah . So you should , definitely .&#10;Speaker: Grad C&#10;Content: Um , so {disfmarker} so it definitely had that as a concept . So tha it has a single time - line ,&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: and then you can have lots of different sections , each of which have I Ds attached to it , and then you can refer from other sections to those I Ds , if you want to . So that , um {disfmarker} so that you start with {disfmarker} with a time - line tag . &quot; Time - line &quot; . And then you have a bunch of times . I don't e I don't remember exactly what my notation was ,&#10;Speaker: PhD A&#10;Content: Oh , I remember seeing an example of this .&#10;Speaker: Grad C&#10;Content: but it {disfmarker}&#10;Speaker: PhD F&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The concept being discussed refers to the creation of a single time-line with unique identifiers (IDs) attached to different sections in a transcription file. This approach allows for consistent annotation and easy reference between various sections within the transcription. By starting the timeline tag with a consistent marker, such as 'utt' for utterances, the team can maintain clarity and consistency in their annotation process. This concept is significant because it enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database." target="1. The process of handling forced alignment when time boundaries of words in transcriptions change due to a new recognition output involves using a flexible database format, such as a relational database that stores information from XML files. This allows for easy input and output of data while maintaining flexibility for further computations or modifications by researchers.&#10;2. When alignments, word transcripts, or utterance boundaries are changed, the system enables recomputing features and updating entries. The time-stamps in the time-line can be adjusted without altering the IDs of the entries, preserving the original time-stamps and IDs.&#10;3. A consistent marker, like 'utt' for utterances, is used to start the timeline tag, ensuring clarity and consistency in the annotation process. This concept enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;4. When different annotations of the same word sequences emerge as the data gets shared and used by others, a tool or method to reliably merge these different annotations into a single version is needed. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The concept being discussed refers to the creation of a single time-line with unique identifiers (IDs) attached to different sections in a transcription file. This approach allows for consistent annotation and easy reference between various sections within the transcription. By starting the timeline tag with a consistent marker, such as 'utt' for utterances, the team can maintain clarity and consistency in their annotation process. This concept is significant because it enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database." target="The issue at hand is the difficulty in reliably merging different versions of word transcripts, especially when those changes are tied to acoustic segments. The speakers express concern that crucial information may be lost during the merge process, making it challenging to combine different annotations or coding methods. They discuss the need for a stable reference point, such as speaker ons and offs or acoustic segments with time-marks, to reduce ambiguity in the merging process. However, they acknowledge that this information might not always be available or correctly preserved during merges. The aim is to create a consistent and clear annotation system that can handle changes in word transcripts while maintaining the original time-stamps and IDs of entries in a relational database.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The concept being discussed refers to the creation of a single time-line with unique identifiers (IDs) attached to different sections in a transcription file. This approach allows for consistent annotation and easy reference between various sections within the transcription. By starting the timeline tag with a consistent marker, such as 'utt' for utterances, the team can maintain clarity and consistency in their annotation process. This concept is significant because it enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database." target="1. Based on Grad C's observations a year ago, the annotated transcription graph system was not very complete, lacking external file format representation at that time. The only existing component was a conceptual node annotated transcription graph, which Grad C found appealing. Since then, there have been new developments, such as the creation of their own external file format and the development of numerous tools, although Grad C has not yet reviewed these updates.&#10;2. The team is considering using a relational database to express sentence types from XML files, which would allow flexibility for further computations or modifications by researchers. No specific tag or marker for this information has been determined in the transcript. However, the team emphasizes the importance of maintaining consistency and clarity in the annotation process and ensuring that any new types created can interact properly with existing types in the database.&#10;3. The use of a single timeline with unique identifiers (IDs) attached to different sections in a transcription file ensures consistent annotation and easy reference between various sections within the transcription. This concept, which starts the timeline tag with a consistent marker like 'utt' for utterances, enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;&#10;In summary, the annotated transcription graph system has made progress since Grad C's observations a year ago, with new developments such as an external file format and various tools. However, these updates have not been reviewed by Grad C yet. The team is considering using a relational database for flexibility in expressing sentence types from XML files while maintaining consistency and clarity in the annotation process. The single timeline concept allows for consistent annotation and reference between sections within the transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments." target=" nature . They 'll be looking at either a word - level prosodic , uh , an {disfmarker} a value ,&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: like a continuous value , like the slope of something . But you know , we 'll do something where we {disfmarker} some kind of data reduction where the prosodic features are sort o uh , either at the word - level or at the segment - level ,&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: or {disfmarker} or something like that . They 're not gonna be at the phone - level and they 're no not gonna be at the frame - level when we get done with sort of giving them simpler shapes and things . And so the main thing is just being able {disfmarker} Well , I guess , the two goals . Um , one that Chuck mentioned is starting out with something that we don't have to start over , that we don't have to throw away if other people want to extend it for other kinds of questions ,&#10;Speaker: Grad C&#10;Content: Right .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments." target="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments." target="1. To efficiently extract and distinguish words within a prosodic boundary and those within a &quot;disfluency marker,&quot; the researchers can utilize their proposed relational database format to store this information from XML files. This would allow for flexibility in computations and modifications by researchers. They can create specific tags or identifiers for prosodic boundaries, disfluency markers, and individual words. By querying the database with these specific tags, they can extract all the words associated with a particular prosodic boundary or disfluency marker.&#10;2. For example, they can use a tag like 'utt' to identify utterances (units of speech starting with a pause and ending with a pause) and 'disfmarker' for disfluency markers. When a word is associated with both a prosodic boundary and a disfluency marker, it would have both tags. By using a relational database, they can efficiently query the data to extract words based on these tags and distinguish between words within prosodic boundaries and those within disfluency markers.&#10;&#10;In summary, by implementing a relational database system with specific tags for prosodic boundaries, sentences, and disfluency markers, the researchers can efficiently extract and distinguish words associated with each category. This system allows for easier computation and analysis of speech data while maintaining clarity and consistency in the annotation process.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments." target="The sharing aspect is important in the context of storing feature vectors on a frame-by-frame basis for two main reasons: adaptability and collaboration. By making the data more accessible and usable, other researchers can adopt and build upon this work more easily.&#10;&#10;1. Adaptability: Creating a system where prosodic features are represented as continuous values at the word or segment level allows for greater flexibility in analyzing speech data. This approach makes it easier to adapt the system to various research questions and domains, as researchers can work with simpler and more meaningful structures (words or segments) instead of being constrained by specific phones or frames.&#10;2. Collaboration: By sharing the feature vectors and related data, researchers enable better collaboration within their field. Other researchers can use this data for their own studies, validate findings, or develop new methods based on the existing work. This not only fosters a sense of community but also helps to advance the field more rapidly by building upon previous research efforts.&#10;&#10;In the context of ten-minute Switchboard conversations, sharing feature vectors on a frame-by-frame basis allows researchers to create a robust and adaptable system for prosodic analysis. The shared data can help address the challenges associated with managing large files containing numerous frames while maintaining flexibility for further computations or modifications by researchers. Additionally, having easily accessible and usable data promotes collaboration and knowledge sharing within the research community, which is essential for advancing our understanding of speech processing and analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments." target="1. The limitation of frame-level analysis for phone-level speech processing, as discussed by the Ph.D. speakers, is that most frames are not actual speech, but silent or transitional periods between sounds. This makes frame-level analysis less reliable for phone-level speech processing since phones can undergo significant modification during merging processes, and analyzing at this level increases computational burden without adding meaningful value to the analysis of prosodic features.&#10;   &#10;2. The average number of phones in an English word is not explicitly stated in the transcript, but it is mentioned that there are approximately five frames per phone. This suggests that the average number of phones in an English word could be around five or more, given that some words might have additional phones due to affixes or other phonetic processes.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments." target="The goals of the project involve creating a system where prosodic features are represented as continuous values at the word or segment level, rather than being tied to specific phones or frames. This approach allows for more flexibility and easier computation in understanding and analyzing speech data. The researchers prefer simpler shapes at the word or segment level because it provides a good balance between granularity and complexity, reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;&#10;The methods to achieve these goals include using a relational database format to store word transcripts, discourse codings, and time-marked data from XML files. This approach provides flexibility for further computations or modifications by researchers. The team is also considering using consistent markers (e.g., 'utt' for utterances) to identify units of speech and potentially prosody markers to denote aspects related to pitch, stress, or intonation in speech data. Additionally, the project aims to define clear merging criteria and rules while utilizing speaker ons and offs as reliable reference points for tying other annotations together, thereby reducing ambiguity in the merging process.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The goal of the researchers is to create a system where prosodic features are represented as continuous values (like the slope of something) at either the word or segment level, instead of being tied to specific phones or frames. This allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;2. They prefer simpler shapes at the word or segment level rather than the phone or frame level because this approach provides a good balance between granularity and complexity. Prosodic features can still be meaningfully analyzed while reducing the computational burden that would come with analyzing these aspects at a more detailed level (phone or frame). Additionally, phones and frames can undergo significant modification during merging processes, making them less reliable as reference points compared to words or segments." target="Yes, it is possible for a prosodic phrase to span across two sentences in the scenario described. This possibility is acknowledged by the researchers when they discuss different segmentation levels that do not nest, meaning they may not fit neatly within each other. In this case, a prosodic phrase might start in one sentence and end in another without being contained entirely within either sentence.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The abbreviation &quot;P&quot; in &quot;P-files&quot; most likely stands for &quot;Phil,&quot; referring to Phil Kohn. This conclusion can be drawn from the comment made by PhD D: &quot;Probably stands for 'Phil.' Phil Kohn.&quot; (Speaker: PhD D, Content: Probably stands for &quot;Phil&quot;. Phil Kohn.)&#10;&#10;2. The significance of P-files in this context is their role as a file format used for storing time-marked data, word transcripts, and annotations in speech research. As mentioned by Grad C, the Quicknet library has utilities to handle these files efficiently. (Speaker: Grad C, Content: But there are nice they're {disfmarker} The {pause} Quicknet library has a bunch of things in it to handle P-files.)" target="1. Frames: The basic unit of data storage in a P-file is the frame, which likely corresponds to a small segment of audio or speech.&#10;2. Utterances and Sentences: Frames are grouped together into higher-level structures such as utterances and sentences. These groupings provide a more meaningful organization for time-marked data, word transcripts, and annotations.&#10;3. Arbitrary Vector of Values: Each P-file can contain an arbitrary vector of values, which may include different types such as floats, integers, or doubles. This vector could be used to store various features or attributes related to the frames, utterances, or sentences within the file.&#10;4. Header Format: The P-file has a header format that describes some aspects of its contents. Specifically, it stores information about utterance numbers and frame numbers, allowing for easy reference and organization of the data within the file.&#10;5. Cepstral or PLP Values: In the context of speech research, each P-file might store a vector of cepstral or Perceptual Linear Prediction (PLP) values for each frame, which are commonly used features in speech processing and analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data." target="1. To efficiently extract and distinguish words within a prosodic boundary and those within a &quot;disfluency marker,&quot; the researchers can utilize their proposed relational database format to store this information from XML files. This would allow for flexibility in computations and modifications by researchers. They can create specific tags or identifiers for prosodic boundaries, disfluency markers, and individual words. By querying the database with these specific tags, they can extract all the words associated with a particular prosodic boundary or disfluency marker.&#10;2. For example, they can use a tag like 'utt' to identify utterances (units of speech starting with a pause and ending with a pause) and 'disfmarker' for disfluency markers. When a word is associated with both a prosodic boundary and a disfluency marker, it would have both tags. By using a relational database, they can efficiently query the data to extract words based on these tags and distinguish between words within prosodic boundaries and those within disfluency markers.&#10;&#10;In summary, by implementing a relational database system with specific tags for prosodic boundaries, sentences, and disfluency markers, the researchers can efficiently extract and distinguish words associated with each category. This system allows for easier computation and analysis of speech data while maintaining clarity and consistency in the annotation process.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data." target="The sharing aspect is important in the context of storing feature vectors on a frame-by-frame basis for two main reasons: adaptability and collaboration. By making the data more accessible and usable, other researchers can adopt and build upon this work more easily.&#10;&#10;1. Adaptability: Creating a system where prosodic features are represented as continuous values at the word or segment level allows for greater flexibility in analyzing speech data. This approach makes it easier to adapt the system to various research questions and domains, as researchers can work with simpler and more meaningful structures (words or segments) instead of being constrained by specific phones or frames.&#10;2. Collaboration: By sharing the feature vectors and related data, researchers enable better collaboration within their field. Other researchers can use this data for their own studies, validate findings, or develop new methods based on the existing work. This not only fosters a sense of community but also helps to advance the field more rapidly by building upon previous research efforts.&#10;&#10;In the context of ten-minute Switchboard conversations, sharing feature vectors on a frame-by-frame basis allows researchers to create a robust and adaptable system for prosodic analysis. The shared data can help address the challenges associated with managing large files containing numerous frames while maintaining flexibility for further computations or modifications by researchers. Additionally, having easily accessible and usable data promotes collaboration and knowledge sharing within the research community, which is essential for advancing our understanding of speech processing and analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data." target="1. The limitation of frame-level analysis for phone-level speech processing, as discussed by the Ph.D. speakers, is that most frames are not actual speech, but silent or transitional periods between sounds. This makes frame-level analysis less reliable for phone-level speech processing since phones can undergo significant modification during merging processes, and analyzing at this level increases computational burden without adding meaningful value to the analysis of prosodic features.&#10;   &#10;2. The average number of phones in an English word is not explicitly stated in the transcript, but it is mentioned that there are approximately five frames per phone. This suggests that the average number of phones in an English word could be around five or more, given that some words might have additional phones due to affixes or other phonetic processes.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data." target="The issue at hand is the difficulty in reliably merging different versions of word transcripts, especially when those changes are tied to acoustic segments. The speakers express concern that crucial information may be lost during the merge process, making it challenging to combine different annotations or coding methods. They discuss the need for a stable reference point, such as speaker ons and offs or acoustic segments with time-marks, to reduce ambiguity in the merging process. However, they acknowledge that this information might not always be available or correctly preserved during merges. The aim is to create a consistent and clear annotation system that can handle changes in word transcripts while maintaining the original time-stamps and IDs of entries in a relational database.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data." target="The goals of the project involve creating a system where prosodic features are represented as continuous values at the word or segment level, rather than being tied to specific phones or frames. This approach allows for more flexibility and easier computation in understanding and analyzing speech data. The researchers prefer simpler shapes at the word or segment level because it provides a good balance between granularity and complexity, reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;&#10;The methods to achieve these goals include using a relational database format to store word transcripts, discourse codings, and time-marked data from XML files. This approach provides flexibility for further computations or modifications by researchers. The team is also considering using consistent markers (e.g., 'utt' for utterances) to identify units of speech and potentially prosody markers to denote aspects related to pitch, stress, or intonation in speech data. Additionally, the project aims to define clear merging criteria and rules while utilizing speaker ons and offs as reliable reference points for tying other annotations together, thereby reducing ambiguity in the merging process.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data." target="The speakers discuss an efficient way to extract and find information in a complex structure by using a relational database that stores information from XML files. This approach allows flexibility for further computations or modifications by researchers, ensuring consistency and clarity in the annotation process. They also mention the importance of maintaining unique word boundaries while incorporating different annotations or coding methods, acknowledging that changes in word transcripts might create challenges during merges. However, they do not explicitly provide a solution for efficiently extracting and finding information in complex structures in this transcript.&#10;&#10;To efficiently extract and find information in a complex structure like an XML file or relational database, you can use query languages such as XQuery for XML files or SQL (Structured Query Language) for relational databases. These query languages allow users to search, filter, and manipulate data based on specific criteria, making it easier to locate the desired information within large and complex datasets. To implement this solution, further details about the structure of the data and the desired information would be required.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data." target="1. Potential uses of a system for extracting phone-level information from XML files include:&#10;   - Prosodic feature analysis at a fine-grained level (although the speakers express concerns about reliability and increased computational burden)&#10;   - Facilitating time-aligned annotation of phones, which could be useful for specific research questions or applications requiring detailed phonetic information.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not explicitly express their views on using pause phones in a study in the given transcript. However, they mention the reliability of speaker ons and offs as reference points for merging different annotations or coding methods. Pause phones could potentially serve a similar purpose, but the group does not discuss this specifically.&#10;   &#10;2. The speakers do discuss their preference for simpler shapes at the word or segment level rather than the phone or frame level regarding prosodic feature annotation. They argue that this approach provides a good balance between granularity and complexity while also reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data." target="Yes, it is possible for a prosodic phrase to span across two sentences in the scenario described. This possibility is acknowledged by the researchers when they discuss different segmentation levels that do not nest, meaning they may not fit neatly within each other. In this case, a prosodic phrase might start in one sentence and end in another without being contained entirely within either sentence.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of handling forced alignment when time boundaries of words in transcriptions change due to a new recognition output involves using a flexible database format, such as a relational database that stores information from XML files. This allows for easy input and output of data while maintaining flexibility for further computations or modifications by researchers.&#10;2. When alignments, word transcripts, or utterance boundaries are changed, the system enables recomputing features and updating entries. The time-stamps in the time-line can be adjusted without altering the IDs of the entries, preserving the original time-stamps and IDs.&#10;3. A consistent marker, like 'utt' for utterances, is used to start the timeline tag, ensuring clarity and consistency in the annotation process. This concept enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;4. When different annotations of the same word sequences emerge as the data gets shared and used by others, a tool or method to reliably merge these different annotations into a single version is needed. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="1. The recommended way to handle data that has been edited and distributed among multiple parties for accurate merging involves the use of a reliable merging tool or method that can incorporate different annotations or coding methods while maintaining unique word boundaries in a consistent manner.&#10;2. Utilizing a stable reference point, such as speaker onsets and offsets or acoustic segments with time-marks, can help reduce ambiguity during the merging process.&#10;3. Carefully defining the criteria and rules for merging is crucial to specify what is meant by &quot;merging&quot; different annotations or coding methods.&#10;4. Implementing a flexible database system, like a relational database that stores information from XML files, can accommodate modifications in word transcripts and time boundaries due to new recognition outputs or changes in forced alignment. This system allows for recomputing features and updating entries while preserving original time-stamps and IDs of the entries.&#10;5. Ensuring consistency and clarity in the annotation process by using clear markers, like 'utt' for utterances, can help manage changes in word transcripts or utterance boundaries, ultimately facilitating accurate merging of edited data from multiple parties.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of handling forced alignment when time boundaries of words in transcriptions change due to a new recognition output involves using a flexible database format, such as a relational database that stores information from XML files. This allows for easy input and output of data while maintaining flexibility for further computations or modifications by researchers.&#10;2. When alignments, word transcripts, or utterance boundaries are changed, the system enables recomputing features and updating entries. The time-stamps in the time-line can be adjusted without altering the IDs of the entries, preserving the original time-stamps and IDs.&#10;3. A consistent marker, like 'utt' for utterances, is used to start the timeline tag, ensuring clarity and consistency in the annotation process. This concept enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;4. When different annotations of the same word sequences emerge as the data gets shared and used by others, a tool or method to reliably merge these different annotations into a single version is needed. The group discussed potential challenges in merging when there are changes in word transcripts and how to handle those cases." target="1. Based on Grad C's observations a year ago, the annotated transcription graph system was not very complete, lacking external file format representation at that time. The only existing component was a conceptual node annotated transcription graph, which Grad C found appealing. Since then, there have been new developments, such as the creation of their own external file format and the development of numerous tools, although Grad C has not yet reviewed these updates.&#10;2. The team is considering using a relational database to express sentence types from XML files, which would allow flexibility for further computations or modifications by researchers. No specific tag or marker for this information has been determined in the transcript. However, the team emphasizes the importance of maintaining consistency and clarity in the annotation process and ensuring that any new types created can interact properly with existing types in the database.&#10;3. The use of a single timeline with unique identifiers (IDs) attached to different sections in a transcription file ensures consistent annotation and easy reference between various sections within the transcription. This concept, which starts the timeline tag with a consistent marker like 'utt' for utterances, enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;&#10;In summary, the annotated transcription graph system has made progress since Grad C's observations a year ago, with new developments such as an external file format and various tools. However, these updates have not been reviewed by Grad C yet. The team is considering using a relational database for flexibility in expressing sentence types from XML files while maintaining consistency and clarity in the annotation process. The single timeline concept allows for consistent annotation and reference between sections within the transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Frames: The basic unit of data storage in a P-file is the frame, which likely corresponds to a small segment of audio or speech.&#10;2. Utterances and Sentences: Frames are grouped together into higher-level structures such as utterances and sentences. These groupings provide a more meaningful organization for time-marked data, word transcripts, and annotations.&#10;3. Arbitrary Vector of Values: Each P-file can contain an arbitrary vector of values, which may include different types such as floats, integers, or doubles. This vector could be used to store various features or attributes related to the frames, utterances, or sentences within the file.&#10;4. Header Format: The P-file has a header format that describes some aspects of its contents. Specifically, it stores information about utterance numbers and frame numbers, allowing for easy reference and organization of the data within the file.&#10;5. Cepstral or PLP Values: In the context of speech research, each P-file might store a vector of cepstral or Perceptual Linear Prediction (PLP) values for each frame, which are commonly used features in speech processing and analysis." target="aker: PhD D&#10;Content: right ? Right .&#10;Speaker: Grad C&#10;Content: So that {disfmarker} what 's nice about the P - file {disfmarker} It {disfmarker} i Built into it is the concept of {pause} frames , utterances , sentences , that sort of thing , that structure . And then also attached to it is an arbitrary vector of values . And it can take different types .&#10;Speaker: PhD F&#10;Content: Oh .&#10;Speaker: Grad C&#10;Content: So it {disfmarker} th they don't all have to be floats . You know , you can have integers and you can have doubles , and all that sort of stuff .&#10;Speaker: PhD F&#10;Content: So that {disfmarker} that sounds {disfmarker} that sounds about what I w&#10;Speaker: Grad C&#10;Content: Um . Right ? And it has a header {disfmarker} it has a header format that {pause} describes it {pause} to some extent . So , the only problem with it is it 's actually storing the {pause} utterance numbers and the {pause} frame numbers in the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Frames: The basic unit of data storage in a P-file is the frame, which likely corresponds to a small segment of audio or speech.&#10;2. Utterances and Sentences: Frames are grouped together into higher-level structures such as utterances and sentences. These groupings provide a more meaningful organization for time-marked data, word transcripts, and annotations.&#10;3. Arbitrary Vector of Values: Each P-file can contain an arbitrary vector of values, which may include different types such as floats, integers, or doubles. This vector could be used to store various features or attributes related to the frames, utterances, or sentences within the file.&#10;4. Header Format: The P-file has a header format that describes some aspects of its contents. Specifically, it stores information about utterance numbers and frame numbers, allowing for easy reference and organization of the data within the file.&#10;5. Cepstral or PLP Values: In the context of speech research, each P-file might store a vector of cepstral or Perceptual Linear Prediction (PLP) values for each frame, which are commonly used features in speech processing and analysis." target="Content: OK . So {disfmarker}&#10;Speaker: Grad C&#10;Content: I would use something tighter than P - files .&#10;Speaker: PhD F&#10;Content: Do you {disfmarker} Are you familiar with it ?&#10;Speaker: Grad C&#10;Content: So .&#10;Speaker: PhD F&#10;Content: I haven't seen this particular format ,&#10;Speaker: PhD A&#10;Content: I mean , I 've {disfmarker} I 've used them .&#10;Speaker: PhD F&#10;Content: but {disfmarker}&#10;Speaker: PhD A&#10;Content: I don't know what their structure is .&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: I 've forgot what the str&#10;Speaker: PhD D&#10;Content: But , wait a minute , P - file for each frame is storing a vector of cepstral or PLP values ,&#10;Speaker: Grad C&#10;Content: It 's whatever you want , actually .&#10;Speaker: PhD D&#10;Content: right ? Right .&#10;Speaker: Grad C&#10;Content: So that {disfmarker} what 's nice">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Frames: The basic unit of data storage in a P-file is the frame, which likely corresponds to a small segment of audio or speech.&#10;2. Utterances and Sentences: Frames are grouped together into higher-level structures such as utterances and sentences. These groupings provide a more meaningful organization for time-marked data, word transcripts, and annotations.&#10;3. Arbitrary Vector of Values: Each P-file can contain an arbitrary vector of values, which may include different types such as floats, integers, or doubles. This vector could be used to store various features or attributes related to the frames, utterances, or sentences within the file.&#10;4. Header Format: The P-file has a header format that describes some aspects of its contents. Specifically, it stores information about utterance numbers and frame numbers, allowing for easy reference and organization of the data within the file.&#10;5. Cepstral or PLP Values: In the context of speech research, each P-file might store a vector of cepstral or Perceptual Linear Prediction (PLP) values for each frame, which are commonly used features in speech processing and analysis." target=" values&#10;Speaker: Grad C&#10;Content: Yeah , I mean , for something like that I would use P - file&#10;Speaker: PhD F&#10;Content: depending on {disfmarker}&#10;Speaker: Grad C&#10;Content: or {disfmarker} or any frame - level stuff I would use P - file .&#10;Speaker: PhD F&#10;Content: Meaning {disfmarker} ?&#10;Speaker: Grad C&#10;Content: Uh , that 's a {disfmarker} well , or something like it . It 's ICS uh , ICSI has a format for frame - level representation of features . Um .&#10;Speaker: PhD F&#10;Content: OK . That you could call {disfmarker} that you would tie into this representation with like an ID .&#10;Speaker: Grad C&#10;Content: Right . Right . Or {disfmarker} or there 's a {disfmarker} there 's a particular way in XML to refer to external resources .&#10;Speaker: PhD F&#10;Content: And {disfmarker} OK .&#10;Speaker: Grad C&#10;Content: So you would say &quot; refer to this external file &quot; .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Frames: The basic unit of data storage in a P-file is the frame, which likely corresponds to a small segment of audio or speech.&#10;2. Utterances and Sentences: Frames are grouped together into higher-level structures such as utterances and sentences. These groupings provide a more meaningful organization for time-marked data, word transcripts, and annotations.&#10;3. Arbitrary Vector of Values: Each P-file can contain an arbitrary vector of values, which may include different types such as floats, integers, or doubles. This vector could be used to store various features or attributes related to the frames, utterances, or sentences within the file.&#10;4. Header Format: The P-file has a header format that describes some aspects of its contents. Specifically, it stores information about utterance numbers and frame numbers, allowing for easy reference and organization of the data within the file.&#10;5. Cepstral or PLP Values: In the context of speech research, each P-file might store a vector of cepstral or Perceptual Linear Prediction (PLP) values for each frame, which are commonly used features in speech processing and analysis." target="The sharing aspect is important in the context of storing feature vectors on a frame-by-frame basis for two main reasons: adaptability and collaboration. By making the data more accessible and usable, other researchers can adopt and build upon this work more easily.&#10;&#10;1. Adaptability: Creating a system where prosodic features are represented as continuous values at the word or segment level allows for greater flexibility in analyzing speech data. This approach makes it easier to adapt the system to various research questions and domains, as researchers can work with simpler and more meaningful structures (words or segments) instead of being constrained by specific phones or frames.&#10;2. Collaboration: By sharing the feature vectors and related data, researchers enable better collaboration within their field. Other researchers can use this data for their own studies, validate findings, or develop new methods based on the existing work. This not only fosters a sense of community but also helps to advance the field more rapidly by building upon previous research efforts.&#10;&#10;In the context of ten-minute Switchboard conversations, sharing feature vectors on a frame-by-frame basis allows researchers to create a robust and adaptable system for prosodic analysis. The shared data can help address the challenges associated with managing large files containing numerous frames while maintaining flexibility for further computations or modifications by researchers. Additionally, having easily accessible and usable data promotes collaboration and knowledge sharing within the research community, which is essential for advancing our understanding of speech processing and analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Frames: The basic unit of data storage in a P-file is the frame, which likely corresponds to a small segment of audio or speech.&#10;2. Utterances and Sentences: Frames are grouped together into higher-level structures such as utterances and sentences. These groupings provide a more meaningful organization for time-marked data, word transcripts, and annotations.&#10;3. Arbitrary Vector of Values: Each P-file can contain an arbitrary vector of values, which may include different types such as floats, integers, or doubles. This vector could be used to store various features or attributes related to the frames, utterances, or sentences within the file.&#10;4. Header Format: The P-file has a header format that describes some aspects of its contents. Specifically, it stores information about utterance numbers and frame numbers, allowing for easy reference and organization of the data within the file.&#10;5. Cepstral or PLP Values: In the context of speech research, each P-file might store a vector of cepstral or Perceptual Linear Prediction (PLP) values for each frame, which are commonly used features in speech processing and analysis." target="1. More compact data storage: By referring to an external file, the size of the primary file or format can be significantly reduced, making it more efficient and easier to manage during long meetings with multiple frames. This is especially beneficial when dealing with large datasets.&#10;2. Reduced computational burden: Storing information in an external file may decrease the computational load on the system since only the essential data needs to be loaded into memory for processing during the meeting.&#10;3. Easier organization and maintenance: External files can help keep related but separate information organized, making it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files can facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To efficiently extract and distinguish words within a prosodic boundary and those within a &quot;disfluency marker,&quot; the researchers can utilize their proposed relational database format to store this information from XML files. This would allow for flexibility in computations and modifications by researchers. They can create specific tags or identifiers for prosodic boundaries, disfluency markers, and individual words. By querying the database with these specific tags, they can extract all the words associated with a particular prosodic boundary or disfluency marker.&#10;2. For example, they can use a tag like 'utt' to identify utterances (units of speech starting with a pause and ending with a pause) and 'disfmarker' for disfluency markers. When a word is associated with both a prosodic boundary and a disfluency marker, it would have both tags. By using a relational database, they can efficiently query the data to extract words based on these tags and distinguish between words within prosodic boundaries and those within disfluency markers.&#10;&#10;In summary, by implementing a relational database system with specific tags for prosodic boundaries, sentences, and disfluency markers, the researchers can efficiently extract and distinguish words associated with each category. This system allows for easier computation and analysis of speech data while maintaining clarity and consistency in the annotation process." target=" Grad C&#10;Content: And then lower down you would say &quot; here 's a prosodic boundary and it has these words in it &quot; . And lower down you 'd have &quot; here 's a sentence ,&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: An - Right .&#10;Speaker: Grad C&#10;Content: and it has these words in it &quot; .&#10;Speaker: PhD F&#10;Content: So you would be able to go in and say , you know , &quot; give me all the words in the bound in the prosodic phrase&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD F&#10;Content: and give me all the words in the {disfmarker} &quot; Yeah .&#10;Speaker: Grad C&#10;Content: So I think that 's {disfmarker} that would wor&#10;Speaker: PhD F&#10;Content: Um , OK .&#10;Speaker: Grad C&#10;Content: Let me look at it again .&#10;Speaker: PhD A&#10;Content: Mm - hmm . The {disfmarker} the o the other issue that you had was , how do you actually efficiently extract , um {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To efficiently extract and distinguish words within a prosodic boundary and those within a &quot;disfluency marker,&quot; the researchers can utilize their proposed relational database format to store this information from XML files. This would allow for flexibility in computations and modifications by researchers. They can create specific tags or identifiers for prosodic boundaries, disfluency markers, and individual words. By querying the database with these specific tags, they can extract all the words associated with a particular prosodic boundary or disfluency marker.&#10;2. For example, they can use a tag like 'utt' to identify utterances (units of speech starting with a pause and ending with a pause) and 'disfmarker' for disfluency markers. When a word is associated with both a prosodic boundary and a disfluency marker, it would have both tags. By using a relational database, they can efficiently query the data to extract words based on these tags and distinguish between words within prosodic boundaries and those within disfluency markers.&#10;&#10;In summary, by implementing a relational database system with specific tags for prosodic boundaries, sentences, and disfluency markers, the researchers can efficiently extract and distinguish words associated with each category. This system allows for easier computation and analysis of speech data while maintaining clarity and consistency in the annotation process." target=" say this {disfmarker} this word is part of that sentence and this prosodic phrase .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: But the phrase is not part of the sentence&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and neither is the sentence part of the phrase .&#10;Speaker: PhD F&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: I I 'm pretty sure that you can do that , but I 'm forgetting the exact level of nesting .&#10;Speaker: PhD A&#10;Content: So , you would have to have {vocalsound} two different pointers from the word up {disfmarker} one level up , one to the sent&#10;Speaker: Grad C&#10;Content: So {disfmarker} so what you would end up having is a tag saying &quot; here 's a word , and it starts here and it ends here &quot; .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: And then lower down you would say &quot; here 's a prosodic boundary and it has these words in it &quot; . And lower down">
      <data key="d0">1</data>
    </edge>
    <edge source=" Grad C&#10;Content: And then lower down you would say &quot; here 's a prosodic boundary and it has these words in it &quot; . And lower down you 'd have &quot; here 's a sentence ,&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: An - Right .&#10;Speaker: Grad C&#10;Content: and it has these words in it &quot; .&#10;Speaker: PhD F&#10;Content: So you would be able to go in and say , you know , &quot; give me all the words in the bound in the prosodic phrase&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD F&#10;Content: and give me all the words in the {disfmarker} &quot; Yeah .&#10;Speaker: Grad C&#10;Content: So I think that 's {disfmarker} that would wor&#10;Speaker: PhD F&#10;Content: Um , OK .&#10;Speaker: Grad C&#10;Content: Let me look at it again .&#10;Speaker: PhD A&#10;Content: Mm - hmm . The {disfmarker} the o the other issue that you had was , how do you actually efficiently extract , um {disfmark" target="Grad C is referring to the possibility of an additional level of complexity in the segmentation of a word sequence. In this conversation, two levels of segmentation have already been mentioned: sentences and prosodic phrases. Grad C suggests that there might be another level, as yet unconsidered, which adds further structure to the word sequence.">
      <data key="d0">1</data>
    </edge>
    <edge source=" say this {disfmarker} this word is part of that sentence and this prosodic phrase .&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: But the phrase is not part of the sentence&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and neither is the sentence part of the phrase .&#10;Speaker: PhD F&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: I I 'm pretty sure that you can do that , but I 'm forgetting the exact level of nesting .&#10;Speaker: PhD A&#10;Content: So , you would have to have {vocalsound} two different pointers from the word up {disfmarker} one level up , one to the sent&#10;Speaker: Grad C&#10;Content: So {disfmarker} so what you would end up having is a tag saying &quot; here 's a word , and it starts here and it ends here &quot; .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: And then lower down you would say &quot; here 's a prosodic boundary and it has these words in it &quot; . And lower down" target="Yes, it is possible for a prosodic phrase to span across two sentences in the scenario described. This possibility is acknowledged by the researchers when they discuss different segmentation levels that do not nest, meaning they may not fit neatly within each other. In this case, a prosodic phrase might start in one sentence and end in another without being contained entirely within either sentence.">
      <data key="d0">1</data>
    </edge>
    <edge source="The sharing aspect is important in the context of storing feature vectors on a frame-by-frame basis for two main reasons: adaptability and collaboration. By making the data more accessible and usable, other researchers can adopt and build upon this work more easily.&#10;&#10;1. Adaptability: Creating a system where prosodic features are represented as continuous values at the word or segment level allows for greater flexibility in analyzing speech data. This approach makes it easier to adapt the system to various research questions and domains, as researchers can work with simpler and more meaningful structures (words or segments) instead of being constrained by specific phones or frames.&#10;2. Collaboration: By sharing the feature vectors and related data, researchers enable better collaboration within their field. Other researchers can use this data for their own studies, validate findings, or develop new methods based on the existing work. This not only fosters a sense of community but also helps to advance the field more rapidly by building upon previous research efforts.&#10;&#10;In the context of ten-minute Switchboard conversations, sharing feature vectors on a frame-by-frame basis allows researchers to create a robust and adaptable system for prosodic analysis. The shared data can help address the challenges associated with managing large files containing numerous frames while maintaining flexibility for further computations or modifications by researchers. Additionally, having easily accessible and usable data promotes collaboration and knowledge sharing within the research community, which is essential for advancing our understanding of speech processing and analysis." target=" ten - minute Switchboard conversations ,&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: and {disfmarker} So it 's doable , it 's just that you can only store a feature vector at frame - by - frame and it doesn't have any kind of ,&#10;Speaker: PhD D&#10;Content: Is {disfmarker} is the sharing part of this a pretty important {pause} consideration&#10;Speaker: PhD F&#10;Content: um {disfmarker}&#10;Speaker: PhD D&#10;Content: or does that just sort of , uh {disfmarker} a nice thing to have ?&#10;Speaker: PhD F&#10;Content: I {disfmarker} I don't know enough about what we 're gonna do with the data . But I thought it would be good to get something that we can {disfmarker} that other people can use or adopt for their own kinds of encoding . And just , I mean we have to use some we have to make some decision about what to do .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: And especially for the prosody work ,">
      <data key="d0">1</data>
    </edge>
    <edge source="The sharing aspect is important in the context of storing feature vectors on a frame-by-frame basis for two main reasons: adaptability and collaboration. By making the data more accessible and usable, other researchers can adopt and build upon this work more easily.&#10;&#10;1. Adaptability: Creating a system where prosodic features are represented as continuous values at the word or segment level allows for greater flexibility in analyzing speech data. This approach makes it easier to adapt the system to various research questions and domains, as researchers can work with simpler and more meaningful structures (words or segments) instead of being constrained by specific phones or frames.&#10;2. Collaboration: By sharing the feature vectors and related data, researchers enable better collaboration within their field. Other researchers can use this data for their own studies, validate findings, or develop new methods based on the existing work. This not only fosters a sense of community but also helps to advance the field more rapidly by building upon previous research efforts.&#10;&#10;In the context of ten-minute Switchboard conversations, sharing feature vectors on a frame-by-frame basis allows researchers to create a robust and adaptable system for prosodic analysis. The shared data can help address the challenges associated with managing large files containing numerous frames while maintaining flexibility for further computations or modifications by researchers. Additionally, having easily accessible and usable data promotes collaboration and knowledge sharing within the research community, which is essential for advancing our understanding of speech processing and analysis." target=" of frame - by - frame .&#10;Speaker: Grad C&#10;Content: Ah , right .&#10;Speaker: PhD F&#10;Content: Which is fine , but it gets unwieldy to go in and {disfmarker} and query these files with really huge files .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: I mean , we could do it . I was just thinking if there 's something that {disfmarker} where all the frame values are {disfmarker}&#10;Speaker: Grad C&#10;Content: And a and again , if you have a {disfmarker} if you have a two - hour - long meeting , that 's gonna {disfmarker}&#10;Speaker: PhD F&#10;Content: Hmm ? They 're {disfmarker} they 're fair they 're quite large .&#10;Speaker: Grad C&#10;Content: Yeah , I mean , they 'd be emo enormous .&#10;Speaker: PhD F&#10;Content: And these are for ten - minute Switchboard conversations ,&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: and {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="The sharing aspect is important in the context of storing feature vectors on a frame-by-frame basis for two main reasons: adaptability and collaboration. By making the data more accessible and usable, other researchers can adopt and build upon this work more easily.&#10;&#10;1. Adaptability: Creating a system where prosodic features are represented as continuous values at the word or segment level allows for greater flexibility in analyzing speech data. This approach makes it easier to adapt the system to various research questions and domains, as researchers can work with simpler and more meaningful structures (words or segments) instead of being constrained by specific phones or frames.&#10;2. Collaboration: By sharing the feature vectors and related data, researchers enable better collaboration within their field. Other researchers can use this data for their own studies, validate findings, or develop new methods based on the existing work. This not only fosters a sense of community but also helps to advance the field more rapidly by building upon previous research efforts.&#10;&#10;In the context of ten-minute Switchboard conversations, sharing feature vectors on a frame-by-frame basis allows researchers to create a robust and adaptable system for prosodic analysis. The shared data can help address the challenges associated with managing large files containing numerous frames while maintaining flexibility for further computations or modifications by researchers. Additionally, having easily accessible and usable data promotes collaboration and knowledge sharing within the research community, which is essential for advancing our understanding of speech processing and analysis." target="The goals of the project involve creating a system where prosodic features are represented as continuous values at the word or segment level, rather than being tied to specific phones or frames. This approach allows for more flexibility and easier computation in understanding and analyzing speech data. The researchers prefer simpler shapes at the word or segment level because it provides a good balance between granularity and complexity, reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;&#10;The methods to achieve these goals include using a relational database format to store word transcripts, discourse codings, and time-marked data from XML files. This approach provides flexibility for further computations or modifications by researchers. The team is also considering using consistent markers (e.g., 'utt' for utterances) to identify units of speech and potentially prosody markers to denote aspects related to pitch, stress, or intonation in speech data. Additionally, the project aims to define clear merging criteria and rules while utilizing speaker ons and offs as reliable reference points for tying other annotations together, thereby reducing ambiguity in the merging process.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, the restriction for adding supplementary information to a type specified as 'utt' is that it cannot be directly added to the existing 'utt' type. This is because 'utt' is used to identify an utterance in the transcription file, and changing this type might affect its consistency across different annotation files.&#10;&#10;If the team wants to add supplementary information, they would need to create a new type, which would require defining the criteria and rules for that specific type. This process involves careful consideration of how the new type will interact with existing types and the overall structure of the database. It may also involve additional work in modifying annotation scripts or tools to accommodate the new type.&#10;&#10;To make a different type, the team would need to:&#10;1. Determine the purpose and criteria for the new type&#10;2. Define how it will interact with other types in the database&#10;3. Modify annotation scripts or tools to recognize and process the new type, if necessary&#10;4. Test the new type to ensure compatibility with existing annotations and the forced alignment program." target="Based on the discussion transcript, the team is considering using a relational database in a format that stores information from XML files to express the type of a sentence in a file. This would allow flexibility for further computations or modifications by researchers. The exact tag or marker for this information is not explicitly stated in the transcript. However, they discuss the importance of maintaining consistency and clarity in the annotation process, as well as ensuring that any new types created can interact properly with existing types in the database. Therefore, careful consideration and definition of the criteria and rules for the new type would be required before implementation.">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C is referring to the possibility of an additional level of complexity in the segmentation of a word sequence. In this conversation, two levels of segmentation have already been mentioned: sentences and prosodic phrases. Grad C suggests that there might be another level, as yet unconsidered, which adds further structure to the word sequence." target="aker: PhD F&#10;Content: You have to have another type then , I guess .&#10;Speaker: PhD A&#10;Content: s Um , well , s let 's {disfmarker} let 's ta so let 's {disfmarker}&#10;Speaker: Grad C&#10;Content: Well , I think I 'm {disfmarker} I think w I had better look at it again&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: so {disfmarker}&#10;Speaker: Grad C&#10;Content: because I {disfmarker} I 'm {disfmarker}&#10;Speaker: PhD F&#10;Content: OK . OK .&#10;Speaker: PhD A&#10;Content: y So for instance @ @ {comment} sup&#10;Speaker: Grad C&#10;Content: There 's one level {disfmarker} there 's one more level of indirection that I 'm forgetting .&#10;Speaker: PhD A&#10;Content: Suppose you have a word sequence and you have two different segmentations of that same word sequence . f Say , one segmentation is in terms of , um , you know , uh , sentences .">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C is referring to the possibility of an additional level of complexity in the segmentation of a word sequence. In this conversation, two levels of segmentation have already been mentioned: sentences and prosodic phrases. Grad C suggests that there might be another level, as yet unconsidered, which adds further structure to the word sequence." target=" you have two different segmentations of that same word sequence . f Say , one segmentation is in terms of , um , you know , uh , sentences . And another segmentation is in terms of , um , {vocalsound} I don't know , {comment} prosodic phrases . And let 's say that they don't {pause} nest . So , you know , a prosodic phrase may cross two sentences or something .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: I don't know if that 's true or not but {vocalsound} let 's as&#10;Speaker: PhD F&#10;Content: Well , it 's definitely true with the segment .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: That 's what I {disfmarker} exactly what I meant by the utterances versus the sentence could be sort of {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah . So , you want to be s you want to say this {disfmarker} this word is part of that sentence and this prosodic phrase .&#10;Speaker: PhD F&#10;Content: Yeah .">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C is referring to the possibility of an additional level of complexity in the segmentation of a word sequence. In this conversation, two levels of segmentation have already been mentioned: sentences and prosodic phrases. Grad C suggests that there might be another level, as yet unconsidered, which adds further structure to the word sequence." target=" C&#10;Content: So {disfmarker} so imagine {disfmarker} I think his {disfmarker} his example is a good one . Imagine that this person who developed the corpus of the referring expressions didn't include time .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Grad C&#10;Content: He included references to words .&#10;Speaker: Postdoc E&#10;Content: Ach !&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: He said that at this word is when {disfmarker} when it happened .&#10;Speaker: Postdoc E&#10;Content: Well , then {disfmarker}&#10;Speaker: PhD A&#10;Content: Or she .&#10;Speaker: Grad C&#10;Content: Or she .&#10;Speaker: Postdoc E&#10;Content: But then couldn't you just indirectly figure out the time {pause} tied to the word ?&#10;Speaker: PhD F&#10;Content: But still they {disfmarker} Exactly .&#10;Speaker: Grad C&#10;Content: Sure . But what if {disfmarker} what if they change the words ?&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C is referring to the possibility of an additional level of complexity in the segmentation of a word sequence. In this conversation, two levels of segmentation have already been mentioned: sentences and prosodic phrases. Grad C suggests that there might be another level, as yet unconsidered, which adds further structure to the word sequence." target="Content: Well , channel or speaker or whatever .&#10;Speaker: PhD F&#10;Content: I mean , w yeah , channel is what the channelized output out&#10;Speaker: PhD A&#10;Content: It doesn't {disfmarker}&#10;Speaker: Grad C&#10;Content: This isn't quite right .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: I have to look at it again .&#10;Speaker: PhD F&#10;Content: Yeah , but {disfmarker}&#10;Speaker: PhD A&#10;Content: But {disfmarker} but {disfmarker} so how in the NIST format do we express {vocalsound} a hierarchical relationship between , um , say , an utterance and the words within it ? So how do you {pause} tell {pause} that {pause} these are the words that belong to that utterance ?&#10;Speaker: Grad C&#10;Content: Um , you would have another structure lower down than this that would be saying they 're all belonging to this ID .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: So each thing refers to">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C is referring to the possibility of an additional level of complexity in the segmentation of a word sequence. In this conversation, two levels of segmentation have already been mentioned: sentences and prosodic phrases. Grad C suggests that there might be another level, as yet unconsidered, which adds further structure to the word sequence." target=" this ID .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: So each thing refers to the {pause} utterance that it belongs to .&#10;Speaker: Grad C&#10;Content: Right . And then each utterance could refer to a turn ,&#10;Speaker: PhD D&#10;Content: So it 's {disfmarker} it 's not hi it 's sort of bottom - up .&#10;Speaker: Grad C&#10;Content: and each turn could refer to something higher up .&#10;Speaker: PhD F&#10;Content: And what if you actually have {disfmarker} So right now what you have as utterance , um , the closest thing that comes out of the channelized is the stuff between the segment boundaries that the transcribers put in or that Thilo put in , which may or may not actually be , like , a s it 's usually not {disfmarker} um , the beginning and end of a sentence , say .&#10;Speaker: Grad C&#10;Content: Well , that 's why I didn't call it &quot; sentence &quot; .&#10;Speaker: PhD F&#10;Content: So , right . Um , so">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C is referring to the possibility of an additional level of complexity in the segmentation of a word sequence. In this conversation, two levels of segmentation have already been mentioned: sentences and prosodic phrases. Grad C suggests that there might be another level, as yet unconsidered, which adds further structure to the word sequence." target="1. The limitation of frame-level analysis for phone-level speech processing, as discussed by the Ph.D. speakers, is that most frames are not actual speech, but silent or transitional periods between sounds. This makes frame-level analysis less reliable for phone-level speech processing since phones can undergo significant modification during merging processes, and analyzing at this level increases computational burden without adding meaningful value to the analysis of prosodic features.&#10;   &#10;2. The average number of phones in an English word is not explicitly stated in the transcript, but it is mentioned that there are approximately five frames per phone. This suggests that the average number of phones in an English word could be around five or more, given that some words might have additional phones due to affixes or other phonetic processes.">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C is referring to the possibility of an additional level of complexity in the segmentation of a word sequence. In this conversation, two levels of segmentation have already been mentioned: sentences and prosodic phrases. Grad C suggests that there might be another level, as yet unconsidered, which adds further structure to the word sequence." target="The goals of the project involve creating a system where prosodic features are represented as continuous values at the word or segment level, rather than being tied to specific phones or frames. This approach allows for more flexibility and easier computation in understanding and analyzing speech data. The researchers prefer simpler shapes at the word or segment level because it provides a good balance between granularity and complexity, reducing computational burden and ambiguity during merging processes. Annotating prosodic features at the word or segment level allows for more flexibility and easier computation in understanding and analyzing speech data.&#10;&#10;The methods to achieve these goals include using a relational database format to store word transcripts, discourse codings, and time-marked data from XML files. This approach provides flexibility for further computations or modifications by researchers. The team is also considering using consistent markers (e.g., 'utt' for utterances) to identify units of speech and potentially prosody markers to denote aspects related to pitch, stress, or intonation in speech data. Additionally, the project aims to define clear merging criteria and rules while utilizing speaker ons and offs as reliable reference points for tying other annotations together, thereby reducing ambiguity in the merging process.">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C is referring to the possibility of an additional level of complexity in the segmentation of a word sequence. In this conversation, two levels of segmentation have already been mentioned: sentences and prosodic phrases. Grad C suggests that there might be another level, as yet unconsidered, which adds further structure to the word sequence." target="Based on the discussion transcript, the team is considering using a relational database in a format that stores information from XML files to express the type of a sentence in a file. This would allow flexibility for further computations or modifications by researchers. The exact tag or marker for this information is not explicitly stated in the transcript. However, they discuss the importance of maintaining consistency and clarity in the annotation process, as well as ensuring that any new types created can interact properly with existing types in the database. Therefore, careful consideration and definition of the criteria and rules for the new type would be required before implementation.">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C is referring to the possibility of an additional level of complexity in the segmentation of a word sequence. In this conversation, two levels of segmentation have already been mentioned: sentences and prosodic phrases. Grad C suggests that there might be another level, as yet unconsidered, which adds further structure to the word sequence." target="Yes, it is possible for a prosodic phrase to span across two sentences in the scenario described. This possibility is acknowledged by the researchers when they discuss different segmentation levels that do not nest, meaning they may not fit neatly within each other. In this case, a prosodic phrase might start in one sentence and end in another without being contained entirely within either sentence.">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C is referring to the possibility of an additional level of complexity in the segmentation of a word sequence. In this conversation, two levels of segmentation have already been mentioned: sentences and prosodic phrases. Grad C suggests that there might be another level, as yet unconsidered, which adds further structure to the word sequence." target="1. The group is discussing the possibility of an additional level of complexity in segmenting a word sequence, beyond sentences and prosodic phrases (as suggested by Grad C). This third level would add further structure to the word sequence.&#10;2. It is acknowledged that prosodic phrases can span across two sentences, not being contained entirely within either sentence (this is accepted when they discuss different segmentation levels that do not nest).&#10;3. The idea of overlapping codes or concepts is mentioned, but it is decided that they don't need to delve into the specifics of the coding system since it's not the main topic of the meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD F&#10;Content: You have to have another type then , I guess .&#10;Speaker: PhD A&#10;Content: s Um , well , s let 's {disfmarker} let 's ta so let 's {disfmarker}&#10;Speaker: Grad C&#10;Content: Well , I think I 'm {disfmarker} I think w I had better look at it again&#10;Speaker: PhD F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: so {disfmarker}&#10;Speaker: Grad C&#10;Content: because I {disfmarker} I 'm {disfmarker}&#10;Speaker: PhD F&#10;Content: OK . OK .&#10;Speaker: PhD A&#10;Content: y So for instance @ @ {comment} sup&#10;Speaker: Grad C&#10;Content: There 's one level {disfmarker} there 's one more level of indirection that I 'm forgetting .&#10;Speaker: PhD A&#10;Content: Suppose you have a word sequence and you have two different segmentations of that same word sequence . f Say , one segmentation is in terms of , um , you know , uh , sentences ." target="Yes, it is possible for a prosodic phrase to span across two sentences in the scenario described. This possibility is acknowledged by the researchers when they discuss different segmentation levels that do not nest, meaning they may not fit neatly within each other. In this case, a prosodic phrase might start in one sentence and end in another without being contained entirely within either sentence.">
      <data key="d0">1</data>
    </edge>
    <edge source=" you have two different segmentations of that same word sequence . f Say , one segmentation is in terms of , um , you know , uh , sentences . And another segmentation is in terms of , um , {vocalsound} I don't know , {comment} prosodic phrases . And let 's say that they don't {pause} nest . So , you know , a prosodic phrase may cross two sentences or something .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: I don't know if that 's true or not but {vocalsound} let 's as&#10;Speaker: PhD F&#10;Content: Well , it 's definitely true with the segment .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: That 's what I {disfmarker} exactly what I meant by the utterances versus the sentence could be sort of {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah . So , you want to be s you want to say this {disfmarker} this word is part of that sentence and this prosodic phrase .&#10;Speaker: PhD F&#10;Content: Yeah ." target="Yes, it is possible for a prosodic phrase to span across two sentences in the scenario described. This possibility is acknowledged by the researchers when they discuss different segmentation levels that do not nest, meaning they may not fit neatly within each other. In this case, a prosodic phrase might start in one sentence and end in another without being contained entirely within either sentence.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The limitation of frame-level analysis for phone-level speech processing, as discussed by the Ph.D. speakers, is that most frames are not actual speech, but silent or transitional periods between sounds. This makes frame-level analysis less reliable for phone-level speech processing since phones can undergo significant modification during merging processes, and analyzing at this level increases computational burden without adding meaningful value to the analysis of prosodic features.&#10;   &#10;2. The average number of phones in an English word is not explicitly stated in the transcript, but it is mentioned that there are approximately five frames per phone. This suggests that the average number of phones in an English word could be around five or more, given that some words might have additional phones due to affixes or other phonetic processes." target=" limit on most O Ss .&#10;Speaker: PhD A&#10;Content: Right , OK . I would say {disfmarker} OK , so frame - level is probably not a good idea . But for phone - level stuff it 's perfectly {disfmarker}&#10;Speaker: PhD F&#10;Content: And th it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Like phones , or syllables , or anything like that .&#10;Speaker: PhD F&#10;Content: Phones are every five frames though , so . Or something like that .&#10;Speaker: PhD A&#10;Content: But {disfmarker} but {disfmarker} but most of the frames are actually not speech . So , you know , people don't {disfmarker} v Look at it , words times the average {disfmarker} The average number of phones in an English word is , I don't know , {comment} five maybe ?&#10;Speaker: PhD F&#10;Content: Yeah , but we actually {disfmarker}&#10;Speaker: PhD A&#10;Content: So , look at it , t number of words times five . That 's not {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The limitation of frame-level analysis for phone-level speech processing, as discussed by the Ph.D. speakers, is that most frames are not actual speech, but silent or transitional periods between sounds. This makes frame-level analysis less reliable for phone-level speech processing since phones can undergo significant modification during merging processes, and analyzing at this level increases computational burden without adding meaningful value to the analysis of prosodic features.&#10;   &#10;2. The average number of phones in an English word is not explicitly stated in the transcript, but it is mentioned that there are approximately five frames per phone. This suggests that the average number of phones in an English word could be around five or more, given that some words might have additional phones due to affixes or other phonetic processes." target="marker}&#10;Speaker: PhD A&#10;Content: So , look at it , t number of words times five . That 's not {disfmarker} that not {disfmarker}&#10;Speaker: PhD F&#10;Content: Oh , so you mean pause phones take up a lot of the {disfmarker} long pause phones .&#10;Speaker: PhD A&#10;Content: Exactly .&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: Yeah . OK . That 's true . But you do have to keep them in there . Y yeah .&#10;Speaker: Grad C&#10;Content: So I think it {disfmarker} it 's debatable whether you want to do phone - level in the same thing .&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Grad C&#10;Content: But I think , a anything at frame - level , even P - file , is too verbose .&#10;Speaker: PhD F&#10;Content: OK . So {disfmarker}&#10;Speaker: Grad C&#10;Content: I would use something tighter than P - files .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The limitation of frame-level analysis for phone-level speech processing, as discussed by the Ph.D. speakers, is that most frames are not actual speech, but silent or transitional periods between sounds. This makes frame-level analysis less reliable for phone-level speech processing since phones can undergo significant modification during merging processes, and analyzing at this level increases computational burden without adding meaningful value to the analysis of prosodic features.&#10;   &#10;2. The average number of phones in an English word is not explicitly stated in the transcript, but it is mentioned that there are approximately five frames per phone. This suggests that the average number of phones in an English word could be around five or more, given that some words might have additional phones due to affixes or other phonetic processes." target="The issue at hand is the difficulty in reliably merging different versions of word transcripts, especially when those changes are tied to acoustic segments. The speakers express concern that crucial information may be lost during the merge process, making it challenging to combine different annotations or coding methods. They discuss the need for a stable reference point, such as speaker ons and offs or acoustic segments with time-marks, to reduce ambiguity in the merging process. However, they acknowledge that this information might not always be available or correctly preserved during merges. The aim is to create a consistent and clear annotation system that can handle changes in word transcripts while maintaining the original time-stamps and IDs of entries in a relational database.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The limitation of frame-level analysis for phone-level speech processing, as discussed by the Ph.D. speakers, is that most frames are not actual speech, but silent or transitional periods between sounds. This makes frame-level analysis less reliable for phone-level speech processing since phones can undergo significant modification during merging processes, and analyzing at this level increases computational burden without adding meaningful value to the analysis of prosodic features.&#10;   &#10;2. The average number of phones in an English word is not explicitly stated in the transcript, but it is mentioned that there are approximately five frames per phone. This suggests that the average number of phones in an English word could be around five or more, given that some words might have additional phones due to affixes or other phonetic processes." target="1. Potential uses of a system for extracting phone-level information from XML files include:&#10;   - Prosodic feature analysis at a fine-grained level (although the speakers express concerns about reliability and increased computational burden)&#10;   - Facilitating time-aligned annotation of phones, which could be useful for specific research questions or applications requiring detailed phonetic information.">
      <data key="d0">1</data>
    </edge>
    <edge source="The issue at hand is the difficulty in reliably merging different versions of word transcripts, especially when those changes are tied to acoustic segments. The speakers express concern that crucial information may be lost during the merge process, making it challenging to combine different annotations or coding methods. They discuss the need for a stable reference point, such as speaker ons and offs or acoustic segments with time-marks, to reduce ambiguity in the merging process. However, they acknowledge that this information might not always be available or correctly preserved during merges. The aim is to create a consistent and clear annotation system that can handle changes in word transcripts while maintaining the original time-stamps and IDs of entries in a relational database." target=" so {disfmarker}&#10;Speaker: PhD F&#10;Content: But , d isn't that something where whoever {disfmarker} if {vocalsound} {disfmarker} if the people who are making changes , say in the transcripts , cuz this all happened when the transcripts were different {disfmarker} ye um , if they tie it to something , like if they tied it to the acoustic segment {disfmarker} if they {disfmarker} You know what I mean ? Then {disfmarker} Or if they tied it to an acoustic segment and we had the time - marks , that would help .&#10;Speaker: Grad C&#10;Content: Yep .&#10;Speaker: PhD F&#10;Content: But the problem is exactly as Adam said , that you get , you know , y you don't have that information or it 's lost in the merge somehow ,&#10;Speaker: Postdoc E&#10;Content: Well , can I ask one question ?&#10;Speaker: PhD F&#10;Content: so {disfmarker}&#10;Speaker: Postdoc E&#10;Content: It {disfmarker} it seems to me that , um , we will have o an">
      <data key="d0">1</data>
    </edge>
    <edge source="The issue at hand is the difficulty in reliably merging different versions of word transcripts, especially when those changes are tied to acoustic segments. The speakers express concern that crucial information may be lost during the merge process, making it challenging to combine different annotations or coding methods. They discuss the need for a stable reference point, such as speaker ons and offs or acoustic segments with time-marks, to reduce ambiguity in the merging process. However, they acknowledge that this information might not always be available or correctly preserved during merges. The aim is to create a consistent and clear annotation system that can handle changes in word transcripts while maintaining the original time-stamps and IDs of entries in a relational database." target="1. The recommended way to handle data that has been edited and distributed among multiple parties for accurate merging involves the use of a reliable merging tool or method that can incorporate different annotations or coding methods while maintaining unique word boundaries in a consistent manner.&#10;2. Utilizing a stable reference point, such as speaker onsets and offsets or acoustic segments with time-marks, can help reduce ambiguity during the merging process.&#10;3. Carefully defining the criteria and rules for merging is crucial to specify what is meant by &quot;merging&quot; different annotations or coding methods.&#10;4. Implementing a flexible database system, like a relational database that stores information from XML files, can accommodate modifications in word transcripts and time boundaries due to new recognition outputs or changes in forced alignment. This system allows for recomputing features and updating entries while preserving original time-stamps and IDs of the entries.&#10;5. Ensuring consistency and clarity in the annotation process by using clear markers, like 'utt' for utterances, can help manage changes in word transcripts or utterance boundaries, ultimately facilitating accurate merging of edited data from multiple parties.">
      <data key="d0">1</data>
    </edge>
    <edge source="The issue at hand is the difficulty in reliably merging different versions of word transcripts, especially when those changes are tied to acoustic segments. The speakers express concern that crucial information may be lost during the merge process, making it challenging to combine different annotations or coding methods. They discuss the need for a stable reference point, such as speaker ons and offs or acoustic segments with time-marks, to reduce ambiguity in the merging process. However, they acknowledge that this information might not always be available or correctly preserved during merges. The aim is to create a consistent and clear annotation system that can handle changes in word transcripts while maintaining the original time-stamps and IDs of entries in a relational database." target="The speakers discuss an efficient way to extract and find information in a complex structure by using a relational database that stores information from XML files. This approach allows flexibility for further computations or modifications by researchers, ensuring consistency and clarity in the annotation process. They also mention the importance of maintaining unique word boundaries while incorporating different annotations or coding methods, acknowledging that changes in word transcripts might create challenges during merges. However, they do not explicitly provide a solution for efficiently extracting and finding information in complex structures in this transcript.&#10;&#10;To efficiently extract and find information in a complex structure like an XML file or relational database, you can use query languages such as XQuery for XML files or SQL (Structured Query Language) for relational databases. These query languages allow users to search, filter, and manipulate data based on specific criteria, making it easier to locate the desired information within large and complex datasets. To implement this solution, further details about the structure of the data and the desired information would be required.">
      <data key="d0">1</data>
    </edge>
    <edge source="The issue at hand is the difficulty in reliably merging different versions of word transcripts, especially when those changes are tied to acoustic segments. The speakers express concern that crucial information may be lost during the merge process, making it challenging to combine different annotations or coding methods. They discuss the need for a stable reference point, such as speaker ons and offs or acoustic segments with time-marks, to reduce ambiguity in the merging process. However, they acknowledge that this information might not always be available or correctly preserved during merges. The aim is to create a consistent and clear annotation system that can handle changes in word transcripts while maintaining the original time-stamps and IDs of entries in a relational database." target="1. Potential uses of a system for extracting phone-level information from XML files include:&#10;   - Prosodic feature analysis at a fine-grained level (although the speakers express concerns about reliability and increased computational burden)&#10;   - Facilitating time-aligned annotation of phones, which could be useful for specific research questions or applications requiring detailed phonetic information.">
      <data key="d0">1</data>
    </edge>
    <edge source="The issue at hand is the difficulty in reliably merging different versions of word transcripts, especially when those changes are tied to acoustic segments. The speakers express concern that crucial information may be lost during the merge process, making it challenging to combine different annotations or coding methods. They discuss the need for a stable reference point, such as speaker ons and offs or acoustic segments with time-marks, to reduce ambiguity in the merging process. However, they acknowledge that this information might not always be available or correctly preserved during merges. The aim is to create a consistent and clear annotation system that can handle changes in word transcripts while maintaining the original time-stamps and IDs of entries in a relational database." target="1. More compact data storage: By storing additional or detailed information in an external file, the primary file remains smaller and more manageable during long meetings with multiple frames. This is particularly useful when dealing with large datasets.&#10;2. Reduced computational burden: Loading only essential data into memory for processing can decrease the computational load on the system, making it run more efficiently.&#10;3. Easier organization and maintenance: External files help keep related but separate information organized and make it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components.">
      <data key="d0">1</data>
    </edge>
    <edge source="The issue at hand is the difficulty in reliably merging different versions of word transcripts, especially when those changes are tied to acoustic segments. The speakers express concern that crucial information may be lost during the merge process, making it challenging to combine different annotations or coding methods. They discuss the need for a stable reference point, such as speaker ons and offs or acoustic segments with time-marks, to reduce ambiguity in the merging process. However, they acknowledge that this information might not always be available or correctly preserved during merges. The aim is to create a consistent and clear annotation system that can handle changes in word transcripts while maintaining the original time-stamps and IDs of entries in a relational database." target="1. Based on Grad C's observations a year ago, the annotated transcription graph system was not very complete, lacking external file format representation at that time. The only existing component was a conceptual node annotated transcription graph, which Grad C found appealing. Since then, there have been new developments, such as the creation of their own external file format and the development of numerous tools, although Grad C has not yet reviewed these updates.&#10;2. The team is considering using a relational database to express sentence types from XML files, which would allow flexibility for further computations or modifications by researchers. No specific tag or marker for this information has been determined in the transcript. However, the team emphasizes the importance of maintaining consistency and clarity in the annotation process and ensuring that any new types created can interact properly with existing types in the database.&#10;3. The use of a single timeline with unique identifiers (IDs) attached to different sections in a transcription file ensures consistent annotation and easy reference between various sections within the transcription. This concept, which starts the timeline tag with a consistent marker like 'utt' for utterances, enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;&#10;In summary, the annotated transcription graph system has made progress since Grad C's observations a year ago, with new developments such as an external file format and various tools. However, these updates have not been reviewed by Grad C yet. The team is considering using a relational database for flexibility in expressing sentence types from XML files while maintaining consistency and clarity in the annotation process. The single timeline concept allows for consistent annotation and reference between sections within the transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion transcript, the team is considering using a relational database in a format that stores information from XML files to express the type of a sentence in a file. This would allow flexibility for further computations or modifications by researchers. The exact tag or marker for this information is not explicitly stated in the transcript. However, they discuss the importance of maintaining consistency and clarity in the annotation process, as well as ensuring that any new types created can interact properly with existing types in the database. Therefore, careful consideration and definition of the criteria and rules for the new type would be required before implementation." target="1. The recommended way to handle data that has been edited and distributed among multiple parties for accurate merging involves the use of a reliable merging tool or method that can incorporate different annotations or coding methods while maintaining unique word boundaries in a consistent manner.&#10;2. Utilizing a stable reference point, such as speaker onsets and offsets or acoustic segments with time-marks, can help reduce ambiguity during the merging process.&#10;3. Carefully defining the criteria and rules for merging is crucial to specify what is meant by &quot;merging&quot; different annotations or coding methods.&#10;4. Implementing a flexible database system, like a relational database that stores information from XML files, can accommodate modifications in word transcripts and time boundaries due to new recognition outputs or changes in forced alignment. This system allows for recomputing features and updating entries while preserving original time-stamps and IDs of the entries.&#10;5. Ensuring consistency and clarity in the annotation process by using clear markers, like 'utt' for utterances, can help manage changes in word transcripts or utterance boundaries, ultimately facilitating accurate merging of edited data from multiple parties.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion transcript, the team is considering using a relational database in a format that stores information from XML files to express the type of a sentence in a file. This would allow flexibility for further computations or modifications by researchers. The exact tag or marker for this information is not explicitly stated in the transcript. However, they discuss the importance of maintaining consistency and clarity in the annotation process, as well as ensuring that any new types created can interact properly with existing types in the database. Therefore, careful consideration and definition of the criteria and rules for the new type would be required before implementation." target="The speakers discuss an efficient way to extract and find information in a complex structure by using a relational database that stores information from XML files. This approach allows flexibility for further computations or modifications by researchers, ensuring consistency and clarity in the annotation process. They also mention the importance of maintaining unique word boundaries while incorporating different annotations or coding methods, acknowledging that changes in word transcripts might create challenges during merges. However, they do not explicitly provide a solution for efficiently extracting and finding information in complex structures in this transcript.&#10;&#10;To efficiently extract and find information in a complex structure like an XML file or relational database, you can use query languages such as XQuery for XML files or SQL (Structured Query Language) for relational databases. These query languages allow users to search, filter, and manipulate data based on specific criteria, making it easier to locate the desired information within large and complex datasets. To implement this solution, further details about the structure of the data and the desired information would be required.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion transcript, the team is considering using a relational database in a format that stores information from XML files to express the type of a sentence in a file. This would allow flexibility for further computations or modifications by researchers. The exact tag or marker for this information is not explicitly stated in the transcript. However, they discuss the importance of maintaining consistency and clarity in the annotation process, as well as ensuring that any new types created can interact properly with existing types in the database. Therefore, careful consideration and definition of the criteria and rules for the new type would be required before implementation." target="1. Potential uses of a system for extracting phone-level information from XML files include:&#10;   - Prosodic feature analysis at a fine-grained level (although the speakers express concerns about reliability and increased computational burden)&#10;   - Facilitating time-aligned annotation of phones, which could be useful for specific research questions or applications requiring detailed phonetic information.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion transcript, the team is considering using a relational database in a format that stores information from XML files to express the type of a sentence in a file. This would allow flexibility for further computations or modifications by researchers. The exact tag or marker for this information is not explicitly stated in the transcript. However, they discuss the importance of maintaining consistency and clarity in the annotation process, as well as ensuring that any new types created can interact properly with existing types in the database. Therefore, careful consideration and definition of the criteria and rules for the new type would be required before implementation." target="1. Based on Grad C's observations a year ago, the annotated transcription graph system was not very complete, lacking external file format representation at that time. The only existing component was a conceptual node annotated transcription graph, which Grad C found appealing. Since then, there have been new developments, such as the creation of their own external file format and the development of numerous tools, although Grad C has not yet reviewed these updates.&#10;2. The team is considering using a relational database to express sentence types from XML files, which would allow flexibility for further computations or modifications by researchers. No specific tag or marker for this information has been determined in the transcript. However, the team emphasizes the importance of maintaining consistency and clarity in the annotation process and ensuring that any new types created can interact properly with existing types in the database.&#10;3. The use of a single timeline with unique identifiers (IDs) attached to different sections in a transcription file ensures consistent annotation and easy reference between various sections within the transcription. This concept, which starts the timeline tag with a consistent marker like 'utt' for utterances, enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;&#10;In summary, the annotated transcription graph system has made progress since Grad C's observations a year ago, with new developments such as an external file format and various tools. However, these updates have not been reviewed by Grad C yet. The team is considering using a relational database for flexibility in expressing sentence types from XML files while maintaining consistency and clarity in the annotation process. The single timeline concept allows for consistent annotation and reference between sections within the transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The recommended way to handle data that has been edited and distributed among multiple parties for accurate merging involves the use of a reliable merging tool or method that can incorporate different annotations or coding methods while maintaining unique word boundaries in a consistent manner.&#10;2. Utilizing a stable reference point, such as speaker onsets and offsets or acoustic segments with time-marks, can help reduce ambiguity during the merging process.&#10;3. Carefully defining the criteria and rules for merging is crucial to specify what is meant by &quot;merging&quot; different annotations or coding methods.&#10;4. Implementing a flexible database system, like a relational database that stores information from XML files, can accommodate modifications in word transcripts and time boundaries due to new recognition outputs or changes in forced alignment. This system allows for recomputing features and updating entries while preserving original time-stamps and IDs of the entries.&#10;5. Ensuring consistency and clarity in the annotation process by using clear markers, like 'utt' for utterances, can help manage changes in word transcripts or utterance boundaries, ultimately facilitating accurate merging of edited data from multiple parties." target="1. More compact data storage: By referring to an external file, the size of the primary file or format can be significantly reduced, making it more efficient and easier to manage during long meetings with multiple frames. This is especially beneficial when dealing with large datasets.&#10;2. Reduced computational burden: Storing information in an external file may decrease the computational load on the system since only the essential data needs to be loaded into memory for processing during the meeting.&#10;3. Easier organization and maintenance: External files can help keep related but separate information organized, making it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files can facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The recommended way to handle data that has been edited and distributed among multiple parties for accurate merging involves the use of a reliable merging tool or method that can incorporate different annotations or coding methods while maintaining unique word boundaries in a consistent manner.&#10;2. Utilizing a stable reference point, such as speaker onsets and offsets or acoustic segments with time-marks, can help reduce ambiguity during the merging process.&#10;3. Carefully defining the criteria and rules for merging is crucial to specify what is meant by &quot;merging&quot; different annotations or coding methods.&#10;4. Implementing a flexible database system, like a relational database that stores information from XML files, can accommodate modifications in word transcripts and time boundaries due to new recognition outputs or changes in forced alignment. This system allows for recomputing features and updating entries while preserving original time-stamps and IDs of the entries.&#10;5. Ensuring consistency and clarity in the annotation process by using clear markers, like 'utt' for utterances, can help manage changes in word transcripts or utterance boundaries, ultimately facilitating accurate merging of edited data from multiple parties." target="1. More compact data storage: By storing additional or detailed information in an external file, the primary file remains smaller and more manageable during long meetings with multiple frames. This is particularly useful when dealing with large datasets.&#10;2. Reduced computational burden: Loading only essential data into memory for processing can decrease the computational load on the system, making it run more efficiently.&#10;3. Easier organization and maintenance: External files help keep related but separate information organized and make it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers discuss an efficient way to extract and find information in a complex structure by using a relational database that stores information from XML files. This approach allows flexibility for further computations or modifications by researchers, ensuring consistency and clarity in the annotation process. They also mention the importance of maintaining unique word boundaries while incorporating different annotations or coding methods, acknowledging that changes in word transcripts might create challenges during merges. However, they do not explicitly provide a solution for efficiently extracting and finding information in complex structures in this transcript.&#10;&#10;To efficiently extract and find information in a complex structure like an XML file or relational database, you can use query languages such as XQuery for XML files or SQL (Structured Query Language) for relational databases. These query languages allow users to search, filter, and manipulate data based on specific criteria, making it easier to locate the desired information within large and complex datasets. To implement this solution, further details about the structure of the data and the desired information would be required." target=" hmm . The {disfmarker} the o the other issue that you had was , how do you actually efficiently extract , um {disfmarker} find and extract information in a structure of this type ?&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Grad C&#10;Content: So .&#10;Speaker: PhD F&#10;Content: That 's good .&#10;Speaker: PhD A&#10;Content: So you gave some examples like {disfmarker}&#10;Speaker: PhD F&#10;Content: Well , uh , and , I mean , you guys might {disfmarker} I don't know if this is premature because I suppose once you get the representation you can do this , but the kinds of things I was worried about is ,&#10;Speaker: PhD A&#10;Content: No , that 's not clear .&#10;Speaker: PhD F&#10;Content: uh {disfmarker}&#10;Speaker: PhD A&#10;Content: I mean , yeah , you c sure you can do it ,&#10;Speaker: PhD F&#10;Content: Well , OK . So i if it {disfmarker}&#10;Speaker: PhD A&#10;Content: but can you do it sort of l">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers discuss an efficient way to extract and find information in a complex structure by using a relational database that stores information from XML files. This approach allows flexibility for further computations or modifications by researchers, ensuring consistency and clarity in the annotation process. They also mention the importance of maintaining unique word boundaries while incorporating different annotations or coding methods, acknowledging that changes in word transcripts might create challenges during merges. However, they do not explicitly provide a solution for efficiently extracting and finding information in complex structures in this transcript.&#10;&#10;To efficiently extract and find information in a complex structure like an XML file or relational database, you can use query languages such as XQuery for XML files or SQL (Structured Query Language) for relational databases. These query languages allow users to search, filter, and manipulate data based on specific criteria, making it easier to locate the desired information within large and complex datasets. To implement this solution, further details about the structure of the data and the desired information would be required." target="1. Potential uses of a system for extracting phone-level information from XML files include:&#10;   - Prosodic feature analysis at a fine-grained level (although the speakers express concerns about reliability and increased computational burden)&#10;   - Facilitating time-aligned annotation of phones, which could be useful for specific research questions or applications requiring detailed phonetic information.">
      <data key="d0">1</data>
    </edge>
    <edge source=" hmm . The {disfmarker} the o the other issue that you had was , how do you actually efficiently extract , um {disfmarker} find and extract information in a structure of this type ?&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Grad C&#10;Content: So .&#10;Speaker: PhD F&#10;Content: That 's good .&#10;Speaker: PhD A&#10;Content: So you gave some examples like {disfmarker}&#10;Speaker: PhD F&#10;Content: Well , uh , and , I mean , you guys might {disfmarker} I don't know if this is premature because I suppose once you get the representation you can do this , but the kinds of things I was worried about is ,&#10;Speaker: PhD A&#10;Content: No , that 's not clear .&#10;Speaker: PhD F&#10;Content: uh {disfmarker}&#10;Speaker: PhD A&#10;Content: I mean , yeah , you c sure you can do it ,&#10;Speaker: PhD F&#10;Content: Well , OK . So i if it {disfmarker}&#10;Speaker: PhD A&#10;Content: but can you do it sort of l" target="Speaker PhD A expresses skepticism about being able to perform meaningful actions or recover information from a text representation that does not reflect the structure of the original words and annotations. They argue that the text representation will not contain sufficient information to accurately reconstruct the original annotations, as it does not capture the structural relationships between words and annotations.&#10;&#10;Speaker Grad C disagrees with this assessment, arguing that it is still possible to recover the original structure from the text representation. They suggest that by tagging certain features of the text (such as sentence boundaries), it may be possible to reconstruct the original annotations with a high degree of accuracy.&#10;&#10;Overall, the disagreement between Speaker PhD A and Speaker Grad C centers on whether it is possible to accurately recover the original structure of words and annotations from a text representation that does not explicitly encode this information. While Speaker PhD A is skeptical of this approach, Speaker Grad C believes that it may be possible with the use of additional tags or markers.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker PhD A expresses skepticism about being able to perform meaningful actions or recover information from a text representation that does not reflect the structure of the original words and annotations. They argue that the text representation will not contain sufficient information to accurately reconstruct the original annotations, as it does not capture the structural relationships between words and annotations.&#10;&#10;Speaker Grad C disagrees with this assessment, arguing that it is still possible to recover the original structure from the text representation. They suggest that by tagging certain features of the text (such as sentence boundaries), it may be possible to reconstruct the original annotations with a high degree of accuracy.&#10;&#10;Overall, the disagreement between Speaker PhD A and Speaker Grad C centers on whether it is possible to accurately recover the original structure of words and annotations from a text representation that does not explicitly encode this information. While Speaker PhD A is skeptical of this approach, Speaker Grad C believes that it may be possible with the use of additional tags or markers." target=" ?&#10;Speaker: PhD A&#10;Content: But this is {disfmarker}&#10;Speaker: Grad C&#10;Content: So , as opposed to {disfmarker}&#10;Speaker: PhD A&#10;Content: I {disfmarker} I 'm still , um , {vocalsound} not convinced that you can do much at all on the text {disfmarker} on the flat file that {disfmarker} that {disfmarker} you know , the text representation . e Because the text representation is gonna be , uh , not reflecting the structure of {disfmarker} of your words and annotations . It 's just {disfmarker} it 's {disfmarker}&#10;Speaker: Grad C&#10;Content: Well , if it 's not representing it , then how do you recover it ? Of course it 's representing it .&#10;Speaker: PhD A&#10;Content: No . You {disfmarker} you have to {disfmarker} what you have to do is you have to basically {disfmarker}&#10;Speaker: Grad C&#10;Content: That 's the whole point .&#10;Speaker: PhD A">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker PhD A expresses skepticism about being able to perform meaningful actions or recover information from a text representation that does not reflect the structure of the original words and annotations. They argue that the text representation will not contain sufficient information to accurately reconstruct the original annotations, as it does not capture the structural relationships between words and annotations.&#10;&#10;Speaker Grad C disagrees with this assessment, arguing that it is still possible to recover the original structure from the text representation. They suggest that by tagging certain features of the text (such as sentence boundaries), it may be possible to reconstruct the original annotations with a high degree of accuracy.&#10;&#10;Overall, the disagreement between Speaker PhD A and Speaker Grad C centers on whether it is possible to accurately recover the original structure of words and annotations from a text representation that does not explicitly encode this information. While Speaker PhD A is skeptical of this approach, Speaker Grad C believes that it may be possible with the use of additional tags or markers." target=": Well , that 's why I didn't call it &quot; sentence &quot; .&#10;Speaker: PhD F&#10;Content: So , right . Um , so it 's like a segment or something .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: So , I mean , I assume this is possible , that if you have {disfmarker} someone annotates the punctuation or whatever when they transcribe , you can say , you know , from {disfmarker} for {disfmarker} from the c beginning of the sentence to the end of the sentence , from the annotations , this is a unit , even though it never actually {disfmarker} i It 's only a unit by virtue of the annotations {pause} at the word - level .&#10;Speaker: Grad C&#10;Content: Sure . I mean , so you would {disfmarker} you would have yet another tag .&#10;Speaker: PhD F&#10;Content: And then that would get a tag somehow .&#10;Speaker: Grad C&#10;Content: You 'd have another tag which says this is of type &quot; sentence &quot; .&#10;Speaker: PhD F&#10;Content: OK . OK">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential uses of a system for extracting phone-level information from XML files include:&#10;   - Prosodic feature analysis at a fine-grained level (although the speakers express concerns about reliability and increased computational burden)&#10;   - Facilitating time-aligned annotation of phones, which could be useful for specific research questions or applications requiring detailed phonetic information." target="1. More compact data storage: By referring to an external file, the size of the primary file or format can be significantly reduced, making it more efficient and easier to manage during long meetings with multiple frames. This is especially beneficial when dealing with large datasets.&#10;2. Reduced computational burden: Storing information in an external file may decrease the computational load on the system since only the essential data needs to be loaded into memory for processing during the meeting.&#10;3. Easier organization and maintenance: External files can help keep related but separate information organized, making it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files can facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential uses of a system for extracting phone-level information from XML files include:&#10;   - Prosodic feature analysis at a fine-grained level (although the speakers express concerns about reliability and increased computational burden)&#10;   - Facilitating time-aligned annotation of phones, which could be useful for specific research questions or applications requiring detailed phonetic information." target="1. More compact data storage: By storing additional or detailed information in an external file, the primary file remains smaller and more manageable during long meetings with multiple frames. This is particularly useful when dealing with large datasets.&#10;2. Reduced computational burden: Loading only essential data into memory for processing can decrease the computational load on the system, making it run more efficiently.&#10;3. Easier organization and maintenance: External files help keep related but separate information organized and make it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Potential uses of a system for extracting phone-level information from XML files include:&#10;   - Prosodic feature analysis at a fine-grained level (although the speakers express concerns about reliability and increased computational burden)&#10;   - Facilitating time-aligned annotation of phones, which could be useful for specific research questions or applications requiring detailed phonetic information." target="1. Based on Grad C's observations a year ago, the annotated transcription graph system was not very complete, lacking external file format representation at that time. The only existing component was a conceptual node annotated transcription graph, which Grad C found appealing. Since then, there have been new developments, such as the creation of their own external file format and the development of numerous tools, although Grad C has not yet reviewed these updates.&#10;2. The team is considering using a relational database to express sentence types from XML files, which would allow flexibility for further computations or modifications by researchers. No specific tag or marker for this information has been determined in the transcript. However, the team emphasizes the importance of maintaining consistency and clarity in the annotation process and ensuring that any new types created can interact properly with existing types in the database.&#10;3. The use of a single timeline with unique identifiers (IDs) attached to different sections in a transcription file ensures consistent annotation and easy reference between various sections within the transcription. This concept, which starts the timeline tag with a consistent marker like 'utt' for utterances, enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;&#10;In summary, the annotated transcription graph system has made progress since Grad C's observations a year ago, with new developments such as an external file format and various tools. However, these updates have not been reviewed by Grad C yet. The team is considering using a relational database for flexibility in expressing sentence types from XML files while maintaining consistency and clarity in the annotation process. The single timeline concept allows for consistent annotation and reference between sections within the transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible for a prosodic phrase to span across two sentences in the scenario described. This possibility is acknowledged by the researchers when they discuss different segmentation levels that do not nest, meaning they may not fit neatly within each other. In this case, a prosodic phrase might start in one sentence and end in another without being contained entirely within either sentence." target="1. The group is discussing the possibility of an additional level of complexity in segmenting a word sequence, beyond sentences and prosodic phrases (as suggested by Grad C). This third level would add further structure to the word sequence.&#10;2. It is acknowledged that prosodic phrases can span across two sentences, not being contained entirely within either sentence (this is accepted when they discuss different segmentation levels that do not nest).&#10;3. The idea of overlapping codes or concepts is mentioned, but it is decided that they don't need to delve into the specifics of the coding system since it's not the main topic of the meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group is discussing the possibility of an additional level of complexity in segmenting a word sequence, beyond sentences and prosodic phrases (as suggested by Grad C). This third level would add further structure to the word sequence.&#10;2. It is acknowledged that prosodic phrases can span across two sentences, not being contained entirely within either sentence (this is accepted when they discuss different segmentation levels that do not nest).&#10;3. The idea of overlapping codes or concepts is mentioned, but it is decided that they don't need to delve into the specifics of the coding system since it's not the main topic of the meeting." target=" A&#10;Content: Hhh .&#10;Speaker: PhD F&#10;Content: it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: You would just have a r&#10;Speaker: PhD F&#10;Content: S&#10;Speaker: Grad C&#10;Content: or do you say this is part of this ? I think {disfmarker}&#10;Speaker: PhD D&#10;Content: You would refer up to the sentence .&#10;Speaker: PhD F&#10;Content: But they 're {disfmarker}&#10;Speaker: PhD A&#10;Content: Well , the thing {disfmarker}&#10;Speaker: PhD F&#10;Content: they 're actually overlapping each other , sort of .&#10;Speaker: Grad C&#10;Content: So {disfmarker}&#10;Speaker: PhD A&#10;Content: the thing is that some something may be a part of one thing for one purpose and another thing of another purpose .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: So f&#10;Speaker: PhD F&#10;Content: You have to have another type then , I guess .&#10;Speaker: PhD A&#10;Content: s Um , well ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group is discussing the possibility of an additional level of complexity in segmenting a word sequence, beyond sentences and prosodic phrases (as suggested by Grad C). This third level would add further structure to the word sequence.&#10;2. It is acknowledged that prosodic phrases can span across two sentences, not being contained entirely within either sentence (this is accepted when they discuss different segmentation levels that do not nest).&#10;3. The idea of overlapping codes or concepts is mentioned, but it is decided that they don't need to delve into the specifics of the coding system since it's not the main topic of the meeting." target=": PhD D&#10;Content: Uh .&#10;Speaker: Postdoc E&#10;Content: What , the codes themselves ?&#10;Speaker: PhD D&#10;Content: Well , th overlap codes .&#10;Speaker: Postdoc E&#10;Content: Or the {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I 'm not sure what that @ @ {disfmarker}&#10;Speaker: Grad C&#10;Content: Well , I mean , is that {disfmarker}&#10;Speaker: PhD D&#10;Content: It probably doesn't matter .&#10;Speaker: Postdoc E&#10;Content: Well , we don't have to go into the codes .&#10;Speaker: Grad C&#10;Content: I mean , it doesn't .&#10;Speaker: PhD D&#10;Content: No , I d&#10;Speaker: Postdoc E&#10;Content: We don't have to go into the codes .&#10;Speaker: Grad C&#10;Content: I mean , that {disfmarker} not for the topic of this meeting .&#10;Speaker: Postdoc E&#10;Content: But let me just {disfmarker} No . W the idea is just to have a separate green ribbon , you know , and {">
      <data key="d0">1</data>
    </edge>
    <edge source="1. More compact data storage: By referring to an external file, the size of the primary file or format can be significantly reduced, making it more efficient and easier to manage during long meetings with multiple frames. This is especially beneficial when dealing with large datasets.&#10;2. Reduced computational burden: Storing information in an external file may decrease the computational load on the system since only the essential data needs to be loaded into memory for processing during the meeting.&#10;3. Easier organization and maintenance: External files can help keep related but separate information organized, making it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files can facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components." target="Content: And {disfmarker} OK .&#10;Speaker: Grad C&#10;Content: So you would say &quot; refer to this external file &quot; . Um , so that external file wouldn't be in {disfmarker}&#10;Speaker: PhD F&#10;Content: So that might {disfmarker} that might work .&#10;Speaker: PhD D&#10;Content: But what {disfmarker} what 's the advantage of doing that versus just putting it into this format ?&#10;Speaker: Grad C&#10;Content: More compact , which I think is {disfmarker} is better .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Grad C&#10;Content: I mean , if you did it at this {disfmarker}&#10;Speaker: PhD F&#10;Content: I mean these are long meetings and with {disfmarker} for every frame ,&#10;Speaker: Grad C&#10;Content: You don't want to do it with that {disfmarker} Anything at frame - level you had better encode binary&#10;Speaker: PhD F&#10;Content: um {disfmarker}&#10;Speaker: Grad C&#10;Content: or it 's gonna">
      <data key="d0">1</data>
    </edge>
    <edge source="1. More compact data storage: By referring to an external file, the size of the primary file or format can be significantly reduced, making it more efficient and easier to manage during long meetings with multiple frames. This is especially beneficial when dealing with large datasets.&#10;2. Reduced computational burden: Storing information in an external file may decrease the computational load on the system since only the essential data needs to be loaded into memory for processing during the meeting.&#10;3. Easier organization and maintenance: External files can help keep related but separate information organized, making it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files can facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components." target="1. More compact data storage: By storing additional or detailed information in an external file, the primary file remains smaller and more manageable during long meetings with multiple frames. This is particularly useful when dealing with large datasets.&#10;2. Reduced computational burden: Loading only essential data into memory for processing can decrease the computational load on the system, making it run more efficiently.&#10;3. Easier organization and maintenance: External files help keep related but separate information organized and make it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: And {disfmarker} OK .&#10;Speaker: Grad C&#10;Content: So you would say &quot; refer to this external file &quot; . Um , so that external file wouldn't be in {disfmarker}&#10;Speaker: PhD F&#10;Content: So that might {disfmarker} that might work .&#10;Speaker: PhD D&#10;Content: But what {disfmarker} what 's the advantage of doing that versus just putting it into this format ?&#10;Speaker: Grad C&#10;Content: More compact , which I think is {disfmarker} is better .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Grad C&#10;Content: I mean , if you did it at this {disfmarker}&#10;Speaker: PhD F&#10;Content: I mean these are long meetings and with {disfmarker} for every frame ,&#10;Speaker: Grad C&#10;Content: You don't want to do it with that {disfmarker} Anything at frame - level you had better encode binary&#10;Speaker: PhD F&#10;Content: um {disfmarker}&#10;Speaker: Grad C&#10;Content: or it 's gonna" target="1. More compact data storage: By storing additional or detailed information in an external file, the primary file remains smaller and more manageable during long meetings with multiple frames. This is particularly useful when dealing with large datasets.&#10;2. Reduced computational burden: Loading only essential data into memory for processing can decrease the computational load on the system, making it run more efficiently.&#10;3. Easier organization and maintenance: External files help keep related but separate information organized and make it easier to maintain and update as needed without affecting the primary format's integrity.&#10;4. Improved collaboration: When working with multiple parties, external files facilitate better collaboration by allowing individuals to focus on specific aspects of the data while maintaining a clear connection between different components.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on Grad C's observations a year ago, the annotated transcription graph system was not very complete, lacking external file format representation at that time. The only existing component was a conceptual node annotated transcription graph, which Grad C found appealing. Since then, there have been new developments, such as the creation of their own external file format and the development of numerous tools, although Grad C has not yet reviewed these updates.&#10;2. The team is considering using a relational database to express sentence types from XML files, which would allow flexibility for further computations or modifications by researchers. No specific tag or marker for this information has been determined in the transcript. However, the team emphasizes the importance of maintaining consistency and clarity in the annotation process and ensuring that any new types created can interact properly with existing types in the database.&#10;3. The use of a single timeline with unique identifiers (IDs) attached to different sections in a transcription file ensures consistent annotation and easy reference between various sections within the transcription. This concept, which starts the timeline tag with a consistent marker like 'utt' for utterances, enables the team to handle changes in word transcripts or utterance boundaries while preserving the original time-stamps and IDs of the entries in the relational database.&#10;&#10;In summary, the annotated transcription graph system has made progress since Grad C's observations a year ago, with new developments such as an external file format and various tools. However, these updates have not been reviewed by Grad C yet. The team is considering using a relational database for flexibility in expressing sentence types from XML files while maintaining consistency and clarity in the annotation process. The single timeline concept allows for consistent annotation and reference between sections within the transcription." target=" Yeah . See , that 's the question . I mean , how stable is their {disfmarker} Are they ready to go ,&#10;Speaker: Grad C&#10;Content: The {disfmarker} I looked at it {disfmarker}&#10;Speaker: PhD D&#10;Content: or {disfmarker} ?&#10;Speaker: Grad C&#10;Content: The last time I looked at it was a while ago , probably a year ago , uh , when we first started talking about this .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Grad C&#10;Content: And at that time at least {vocalsound} it was still not very {pause} complete . And so , specifically they didn't have any external format representation at that time . They just had the sort of conceptual {pause} node {disfmarker} uh , annotated transcription graph , which I really liked . And that 's exactly what this stuff is based on . Since then , they 've developed their own external file format , which is , uh , you know , this sort of s this sort of thing . Um , and apparently they 've also developed a lot of tools , but I haven't looked at">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
