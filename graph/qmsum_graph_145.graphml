<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." />
    <node id=": I don't think so . Well , they {disfmarker} they {disfmarker} they 're going actually the {disfmarker} the other way , defining uh , phoneme clusters , apparently . Well .&#10;Speaker: Professor F&#10;Content: Aha . That 's right . Uh , and that 's an interesting {pause} way to go too .&#10;Speaker: PhD A&#10;Content: So they just throw the speech from all different languages together , then cluster it into sixty or fifty or whatever clusters ?&#10;Speaker: PhD G&#10;Content: I think they 've not done it , uh , doing , uh , multiple language yet , but what they did is to training , uh , English nets with all the phonemes , and then training it in English nets with , uh , kind of seventeen , I think it was {disfmarker} seventeen , uh , broad classes .&#10;Speaker: PhD A&#10;Content: Automatically derived {disfmarker} Mm - hmm . Automatically derived broad classes , or {disfmarker} ?&#10;Speaker: PhD G&#10;Content: Yeah . Yeah , I think so .&#10;Speaker: PhD A&#10;" />
    <node id=" with neural networks trained on , {vocalsound} uh , data from the same language , if possible , with , well , using a more generic database , which is phonetically {disfmarker} phonetically balanced , and . Um .&#10;Speaker: Professor F&#10;Content: So - so we had talked {disfmarker} I guess we had talked at one point about maybe , the language ID corpus ?&#10;Speaker: PhD G&#10;Content: Yeah . So .&#10;Speaker: Professor F&#10;Content: Is that a possibility for that ?&#10;Speaker: PhD G&#10;Content: Ye - uh {disfmarker} {pause} Yeah , but , uh these corpus , w w there is a CallHome and a CallFriend also , The CallFriend is for language ind identification . Well , anyway , these corpus are all telephone speech . So , um . {vocalsound} This could be a {disfmarker} {pause} a problem for {disfmarker} Why ? Because uh , uh , the {disfmarker} the SpeechDat databases are not telephone speech . They are downsampled to eight kilohertz but {disfmarker} but they are not {voc" />
    <node id="aker: PhD G&#10;Content: Yep .&#10;Speaker: Grad C&#10;Content: Out of that fifty - six .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: Grad C&#10;Content: Yeah . So , it 's {disfmarker} it 's definitely broader , yeah .&#10;Speaker: PhD G&#10;Content: But , actually , the issue of phoneti phon uh phone phoneme mappings will arise when we will do severa use several languages&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: because you {disfmarker} Well , some phonemes are not , uh , in every languages , and {disfmarker} So we plan to develop a subset of the phonemes , uh , that includes , uh , all the phonemes of our training languages ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: and use a network with kind of one hundred outputs or something like that .&#10;Speaker: Professor F&#10;Content: Mm - hmm . You mean a superset , sort of .&#10;Speaker: PhD G&#10;Content" />
    <node id=" mean , we have {disfmarker} {pause} i we have the {disfmarker} the trainings with our own categories . And now we 're saying , &quot; Well , how do we handle cross - language ? &quot; And one way is to come up with a superset , but they are als they 're trying coming up with clustered , and do we think there 's something wrong with that ?&#10;Speaker: PhD G&#10;Content: I think that there 's something wrong&#10;Speaker: Professor F&#10;Content: OK . What w&#10;Speaker: PhD G&#10;Content: or {disfmarker} Well , because {disfmarker} Well , for the moment we are testing on digits , and e i perhaps u using broad phoneme classes , it 's {disfmarker} it 's OK for um , uh classifying the digits , but as soon as you will have more words , well , words can differ with only a single phoneme , and {disfmarker} which could be the same , uh , class .&#10;Speaker: Professor F&#10;Content: I see .&#10;Speaker: PhD G&#10;Content: Well . So .&#10;Speaker: Professor" />
    <node id=" about the cube ?&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Oh ! Cube . Yeah .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Fill in the cube .&#10;Speaker: PhD G&#10;Content: Uh we {disfmarker} actually we want to , mmm , Uh , {vocalsound} uh , analyze three dimensions , the feature dimension , the {pause} training data dimension , and the test data dimension . Um . Well , what we want to do is first we have number for each {pause} uh task . So we have the um , TI - digit task , the Italian task , the French task {pause} and the Finnish task .&#10;Speaker: Professor F&#10;Content: Yeah ?&#10;Speaker: PhD G&#10;Content: So we have numbers with {pause} uh {disfmarker} systems {disfmarker} I mean {disfmarker} I mean neural networks trained on the task data . And then to have systems with neural networks trained on , {vocalsound} uh , data from the same language , if possible , with , well , using a more generic database ," />
    <node id=": Grad C&#10;Content: Um , for the {disfmarker} for nets trained on digits , {comment} um , we have been using , uh , four hundred order hidden units . And , um , for the broader class nets we 're {disfmarker} we 're going to increase that because the , um , the digits nets only correspond to about twenty phonemes .&#10;Speaker: PhD B&#10;Content: Uh - huh .&#10;Speaker: Grad C&#10;Content: So .&#10;Speaker: Professor F&#10;Content: Broader class ?&#10;Speaker: Grad C&#10;Content: Um , the broader {disfmarker} broader training corpus nets like TIMIT . Um , w we 're gonna {disfmarker}&#10;Speaker: Professor F&#10;Content: Oh , it 's not actually broader class , it 's actually finer class , but you mean {disfmarker} y You mean {vocalsound} more classes .&#10;Speaker: Grad C&#10;Content: Right . Right . Yeah . More classes . Right , right . More classes .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: That 's what I mean" />
    <node id="The speakers, PhD A and Professor F, discuss the difference between silence at the beginning and end of a signal, but ultimately conclude that they are both similar forms of silence. They mention that distinctions between these types of silence may have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. However, it is not explicitly pointed out that this difference has any significant impact on the mapping process. It is also noted that there was a glottal stop in the TIMIT dataset, which is a short period of silence, and this might have been relevant for the mapping as well." />
    <node id=" .&#10;Speaker: PhD A&#10;Content: Yeah , and I think some of them , they were making distinctions between silence at the end and silence at the beginning , when really they 're {pause} both silence .&#10;Speaker: PhD B&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: I th I think it was things like that that got it mapped down to fifty - six .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: Yeah , especially in a system like ours , which is a discriminative system . You know , you 're really asking this net to learn .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: It 's {disfmarker} it 's kind of hard .&#10;Speaker: PhD A&#10;Content: There 's not much difference , really . And {pause} the ones that are gone , I think are {disfmarker} I think there was {disfmarker} they also in TIMIT had like a glottal stop , which was basically a short period of silence ,&#10;Speaker: PhD B" />
    <node id=" A&#10;Content: Yeah . Right .&#10;Speaker: Professor F&#10;Content: then it 's probably {disfmarker}&#10;Speaker: PhD A&#10;Content: What about the differences in the phone sets ?&#10;Speaker: Grad C&#10;Content: Uh , between languages ?&#10;Speaker: PhD A&#10;Content: No , between TIMIT and the {disfmarker} the digits .&#10;Speaker: Grad C&#10;Content: Oh , um , right . Well , there 's a mapping from the sixty - one phonemes in TIMIT to {disfmarker} to fifty - six , the ICSI fifty - six .&#10;Speaker: PhD E&#10;Content: Sixty - one .&#10;Speaker: PhD A&#10;Content: Oh , OK . I see .&#10;Speaker: Grad C&#10;Content: And then the digits phonemes , um , there 's about twenty twenty - two or twenty - four of them ? Is that right ?&#10;Speaker: PhD A&#10;Content: Out of that fifty - six ?&#10;Speaker: PhD G&#10;Content: Yep .&#10;Speaker: Grad C&#10;Content: Out of that fifty - six .&#10;Speaker: PhD A&#10;" />
    <node id=" , it needs {disfmarker} we must r h do a lot of work {vocalsound} because we need to generate new tran transcription for the database that we have .&#10;Speaker: Professor F&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD B&#10;Content: Other than the language , is there a reason not to use the TIMIT phone set ? Cuz it 's larger ? As opposed to the ICSI {pause} phone set ?&#10;Speaker: Grad C&#10;Content: Oh , you mean why map the sixty - one to the fifty - six ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: I don't know . I have {disfmarker}&#10;Speaker: Professor F&#10;Content: Um , I forget if that happened starting with you , or was it {disfmarker} o or if it was Eric , afterwards who did that . But I think , basically , there were several of the phones that were just hardly ever there .&#10;Speaker: PhD A&#10;Content: Yeah , and I think some of them , they were making distinctions between silence at the end and silence at the" />
    <node id="isfmarker} they also in TIMIT had like a glottal stop , which was basically a short period of silence ,&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: and so .&#10;Speaker: PhD B&#10;Content: Well , we have that now , too , right ?&#10;Speaker: PhD A&#10;Content: I don't know .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So .&#10;Speaker: Professor F&#10;Content: i It 's actually pretty common that a lot of the recognition systems people use have things like {disfmarker} like , say thirty - nine , phone symbols , right ? Uh , and then they get the variety by {disfmarker} by bringing in the context , the phonetic context . Uh . So we actually have an unusually large number in {disfmarker} in what we tend to use here . Um . So , a a actually {disfmarker} maybe {disfmarker} now you 've got me sort of intrigued . What {disfmarker} there 's {disfmarker} Can you describe what" />
    <node id=" differences between these different databases . I mean some of this stuff 's recorded in the car , and some of it 's {disfmarker} I mean there 's {disfmarker} there 's many different acoustic differences . So I 'm not sure if {disfmarker} . I mean , unless we 're going to include a bunch of car recordings in the {disfmarker} in the training database , I 'm not sure if it 's {disfmarker} completely rules it out&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: if our {disfmarker} if we {disfmarker} if our major goal is to have phonetic context and you figure that there 's gonna be a mismatch in acoustic conditions does it make it much worse f to sort of add another mismatch , if you will .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Uh , i i I {disfmarker} I guess the question is how important is it to {disfmarker} for us to get multiple languages uh , in there .&#10;Speaker: PhD G&#10;Content" />
    <node id="The individual, presumably Professor F, is arranging to meet everyone at 6 AM in the morning at an unspecified location, as they plan to drive them to the airport. The backup plan if they are not there on time is not explicitly mentioned in the transcript, but it can be inferred that they would likely wait for a reasonable amount of time before leaving without them or making alternative arrangements." />
    <node id=" how would we ever accomplish that ? Uh {disfmarker} what {disfmarker} what {disfmarker} what part of town do you live in ?&#10;Speaker: Grad C&#10;Content: Um , I live in , um , the corner of campus . The , um , southeast corner .&#10;Speaker: Professor F&#10;Content: OK . OK , so would it be easier {disfmarker} those of you who are not , you know , used to this area , it can be very tricky to get to the airport at {disfmarker} at uh , you know , six thirty . Um . So . Would it be easier for you if you came here and I drove you ? Yeah ? Yeah , yeah , OK .&#10;Speaker: PhD G&#10;Content: Yeah , perhaps , yeah .&#10;Speaker: Grad C&#10;Content: Yeah . Sure .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: OK , so if {disfmarker} if everybody can get here at six .&#10;Speaker: PhD E&#10;Content: At six .&#10;Speaker: Professor F&#10;Content: Yeah , I 'm afraid we need to do that" />
    <node id="Speaker: Professor F&#10;Content: Nah , I 'll be fine . I just , uh {disfmarker} it {disfmarker} for me it just means getting up a half an hour earlier than I usually do . Not {disfmarker} not {disfmarker} not a lot ,&#10;Speaker: Grad C&#10;Content: OK . Wednesday .&#10;Speaker: Professor F&#10;Content: so OK , that was the real real important stuff . Um , I {disfmarker} I {disfmarker} I figured maybe wait on the potential goals for the meeting uh {disfmarker} until we talk about wh what 's been going on . So , uh , what 's been going on ? Why don't we start {disfmarker} start over here .&#10;Speaker: PhD G&#10;Content: Um . {vocalsound} Well , preparation of the French test data actually .&#10;Speaker: Professor F&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: So , {vocalsound} it means that um , well , it is , uh , a digit French database of microphone speech , downsampled to eight kilohertz and" />
    <node id="&#10;Speaker: PhD E&#10;Content: At six .&#10;Speaker: Professor F&#10;Content: Yeah , I 'm afraid we need to do that to get there on time .&#10;Speaker: Grad C&#10;Content: Six , OK .&#10;Speaker: Professor F&#10;Content: Yeah , so . Oh boy . Anyway , so .&#10;Speaker: PhD A&#10;Content: Will that {pause} be enough time ?&#10;Speaker: Professor F&#10;Content: Yeah . Yeah , so I 'll just pull up in front at six and just be out front . And , uh , and yeah , that 'll be plenty of time . It 'll take {disfmarker} it {disfmarker} it {disfmarker} it won't be bad traffic that time of day and {disfmarker} and uh&#10;Speaker: PhD A&#10;Content: I guess once you get past the bridge {pause} that that would be the worst .&#10;Speaker: PhD B&#10;Content: Yeah , Oakland .&#10;Speaker: Professor F&#10;Content: Going to Oakland .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Oakland .&#10;Speaker:" />
    <node id="} or anything , but&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: Uh , anyway . Uh {disfmarker} so here 's what I have for {disfmarker} I {disfmarker} I was just jotting down things I think th w that we should do today . Uh {disfmarker} This is what I have for an agenda so far Um , We should talk a little bit about the plans for the uh {disfmarker} the field trip next week . Uh {disfmarker} a number of us are doing a field trip to uh Uh {disfmarker} OGI And uh {disfmarker} mostly uh First though about the logistics for it . Then maybe later on in the meeting we should talk about what we actually you know , might accomplish . Uh {disfmarker}&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: Uh , in and {pause} kind of go around {disfmarker} see what people have been doing {disfmarker} talk about that , {pause} a r progress report . Um , Essentially . Um" />
    <node id=" Uh , are we still using P - make ? Is that {disfmarker} ?&#10;Speaker: Grad C&#10;Content: Oh , I don't know how w how we would P - make this , though . Um .&#10;Speaker: Professor F&#10;Content: Well , you have a {disfmarker} I mean , once you get the basic thing set up , you have just all the {disfmarker} uh , a all these combinations ,&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: right ?&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: Um . It 's {disfmarker} it 's {disfmarker} let 's say it 's six hours or eight hours , or something for the training of HTK . How long is it for training of {disfmarker} of , uh , the neural net ?&#10;Speaker: Grad C&#10;Content: The neural net ? Um .&#10;Speaker: PhD G&#10;Content: I would say two days .&#10;Speaker: PhD A&#10;Content: Depends on the corpuses , right ?&#10;" />
    <node id="The speaker sees several benefits in the current situation, despite the limitations of time and the need to address a new problem. Firstly, they recognize that there is significant computational power available across the institute, which can be leveraged to tackle this issue. They also highlight that while working on something new inevitably involves a learning and development process, in this case, they have the advantage of being able to build upon existing knowledge and data that they already possess. This includes having experience with techniques such as voiced-unvoiced-silence detection features. Therefore, instead of starting from scratch or exploring entirely unknown territory, they can focus on applying proven methods and utilizing available resources to make progress in the new problem area." />
    <node id=" of thing , or {disfmarker} or we 're not going to get through any significant number of these .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content:  So this is {disfmarker} Yeah , I mean , I kind of like this because what it {disfmarker} No {disfmarker}&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: uh , no , what I like about it is we {disfmarker} we {disfmarker} we do have a problem that we have very limited time . You know , so , with very limited time , we actually have really quite a {disfmarker} quite a bit of computational resource available if you , you know , get a look across the institute and how little things are being used . And uh , on the other hand , almost anything that really i you know , is {disfmarker} is new , where we 're saying , &quot; Well , let 's look at , like we were talking before about , uh , uh , voiced - unvoiced - silence detection features and all those sort {disfmarker" />
    <node id="} {disfmarker} he 's working on other things , but to {disfmarker} to do something on this project . So the question is , &quot; Where {disfmarker} where could we , uh , uh , most use Dave 's help ? &quot;&#10;Speaker: PhD G&#10;Content: Um , yeah , I was thinking perhaps if , um , additionally to all these experiments , which is not really research , well I mean it 's , uh , running programs&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: and , um , {vocalsound} trying to have a closer look at the {disfmarker} perhaps the , um , {vocalsound} speech , uh , noise detection or , uh , voiced - sound - unvoiced - sound detection and {disfmarker} Which could be important in {disfmarker} i for noise {disfmarker} noise {disfmarker}&#10;Speaker: PhD A&#10;Content: I think that would be a {disfmarker} I think that 's a big {disfmarker} big deal . Because the {disfmarker} you" />
    <node id=" look at , like we were talking before about , uh , uh , voiced - unvoiced - silence detection features and all those sort {disfmarker} &quot; that 's {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: I think it 's a great thing to go to . But if it 's new , then we have this development and {disfmarker} and {disfmarker} and learning process t to {disfmarker} to go through on top of {disfmarker} just the {disfmarker} the {disfmarker} all the {disfmarker} all the work . So , I {disfmarker} I {disfmarker} I don't see how we 'd do it . So what I like about this is you basically have listed all the things that we already know how to do .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: And {disfmarker} and all the kinds of data that we , at this point , already have . And , uh , you 're just saying let 's look" />
    <node id="disfmarker} Mm - hmm . OK .&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: So , presumably , that 'll be part of the topic of analysis of the {disfmarker} the test results , is how well you do when it 's matching noise and how well you do where it 's not .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: I think that 's right .&#10;Speaker: Grad C&#10;Content: So , I guess we can't train on {disfmarker} on the {disfmarker} the unseen noise conditions .&#10;Speaker: Professor F&#10;Content: Well , not if it 's not seen ,&#10;Speaker: Grad C&#10;Content: Right . If {disfmarker} Not if it 's unseen .&#10;Speaker: Professor F&#10;Content: yeah .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: OK . I mean , i i i i it does seem to me that a lot of times when you train with something that 's at least a little bit" />
    <node id="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is mentioned by Professor F when discussing the analysis of test results.&#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus.&#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments.&#10;4. Computational cost: While combining noise conditions for testing doesn't require as much computation as initial training, there is still some computational overhead for computing the KL transformation. Additionally, there will be further computational costs for training and testing the HTK models with these combinations." />
    <node id=" all {disfmarker} they 're all gonna be test tested , um , with one training of the HTK system . Um , there 's a script that tests all three different types of noise conditions . Test - A is like a matched noise . Test - B is a {disfmarker} is a slightly mismatched . And test - C is a , um , mismatched channel .&#10;Speaker: Grad D&#10;Content: And do we do all our {pause} training on clean data ?&#10;Speaker: Grad C&#10;Content: Um , no , no ,&#10;Speaker: PhD E&#10;Content: Also , we can clean that .&#10;Speaker: Grad C&#10;Content: we 're {disfmarker} we 're gonna be , um , training on the noise files that we do have .&#10;Speaker: PhD G&#10;Content: No .&#10;Speaker: Professor F&#10;Content: So , um {disfmarker} Yeah , so I guess the question is how long does it take to do a {disfmarker} a training ? I mean , it 's not totally crazy t I mean , these are {disfmarker} a lot of these are built - in" />
    <node id=" deep South .&#10;Speaker: Professor F&#10;Content: So - s so it 's not really from the US either .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Is that {disfmarker} ? OK .&#10;Speaker: Grad C&#10;Content: Yeah . OK . And , um , with within the training corporas um , we 're , uh , thinking about , um , training with noise . So , incorporating the same kinds of noises that , um , Aurora is in incorporating in their , um {disfmarker} in their training corpus . Um , I don't think we we 're given the , uh {disfmarker} the unseen noise conditions , though , right ?&#10;Speaker: Professor F&#10;Content: I think what they were saying was that , um , for this next test there 's gonna be some of the cases where they have the same type of noise as you were given before hand and some cases where you 're not .&#10;Speaker: Grad C&#10;Content: Like {disfmarker} Mm - hmm . OK .&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker:" />
    <node id=" C&#10;Content: Yeah . One hundred each , about .&#10;Speaker: Professor F&#10;Content: Uh , so that 's hundred and {disfmarker} {vocalsound} hundred and fourteen each .&#10;Speaker: Grad D&#10;Content: What a what about noise conditions ?&#10;Speaker: Professor F&#10;Content: What ?&#10;Speaker: Grad D&#10;Content: w Don't we need to put in the column for noise conditions ?&#10;Speaker: Professor F&#10;Content: Are you just trying to be difficult ?&#10;Speaker: Grad D&#10;Content: No , I just don't understand .&#10;Speaker: Grad C&#10;Content: Well , th uh , when {disfmarker} when I put these testings on there , I 'm assumi&#10;Speaker: Professor F&#10;Content: I 'm just kidding . Yeah .&#10;Speaker: Grad C&#10;Content: There - there 's three {disfmarker} three tests . Um , type - A , type - B , and type - C . And they 're all {disfmarker} they 're all gonna be test tested , um , with one training of the HTK system . Um , there 's" />
    <node id=" long a {disfmarker} a training takes , if we can train up all these {disfmarker} these combinations , uh , then we can start working on testing of them individually , and in combination . Right ?&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: Because the putting them in combination , I think , is not as much computationally as the r training of the nets in the first place . Right ?&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: So y you do have to compute the KL transformation . Uh , which is a little bit , but it 's not too much .&#10;Speaker: PhD G&#10;Content: It 's not too much ,&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: no .&#10;Speaker: Professor F&#10;Content: So it 's {disfmarker}&#10;Speaker: PhD G&#10;Content: But {disfmarker} Yeah . But there is the testing also , which implies training , uh , the HTK models&#10;Speaker: PhD E&#10;Content: The {disfmarker}" />
    <node id="The suggestion made by Professor F was to approach the task as a series of nested loops along the time axis, with a choice at each stage. This implies that instead of thinking of the process in stages, they should consider it as a single stream of data over time, with different choices to be made at various points. He also illustrates this concept by comparing it to having three nested loops with a choice for each one." />
    <node id=" Yeah . Um , I {disfmarker} I was thinking two things . Uh , the first thing was , um {disfmarker} we {disfmarker} we actually had thought of this as sort of like , um {disfmarker} not {disfmarker} not in stages , {comment} but more along the {disfmarker} the time axis . Just kind of like one stream at a time ,&#10;Speaker: Professor F&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: je - je - je - je - je {comment} check out the results and {disfmarker} and go that way .&#10;Speaker: Professor F&#10;Content: Oh , yeah , yeah , sure . No , I 'm just saying , I 'm just thinking of it like loops ,&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: Professor F&#10;Content: right ? And so , y y y if you had three nested loops , that you have a choice for this , a choice for this , and a choice for that ,&#10;Speaker: Grad C&#10;Content: Yeah . Mm - hmm .&#10;Spe" />
    <node id=" putting those together .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: Um . Yeah .&#10;Speaker: PhD A&#10;Content: When you do that , you 're increasing the size of the inputs to the net . Do you have to reduce the hidden layer , or something ?&#10;Speaker: Professor F&#10;Content: Well , so {disfmarker} I mean , so i it doesn't increase the number of trainings .&#10;Speaker: PhD A&#10;Content: No , no , I 'm {disfmarker} I 'm just wondering about number of parameters in the net . Do you have to worry about keeping that the same , or {disfmarker} ?&#10;Speaker: Professor F&#10;Content: Uh , I don't think so .&#10;Speaker: PhD B&#10;Content: There 's a computation limit , though , isn't there ?&#10;Speaker: Professor F&#10;Content: Yeah , I mean , it 's just more compu Excuse me ?&#10;Speaker: PhD B&#10;Content: Isn't there like a limit {pause} on the computation load , or d latency , or something like that for Aurora task" />
    <node id="er} and the inputs are PLP and delta and that sort of thing ,&#10;Speaker: Grad C&#10;Content: Well , the inputs are one dimension of the cube ,&#10;Speaker: Professor F&#10;Content: or {disfmarker} ?&#10;Speaker: Grad C&#10;Content: which , um , we 've talked about it being , uh , PLP , um , M F C Cs , um , J - JRASTA , JRASTA - LDA {disfmarker}&#10;Speaker: PhD G&#10;Content: Hmm .&#10;Speaker: Professor F&#10;Content: Yeah , but your initial things you 're making one choice there ,&#10;Speaker: Grad C&#10;Content: Yeah ,&#10;Speaker: Professor F&#10;Content: right ?&#10;Speaker: Grad C&#10;Content: right .&#10;Speaker: Professor F&#10;Content: Which is PLP , or something ?&#10;Speaker: Grad C&#10;Content: Um , I {disfmarker} I haven't {disfmarker} I haven't decided on {disfmarker} on the initial thing .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Probably {" />
    <node id="marker} and they 're han hand uh {disfmarker} hand - marked . Uh , I guess , actually , TIMIT was not entirely hand - marked . It was automatically first , and then hand {disfmarker} hand - corrected .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: Professor F&#10;Content: But {disfmarker} but , um , uh , it {disfmarker} it , um , it might be a better source . So , i it 's {disfmarker} you 're right . It would be another interesting scientific question to ask , &quot; Is it because it 's a broad source or because it was , you know , carefully ? &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: uh . And that 's something you could ask , but given limited time , I think the main thing is if it 's a better thing for going across languages on this training tandem system ,&#10;Speaker: PhD A&#10;Content: Yeah . Right .&#10;Speaker: Professor F&#10;Content: then it 's probably {disfmarker}&#10;Speaker:" />
    <node id="The speakers are discussing the use of a neural network to classify phonemes using English neural networks that were trained with phonemes and seventeen broad classes. They mention the idea of creating a superset of broad phoneme classes, which could be used for cross-language handling. However, one speaker thinks there is something wrong with this approach, suggesting that using the same language for training the neural network and having a phonetically balanced database would be better.&#10;&#10;Later in the discussion, they talk about potential issues when discriminating words based on the output of the neural network. They mention that this may not provide enough information to distinguish between words due to the fact that most confusions occur within phone classes. This is because words can differ with only a single phoneme, which could be in the same broad class. The speakers note that this might have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. Additionally, they mention a glottal stop in the TIMIT dataset, which is a short period of silence, and this might also be relevant for the mapping process." />
    <node id="aker: PhD G&#10;Content: I would say two days .&#10;Speaker: PhD A&#10;Content: Depends on the corpuses , right ?&#10;Speaker: PhD E&#10;Content: It depends .&#10;Speaker: PhD B&#10;Content: It s also depends on the net .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Depends on the corpus .&#10;Speaker: PhD B&#10;Content: How big is the net ?&#10;Speaker: PhD E&#10;Content: For Albayzin I trained on neural network , uh , was , um , one day also .&#10;Speaker: Professor F&#10;Content: Uh , but on what machine ?&#10;Speaker: Grad C&#10;Content: On a SPERT board .&#10;Speaker: PhD E&#10;Content: Uh . I {disfmarker} I think the neural net SPERT .&#10;Speaker: Grad C&#10;Content: Y you did a {disfmarker} you did it on a SPERT board .&#10;Speaker: PhD E&#10;Content: Yes .&#10;Speaker: Professor F&#10;Content: OK , again , we do have a bunch of" />
    <node id=" class .&#10;Speaker: Professor F&#10;Content: I see .&#10;Speaker: PhD G&#10;Content: Well . So .&#10;Speaker: Professor F&#10;Content: Right . Although , you are not using this for the {disfmarker}&#10;Speaker: PhD G&#10;Content: So , I 'm&#10;Speaker: Professor F&#10;Content: You 're using this for the feature generation , though , not the {disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah , but you will ask the net to put one for th th the phoneme class&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: and {disfmarker} So .&#10;Speaker: PhD A&#10;Content: So you 're saying that there may not be enough information coming out of the net to help you discriminate the words ?&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Well . Yeah , yeah . Mmm .&#10;Speaker: PhD B&#10;Content: Fact , most confusions are within the phone {disfmarker} phone classes , right ? I think , uh , Larry was saying like obstruents are" />
    <node id="1. HTK Training and Testing: The speakers discuss conducting tests using the Hidden Markov Model Toolkit (HTK) system. This involves training the HTK models with one set of data and testing it with another. The computational cost and time required for an HTK training, which is around six hours or more, are considered.&#10;2. Noise Condition Tests: Grad D brings up the topic of noise conditions, questioning whether they should include a column for it in their analysis. Three types of noise conditions (A, B, and C) will be used for testing the model's performance with different noisy environments. Type A is matched noise, Type B is slightly mismatched, and Type C is mismatched channel noise.&#10;3. Cross-Language Testing: PhD G mentions a cross-language testing scenario where results showed better performance. This raises an interesting question about whether they should leverage this method for their own testing purposes.&#10;4. Training Corpus Tests: The discussion also involves tests based on the training corpus, specifically for the ANN (Artificial Neural Network) model. The team considers various features and different combinations of the training corpus to optimize the ANN performance.&#10;5. Discriminant Neural Net Testing: Grad C mentions a test related to the discriminant neural net, where they analyze the first dimension of features used in the testing process." />
    <node id=" there is the testing also , which implies training , uh , the HTK models&#10;Speaker: PhD E&#10;Content: The {disfmarker} the model {disfmarker} the HTK model .&#10;Speaker: PhD G&#10;Content: and , well ,&#10;Speaker: Professor F&#10;Content: Uh , right .&#10;Speaker: PhD G&#10;Content: it 's {disfmarker}&#10;Speaker: Professor F&#10;Content: Right . So if you do have lots of combinations , it 's {disfmarker}&#10;Speaker: PhD G&#10;Content: yeah . But it 's {disfmarker} it 's {disfmarker} it 's not so long . It @ @ {disfmarker} Yeah .&#10;Speaker: Professor F&#10;Content: How long does it take for an , uh , HTK training ?&#10;Speaker: PhD G&#10;Content: It 's around six hours , I think .&#10;Speaker: PhD E&#10;Content: It depends on the {disfmarker}&#10;Speaker: PhD G&#10;Content: For training and testing , yeah .&#10;Speaker: PhD E&#10;Content: More than six hours" />
    <node id=" 's tethered .&#10;Speaker: Grad C&#10;Content: The first dimension is the {disfmarker} the features that we 're going to use . And the second dimension , um , is the training corpus . And that 's the training on the discriminant neural net . Um and the last dimension happens to be {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah and again {disfmarker} Yeah . So the {disfmarker} the training for HTK is always {disfmarker} that 's always set up for the individual test , right ? That there 's some training data and some test data . So that 's different than this .&#10;Speaker: Grad C&#10;Content: Right , right . This is {disfmarker} this is for {disfmarker} for ANN only . And , yeah , the training for the HTK models is always , uh , fixed for whatever language you 're testing on .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: And then , there 's the testing corpus . So , then I think it 's probably instructive to go and {disfmark" />
    <node id=" or {disfmarker} ?&#10;Speaker: PhD G&#10;Content: Yeah . Yeah , I think so .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD G&#10;Content: Uh , and , yeah . And the result was that apparently , when testing on cross - language it was better . I think so . But Hynek didn't add {disfmarker} didn't have all the results when he showed me that , so , well .&#10;Speaker: Professor F&#10;Content: So that does make an interesting question , though .&#10;Speaker: PhD G&#10;Content: But {disfmarker}&#10;Speaker: Professor F&#10;Content: Is there 's some way that we should tie into that with this . Um . Right ? I mean , if {disfmarker} if in fact that is a better thing to do , {pause} should we leverage that , rather than doing , {pause} um , our own . Right ? So , if i if {disfmarker} if they s I mean , we have {disfmarker} {pause} i we have the {disfmarker} the trainings with our own categories . And" />
    <node id="If they are able to successfully get all the specified features correct in an experiment, it would mean that they have defined and correctly identified all the phones (distinct units of sound) in the data. This is significant because phones are the building blocks of speech and language, and accurately identifying them is crucial for many applications such as speech recognition and synthesis. Professor F also mentions that this would be equivalent to saying that all the phones are correct. However, it's important to note that just getting all the features right may not necessarily guarantee perfect results in other aspects of their work, such as normalization or word discrimination, which are also discussed in the conversation." />
    <node id="disfmarker}&#10;Speaker: Professor F&#10;Content: He {disfmarker} he di he didn't mention that part .&#10;Speaker: PhD A&#10;Content: Well , Hynek said that {disfmarker} that , I guess before they had him work on this , they had done some experiment where if they could get that one feature right , it dramatically improved the result .&#10;Speaker: Professor F&#10;Content: But . I see . OK .&#10;Speaker: PhD A&#10;Content: So I was thinking , you know {disfmarker} it made me think about this , that if {disfmarker} it 'd be an interesting experiment just to see , you know , if you did get all of those right .&#10;Speaker: Professor F&#10;Content: Should be . Because if you get all of them in there , that defines all of the phones . So that 's {disfmarker} that 's equivalent to saying that you 've got {disfmarker} {vocalsound} got all the phones right .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: So , if that doesn't help ," />
    <node id=" an another question occurred to me is {disfmarker} is what were you folks planning to do about normalization ?&#10;Speaker: PhD G&#10;Content: Um . Well , we were thinking about using this systematically for all the experiments . Um .&#10;Speaker: Professor F&#10;Content: This being {disfmarker} ?&#10;Speaker: PhD G&#10;Content: So , but {disfmarker} Uh . So that this could be another dimension , but we think perhaps we can use the {disfmarker} the best , uh , um , uh , normalization scheme as OGI is using , so , with parameters that they use there , &#10;Speaker: Professor F&#10;Content: Yeah , I think that 's a good idea .&#10;Speaker: PhD G&#10;Content: u {vocalsound} u&#10;Speaker: Professor F&#10;Content: I mean it 's i i we {disfmarker} we seem to have enough dimensions as it is . So probably if we {vocalsound} sort of take their {disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah , yeah , yeah .&#10;Speaker: Professor F&#10;Content: probably the on -" />
    <node id=" , with a general database {disfmarker} general databases . u So that th Well , the {disfmarker} the guy who has to develop an application with one language can use the net trained o on that language , or a generic net ,&#10;Speaker: Professor F&#10;Content: Uh , depen it depen it depends how you mean &quot; using the net &quot; .&#10;Speaker: PhD G&#10;Content: but not trained on a {disfmarker}&#10;Speaker: Professor F&#10;Content: So , if you 're talking about for producing these discriminative features {pause} that we 're talking about {pause} you can't do that .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Because {disfmarker} because the {disfmarker} what they 're asking for is {disfmarker} is a feature set . Right ? And so , uh , we 're the ones who have been weird by {disfmarker} by doing this training . But if we say , &quot; No , you have to have a different feature set for each language , &quot; I think this is ver gonna be very bad .&#10;" />
    <node id="1. The targets of the net, as discussed in this context, appear to be articulatory features. Articulatory features are a way to describe the position and movement of speech organs when producing sounds. They are typically used in speech synthesis and analysis.&#10;2. Yes, articulatory features imply that multiple features can be active at the same time in this discussion. Professor F confirms that having more than one articulatory feature active at a time is correct, indicating that they are considering a multi-valued or multi-dimensional representation for these features. This allows for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures." />
    <node id=" really mark articulatory features , you really wanna look at the acoustics and {disfmarker} and see where everything is , and we 're not gonna do that . So , uh , the second class way of doing it is {pause} to look at the , uh , phones that are labeled and translate them into acoustic {disfmarker} uh , uh {disfmarker} articulatory , uh , uh , features . So it won't really be right . You won't really have these overlapping {pause} things and so forth ,&#10;Speaker: PhD A&#10;Content: So the targets of the net {disfmarker} are these {disfmarker} ?&#10;Speaker: Professor F&#10;Content: but {disfmarker}&#10;Speaker: PhD A&#10;Content: Articulatory features .&#10;Speaker: Professor F&#10;Content: Articulatory feature .&#10;Speaker: PhD A&#10;Content: But that implies that you can have more than one on at a time ?&#10;Speaker: Professor F&#10;Content: Right . That 's right .&#10;Speaker: PhD A&#10;Content: Ah . OK .&#10;Speaker: Professor F&#10;Content: You either do that or" />
    <node id=" be done {disfmarker}&#10;Speaker: PhD B&#10;Content: Um .&#10;Speaker: Professor F&#10;Content: is that we could {disfmarker} we could , uh , just translate {disfmarker} instead of translating to a superset , {pause} just translate to articulatory features , some set of articulatory features and train with that . Now the fact {disfmarker} even though it 's a smaller number , {pause} it 's still fine because you have the {disfmarker} the , uh , combinations . So , in fact , it has every , you know {disfmarker} it had {disfmarker} has {disfmarker} has every distinction in it that you would have the other way .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: But it should go across languages better .&#10;Speaker: PhD A&#10;Content: We could do an interesting cheating experiment with that too . We could {disfmarker} I don't know , if you had uh the phone labels , you could replace them by their articulatory features and then feed in a vector with those uh , things" />
    <node id="marker} which I {disfmarker} I guess they 're starting to look at up there , {comment} training to something more like articulatory features . Uh , and if you have something that 's just good for distinguishing different articulatory features that should just be good across , you know , a wide range of languages .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Uh , but {disfmarker} Yeah , so I don't th I know {disfmarker} unfortunately I don't {disfmarker} I see what you 're comi where you 're coming from , I think , but I don't think we can ignore it .&#10;Speaker: PhD G&#10;Content: So we {disfmarker} we really have to do test with a real cross - language . I mean , tr for instance training on English and testing on Italian , or {disfmarker} Or we can train {disfmarker} or else , uh , can we train a net on , uh , a range of languages and {disfmarker} which can include the test {disfmarker} the test @ @ the target language ," />
    <node id="The speaker, presumably Professor F, is arranging to meet with the group at 6 AM in front of an unspecified location. If there are any delays, Professor F mentioned that they will be fine and do not need to be woken up, but they offered to provide their phone number just in case." />
    <node id="&#10;Speaker: Grad C&#10;Content: s So just {disfmarker}&#10;Speaker: Professor F&#10;Content: W you 're staying overnight . I figured you wouldn't need a great big suitcase , yeah .&#10;Speaker: PhD G&#10;Content: Oh yeah . Yeah .&#10;Speaker: Professor F&#10;Content: That 's sort of {pause} {vocalsound} one night . So . Anyway . OK .&#10;Speaker: Grad C&#10;Content: So , s six AM , in front .&#10;Speaker: Professor F&#10;Content: Six AM in front .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: Uh , I 'll be here . Uh {disfmarker} I 'll {disfmarker} I 'll {disfmarker} I 'll {disfmarker} I 'll give you my phone number , If I 'm not here for a few m after a few minutes then&#10;Speaker: Grad C&#10;Content: Wake you up .&#10;Speaker: Professor F&#10;Content: Nah , I 'll be fine . I just , uh {disfmarker} it {disfmarker" />
    <node id=" That 's right .&#10;Speaker: PhD A&#10;Content: Ah . OK .&#10;Speaker: Professor F&#10;Content: You either do that or you have multiple nets .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: Professor F&#10;Content:  Um . And , um I don't know if our software {disfmarker} this {disfmarker} if the qu versions of the Quicknet that we 're using allows for that . Do you know ?&#10;Speaker: Grad C&#10;Content: Allows for {disfmarker} ?&#10;Speaker: Professor F&#10;Content: Multiple targets being one ?&#10;Speaker: Grad C&#10;Content: Oh , um , we have gotten soft targets to {disfmarker} to work .&#10;Speaker: Professor F&#10;Content: OK . So that {disfmarker} that 'll work , yeah .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: OK . So , um , that 's another thing that could be done {disfmarker}&#10;Speaker: PhD B&#10;Content: Um .&#10;Speaker: Professor F&#10;Content: is that we could" />
    <node id=" , most confusions are within the phone {disfmarker} phone classes , right ? I think , uh , Larry was saying like obstruents are only confused with other obstruents , et cetera , et cetera .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor F&#10;Content: Yeah . Yeah .&#10;Speaker: PhD G&#10;Content: Yeah , this is another p yeah , another point .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: So {disfmarker} so , maybe we could look at articulatory type stuff ,&#10;Speaker: Professor F&#10;Content: But that 's what I thought they were gonna {disfmarker}&#10;Speaker: Grad C&#10;Content: right ?&#10;Speaker: Professor F&#10;Content: Did they not do that , or {disfmarker} ?&#10;Speaker: PhD G&#10;Content: I don't think so . Well ,&#10;Speaker: Professor F&#10;Content: So {disfmarker}&#10;Speaker: PhD G&#10;Content: they were talking about , perhaps , but they d&#10;" />
    <node id="Based on the information provided in the transcripts, it is stated that an HTK training takes around six hours or more. The discussion includes testing the model's performance on three different types of noise conditions (matched, mismatched, and mismatched channel) using a script. However, there is no explicit mention of the time required to complete a training session with the specific configuration you mentioned. To provide an accurate answer, further details or specifications about the computational cost associated with this particular scenario are needed." />
    <node id="Four hundred order hidden units are being used for nets trained on digits. The plan to increase the number of units for broader class nets, which actually means using more classes, is because these broader training corpus nets like TIMIT correspond to a larger number of phoneme classes compared to the digits nets that only correspond to about twenty phonemes." />
    <node id=" . Um , yeah , so I I 've been looking at , uh , uh , TIMIT stuff . Um , the {disfmarker} the stuff that we 've been working on with TIMIT , trying to get a , um {disfmarker} a labels file so we can , uh , train up a {disfmarker} train up a net on TIMIT and test , um , the difference between this net trained on TIMIT and a net trained on digits alone . Um , and seeing if {disfmarker} if it hurts or helps .&#10;Speaker: Professor F&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: Anyway .&#10;Speaker: Professor F&#10;Content: And again , when y just to clarify , when you 're talking about training up a net , you 're talking about training up a net for a tandem approach ?&#10;Speaker: Grad C&#10;Content: Yeah , yeah . Um . Mm - hmm .&#10;Speaker: Professor F&#10;Content: And {disfmarker} and the inputs are PLP and delta and that sort of thing ,&#10;Speaker: Grad C&#10;Content: Well , the inputs are one dimension" />
    <node id="One major criticism against current speech recognition systems is that they throw away all the harmonicity information and try to get spectral envelopes, discarding most of the information about the phonetic identity which is contained in the harmonics. Incorporating pitch or harmonicity information could address this issue by preserving some of this lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds." />
    <node id=" So , um {disfmarker} And th th the other tricky thing is , since we are , uh , even though we 're not {disfmarker} we don't have a strict prohibition on memory size , and {disfmarker} and computational complexity , uh , clearly there 's some limitation to it . So if we have to {disfmarker} if we say we have to have a pitch detector , say , if we {disfmarker} if we 're trying to incorporate pitch information , or at least some kind of harmonic {disfmarker} harmonicity , or something , this is another whole thing , take a while to develop . Anyway , it 's a very very interesting topic . I mean , one {disfmarker} I think one of the {disfmarker} a lot of people would say , and I think Dan would also , uh , that one of the things wrong with current speech recognition is that we {disfmarker} we really do throw away all the harmonicity information . Uh , we try to get spectral envelopes . Reason for doing that is that most of the information about the phonetic identity is in the spectral envelopes are not in the harmonic" />
    <node id="The goal of computing and sending features instead of actual speech in the development of new telephone standards, as discussed by the speakers, is to improve speech recognition and synthesis by accurately identifying phones (distinct units of sound) in the data. This would allow for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures. Additionally, incorporating pitch or harmonicity information could address current criticism of speech recognition systems by preserving some of the lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.&#10;&#10;One challenge in achieving this goal is the differences in phone sets between different databases and languages. In the conversation, they mention a mapping from the 61 phonemes in TIMIT to the 56 phonemes in ICSI and the differences in phone sets between the two databases. This shows that addressing the issue of phone set differences is an important step towards achieving the goal of accurately identifying phones and improving speech recognition and synthesis in new telephone standards." />
    <node id=" SpeechDat databases are not telephone speech . They are downsampled to eight kilohertz but {disfmarker} but they are not {vocalsound} uh with telephone bandwidth .&#10;Speaker: Professor F&#10;Content: Yeah . That 's really funny isn't it ? I mean cuz th this whole thing is for {pause} developing new standards for the telephone .&#10;Speaker: Grad C&#10;Content: Telephone .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Yeah , but the {disfmarker} the idea is to compute the feature before {pause} the {disfmarker} before sending them to the {disfmarker} Well , {pause} you don't {disfmarker} do not send speech , you send features , computed on th the {disfmarker} {pause} the device ,&#10;Speaker: Professor F&#10;Content: Mm - hmm . Yeah , I know , but the reason {disfmarker}&#10;Speaker: PhD G&#10;Content: or {disfmarker} Well .&#10;Speaker: Professor F&#10;Content: Oh I see , so your point is that it '" />
    <node id="1. Purpose of the trip: The purpose of the trip is not explicitly stated in the transcript, but it involves Professor F, PhD G, and possibly others, conducting experiments and working on a project, which seems to involve analyzing audio recordings based on the context.&#10;2. Preparations for Dave's contribution: In response to Dave's offer to contribute to the project, they are discussing where his help would be most useful. PhD G suggests having Dave focus on research aspects like implementing speech noise detection or voiced-unvoiced sound detection algorithms in addition to the ongoing experiments. The team should consider how best to integrate Dave into their workflow and delegate tasks based on his skills and interests, as well as ensure he has access to necessary resources such as data and computational power.&#10;&#10;However, there is no information provided about specific preparations related to time or disk space mentioned in the transcripts. These considerations are discussed separately in the context of the project's progress but not directly linked to Dave's offer to help." />
    <node id=" trip started off with {disfmarker} with , uh , Stephane talking to Hynek , so you may have {disfmarker} you may have had other goals , uh , for going up , and any anything else you can think of would be {disfmarker} we should think about {pause} accomplishing ? I mean , I 'm just saying this because {pause} maybe there 's things we need to do in preparation .&#10;Speaker: PhD G&#10;Content: Oh , I think basically , this is {disfmarker} this is , uh , yeah .&#10;Speaker: Professor F&#10;Content: OK . OK . Uh . Alright . And uh {disfmarker} and the other {disfmarker} the {disfmarker} the last topic I had here was , um , uh d Dave 's fine offer to {disfmarker} to , uh , do something {pause} {vocalsound} on this . I mean he 's doing {disfmarker} {vocalsound} {disfmarker} he 's working on other things , but to {disfmarker} to do something on this project . So the" />
    <node id="} It 's gonna take us a couple weeks at least to get the , uh , uh , the amount of disk we 're gonna be getting . We 're actually gonna get , uh , I think four more , uh , thirty - six gigabyte drives and , uh , put them on another {disfmarker} another disk rack . We ran out of space on the disk rack that we had , so we 're getting another disk rack and {vocalsound} four more drives to share between , uh {disfmarker} primarily between this project and the Meetings {disfmarker} Meetings Project . Um . But , uh , we 've put another {disfmarker} I guess there 's another eighteen gigabytes that 's {disfmarker} that 's in there now to help us with the immediate crunch . But , uh , are you saying {disfmarker} So I don't know where {pause} you 're {disfmarker} Stephane , where you 're doing your computations . If {disfmarker} i so , you 're on an NT machine , so you 're using some external machine&#10;Speaker: PhD G&#10;Content" />
    <node id=" have with it is exactly the same reason why you thought it 'd be a good thing to do . Um , I {disfmarker} I think that {disfmarker} Let 's fall back to that . But I think the first responsibility is sort of to figure out if there 's something {pause} that , uh , an {disfmarker} an additional {disfmarker} Uh , that 's a good thing you {disfmarker} remove the mike . Go ahead , good . Uh , uh . What an additional clever person could help with when we 're really in a crunch for time . Right ? Cuz Dave 's gonna be around for a long time ,&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: right ? He 's {disfmarker} he 's gonna be here for years . And so , um ,&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: over years , if he 's {disfmarker} if he 's interested in , you know , voiced - unvoiced - silence , he could do a lot . But if there {d" />
    <node id="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is because the model will have limited exposure and experience in handling diverse types of unseen noise, which could lead to suboptimal performance when dealing with real-world scenarios where various kinds of noises are present.&#10;   &#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus. By training with seen noise conditions, the model can become more robust in handling those specific types of noises, improving its overall performance in noisy environments that it has encountered before.&#10;   &#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments. While this doesn't directly address the issue of not being able to train on unseen noise conditions, combination testing can provide valuable insights into how the model performs under various noisy scenarios and potentially lead to improvements in the model's ability to generalize to new, unseen noise conditions." />
    <node id="Based on the information provided in the transcripts, specifically the discussion about HTK training taking around six hours or more, and considering that there will be some computational overhead for computing the KL transformation when testing combinations of built-in training programs like PLP, MSG, and JRA, it can be estimated that conducting experiments on these combinations might take a significant amount of time. However, the exact duration is not explicitly mentioned in the transcripts.&#10;&#10;To provide an accurate answer, additional information about the computational resources available for running these experiments would be helpful. Factors such as the number of possible combinations, the complexity of each combination, and the computational power of the hardware being used would all impact the total time required to complete the experiments.&#10;&#10;In summary, while it is not possible to provide a precise estimate based on the given transcripts, conducting experiments on combinations of built-in training programs such as PLP, MSG, and JRA is likely to take a considerable amount of time, possibly extending beyond a few days, considering the estimated six hours or more for each HTK training session and additional computational overhead." />
    <node id=" a training ? I mean , it 's not totally crazy t I mean , these are {disfmarker} a lot of these are built - in things and we know {disfmarker} we have programs that compute PLP , we have MSG , we have JRA you know , a lot of these things will just kind of happen , won't take uh a huge amount of development , it 's just trying it out . So , we actually can do quite a few experiments .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: But how {disfmarker} how long does it take , do we think , for one of these {pause} {comment} trainings ?&#10;Speaker: Grad C&#10;Content: That 's a good question .&#10;Speaker: PhD A&#10;Content: What about combinations of things ?&#10;Speaker: Professor F&#10;Content: Oh yeah , that 's right . I mean , cuz , so , for instance , I think the major advantage of MSG {disfmarker}&#10;Speaker: Grad C&#10;Content: Oh !&#10;Speaker: Professor F&#10;Content: Yeah ,&#10;Speaker: Grad C" />
    <node id="The &quot;unusual number&quot; the speakers refer to is the larger number of phone symbols used in their recognition system, which is 120 compared to the typical 39 in other systems. This larger set allows for a more nuanced identification of sounds, as it includes additional phonetic context beyond the basic sound itself. The use of this larger set enables the system to bring in context and better distinguish between similar sounds based on their surrounding phonetic environment.&#10;&#10;This approach aims to improve accuracy by considering how individual phones are influenced by neighboring sounds within words or phrases, which can help resolve ambiguities that may arise from using a smaller phone set. However, this increased level of detail also presents challenges in accurately training and fine-tuning the recognition system to effectively use this larger phone set for speech processing tasks." />
    <node id="The &quot;slash-scratch&quot; directory in a Unix workstation is intended for temporary storage of files. It is not backed up and may refer to an internal disk of the machine. Therefore, it's not suitable for storing important files long-term because if the machine is replaced or disappears, any files stored in &quot;slash-scratch&quot; will be lost. Additionally, if a user stores large amounts of data in &quot;slash-scratch,&quot; it could potentially impact network performance as the networks may still be using ten megabit lines while machines are getting faster and faster. To check space availability in &quot;slash-scratch,&quot; users can use the command &quot;DF -K&quot; when they're on that machine." />
    <node id=" have a {disfmarker} a {disfmarker} a Unix workstation , and they attach an external disk , {comment} it 'll be called &quot; slash - X - something &quot; uh , if it 's not backed up and it 'll be &quot; slash - D - something &quot; if it is backed up . And if it 's inside the machine on the desk , it 's called &quot; slash - scratch &quot; . But the problem is , if you ever get a new machine , they take your machine away . It 's easy to unhook the external disks , put them back on the new machine , but then your slash - scratch is gone . So , you don't wanna put anything in slash - scratch that you wanna keep around for a long period of time . But if it 's a copy of , say , some data that 's on a server , you can put it on slash - scratch because , um , first of all it 's not backed up , and second it doesn't matter if that machine disappears and you get a new machine because you just recopy it to slash - scratch . So tha that 's why I was saying you could check slash - scratch on those {disfmarker} on {" />
    <node id=": PhD A&#10;Content: Uh , and if there is then , uh {disfmarker}&#10;Speaker: Professor F&#10;Content: But wasn't it , uh {disfmarker} I think Dave was saying that he preferred that people didn't put stuff in slash - scratch . It 's more putting in d s XA or XB or ,&#10;Speaker: PhD A&#10;Content: Well , there 's different {disfmarker} there , um , there 's {disfmarker}&#10;Speaker: Professor F&#10;Content: right ?&#10;Speaker: PhD A&#10;Content: Right . So there 's the slash - X - whatever disks , and then there 's slash - scratch . And both of those two kinds are not backed up . And if it 's called &quot; slash - scratch &quot; , it means it 's probably an internal disk to the machine . Um . And so that 's the kind of thing where , like if {disfmarker} um , OK , if you don't have an NT , but you have a {disfmarker} a {disfmarker} a Unix workstation , and they attach an external disk , {comment} it '" />
    <node id="&#10;Speaker: PhD A&#10;Content: Well , I 'll check on that .&#10;Speaker: Professor F&#10;Content: Yeah . Yeah , so basically , uh , Chuck will be the one who will be sorting out what disk needs to be where , and so on , and I 'll be the one who says , &quot; OK , spend the money . &quot; So . {vocalsound} Which , I mean , n these days , uh , if you 're talking about scratch space , it doesn't increase the , uh , need for backup , and , uh , I think it 's not that big a d and the {disfmarker} the disks themselves are not that expensive . Right now it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: What you can do , when you 're on that machine , is , uh , just go to the slash - scratch directory , and do a DF minus K , and it 'll tell you if there 's space available .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh , and if there is then , uh {disfmarker}&#10;Speaker: Professor F&#10;Content: But wasn" />
    <node id="opy it to slash - scratch . So tha that 's why I was saying you could check slash - scratch on those {disfmarker} on {disfmarker} on , um , Mustard and {disfmarker} and Nutmeg to see if {disfmarker} if there 's space that you could use there .&#10;Speaker: Professor F&#10;Content: I see .&#10;Speaker: PhD A&#10;Content: You could also use slash - X - whatever disks on Mustard and Nutmeg .&#10;Speaker: PhD G&#10;Content: Yeah , yeah .&#10;Speaker: PhD A&#10;Content: Um . Yeah , and we do have {disfmarker} I mean , yeah , so {disfmarker} so you {disfmarker} yeah , it 's better to have things local if you 're gonna run over them lots of times so you don't have to go to the network .&#10;Speaker: Professor F&#10;Content: Right , so es so especially if you 're {disfmarker} right , if you 're {disfmarker} if you 're taking some piece of the training corpus , which usually resides in where Chuck is putting" />
    <node id=" e scratch space for {disfmarker} for people to work on . And I know that , uh , Stephane 's working from an NT machine , so his {disfmarker} his home directory exists somewhere else .&#10;Speaker: Professor F&#10;Content: His {disfmarker} his stuff is somewhere else , yeah . Yeah , I mean , my point I {disfmarker} I want to {disfmarker} Yeah , thanks for bring it back to that . My {disfmarker} th I want to clarify my point about that {disfmarker} that {disfmarker} that Chuck repeated in his note . Um . We 're {disfmarker} over the next year or two , we 're gonna be upgrading the networks in this place ,&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: but right now they 're still all te pretty much all ten megabit lines . And we have reached the {disfmarker} this {disfmarker} the machines are getting faster and faster . So , it actually has reached the point where it 's a significant drag on the time" />
    <node id=" new , um , drive onto Abbott , that 's an X disk , which means it 's not backed up . So , um , I 've been going through and copying data that is , you know , some kind of corpus stuff usually , that {disfmarker} that we 've got on a CD - ROM or something , onto that new disk to free up space {pause} on other disks . And , um , so far , um , I 've copied a couple of Carmen 's , um , databases over there . We haven't deleted them off of the slash - DC disk that they 're on right now in Abbott , um , uh , but we {disfmarker} I would like to go through {disfmarker} sit down with you about some of these other ones and see if we can move them onto , um , this new disk also . There 's {disfmarker} there 's a lot more space there ,&#10;Speaker: PhD G&#10;Content: Yeah , OK .&#10;Speaker: PhD A&#10;Content: and it 'll free up more space for doing the experiments and things . So , anything that {disfmarker} that you don't need backed up ," />
    <node id="Professor F is trying to clarify that despite the upcoming upgrade of the networks in their place, which will improve the network speeds, currently they are still limited by pretty much all ten megabit lines. He mentions that this is becoming a significant drag on their work because the machines and computers they use are getting faster and faster, but the network speeds are unable to keep up with them." />
    <node id=" .&#10;Speaker: Professor F&#10;Content: So .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Uh , haven't been {disfmarker} haven't been doing much computing personally , so . Um . Yeah , so those are computational servers . So I guess the other question is what disk there i space there is there on the computational servers .&#10;Speaker: PhD A&#10;Content: Right . Yeah , I 'm not sure what 's available on {disfmarker} is it {disfmarker} you said Nutmeg and what was the other one ?&#10;Speaker: PhD G&#10;Content: Mustard .&#10;Speaker: PhD A&#10;Content: Mustard . OK .&#10;Speaker: PhD B&#10;Content: Huh .&#10;Speaker: Professor F&#10;Content: Yeah , Well , you 're the {disfmarker} you 're the disk czar now .&#10;Speaker: PhD A&#10;Content: Right , right .&#10;Speaker: Professor F&#10;Content: So&#10;Speaker: PhD A&#10;Content: Well , I 'll check on that .&#10;Speaker: Professor F&#10;Content: Yeah . Yeah , so" />
    <node id=" the phones right .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: So , if that doesn't help , there 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Although , yeah , it would be {disfmarker} make an interesting cheating experiment because we are using it in this funny way ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: where we 're converting it into features .&#10;Speaker: PhD A&#10;Content: And then you also don't know what error they 've got on the HTK side . You know ? It sort of gives you your {disfmarker} the best you could hope for , kind of .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Mmm . Mmm , I see .&#10;Speaker: PhD B&#10;Content: The soft training of the nets still requires the vector to sum to one , though , right ?&#10;Speaker: Grad C&#10;Content: To sum up to one .&#10;Speaker: PhD B&#10;Content: So you can't" />
    <node id="The transcript mentions that training a neural network on multiple languages is probably done using a single network, as suggested by PhD G's statement &quot;probably one net.&quot; The specifics of this method are not detailed in the conversation; however, it can be inferred that this would involve combining data from different languages during the training process.&#10;&#10;The speakers also discuss combining neural networks trained on individual languages with a more generic database, as stated by PhD G: &quot;training a net on the target languages but with a general database.&quot; It is important to note that the conversation does not explicitly confirm whether this method has been implemented or not.&#10;&#10;In summary, while the discussion does imply training a neural network using multiple languages in one network and combining individually trained networks, it does not provide explicit details about these methods." />
    <node id=" , so there 's plenty of stuff around . OK , so anyway , th the basic plan is to , uh , test this cube . Yes .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: To fill in the cube .&#10;Speaker: Professor F&#10;Content: To fill i fill it in , yeah . OK .&#10;Speaker: PhD G&#10;Content: Yeah , and perhaps , um {disfmarker} {pause} We were thinking that perhaps the cross - language issue is not , uh , so big of a issue . Well , w w we {disfmarker} perhaps we should not focus too much on that cross - language stuff . I mean , uh , training {disfmarker} training a net on a language and testing a for another language .&#10;Speaker: Professor F&#10;Content: Uh - huh . But that 's {disfmarker}&#10;Speaker: PhD G&#10;Content: Mmm . Perhaps the most important is to have neural networks trained on the target languages . But , uh , with a general database {disfmarker} general databases . u So that th Well , the {disfmarker} the guy who has to" />
    <node id="} and , uh {disfmarker} So , what it {disfmarker} it blows out is the number of uh testings . And , you know {disfmarker} and the number of times you do that last part . But that last part , I think , is so {disfmarker} has gotta be pretty quick , so . Uh . Right ? I mean , it 's just running the data through {disfmarker}&#10;Speaker: Grad C&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: But wh what about a net that 's trained on multiple languages , though ?&#10;Speaker: Professor F&#10;Content: Well , you gotta do the KL transformation ,&#10;Speaker: PhD G&#10;Content: Eight {disfmarker} y&#10;Speaker: Professor F&#10;Content: but {disfmarker}&#10;Speaker: PhD A&#10;Content: Is that just separate nets for each language then combined , or is that actually one net trained on ?&#10;Speaker: PhD E&#10;Content: Necessary to put in .&#10;Speaker: Professor F&#10;Content: Good question .&#10;Speaker: PhD G&#10;Content: Uh , probably one net ." />
    <node id="1. The discussion was focused on the observation that most confusions in phone classification occur within the same broad classes, such as obstruents being confused with other obstruents. This led to the idea of examining articulatory types as a potential solution.&#10;2. The participants also discussed the possibility of using a multi-valued or multi-dimensional representation for articulatory features, allowing for the modeling of various combinations of articulatory gestures and a more nuanced understanding of speech sounds.&#10;3. Articulatory features were considered as targets for the neural network, which describe the position and movement of speech organs when producing sounds, typically used in speech synthesis and analysis.&#10;4. The challenge of addressing differences in phone sets between different databases and languages was mentioned, with a specific example of mapping from 61 phonemes in TIMIT to 56 phonemes in ICSI given.&#10;&#10;In summary, the discussion centered around using articulatory type information for classifying phones, considering multiple features being active at once, and addressing challenges posed by phone set differences between databases and languages." />
    <node id="The benefits of having multiple SPERT boards for running parallel jobs include:&#10;&#10;1. Increased processing speed and efficiency: As discussed in the meeting, the SPERT board is faster than other alternatives. Having multiple SPERT boards allows users to process and run tasks simultaneously, leading to time savings and increased productivity.&#10;2. Resource allocation: When running out of capacity on one SPERT board, having additional boards enables users to offload tasks and utilize their computer for other activities. This flexibility in resource allocation ensures optimal use of available hardware.&#10;3. Job management: Multiple SPERT boards allow for the setup of multiple jobs, enabling better organization and management of tasks. Users can run different jobs on separate boards and have several jobs running on various computers simultaneously, increasing overall throughput.&#10;4. Parallel processing: Running parallel jobs on multiple SPERT boards is particularly useful in machine learning applications where large data sets require significant processing power. This allows researchers to process massive amounts of data more efficiently than relying on a single board or computer.&#10;5. Improved collaboration: By dedicating specific sections within the cube for people to work on, as suggested during the discussion, multiple SPERT boards can foster better collaboration among team members working on different aspects of a project. This facilitates the division of labor and promotes a more efficient workflow." />
    <node id="isfmarker} or Dan did some testing and they found that the SPERT board 's still {disfmarker} still faster .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: And the benefits is that , you know , you run out of SPERT and then you can do other things on your {disfmarker} your computer ,&#10;Speaker: Professor F&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: and you don't {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah . So you could be {disfmarker} we have quite a few SPERT boards . You could set up , uh , you know , ten different jobs , or something , to run on SPERT {disfmarker} different SPERT boards and {disfmarker} and have ten other jobs running on different computers . So , it 's got to take that sort of thing , or {disfmarker} or we 're not going to get through any significant number of these .&#10;Speaker: Grad C&#10;" />
    <node id=" board .&#10;Speaker: PhD E&#10;Content: Yes .&#10;Speaker: Professor F&#10;Content: OK , again , we do have a bunch of SPERT boards .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: And I think there {disfmarker} there {disfmarker} there 's {disfmarker} I think you folks are probably go the ones using them right now .&#10;Speaker: PhD A&#10;Content: Is it faster to do it on the SPERT , or {disfmarker} ?&#10;Speaker: Professor F&#10;Content: Uh , don't know .&#10;Speaker: Grad C&#10;Content: It 's {disfmarker} it 's still a little faster on the&#10;Speaker: Professor F&#10;Content: Used to be .&#10;Speaker: PhD A&#10;Content: Is it ?&#10;Speaker: Grad C&#10;Content: Yeah , yeah . Ad - Adam {disfmarker} Adam did some testing . Or either Adam or {disfmarker} or Dan did some testing and they found that the SPERT board 's still {disfmarker} still faster .&#10;Speaker" />
    <node id="er} now you 've got me sort of intrigued . What {disfmarker} there 's {disfmarker} Can you describe what {disfmarker} what 's on the cube ?&#10;Speaker: Grad C&#10;Content: Yeah , w I th I think that 's a good idea&#10;Speaker: Professor F&#10;Content: I mean {disfmarker}&#10;Speaker: Grad C&#10;Content: to {disfmarker} to talk about the whole cube&#10;Speaker: Professor F&#10;Content: Yeah , yeah .&#10;Speaker: PhD E&#10;Content: Yeah . Yeah .&#10;Speaker: Grad C&#10;Content: and maybe we could sections in the cube for people to work on .&#10;Speaker: Professor F&#10;Content: Yeah . Yeah .&#10;Speaker: Grad C&#10;Content: Um , OK . Uh , do you wanna do it ?&#10;Speaker: Professor F&#10;Content: OK , so even {disfmarker} even though the meeting recorder doesn't {disfmarker} doesn't , uh {disfmarker} and since you 're not running a video camera we won't get this , but if you use a board it 'll" />
    <node id=" this , a choice for this , and a choice for that ,&#10;Speaker: Grad C&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor F&#10;Content: right ? And you 're going through them all . That {disfmarker} that 's what I meant .&#10;Speaker: Grad C&#10;Content: Right , right .&#10;Speaker: Professor F&#10;Content: And , uh , the thing is that once you get a better handle on how much you can realistically do , uh , um , {vocalsound} concurrently on different machines , different SPERTs , and so forth , uh , and you see how long it takes on what machine and so forth , you can stand back from it and say , &quot; OK , if we look at all these combinations we 're talking about , and combinations of combinations , and so forth , &quot; you 'll probably find you can't do it all .&#10;Speaker: Grad C&#10;Content: Mm - hmm . OK .&#10;Speaker: Professor F&#10;Content: OK , so then at that point , uh , we should sort out which ones do we throw away .&#10;Speaker: Grad C&#10;Content: Mm - hmm ." />
    <node id="1. Spain (Spanish accent)&#10;2. France (French accent, specifically from Paris)&#10;3. Texas, USA (TIMIT corpus is from Texas)&#10;4. Finland (Finnish language)&#10;5. Italy (Italian language)&#10;6. Mexican Spain (possibly a mixed or regional accent)&#10;7. Switzerland (Swiss German or French accent was mentioned as a possibility)" />
    <node id=" Spanish from Spain .&#10;Speaker: Professor F&#10;Content: Yeah , OK .&#10;Speaker: Grad C&#10;Content: From Spain .&#10;Speaker: Professor F&#10;Content: Alright . Spanish from Spain . Yeah , we 're really covered there now . OK .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: And the French from France .&#10;Speaker: PhD G&#10;Content: Yeah , the {disfmarker} No , the French is f yeah , from , uh , Paris ,&#10;Speaker: Grad C&#10;Content: Oh , from Paris , OK .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: And TIMIT 's from {pause} lots of different places .&#10;Speaker: PhD G&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: From TI . From {disfmarker} i It 's from Texas . So may maybe it 's {disfmarker}&#10;Speaker: PhD B&#10;Content: From the deep South .&#10;Speaker: Professor F&#10;Content: So - s so it 's not really from the US either .&#10;Speaker: Grad C" />
    <node id=" ?&#10;Speaker: Professor F&#10;Content: The newer one .&#10;Speaker: PhD G&#10;Content: So English , uh , Finnish and Italian are Aurora .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: And Spanish and French is something that we can use in addition to Aurora . Uh , well .&#10;Speaker: Professor F&#10;Content: Yeah , so Carmen brought the Spanish , and Stephane brought the French .&#10;Speaker: Grad C&#10;Content: OK . And , um , oh yeah , and {disfmarker}&#10;Speaker: Professor F&#10;Content: Is it French French or Belgian French ? There 's a {disfmarker}&#10;Speaker: PhD G&#10;Content: It 's , uh , French French .&#10;Speaker: Grad C&#10;Content: French French .&#10;Speaker: PhD E&#10;Content: Like Mexican Spain and Spain .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Or Swiss .&#10;Speaker: PhD E&#10;Content: I think that is more important ,&#10;Speaker: PhD B&#10;Content: Swiss - German .&#10;Speaker: PhD E&#10;Content: Mexican" />
    <node id="ound} from the various languages . Um , English Spanish um , French What else do we have ?&#10;Speaker: PhD G&#10;Content: And the {pause} Finnish .&#10;Speaker: Grad C&#10;Content: Finnish .&#10;Speaker: PhD A&#10;Content: Where did th where did that come from ?&#10;Speaker: PhD E&#10;Content: And Italian .&#10;Speaker: PhD A&#10;Content: Digits ?&#10;Speaker: PhD E&#10;Content: Uh , no , Italian no . Italian no .&#10;Speaker: PhD A&#10;Content: Oh .&#10;Speaker: Grad C&#10;Content: Oh . Italian .&#10;Speaker: PhD E&#10;Content: I Italian yes . Italian ?&#10;Speaker: Professor F&#10;Content: Italian .&#10;Speaker: PhD A&#10;Content: Is that {disfmarker} Was that distributed with Aurora , or {disfmarker} ?&#10;Speaker: Grad C&#10;Content: One L or two L 's ?&#10;Speaker: PhD A&#10;Content: Where did that {disfmarker} ?&#10;Speaker: Professor F&#10;Content: The newer one .&#10;Speaker: PhD G&#10;Content: So English , uh , Finnish and Italian are" />
    <node id=": Ah , but which Dan ?&#10;Speaker: Grad C&#10;Content: Uh , Ellis . Right ?&#10;Speaker: Professor F&#10;Content: OK . OK .&#10;Speaker: Grad C&#10;Content: Yeah . So .&#10;Speaker: PhD A&#10;Content: I was just wondering because that test you 're t&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: I {disfmarker} I think you 're doing this test because you want to determine whether or not , uh , having s general speech performs as well as having specific {pause} speech .&#10;Speaker: Grad C&#10;Content: That 's right .&#10;Speaker: Professor F&#10;Content: Well , especially when you go over the different languages again , because you 'd {disfmarker} the different languages have different words for the different digits ,&#10;Speaker: PhD A&#10;Content: Mm - hmm . And I was {disfmarker}&#10;Speaker: Professor F&#10;Content: so it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: yeah , so I was just wondering if the fact that TIMIT {disfmark" />
    <node id="1. When evaluating a system's ability to handle a specific language like Portuguese within a multi-language context, it is important to compare the performance of the system with and without including the target language in the training set. This will help determine if the system can adequately process the specific language.&#10;   &#10;2. It may also be beneficial to train separate neural networks for each language and then combine them during the evaluation process, ensuring that the system has been exposed to the phonetic contexts and characteristics of the target language.&#10;&#10;3. Using a multi-valued or multi-dimensional representation for articulatory features can provide a more nuanced understanding of speech sounds, allowing the system to better model various combinations of articulatory gestures and reduce confusions between broad classes like obstruents.&#10;&#10;4. Examining articulatory types as targets for the neural network could help improve phone classification, especially when dealing with differences in phone sets between databases and languages. This might be particularly useful when addressing challenges posed by the specific phoneme set of Portuguese within a multi-language context.&#10;&#10;5. Consider using existing English, Spanish, and French databases as a starting point for the multi-lingual part of the system, as these resources are already available and can provide initial support for processing the target language (Portuguese) in a multi-language context." />
    <node id=" PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Uh , now , what do you do , uh , when somebody has Portuguese ? &quot; you know ? Um , and {disfmarker} Uh , however , you aren't {disfmarker} it isn't actually a constraint in this evaluation . So I would say if it looks like there 's a big difference to put it in , then we 'd make note of it , and then we probably put in the other , because we have so many other problems in trying to get things to work well here that {disfmarker} that , you know , it 's not so bad as long as we {disfmarker} we note it and say , &quot; Look , we did do this &quot; .&#10;Speaker: PhD G&#10;Content: Mmm ?&#10;Speaker: PhD A&#10;Content: And so , ideally , what you 'd wanna do is you 'd wanna run it with and without the target language and the training set for a wide range of languages .&#10;Speaker: Professor F&#10;Content: Uh . Yeah .&#10;Speaker: PhD G&#10;Content: Yeah , perhaps . Yeah .&#10;Speaker: PhD A&#10;" />
    <node id=" uh , a range of languages and {disfmarker} which can include the test {disfmarker} the test @ @ the target language ,&#10;Speaker: Grad C&#10;Content: Test on an unseen .&#10;Speaker: PhD G&#10;Content: or {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah , so , um , there 's {disfmarker} there 's , uh {disfmarker} This is complex . So , ultimately , uh , as I was saying , I think it doesn't fit within their image that you switch nets based on language . Now , can you include , uh , the {disfmarker} the target language ?&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Um , from a purist 's standpoint it 'd be nice not to because then you can say when {disfmarker} because surely someone is going to say at some point , &quot; OK , so you put in the German and the Finnish .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Uh , now , what do you do , uh , when somebody has Portuguese" />
    <node id=" question is how important is it to {disfmarker} for us to get multiple languages uh , in there .&#10;Speaker: PhD G&#10;Content: Yeah , but {disfmarker} Mm - hmm . {vocalsound} Um . Yeah . Well , actually , for the moment if we w do not want to use these phone databases , we {disfmarker} we already have uh {disfmarker} English , Spanish and French uh , with microphone speech . &#10;Speaker: Professor F&#10;Content: Mm - hmm . Yeah .&#10;Speaker: PhD G&#10;Content: So .&#10;Speaker: Professor F&#10;Content: So that 's what you 're thinking of using is sort of the multi the equivalent of the multiple ?&#10;Speaker: PhD G&#10;Content: Well . Yeah , for the multilingual part we were thinking of using these three databases .&#10;Speaker: Professor F&#10;Content: And for the difference in phonetic context {pause} that you {disfmarker} ? Provide that .&#10;Speaker: PhD G&#10;Content: Well , this {disfmarker} Uh , actually , these three databases are um generic databases .&#10;Speaker" />
    <node id="The transcript does not provide specific information about the available disk space on the Nutmeg and Mustard computational servers. However, it is mentioned that they are computational servers and there are discussions around checking for space in slash-scratch and other disks (slash-X-whatever) on Mustard and Nutmeg. It is recommended to use local storage if running experiments multiple times to avoid network overhead. To get the exact available disk space, it is advised to manually check by logging onto those servers and using commands like &quot;DF -K&quot; or checking the specific directories suggested in the transcripts." />
    <node id="disfmarker} i so , you 're on an NT machine , so you 're using some external machine&#10;Speaker: PhD G&#10;Content: Yeah , it , uh {disfmarker} Well , to {disfmarker} It 's Nutmeg and Mustard , I think , &#10;Speaker: Professor F&#10;Content: Do you know these yet ?&#10;Speaker: PhD G&#10;Content: I don't know what kind .&#10;Speaker: PhD A&#10;Content: Nuh - uh .&#10;Speaker: Professor F&#10;Content: Yeah , OK . Uh , are these {disfmarker} are these , uh , computational servers , or something ? I 'm {disfmarker} I 've been kind of out of it .&#10;Speaker: PhD G&#10;Content: Yeah , I think , yeah . I think so .&#10;Speaker: Professor F&#10;Content: Unfortunately , these days my idea of running comput of computa doing computation is running a spread sheet .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: So .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content" />
    <node id="The speakers discussed using the same normalization scheme as OGI is using for all their experiments, as this would add another dimension to their analysis. They acknowledged having enough dimensions already and thus considered adopting the normalization method from OGI to avoid increasing complexity unnecessarily." />
    <node id=" it 'll free up more space for doing the experiments and things . So , anything that {disfmarker} that you don't need backed up , we can put on this new disk . Um , but if it 's experiments and you 're creating files and things that you 're gonna need , you probably wanna have those on a disk that 's backed up , just in case something {comment} goes wrong . So . Um So far I 've {disfmarker} I 've copied a couple of things , but I haven't deleted anything off of the old disk to make room yet . Um , and I haven't looked at the {disfmarker} any of the Aurora stuff , except for the Spanish . So I {disfmarker} I guess I 'll need to get together with you and see what data we can move onto the new disk .&#10;Speaker: PhD G&#10;Content: Yeah , OK .&#10;Speaker: Professor F&#10;Content: Um , yeah , I {disfmarker} I just {disfmarker} an another question occurred to me is {disfmarker} is what were you folks planning to do about normalization ?&#10;Speaker: PhD G&#10;" />
    <node id="Based on the transcript, there is no direct mention of Professor F's decision to schedule a 7:20 AM plane departure. However, it can be inferred that they plan to drive the group to the airport and want everyone to meet at 6 AM to ensure they arrive on time for this departure.&#10;&#10;As for accomplishing a same-day return visit, Professor F suggests driving the group to the airport and offers to pick up those who are not familiar with the area. This implies that they have considered the logistics of getting the group to the airport in time for their flight and making arrangements to facilitate this process. Additionally, since Professor F mentions that network upgrades are making it difficult to work due to slower network speeds compared to their increasingly faster computers and machines, it can be inferred that the purpose of the trip involves tasks that cannot be completed remotely or require access to specific resources at their destination." />
    <node id="Based on the discussion, it is suggested that using the TIMIT dataset as a source for training a tandem system across languages could be a better choice due to its broadness and careful hand-correction. However, given the limited time and resources, the speakers did not explicitly conclude that the TIMIT dataset's broadness or careful hand-correction is the sole reason for its potential superiority. Further investigation would be required to make a definitive conclusion." />
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target=": I don't think so . Well , they {disfmarker} they {disfmarker} they 're going actually the {disfmarker} the other way , defining uh , phoneme clusters , apparently . Well .&#10;Speaker: Professor F&#10;Content: Aha . That 's right . Uh , and that 's an interesting {pause} way to go too .&#10;Speaker: PhD A&#10;Content: So they just throw the speech from all different languages together , then cluster it into sixty or fifty or whatever clusters ?&#10;Speaker: PhD G&#10;Content: I think they 've not done it , uh , doing , uh , multiple language yet , but what they did is to training , uh , English nets with all the phonemes , and then training it in English nets with , uh , kind of seventeen , I think it was {disfmarker} seventeen , uh , broad classes .&#10;Speaker: PhD A&#10;Content: Automatically derived {disfmarker} Mm - hmm . Automatically derived broad classes , or {disfmarker} ?&#10;Speaker: PhD G&#10;Content: Yeah . Yeah , I think so .&#10;Speaker: PhD A&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target=" with neural networks trained on , {vocalsound} uh , data from the same language , if possible , with , well , using a more generic database , which is phonetically {disfmarker} phonetically balanced , and . Um .&#10;Speaker: Professor F&#10;Content: So - so we had talked {disfmarker} I guess we had talked at one point about maybe , the language ID corpus ?&#10;Speaker: PhD G&#10;Content: Yeah . So .&#10;Speaker: Professor F&#10;Content: Is that a possibility for that ?&#10;Speaker: PhD G&#10;Content: Ye - uh {disfmarker} {pause} Yeah , but , uh these corpus , w w there is a CallHome and a CallFriend also , The CallFriend is for language ind identification . Well , anyway , these corpus are all telephone speech . So , um . {vocalsound} This could be a {disfmarker} {pause} a problem for {disfmarker} Why ? Because uh , uh , the {disfmarker} the SpeechDat databases are not telephone speech . They are downsampled to eight kilohertz but {disfmarker} but they are not {voc">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="aker: PhD G&#10;Content: Yep .&#10;Speaker: Grad C&#10;Content: Out of that fifty - six .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: Grad C&#10;Content: Yeah . So , it 's {disfmarker} it 's definitely broader , yeah .&#10;Speaker: PhD G&#10;Content: But , actually , the issue of phoneti phon uh phone phoneme mappings will arise when we will do severa use several languages&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: because you {disfmarker} Well , some phonemes are not , uh , in every languages , and {disfmarker} So we plan to develop a subset of the phonemes , uh , that includes , uh , all the phonemes of our training languages ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: and use a network with kind of one hundred outputs or something like that .&#10;Speaker: Professor F&#10;Content: Mm - hmm . You mean a superset , sort of .&#10;Speaker: PhD G&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target=" mean , we have {disfmarker} {pause} i we have the {disfmarker} the trainings with our own categories . And now we 're saying , &quot; Well , how do we handle cross - language ? &quot; And one way is to come up with a superset , but they are als they 're trying coming up with clustered , and do we think there 's something wrong with that ?&#10;Speaker: PhD G&#10;Content: I think that there 's something wrong&#10;Speaker: Professor F&#10;Content: OK . What w&#10;Speaker: PhD G&#10;Content: or {disfmarker} Well , because {disfmarker} Well , for the moment we are testing on digits , and e i perhaps u using broad phoneme classes , it 's {disfmarker} it 's OK for um , uh classifying the digits , but as soon as you will have more words , well , words can differ with only a single phoneme , and {disfmarker} which could be the same , uh , class .&#10;Speaker: Professor F&#10;Content: I see .&#10;Speaker: PhD G&#10;Content: Well . So .&#10;Speaker: Professor">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target=" about the cube ?&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Oh ! Cube . Yeah .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Fill in the cube .&#10;Speaker: PhD G&#10;Content: Uh we {disfmarker} actually we want to , mmm , Uh , {vocalsound} uh , analyze three dimensions , the feature dimension , the {pause} training data dimension , and the test data dimension . Um . Well , what we want to do is first we have number for each {pause} uh task . So we have the um , TI - digit task , the Italian task , the French task {pause} and the Finnish task .&#10;Speaker: Professor F&#10;Content: Yeah ?&#10;Speaker: PhD G&#10;Content: So we have numbers with {pause} uh {disfmarker} systems {disfmarker} I mean {disfmarker} I mean neural networks trained on the task data . And then to have systems with neural networks trained on , {vocalsound} uh , data from the same language , if possible , with , well , using a more generic database ,">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target=": Grad C&#10;Content: Um , for the {disfmarker} for nets trained on digits , {comment} um , we have been using , uh , four hundred order hidden units . And , um , for the broader class nets we 're {disfmarker} we 're going to increase that because the , um , the digits nets only correspond to about twenty phonemes .&#10;Speaker: PhD B&#10;Content: Uh - huh .&#10;Speaker: Grad C&#10;Content: So .&#10;Speaker: Professor F&#10;Content: Broader class ?&#10;Speaker: Grad C&#10;Content: Um , the broader {disfmarker} broader training corpus nets like TIMIT . Um , w we 're gonna {disfmarker}&#10;Speaker: Professor F&#10;Content: Oh , it 's not actually broader class , it 's actually finer class , but you mean {disfmarker} y You mean {vocalsound} more classes .&#10;Speaker: Grad C&#10;Content: Right . Right . Yeah . More classes . Right , right . More classes .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: That 's what I mean">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="The speakers, PhD A and Professor F, discuss the difference between silence at the beginning and end of a signal, but ultimately conclude that they are both similar forms of silence. They mention that distinctions between these types of silence may have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. However, it is not explicitly pointed out that this difference has any significant impact on the mapping process. It is also noted that there was a glottal stop in the TIMIT dataset, which is a short period of silence, and this might have been relevant for the mapping as well.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="The individual, presumably Professor F, is arranging to meet everyone at 6 AM in the morning at an unspecified location, as they plan to drive them to the airport. The backup plan if they are not there on time is not explicitly mentioned in the transcript, but it can be inferred that they would likely wait for a reasonable amount of time before leaving without them or making alternative arrangements.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="The speaker sees several benefits in the current situation, despite the limitations of time and the need to address a new problem. Firstly, they recognize that there is significant computational power available across the institute, which can be leveraged to tackle this issue. They also highlight that while working on something new inevitably involves a learning and development process, in this case, they have the advantage of being able to build upon existing knowledge and data that they already possess. This includes having experience with techniques such as voiced-unvoiced-silence detection features. Therefore, instead of starting from scratch or exploring entirely unknown territory, they can focus on applying proven methods and utilizing available resources to make progress in the new problem area.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is mentioned by Professor F when discussing the analysis of test results.&#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus.&#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments.&#10;4. Computational cost: While combining noise conditions for testing doesn't require as much computation as initial training, there is still some computational overhead for computing the KL transformation. Additionally, there will be further computational costs for training and testing the HTK models with these combinations.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="The suggestion made by Professor F was to approach the task as a series of nested loops along the time axis, with a choice at each stage. This implies that instead of thinking of the process in stages, they should consider it as a single stream of data over time, with different choices to be made at various points. He also illustrates this concept by comparing it to having three nested loops with a choice for each one.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="The speakers are discussing the use of a neural network to classify phonemes using English neural networks that were trained with phonemes and seventeen broad classes. They mention the idea of creating a superset of broad phoneme classes, which could be used for cross-language handling. However, one speaker thinks there is something wrong with this approach, suggesting that using the same language for training the neural network and having a phonetically balanced database would be better.&#10;&#10;Later in the discussion, they talk about potential issues when discriminating words based on the output of the neural network. They mention that this may not provide enough information to distinguish between words due to the fact that most confusions occur within phone classes. This is because words can differ with only a single phoneme, which could be in the same broad class. The speakers note that this might have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. Additionally, they mention a glottal stop in the TIMIT dataset, which is a short period of silence, and this might also be relevant for the mapping process.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="1. The targets of the net, as discussed in this context, appear to be articulatory features. Articulatory features are a way to describe the position and movement of speech organs when producing sounds. They are typically used in speech synthesis and analysis.&#10;2. Yes, articulatory features imply that multiple features can be active at the same time in this discussion. Professor F confirms that having more than one articulatory feature active at a time is correct, indicating that they are considering a multi-valued or multi-dimensional representation for these features. This allows for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="Four hundred order hidden units are being used for nets trained on digits. The plan to increase the number of units for broader class nets, which actually means using more classes, is because these broader training corpus nets like TIMIT correspond to a larger number of phoneme classes compared to the digits nets that only correspond to about twenty phonemes.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="One major criticism against current speech recognition systems is that they throw away all the harmonicity information and try to get spectral envelopes, discarding most of the information about the phonetic identity which is contained in the harmonics. Incorporating pitch or harmonicity information could address this issue by preserving some of this lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="The goal of computing and sending features instead of actual speech in the development of new telephone standards, as discussed by the speakers, is to improve speech recognition and synthesis by accurately identifying phones (distinct units of sound) in the data. This would allow for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures. Additionally, incorporating pitch or harmonicity information could address current criticism of speech recognition systems by preserving some of the lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.&#10;&#10;One challenge in achieving this goal is the differences in phone sets between different databases and languages. In the conversation, they mention a mapping from the 61 phonemes in TIMIT to the 56 phonemes in ICSI and the differences in phone sets between the two databases. This shows that addressing the issue of phone set differences is an important step towards achieving the goal of accurately identifying phones and improving speech recognition and synthesis in new telephone standards.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is because the model will have limited exposure and experience in handling diverse types of unseen noise, which could lead to suboptimal performance when dealing with real-world scenarios where various kinds of noises are present.&#10;   &#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus. By training with seen noise conditions, the model can become more robust in handling those specific types of noises, improving its overall performance in noisy environments that it has encountered before.&#10;   &#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments. While this doesn't directly address the issue of not being able to train on unseen noise conditions, combination testing can provide valuable insights into how the model performs under various noisy scenarios and potentially lead to improvements in the model's ability to generalize to new, unseen noise conditions.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="The &quot;unusual number&quot; the speakers refer to is the larger number of phone symbols used in their recognition system, which is 120 compared to the typical 39 in other systems. This larger set allows for a more nuanced identification of sounds, as it includes additional phonetic context beyond the basic sound itself. The use of this larger set enables the system to bring in context and better distinguish between similar sounds based on their surrounding phonetic environment.&#10;&#10;This approach aims to improve accuracy by considering how individual phones are influenced by neighboring sounds within words or phrases, which can help resolve ambiguities that may arise from using a smaller phone set. However, this increased level of detail also presents challenges in accurately training and fine-tuning the recognition system to effectively use this larger phone set for speech processing tasks.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="The transcript mentions that training a neural network on multiple languages is probably done using a single network, as suggested by PhD G's statement &quot;probably one net.&quot; The specifics of this method are not detailed in the conversation; however, it can be inferred that this would involve combining data from different languages during the training process.&#10;&#10;The speakers also discuss combining neural networks trained on individual languages with a more generic database, as stated by PhD G: &quot;training a net on the target languages but with a general database.&quot; It is important to note that the conversation does not explicitly confirm whether this method has been implemented or not.&#10;&#10;In summary, while the discussion does imply training a neural network using multiple languages in one network and combining individually trained networks, it does not provide explicit details about these methods.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="1. The discussion was focused on the observation that most confusions in phone classification occur within the same broad classes, such as obstruents being confused with other obstruents. This led to the idea of examining articulatory types as a potential solution.&#10;2. The participants also discussed the possibility of using a multi-valued or multi-dimensional representation for articulatory features, allowing for the modeling of various combinations of articulatory gestures and a more nuanced understanding of speech sounds.&#10;3. Articulatory features were considered as targets for the neural network, which describe the position and movement of speech organs when producing sounds, typically used in speech synthesis and analysis.&#10;4. The challenge of addressing differences in phone sets between different databases and languages was mentioned, with a specific example of mapping from 61 phonemes in TIMIT to 56 phonemes in ICSI given.&#10;&#10;In summary, the discussion centered around using articulatory type information for classifying phones, considering multiple features being active at once, and addressing challenges posed by phone set differences between databases and languages.">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="1. Spain (Spanish accent)&#10;2. France (French accent, specifically from Paris)&#10;3. Texas, USA (TIMIT corpus is from Texas)&#10;4. Finland (Finnish language)&#10;5. Italy (Italian language)&#10;6. Mexican Spain (possibly a mixed or regional accent)&#10;7. Switzerland (Swiss German or French accent was mentioned as a possibility)">
      <data key="d0">1</data>
    </edge>
    <edge source="No, the English neural networks were trained with phonemes and seventeen broad classes, but these broad classes were not derived by automatically clustering speech in multiple languages together. Instead, they were also trained on English nets. The speaker was unsure if a method using multiple languages has been implemented yet. The idea of using a subset of phonemes that includes all the phonemes of training languages is brought up later in the discussion, but it is not mentioned that this has already been done with English neural networks." target="1. When evaluating a system's ability to handle a specific language like Portuguese within a multi-language context, it is important to compare the performance of the system with and without including the target language in the training set. This will help determine if the system can adequately process the specific language.&#10;   &#10;2. It may also be beneficial to train separate neural networks for each language and then combine them during the evaluation process, ensuring that the system has been exposed to the phonetic contexts and characteristics of the target language.&#10;&#10;3. Using a multi-valued or multi-dimensional representation for articulatory features can provide a more nuanced understanding of speech sounds, allowing the system to better model various combinations of articulatory gestures and reduce confusions between broad classes like obstruents.&#10;&#10;4. Examining articulatory types as targets for the neural network could help improve phone classification, especially when dealing with differences in phone sets between databases and languages. This might be particularly useful when addressing challenges posed by the specific phoneme set of Portuguese within a multi-language context.&#10;&#10;5. Consider using existing English, Spanish, and French databases as a starting point for the multi-lingual part of the system, as these resources are already available and can provide initial support for processing the target language (Portuguese) in a multi-language context.">
      <data key="d0">1</data>
    </edge>
    <edge source=" with neural networks trained on , {vocalsound} uh , data from the same language , if possible , with , well , using a more generic database , which is phonetically {disfmarker} phonetically balanced , and . Um .&#10;Speaker: Professor F&#10;Content: So - so we had talked {disfmarker} I guess we had talked at one point about maybe , the language ID corpus ?&#10;Speaker: PhD G&#10;Content: Yeah . So .&#10;Speaker: Professor F&#10;Content: Is that a possibility for that ?&#10;Speaker: PhD G&#10;Content: Ye - uh {disfmarker} {pause} Yeah , but , uh these corpus , w w there is a CallHome and a CallFriend also , The CallFriend is for language ind identification . Well , anyway , these corpus are all telephone speech . So , um . {vocalsound} This could be a {disfmarker} {pause} a problem for {disfmarker} Why ? Because uh , uh , the {disfmarker} the SpeechDat databases are not telephone speech . They are downsampled to eight kilohertz but {disfmarker} but they are not {voc" target="The speakers are discussing the use of a neural network to classify phonemes using English neural networks that were trained with phonemes and seventeen broad classes. They mention the idea of creating a superset of broad phoneme classes, which could be used for cross-language handling. However, one speaker thinks there is something wrong with this approach, suggesting that using the same language for training the neural network and having a phonetically balanced database would be better.&#10;&#10;Later in the discussion, they talk about potential issues when discriminating words based on the output of the neural network. They mention that this may not provide enough information to distinguish between words due to the fact that most confusions occur within phone classes. This is because words can differ with only a single phoneme, which could be in the same broad class. The speakers note that this might have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. Additionally, they mention a glottal stop in the TIMIT dataset, which is a short period of silence, and this might also be relevant for the mapping process.">
      <data key="d0">1</data>
    </edge>
    <edge source=" mean , we have {disfmarker} {pause} i we have the {disfmarker} the trainings with our own categories . And now we 're saying , &quot; Well , how do we handle cross - language ? &quot; And one way is to come up with a superset , but they are als they 're trying coming up with clustered , and do we think there 's something wrong with that ?&#10;Speaker: PhD G&#10;Content: I think that there 's something wrong&#10;Speaker: Professor F&#10;Content: OK . What w&#10;Speaker: PhD G&#10;Content: or {disfmarker} Well , because {disfmarker} Well , for the moment we are testing on digits , and e i perhaps u using broad phoneme classes , it 's {disfmarker} it 's OK for um , uh classifying the digits , but as soon as you will have more words , well , words can differ with only a single phoneme , and {disfmarker} which could be the same , uh , class .&#10;Speaker: Professor F&#10;Content: I see .&#10;Speaker: PhD G&#10;Content: Well . So .&#10;Speaker: Professor" target="The speakers are discussing the use of a neural network to classify phonemes using English neural networks that were trained with phonemes and seventeen broad classes. They mention the idea of creating a superset of broad phoneme classes, which could be used for cross-language handling. However, one speaker thinks there is something wrong with this approach, suggesting that using the same language for training the neural network and having a phonetically balanced database would be better.&#10;&#10;Later in the discussion, they talk about potential issues when discriminating words based on the output of the neural network. They mention that this may not provide enough information to distinguish between words due to the fact that most confusions occur within phone classes. This is because words can differ with only a single phoneme, which could be in the same broad class. The speakers note that this might have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. Additionally, they mention a glottal stop in the TIMIT dataset, which is a short period of silence, and this might also be relevant for the mapping process.">
      <data key="d0">1</data>
    </edge>
    <edge source=" mean , we have {disfmarker} {pause} i we have the {disfmarker} the trainings with our own categories . And now we 're saying , &quot; Well , how do we handle cross - language ? &quot; And one way is to come up with a superset , but they are als they 're trying coming up with clustered , and do we think there 's something wrong with that ?&#10;Speaker: PhD G&#10;Content: I think that there 's something wrong&#10;Speaker: Professor F&#10;Content: OK . What w&#10;Speaker: PhD G&#10;Content: or {disfmarker} Well , because {disfmarker} Well , for the moment we are testing on digits , and e i perhaps u using broad phoneme classes , it 's {disfmarker} it 's OK for um , uh classifying the digits , but as soon as you will have more words , well , words can differ with only a single phoneme , and {disfmarker} which could be the same , uh , class .&#10;Speaker: Professor F&#10;Content: I see .&#10;Speaker: PhD G&#10;Content: Well . So .&#10;Speaker: Professor" target="Four hundred order hidden units are being used for nets trained on digits. The plan to increase the number of units for broader class nets, which actually means using more classes, is because these broader training corpus nets like TIMIT correspond to a larger number of phoneme classes compared to the digits nets that only correspond to about twenty phonemes.">
      <data key="d0">1</data>
    </edge>
    <edge source=" about the cube ?&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Oh ! Cube . Yeah .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Fill in the cube .&#10;Speaker: PhD G&#10;Content: Uh we {disfmarker} actually we want to , mmm , Uh , {vocalsound} uh , analyze three dimensions , the feature dimension , the {pause} training data dimension , and the test data dimension . Um . Well , what we want to do is first we have number for each {pause} uh task . So we have the um , TI - digit task , the Italian task , the French task {pause} and the Finnish task .&#10;Speaker: Professor F&#10;Content: Yeah ?&#10;Speaker: PhD G&#10;Content: So we have numbers with {pause} uh {disfmarker} systems {disfmarker} I mean {disfmarker} I mean neural networks trained on the task data . And then to have systems with neural networks trained on , {vocalsound} uh , data from the same language , if possible , with , well , using a more generic database ," target="The suggestion made by Professor F was to approach the task as a series of nested loops along the time axis, with a choice at each stage. This implies that instead of thinking of the process in stages, they should consider it as a single stream of data over time, with different choices to be made at various points. He also illustrates this concept by comparing it to having three nested loops with a choice for each one.">
      <data key="d0">1</data>
    </edge>
    <edge source=" about the cube ?&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Oh ! Cube . Yeah .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Fill in the cube .&#10;Speaker: PhD G&#10;Content: Uh we {disfmarker} actually we want to , mmm , Uh , {vocalsound} uh , analyze three dimensions , the feature dimension , the {pause} training data dimension , and the test data dimension . Um . Well , what we want to do is first we have number for each {pause} uh task . So we have the um , TI - digit task , the Italian task , the French task {pause} and the Finnish task .&#10;Speaker: Professor F&#10;Content: Yeah ?&#10;Speaker: PhD G&#10;Content: So we have numbers with {pause} uh {disfmarker} systems {disfmarker} I mean {disfmarker} I mean neural networks trained on the task data . And then to have systems with neural networks trained on , {vocalsound} uh , data from the same language , if possible , with , well , using a more generic database ," target="The transcript mentions that training a neural network on multiple languages is probably done using a single network, as suggested by PhD G's statement &quot;probably one net.&quot; The specifics of this method are not detailed in the conversation; however, it can be inferred that this would involve combining data from different languages during the training process.&#10;&#10;The speakers also discuss combining neural networks trained on individual languages with a more generic database, as stated by PhD G: &quot;training a net on the target languages but with a general database.&quot; It is important to note that the conversation does not explicitly confirm whether this method has been implemented or not.&#10;&#10;In summary, while the discussion does imply training a neural network using multiple languages in one network and combining individually trained networks, it does not provide explicit details about these methods.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Grad C&#10;Content: Um , for the {disfmarker} for nets trained on digits , {comment} um , we have been using , uh , four hundred order hidden units . And , um , for the broader class nets we 're {disfmarker} we 're going to increase that because the , um , the digits nets only correspond to about twenty phonemes .&#10;Speaker: PhD B&#10;Content: Uh - huh .&#10;Speaker: Grad C&#10;Content: So .&#10;Speaker: Professor F&#10;Content: Broader class ?&#10;Speaker: Grad C&#10;Content: Um , the broader {disfmarker} broader training corpus nets like TIMIT . Um , w we 're gonna {disfmarker}&#10;Speaker: Professor F&#10;Content: Oh , it 's not actually broader class , it 's actually finer class , but you mean {disfmarker} y You mean {vocalsound} more classes .&#10;Speaker: Grad C&#10;Content: Right . Right . Yeah . More classes . Right , right . More classes .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: That 's what I mean" target="Four hundred order hidden units are being used for nets trained on digits. The plan to increase the number of units for broader class nets, which actually means using more classes, is because these broader training corpus nets like TIMIT correspond to a larger number of phoneme classes compared to the digits nets that only correspond to about twenty phonemes.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, PhD A and Professor F, discuss the difference between silence at the beginning and end of a signal, but ultimately conclude that they are both similar forms of silence. They mention that distinctions between these types of silence may have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. However, it is not explicitly pointed out that this difference has any significant impact on the mapping process. It is also noted that there was a glottal stop in the TIMIT dataset, which is a short period of silence, and this might have been relevant for the mapping as well." target=" .&#10;Speaker: PhD A&#10;Content: Yeah , and I think some of them , they were making distinctions between silence at the end and silence at the beginning , when really they 're {pause} both silence .&#10;Speaker: PhD B&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: I th I think it was things like that that got it mapped down to fifty - six .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: Yeah , especially in a system like ours , which is a discriminative system . You know , you 're really asking this net to learn .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: It 's {disfmarker} it 's kind of hard .&#10;Speaker: PhD A&#10;Content: There 's not much difference , really . And {pause} the ones that are gone , I think are {disfmarker} I think there was {disfmarker} they also in TIMIT had like a glottal stop , which was basically a short period of silence ,&#10;Speaker: PhD B">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, PhD A and Professor F, discuss the difference between silence at the beginning and end of a signal, but ultimately conclude that they are both similar forms of silence. They mention that distinctions between these types of silence may have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. However, it is not explicitly pointed out that this difference has any significant impact on the mapping process. It is also noted that there was a glottal stop in the TIMIT dataset, which is a short period of silence, and this might have been relevant for the mapping as well." target=" A&#10;Content: Yeah . Right .&#10;Speaker: Professor F&#10;Content: then it 's probably {disfmarker}&#10;Speaker: PhD A&#10;Content: What about the differences in the phone sets ?&#10;Speaker: Grad C&#10;Content: Uh , between languages ?&#10;Speaker: PhD A&#10;Content: No , between TIMIT and the {disfmarker} the digits .&#10;Speaker: Grad C&#10;Content: Oh , um , right . Well , there 's a mapping from the sixty - one phonemes in TIMIT to {disfmarker} to fifty - six , the ICSI fifty - six .&#10;Speaker: PhD E&#10;Content: Sixty - one .&#10;Speaker: PhD A&#10;Content: Oh , OK . I see .&#10;Speaker: Grad C&#10;Content: And then the digits phonemes , um , there 's about twenty twenty - two or twenty - four of them ? Is that right ?&#10;Speaker: PhD A&#10;Content: Out of that fifty - six ?&#10;Speaker: PhD G&#10;Content: Yep .&#10;Speaker: Grad C&#10;Content: Out of that fifty - six .&#10;Speaker: PhD A&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, PhD A and Professor F, discuss the difference between silence at the beginning and end of a signal, but ultimately conclude that they are both similar forms of silence. They mention that distinctions between these types of silence may have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. However, it is not explicitly pointed out that this difference has any significant impact on the mapping process. It is also noted that there was a glottal stop in the TIMIT dataset, which is a short period of silence, and this might have been relevant for the mapping as well." target=" , it needs {disfmarker} we must r h do a lot of work {vocalsound} because we need to generate new tran transcription for the database that we have .&#10;Speaker: Professor F&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD B&#10;Content: Other than the language , is there a reason not to use the TIMIT phone set ? Cuz it 's larger ? As opposed to the ICSI {pause} phone set ?&#10;Speaker: Grad C&#10;Content: Oh , you mean why map the sixty - one to the fifty - six ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: I don't know . I have {disfmarker}&#10;Speaker: Professor F&#10;Content: Um , I forget if that happened starting with you , or was it {disfmarker} o or if it was Eric , afterwards who did that . But I think , basically , there were several of the phones that were just hardly ever there .&#10;Speaker: PhD A&#10;Content: Yeah , and I think some of them , they were making distinctions between silence at the end and silence at the">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, PhD A and Professor F, discuss the difference between silence at the beginning and end of a signal, but ultimately conclude that they are both similar forms of silence. They mention that distinctions between these types of silence may have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. However, it is not explicitly pointed out that this difference has any significant impact on the mapping process. It is also noted that there was a glottal stop in the TIMIT dataset, which is a short period of silence, and this might have been relevant for the mapping as well." target="isfmarker} they also in TIMIT had like a glottal stop , which was basically a short period of silence ,&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: and so .&#10;Speaker: PhD B&#10;Content: Well , we have that now , too , right ?&#10;Speaker: PhD A&#10;Content: I don't know .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So .&#10;Speaker: Professor F&#10;Content: i It 's actually pretty common that a lot of the recognition systems people use have things like {disfmarker} like , say thirty - nine , phone symbols , right ? Uh , and then they get the variety by {disfmarker} by bringing in the context , the phonetic context . Uh . So we actually have an unusually large number in {disfmarker} in what we tend to use here . Um . So , a a actually {disfmarker} maybe {disfmarker} now you 've got me sort of intrigued . What {disfmarker} there 's {disfmarker} Can you describe what">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, PhD A and Professor F, discuss the difference between silence at the beginning and end of a signal, but ultimately conclude that they are both similar forms of silence. They mention that distinctions between these types of silence may have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. However, it is not explicitly pointed out that this difference has any significant impact on the mapping process. It is also noted that there was a glottal stop in the TIMIT dataset, which is a short period of silence, and this might have been relevant for the mapping as well." target=" differences between these different databases . I mean some of this stuff 's recorded in the car , and some of it 's {disfmarker} I mean there 's {disfmarker} there 's many different acoustic differences . So I 'm not sure if {disfmarker} . I mean , unless we 're going to include a bunch of car recordings in the {disfmarker} in the training database , I 'm not sure if it 's {disfmarker} completely rules it out&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: if our {disfmarker} if we {disfmarker} if our major goal is to have phonetic context and you figure that there 's gonna be a mismatch in acoustic conditions does it make it much worse f to sort of add another mismatch , if you will .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Uh , i i I {disfmarker} I guess the question is how important is it to {disfmarker} for us to get multiple languages uh , in there .&#10;Speaker: PhD G&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, PhD A and Professor F, discuss the difference between silence at the beginning and end of a signal, but ultimately conclude that they are both similar forms of silence. They mention that distinctions between these types of silence may have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. However, it is not explicitly pointed out that this difference has any significant impact on the mapping process. It is also noted that there was a glottal stop in the TIMIT dataset, which is a short period of silence, and this might have been relevant for the mapping as well." target="The speaker sees several benefits in the current situation, despite the limitations of time and the need to address a new problem. Firstly, they recognize that there is significant computational power available across the institute, which can be leveraged to tackle this issue. They also highlight that while working on something new inevitably involves a learning and development process, in this case, they have the advantage of being able to build upon existing knowledge and data that they already possess. This includes having experience with techniques such as voiced-unvoiced-silence detection features. Therefore, instead of starting from scratch or exploring entirely unknown territory, they can focus on applying proven methods and utilizing available resources to make progress in the new problem area.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, PhD A and Professor F, discuss the difference between silence at the beginning and end of a signal, but ultimately conclude that they are both similar forms of silence. They mention that distinctions between these types of silence may have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. However, it is not explicitly pointed out that this difference has any significant impact on the mapping process. It is also noted that there was a glottal stop in the TIMIT dataset, which is a short period of silence, and this might have been relevant for the mapping as well." target="The speakers are discussing the use of a neural network to classify phonemes using English neural networks that were trained with phonemes and seventeen broad classes. They mention the idea of creating a superset of broad phoneme classes, which could be used for cross-language handling. However, one speaker thinks there is something wrong with this approach, suggesting that using the same language for training the neural network and having a phonetically balanced database would be better.&#10;&#10;Later in the discussion, they talk about potential issues when discriminating words based on the output of the neural network. They mention that this may not provide enough information to distinguish between words due to the fact that most confusions occur within phone classes. This is because words can differ with only a single phoneme, which could be in the same broad class. The speakers note that this might have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. Additionally, they mention a glottal stop in the TIMIT dataset, which is a short period of silence, and this might also be relevant for the mapping process.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, PhD A and Professor F, discuss the difference between silence at the beginning and end of a signal, but ultimately conclude that they are both similar forms of silence. They mention that distinctions between these types of silence may have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. However, it is not explicitly pointed out that this difference has any significant impact on the mapping process. It is also noted that there was a glottal stop in the TIMIT dataset, which is a short period of silence, and this might have been relevant for the mapping as well." target="One major criticism against current speech recognition systems is that they throw away all the harmonicity information and try to get spectral envelopes, discarding most of the information about the phonetic identity which is contained in the harmonics. Incorporating pitch or harmonicity information could address this issue by preserving some of this lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, PhD A and Professor F, discuss the difference between silence at the beginning and end of a signal, but ultimately conclude that they are both similar forms of silence. They mention that distinctions between these types of silence may have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. However, it is not explicitly pointed out that this difference has any significant impact on the mapping process. It is also noted that there was a glottal stop in the TIMIT dataset, which is a short period of silence, and this might have been relevant for the mapping as well." target="Based on the discussion, it is suggested that using the TIMIT dataset as a source for training a tandem system across languages could be a better choice due to its broadness and careful hand-correction. However, given the limited time and resources, the speakers did not explicitly conclude that the TIMIT dataset's broadness or careful hand-correction is the sole reason for its potential superiority. Further investigation would be required to make a definitive conclusion.">
      <data key="d0">1</data>
    </edge>
    <edge source=" A&#10;Content: Yeah . Right .&#10;Speaker: Professor F&#10;Content: then it 's probably {disfmarker}&#10;Speaker: PhD A&#10;Content: What about the differences in the phone sets ?&#10;Speaker: Grad C&#10;Content: Uh , between languages ?&#10;Speaker: PhD A&#10;Content: No , between TIMIT and the {disfmarker} the digits .&#10;Speaker: Grad C&#10;Content: Oh , um , right . Well , there 's a mapping from the sixty - one phonemes in TIMIT to {disfmarker} to fifty - six , the ICSI fifty - six .&#10;Speaker: PhD E&#10;Content: Sixty - one .&#10;Speaker: PhD A&#10;Content: Oh , OK . I see .&#10;Speaker: Grad C&#10;Content: And then the digits phonemes , um , there 's about twenty twenty - two or twenty - four of them ? Is that right ?&#10;Speaker: PhD A&#10;Content: Out of that fifty - six ?&#10;Speaker: PhD G&#10;Content: Yep .&#10;Speaker: Grad C&#10;Content: Out of that fifty - six .&#10;Speaker: PhD A&#10;" target="The goal of computing and sending features instead of actual speech in the development of new telephone standards, as discussed by the speakers, is to improve speech recognition and synthesis by accurately identifying phones (distinct units of sound) in the data. This would allow for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures. Additionally, incorporating pitch or harmonicity information could address current criticism of speech recognition systems by preserving some of the lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.&#10;&#10;One challenge in achieving this goal is the differences in phone sets between different databases and languages. In the conversation, they mention a mapping from the 61 phonemes in TIMIT to the 56 phonemes in ICSI and the differences in phone sets between the two databases. This shows that addressing the issue of phone set differences is an important step towards achieving the goal of accurately identifying phones and improving speech recognition and synthesis in new telephone standards.">
      <data key="d0">1</data>
    </edge>
    <edge source=" A&#10;Content: Yeah . Right .&#10;Speaker: Professor F&#10;Content: then it 's probably {disfmarker}&#10;Speaker: PhD A&#10;Content: What about the differences in the phone sets ?&#10;Speaker: Grad C&#10;Content: Uh , between languages ?&#10;Speaker: PhD A&#10;Content: No , between TIMIT and the {disfmarker} the digits .&#10;Speaker: Grad C&#10;Content: Oh , um , right . Well , there 's a mapping from the sixty - one phonemes in TIMIT to {disfmarker} to fifty - six , the ICSI fifty - six .&#10;Speaker: PhD E&#10;Content: Sixty - one .&#10;Speaker: PhD A&#10;Content: Oh , OK . I see .&#10;Speaker: Grad C&#10;Content: And then the digits phonemes , um , there 's about twenty twenty - two or twenty - four of them ? Is that right ?&#10;Speaker: PhD A&#10;Content: Out of that fifty - six ?&#10;Speaker: PhD G&#10;Content: Yep .&#10;Speaker: Grad C&#10;Content: Out of that fifty - six .&#10;Speaker: PhD A&#10;" target="1. Spain (Spanish accent)&#10;2. France (French accent, specifically from Paris)&#10;3. Texas, USA (TIMIT corpus is from Texas)&#10;4. Finland (Finnish language)&#10;5. Italy (Italian language)&#10;6. Mexican Spain (possibly a mixed or regional accent)&#10;7. Switzerland (Swiss German or French accent was mentioned as a possibility)">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker} they also in TIMIT had like a glottal stop , which was basically a short period of silence ,&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: and so .&#10;Speaker: PhD B&#10;Content: Well , we have that now , too , right ?&#10;Speaker: PhD A&#10;Content: I don't know .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So .&#10;Speaker: Professor F&#10;Content: i It 's actually pretty common that a lot of the recognition systems people use have things like {disfmarker} like , say thirty - nine , phone symbols , right ? Uh , and then they get the variety by {disfmarker} by bringing in the context , the phonetic context . Uh . So we actually have an unusually large number in {disfmarker} in what we tend to use here . Um . So , a a actually {disfmarker} maybe {disfmarker} now you 've got me sort of intrigued . What {disfmarker} there 's {disfmarker} Can you describe what" target="The &quot;unusual number&quot; the speakers refer to is the larger number of phone symbols used in their recognition system, which is 120 compared to the typical 39 in other systems. This larger set allows for a more nuanced identification of sounds, as it includes additional phonetic context beyond the basic sound itself. The use of this larger set enables the system to bring in context and better distinguish between similar sounds based on their surrounding phonetic environment.&#10;&#10;This approach aims to improve accuracy by considering how individual phones are influenced by neighboring sounds within words or phrases, which can help resolve ambiguities that may arise from using a smaller phone set. However, this increased level of detail also presents challenges in accurately training and fine-tuning the recognition system to effectively use this larger phone set for speech processing tasks.">
      <data key="d0">1</data>
    </edge>
    <edge source=" differences between these different databases . I mean some of this stuff 's recorded in the car , and some of it 's {disfmarker} I mean there 's {disfmarker} there 's many different acoustic differences . So I 'm not sure if {disfmarker} . I mean , unless we 're going to include a bunch of car recordings in the {disfmarker} in the training database , I 'm not sure if it 's {disfmarker} completely rules it out&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: if our {disfmarker} if we {disfmarker} if our major goal is to have phonetic context and you figure that there 's gonna be a mismatch in acoustic conditions does it make it much worse f to sort of add another mismatch , if you will .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Uh , i i I {disfmarker} I guess the question is how important is it to {disfmarker} for us to get multiple languages uh , in there .&#10;Speaker: PhD G&#10;Content" target="Based on the information provided in the transcripts, it is stated that an HTK training takes around six hours or more. The discussion includes testing the model's performance on three different types of noise conditions (matched, mismatched, and mismatched channel) using a script. However, there is no explicit mention of the time required to complete a training session with the specific configuration you mentioned. To provide an accurate answer, further details or specifications about the computational cost associated with this particular scenario are needed.">
      <data key="d0">1</data>
    </edge>
    <edge source="The individual, presumably Professor F, is arranging to meet everyone at 6 AM in the morning at an unspecified location, as they plan to drive them to the airport. The backup plan if they are not there on time is not explicitly mentioned in the transcript, but it can be inferred that they would likely wait for a reasonable amount of time before leaving without them or making alternative arrangements." target=" how would we ever accomplish that ? Uh {disfmarker} what {disfmarker} what {disfmarker} what part of town do you live in ?&#10;Speaker: Grad C&#10;Content: Um , I live in , um , the corner of campus . The , um , southeast corner .&#10;Speaker: Professor F&#10;Content: OK . OK , so would it be easier {disfmarker} those of you who are not , you know , used to this area , it can be very tricky to get to the airport at {disfmarker} at uh , you know , six thirty . Um . So . Would it be easier for you if you came here and I drove you ? Yeah ? Yeah , yeah , OK .&#10;Speaker: PhD G&#10;Content: Yeah , perhaps , yeah .&#10;Speaker: Grad C&#10;Content: Yeah . Sure .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: OK , so if {disfmarker} if everybody can get here at six .&#10;Speaker: PhD E&#10;Content: At six .&#10;Speaker: Professor F&#10;Content: Yeah , I 'm afraid we need to do that">
      <data key="d0">1</data>
    </edge>
    <edge source="The individual, presumably Professor F, is arranging to meet everyone at 6 AM in the morning at an unspecified location, as they plan to drive them to the airport. The backup plan if they are not there on time is not explicitly mentioned in the transcript, but it can be inferred that they would likely wait for a reasonable amount of time before leaving without them or making alternative arrangements." target="Speaker: Professor F&#10;Content: Nah , I 'll be fine . I just , uh {disfmarker} it {disfmarker} for me it just means getting up a half an hour earlier than I usually do . Not {disfmarker} not {disfmarker} not a lot ,&#10;Speaker: Grad C&#10;Content: OK . Wednesday .&#10;Speaker: Professor F&#10;Content: so OK , that was the real real important stuff . Um , I {disfmarker} I {disfmarker} I figured maybe wait on the potential goals for the meeting uh {disfmarker} until we talk about wh what 's been going on . So , uh , what 's been going on ? Why don't we start {disfmarker} start over here .&#10;Speaker: PhD G&#10;Content: Um . {vocalsound} Well , preparation of the French test data actually .&#10;Speaker: Professor F&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: So , {vocalsound} it means that um , well , it is , uh , a digit French database of microphone speech , downsampled to eight kilohertz and">
      <data key="d0">1</data>
    </edge>
    <edge source="The individual, presumably Professor F, is arranging to meet everyone at 6 AM in the morning at an unspecified location, as they plan to drive them to the airport. The backup plan if they are not there on time is not explicitly mentioned in the transcript, but it can be inferred that they would likely wait for a reasonable amount of time before leaving without them or making alternative arrangements." target="&#10;Speaker: PhD E&#10;Content: At six .&#10;Speaker: Professor F&#10;Content: Yeah , I 'm afraid we need to do that to get there on time .&#10;Speaker: Grad C&#10;Content: Six , OK .&#10;Speaker: Professor F&#10;Content: Yeah , so . Oh boy . Anyway , so .&#10;Speaker: PhD A&#10;Content: Will that {pause} be enough time ?&#10;Speaker: Professor F&#10;Content: Yeah . Yeah , so I 'll just pull up in front at six and just be out front . And , uh , and yeah , that 'll be plenty of time . It 'll take {disfmarker} it {disfmarker} it {disfmarker} it won't be bad traffic that time of day and {disfmarker} and uh&#10;Speaker: PhD A&#10;Content: I guess once you get past the bridge {pause} that that would be the worst .&#10;Speaker: PhD B&#10;Content: Yeah , Oakland .&#10;Speaker: Professor F&#10;Content: Going to Oakland .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Oakland .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="The individual, presumably Professor F, is arranging to meet everyone at 6 AM in the morning at an unspecified location, as they plan to drive them to the airport. The backup plan if they are not there on time is not explicitly mentioned in the transcript, but it can be inferred that they would likely wait for a reasonable amount of time before leaving without them or making alternative arrangements." target="} or anything , but&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: Uh , anyway . Uh {disfmarker} so here 's what I have for {disfmarker} I {disfmarker} I was just jotting down things I think th w that we should do today . Uh {disfmarker} This is what I have for an agenda so far Um , We should talk a little bit about the plans for the uh {disfmarker} the field trip next week . Uh {disfmarker} a number of us are doing a field trip to uh Uh {disfmarker} OGI And uh {disfmarker} mostly uh First though about the logistics for it . Then maybe later on in the meeting we should talk about what we actually you know , might accomplish . Uh {disfmarker}&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: Uh , in and {pause} kind of go around {disfmarker} see what people have been doing {disfmarker} talk about that , {pause} a r progress report . Um , Essentially . Um">
      <data key="d0">1</data>
    </edge>
    <edge source="The individual, presumably Professor F, is arranging to meet everyone at 6 AM in the morning at an unspecified location, as they plan to drive them to the airport. The backup plan if they are not there on time is not explicitly mentioned in the transcript, but it can be inferred that they would likely wait for a reasonable amount of time before leaving without them or making alternative arrangements." target=" Uh , are we still using P - make ? Is that {disfmarker} ?&#10;Speaker: Grad C&#10;Content: Oh , I don't know how w how we would P - make this , though . Um .&#10;Speaker: Professor F&#10;Content: Well , you have a {disfmarker} I mean , once you get the basic thing set up , you have just all the {disfmarker} uh , a all these combinations ,&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: right ?&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: Um . It 's {disfmarker} it 's {disfmarker} let 's say it 's six hours or eight hours , or something for the training of HTK . How long is it for training of {disfmarker} of , uh , the neural net ?&#10;Speaker: Grad C&#10;Content: The neural net ? Um .&#10;Speaker: PhD G&#10;Content: I would say two days .&#10;Speaker: PhD A&#10;Content: Depends on the corpuses , right ?&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The individual, presumably Professor F, is arranging to meet everyone at 6 AM in the morning at an unspecified location, as they plan to drive them to the airport. The backup plan if they are not there on time is not explicitly mentioned in the transcript, but it can be inferred that they would likely wait for a reasonable amount of time before leaving without them or making alternative arrangements." target="The speaker, presumably Professor F, is arranging to meet with the group at 6 AM in front of an unspecified location. If there are any delays, Professor F mentioned that they will be fine and do not need to be woken up, but they offered to provide their phone number just in case.">
      <data key="d0">1</data>
    </edge>
    <edge source="The individual, presumably Professor F, is arranging to meet everyone at 6 AM in the morning at an unspecified location, as they plan to drive them to the airport. The backup plan if they are not there on time is not explicitly mentioned in the transcript, but it can be inferred that they would likely wait for a reasonable amount of time before leaving without them or making alternative arrangements." target="Based on the transcript, there is no direct mention of Professor F's decision to schedule a 7:20 AM plane departure. However, it can be inferred that they plan to drive the group to the airport and want everyone to meet at 6 AM to ensure they arrive on time for this departure.&#10;&#10;As for accomplishing a same-day return visit, Professor F suggests driving the group to the airport and offers to pick up those who are not familiar with the area. This implies that they have considered the logistics of getting the group to the airport in time for their flight and making arrangements to facilitate this process. Additionally, since Professor F mentions that network upgrades are making it difficult to work due to slower network speeds compared to their increasingly faster computers and machines, it can be inferred that the purpose of the trip involves tasks that cannot be completed remotely or require access to specific resources at their destination.">
      <data key="d0">1</data>
    </edge>
    <edge source=" how would we ever accomplish that ? Uh {disfmarker} what {disfmarker} what {disfmarker} what part of town do you live in ?&#10;Speaker: Grad C&#10;Content: Um , I live in , um , the corner of campus . The , um , southeast corner .&#10;Speaker: Professor F&#10;Content: OK . OK , so would it be easier {disfmarker} those of you who are not , you know , used to this area , it can be very tricky to get to the airport at {disfmarker} at uh , you know , six thirty . Um . So . Would it be easier for you if you came here and I drove you ? Yeah ? Yeah , yeah , OK .&#10;Speaker: PhD G&#10;Content: Yeah , perhaps , yeah .&#10;Speaker: Grad C&#10;Content: Yeah . Sure .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: OK , so if {disfmarker} if everybody can get here at six .&#10;Speaker: PhD E&#10;Content: At six .&#10;Speaker: Professor F&#10;Content: Yeah , I 'm afraid we need to do that" target="Based on the transcript, there is no direct mention of Professor F's decision to schedule a 7:20 AM plane departure. However, it can be inferred that they plan to drive the group to the airport and want everyone to meet at 6 AM to ensure they arrive on time for this departure.&#10;&#10;As for accomplishing a same-day return visit, Professor F suggests driving the group to the airport and offers to pick up those who are not familiar with the area. This implies that they have considered the logistics of getting the group to the airport in time for their flight and making arrangements to facilitate this process. Additionally, since Professor F mentions that network upgrades are making it difficult to work due to slower network speeds compared to their increasingly faster computers and machines, it can be inferred that the purpose of the trip involves tasks that cannot be completed remotely or require access to specific resources at their destination.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: Professor F&#10;Content: Nah , I 'll be fine . I just , uh {disfmarker} it {disfmarker} for me it just means getting up a half an hour earlier than I usually do . Not {disfmarker} not {disfmarker} not a lot ,&#10;Speaker: Grad C&#10;Content: OK . Wednesday .&#10;Speaker: Professor F&#10;Content: so OK , that was the real real important stuff . Um , I {disfmarker} I {disfmarker} I figured maybe wait on the potential goals for the meeting uh {disfmarker} until we talk about wh what 's been going on . So , uh , what 's been going on ? Why don't we start {disfmarker} start over here .&#10;Speaker: PhD G&#10;Content: Um . {vocalsound} Well , preparation of the French test data actually .&#10;Speaker: Professor F&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: So , {vocalsound} it means that um , well , it is , uh , a digit French database of microphone speech , downsampled to eight kilohertz and" target="The speaker, presumably Professor F, is arranging to meet with the group at 6 AM in front of an unspecified location. If there are any delays, Professor F mentioned that they will be fine and do not need to be woken up, but they offered to provide their phone number just in case.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: PhD E&#10;Content: At six .&#10;Speaker: Professor F&#10;Content: Yeah , I 'm afraid we need to do that to get there on time .&#10;Speaker: Grad C&#10;Content: Six , OK .&#10;Speaker: Professor F&#10;Content: Yeah , so . Oh boy . Anyway , so .&#10;Speaker: PhD A&#10;Content: Will that {pause} be enough time ?&#10;Speaker: Professor F&#10;Content: Yeah . Yeah , so I 'll just pull up in front at six and just be out front . And , uh , and yeah , that 'll be plenty of time . It 'll take {disfmarker} it {disfmarker} it {disfmarker} it won't be bad traffic that time of day and {disfmarker} and uh&#10;Speaker: PhD A&#10;Content: I guess once you get past the bridge {pause} that that would be the worst .&#10;Speaker: PhD B&#10;Content: Yeah , Oakland .&#10;Speaker: Professor F&#10;Content: Going to Oakland .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Oakland .&#10;Speaker:" target="The speaker, presumably Professor F, is arranging to meet with the group at 6 AM in front of an unspecified location. If there are any delays, Professor F mentioned that they will be fine and do not need to be woken up, but they offered to provide their phone number just in case.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Uh , are we still using P - make ? Is that {disfmarker} ?&#10;Speaker: Grad C&#10;Content: Oh , I don't know how w how we would P - make this , though . Um .&#10;Speaker: Professor F&#10;Content: Well , you have a {disfmarker} I mean , once you get the basic thing set up , you have just all the {disfmarker} uh , a all these combinations ,&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: right ?&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: Um . It 's {disfmarker} it 's {disfmarker} let 's say it 's six hours or eight hours , or something for the training of HTK . How long is it for training of {disfmarker} of , uh , the neural net ?&#10;Speaker: Grad C&#10;Content: The neural net ? Um .&#10;Speaker: PhD G&#10;Content: I would say two days .&#10;Speaker: PhD A&#10;Content: Depends on the corpuses , right ?&#10;" target="Based on the information provided in the transcripts, specifically the discussion about HTK training taking around six hours or more, and considering that there will be some computational overhead for computing the KL transformation when testing combinations of built-in training programs like PLP, MSG, and JRA, it can be estimated that conducting experiments on these combinations might take a significant amount of time. However, the exact duration is not explicitly mentioned in the transcripts.&#10;&#10;To provide an accurate answer, additional information about the computational resources available for running these experiments would be helpful. Factors such as the number of possible combinations, the complexity of each combination, and the computational power of the hardware being used would all impact the total time required to complete the experiments.&#10;&#10;In summary, while it is not possible to provide a precise estimate based on the given transcripts, conducting experiments on combinations of built-in training programs such as PLP, MSG, and JRA is likely to take a considerable amount of time, possibly extending beyond a few days, considering the estimated six hours or more for each HTK training session and additional computational overhead.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speaker sees several benefits in the current situation, despite the limitations of time and the need to address a new problem. Firstly, they recognize that there is significant computational power available across the institute, which can be leveraged to tackle this issue. They also highlight that while working on something new inevitably involves a learning and development process, in this case, they have the advantage of being able to build upon existing knowledge and data that they already possess. This includes having experience with techniques such as voiced-unvoiced-silence detection features. Therefore, instead of starting from scratch or exploring entirely unknown territory, they can focus on applying proven methods and utilizing available resources to make progress in the new problem area." target=" of thing , or {disfmarker} or we 're not going to get through any significant number of these .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content:  So this is {disfmarker} Yeah , I mean , I kind of like this because what it {disfmarker} No {disfmarker}&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: uh , no , what I like about it is we {disfmarker} we {disfmarker} we do have a problem that we have very limited time . You know , so , with very limited time , we actually have really quite a {disfmarker} quite a bit of computational resource available if you , you know , get a look across the institute and how little things are being used . And uh , on the other hand , almost anything that really i you know , is {disfmarker} is new , where we 're saying , &quot; Well , let 's look at , like we were talking before about , uh , uh , voiced - unvoiced - silence detection features and all those sort {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="The speaker sees several benefits in the current situation, despite the limitations of time and the need to address a new problem. Firstly, they recognize that there is significant computational power available across the institute, which can be leveraged to tackle this issue. They also highlight that while working on something new inevitably involves a learning and development process, in this case, they have the advantage of being able to build upon existing knowledge and data that they already possess. This includes having experience with techniques such as voiced-unvoiced-silence detection features. Therefore, instead of starting from scratch or exploring entirely unknown territory, they can focus on applying proven methods and utilizing available resources to make progress in the new problem area." target="} {disfmarker} he 's working on other things , but to {disfmarker} to do something on this project . So the question is , &quot; Where {disfmarker} where could we , uh , uh , most use Dave 's help ? &quot;&#10;Speaker: PhD G&#10;Content: Um , yeah , I was thinking perhaps if , um , additionally to all these experiments , which is not really research , well I mean it 's , uh , running programs&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: and , um , {vocalsound} trying to have a closer look at the {disfmarker} perhaps the , um , {vocalsound} speech , uh , noise detection or , uh , voiced - sound - unvoiced - sound detection and {disfmarker} Which could be important in {disfmarker} i for noise {disfmarker} noise {disfmarker}&#10;Speaker: PhD A&#10;Content: I think that would be a {disfmarker} I think that 's a big {disfmarker} big deal . Because the {disfmarker} you">
      <data key="d0">1</data>
    </edge>
    <edge source="The speaker sees several benefits in the current situation, despite the limitations of time and the need to address a new problem. Firstly, they recognize that there is significant computational power available across the institute, which can be leveraged to tackle this issue. They also highlight that while working on something new inevitably involves a learning and development process, in this case, they have the advantage of being able to build upon existing knowledge and data that they already possess. This includes having experience with techniques such as voiced-unvoiced-silence detection features. Therefore, instead of starting from scratch or exploring entirely unknown territory, they can focus on applying proven methods and utilizing available resources to make progress in the new problem area." target=" look at , like we were talking before about , uh , uh , voiced - unvoiced - silence detection features and all those sort {disfmarker} &quot; that 's {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: I think it 's a great thing to go to . But if it 's new , then we have this development and {disfmarker} and {disfmarker} and learning process t to {disfmarker} to go through on top of {disfmarker} just the {disfmarker} the {disfmarker} all the {disfmarker} all the work . So , I {disfmarker} I {disfmarker} I don't see how we 'd do it . So what I like about this is you basically have listed all the things that we already know how to do .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: And {disfmarker} and all the kinds of data that we , at this point , already have . And , uh , you 're just saying let 's look">
      <data key="d0">1</data>
    </edge>
    <edge source="The speaker sees several benefits in the current situation, despite the limitations of time and the need to address a new problem. Firstly, they recognize that there is significant computational power available across the institute, which can be leveraged to tackle this issue. They also highlight that while working on something new inevitably involves a learning and development process, in this case, they have the advantage of being able to build upon existing knowledge and data that they already possess. This includes having experience with techniques such as voiced-unvoiced-silence detection features. Therefore, instead of starting from scratch or exploring entirely unknown territory, they can focus on applying proven methods and utilizing available resources to make progress in the new problem area." target="disfmarker} Mm - hmm . OK .&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: So , presumably , that 'll be part of the topic of analysis of the {disfmarker} the test results , is how well you do when it 's matching noise and how well you do where it 's not .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: I think that 's right .&#10;Speaker: Grad C&#10;Content: So , I guess we can't train on {disfmarker} on the {disfmarker} the unseen noise conditions .&#10;Speaker: Professor F&#10;Content: Well , not if it 's not seen ,&#10;Speaker: Grad C&#10;Content: Right . If {disfmarker} Not if it 's unseen .&#10;Speaker: Professor F&#10;Content: yeah .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: OK . I mean , i i i i it does seem to me that a lot of times when you train with something that 's at least a little bit">
      <data key="d0">1</data>
    </edge>
    <edge source="} {disfmarker} he 's working on other things , but to {disfmarker} to do something on this project . So the question is , &quot; Where {disfmarker} where could we , uh , uh , most use Dave 's help ? &quot;&#10;Speaker: PhD G&#10;Content: Um , yeah , I was thinking perhaps if , um , additionally to all these experiments , which is not really research , well I mean it 's , uh , running programs&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: and , um , {vocalsound} trying to have a closer look at the {disfmarker} perhaps the , um , {vocalsound} speech , uh , noise detection or , uh , voiced - sound - unvoiced - sound detection and {disfmarker} Which could be important in {disfmarker} i for noise {disfmarker} noise {disfmarker}&#10;Speaker: PhD A&#10;Content: I think that would be a {disfmarker} I think that 's a big {disfmarker} big deal . Because the {disfmarker} you" target="1. Purpose of the trip: The purpose of the trip is not explicitly stated in the transcript, but it involves Professor F, PhD G, and possibly others, conducting experiments and working on a project, which seems to involve analyzing audio recordings based on the context.&#10;2. Preparations for Dave's contribution: In response to Dave's offer to contribute to the project, they are discussing where his help would be most useful. PhD G suggests having Dave focus on research aspects like implementing speech noise detection or voiced-unvoiced sound detection algorithms in addition to the ongoing experiments. The team should consider how best to integrate Dave into their workflow and delegate tasks based on his skills and interests, as well as ensure he has access to necessary resources such as data and computational power.&#10;&#10;However, there is no information provided about specific preparations related to time or disk space mentioned in the transcripts. These considerations are discussed separately in the context of the project's progress but not directly linked to Dave's offer to help.">
      <data key="d0">1</data>
    </edge>
    <edge source="disfmarker} Mm - hmm . OK .&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: So , presumably , that 'll be part of the topic of analysis of the {disfmarker} the test results , is how well you do when it 's matching noise and how well you do where it 's not .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: I think that 's right .&#10;Speaker: Grad C&#10;Content: So , I guess we can't train on {disfmarker} on the {disfmarker} the unseen noise conditions .&#10;Speaker: Professor F&#10;Content: Well , not if it 's not seen ,&#10;Speaker: Grad C&#10;Content: Right . If {disfmarker} Not if it 's unseen .&#10;Speaker: Professor F&#10;Content: yeah .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: OK . I mean , i i i i it does seem to me that a lot of times when you train with something that 's at least a little bit" target="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is mentioned by Professor F when discussing the analysis of test results.&#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus.&#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments.&#10;4. Computational cost: While combining noise conditions for testing doesn't require as much computation as initial training, there is still some computational overhead for computing the KL transformation. Additionally, there will be further computational costs for training and testing the HTK models with these combinations.">
      <data key="d0">1</data>
    </edge>
    <edge source="disfmarker} Mm - hmm . OK .&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: So , presumably , that 'll be part of the topic of analysis of the {disfmarker} the test results , is how well you do when it 's matching noise and how well you do where it 's not .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: I think that 's right .&#10;Speaker: Grad C&#10;Content: So , I guess we can't train on {disfmarker} on the {disfmarker} the unseen noise conditions .&#10;Speaker: Professor F&#10;Content: Well , not if it 's not seen ,&#10;Speaker: Grad C&#10;Content: Right . If {disfmarker} Not if it 's unseen .&#10;Speaker: Professor F&#10;Content: yeah .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: OK . I mean , i i i i it does seem to me that a lot of times when you train with something that 's at least a little bit" target="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is because the model will have limited exposure and experience in handling diverse types of unseen noise, which could lead to suboptimal performance when dealing with real-world scenarios where various kinds of noises are present.&#10;   &#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus. By training with seen noise conditions, the model can become more robust in handling those specific types of noises, improving its overall performance in noisy environments that it has encountered before.&#10;   &#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments. While this doesn't directly address the issue of not being able to train on unseen noise conditions, combination testing can provide valuable insights into how the model performs under various noisy scenarios and potentially lead to improvements in the model's ability to generalize to new, unseen noise conditions.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is mentioned by Professor F when discussing the analysis of test results.&#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus.&#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments.&#10;4. Computational cost: While combining noise conditions for testing doesn't require as much computation as initial training, there is still some computational overhead for computing the KL transformation. Additionally, there will be further computational costs for training and testing the HTK models with these combinations." target=" all {disfmarker} they 're all gonna be test tested , um , with one training of the HTK system . Um , there 's a script that tests all three different types of noise conditions . Test - A is like a matched noise . Test - B is a {disfmarker} is a slightly mismatched . And test - C is a , um , mismatched channel .&#10;Speaker: Grad D&#10;Content: And do we do all our {pause} training on clean data ?&#10;Speaker: Grad C&#10;Content: Um , no , no ,&#10;Speaker: PhD E&#10;Content: Also , we can clean that .&#10;Speaker: Grad C&#10;Content: we 're {disfmarker} we 're gonna be , um , training on the noise files that we do have .&#10;Speaker: PhD G&#10;Content: No .&#10;Speaker: Professor F&#10;Content: So , um {disfmarker} Yeah , so I guess the question is how long does it take to do a {disfmarker} a training ? I mean , it 's not totally crazy t I mean , these are {disfmarker} a lot of these are built - in">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is mentioned by Professor F when discussing the analysis of test results.&#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus.&#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments.&#10;4. Computational cost: While combining noise conditions for testing doesn't require as much computation as initial training, there is still some computational overhead for computing the KL transformation. Additionally, there will be further computational costs for training and testing the HTK models with these combinations." target=" deep South .&#10;Speaker: Professor F&#10;Content: So - s so it 's not really from the US either .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Is that {disfmarker} ? OK .&#10;Speaker: Grad C&#10;Content: Yeah . OK . And , um , with within the training corporas um , we 're , uh , thinking about , um , training with noise . So , incorporating the same kinds of noises that , um , Aurora is in incorporating in their , um {disfmarker} in their training corpus . Um , I don't think we we 're given the , uh {disfmarker} the unseen noise conditions , though , right ?&#10;Speaker: Professor F&#10;Content: I think what they were saying was that , um , for this next test there 's gonna be some of the cases where they have the same type of noise as you were given before hand and some cases where you 're not .&#10;Speaker: Grad C&#10;Content: Like {disfmarker} Mm - hmm . OK .&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is mentioned by Professor F when discussing the analysis of test results.&#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus.&#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments.&#10;4. Computational cost: While combining noise conditions for testing doesn't require as much computation as initial training, there is still some computational overhead for computing the KL transformation. Additionally, there will be further computational costs for training and testing the HTK models with these combinations." target=" C&#10;Content: Yeah . One hundred each , about .&#10;Speaker: Professor F&#10;Content: Uh , so that 's hundred and {disfmarker} {vocalsound} hundred and fourteen each .&#10;Speaker: Grad D&#10;Content: What a what about noise conditions ?&#10;Speaker: Professor F&#10;Content: What ?&#10;Speaker: Grad D&#10;Content: w Don't we need to put in the column for noise conditions ?&#10;Speaker: Professor F&#10;Content: Are you just trying to be difficult ?&#10;Speaker: Grad D&#10;Content: No , I just don't understand .&#10;Speaker: Grad C&#10;Content: Well , th uh , when {disfmarker} when I put these testings on there , I 'm assumi&#10;Speaker: Professor F&#10;Content: I 'm just kidding . Yeah .&#10;Speaker: Grad C&#10;Content: There - there 's three {disfmarker} three tests . Um , type - A , type - B , and type - C . And they 're all {disfmarker} they 're all gonna be test tested , um , with one training of the HTK system . Um , there 's">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is mentioned by Professor F when discussing the analysis of test results.&#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus.&#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments.&#10;4. Computational cost: While combining noise conditions for testing doesn't require as much computation as initial training, there is still some computational overhead for computing the KL transformation. Additionally, there will be further computational costs for training and testing the HTK models with these combinations." target=" long a {disfmarker} a training takes , if we can train up all these {disfmarker} these combinations , uh , then we can start working on testing of them individually , and in combination . Right ?&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: Because the putting them in combination , I think , is not as much computationally as the r training of the nets in the first place . Right ?&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: So y you do have to compute the KL transformation . Uh , which is a little bit , but it 's not too much .&#10;Speaker: PhD G&#10;Content: It 's not too much ,&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: no .&#10;Speaker: Professor F&#10;Content: So it 's {disfmarker}&#10;Speaker: PhD G&#10;Content: But {disfmarker} Yeah . But there is the testing also , which implies training , uh , the HTK models&#10;Speaker: PhD E&#10;Content: The {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is mentioned by Professor F when discussing the analysis of test results.&#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus.&#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments.&#10;4. Computational cost: While combining noise conditions for testing doesn't require as much computation as initial training, there is still some computational overhead for computing the KL transformation. Additionally, there will be further computational costs for training and testing the HTK models with these combinations." target="1. HTK Training and Testing: The speakers discuss conducting tests using the Hidden Markov Model Toolkit (HTK) system. This involves training the HTK models with one set of data and testing it with another. The computational cost and time required for an HTK training, which is around six hours or more, are considered.&#10;2. Noise Condition Tests: Grad D brings up the topic of noise conditions, questioning whether they should include a column for it in their analysis. Three types of noise conditions (A, B, and C) will be used for testing the model's performance with different noisy environments. Type A is matched noise, Type B is slightly mismatched, and Type C is mismatched channel noise.&#10;3. Cross-Language Testing: PhD G mentions a cross-language testing scenario where results showed better performance. This raises an interesting question about whether they should leverage this method for their own testing purposes.&#10;4. Training Corpus Tests: The discussion also involves tests based on the training corpus, specifically for the ANN (Artificial Neural Network) model. The team considers various features and different combinations of the training corpus to optimize the ANN performance.&#10;5. Discriminant Neural Net Testing: Grad C mentions a test related to the discriminant neural net, where they analyze the first dimension of features used in the testing process.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is mentioned by Professor F when discussing the analysis of test results.&#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus.&#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments.&#10;4. Computational cost: While combining noise conditions for testing doesn't require as much computation as initial training, there is still some computational overhead for computing the KL transformation. Additionally, there will be further computational costs for training and testing the HTK models with these combinations." target="Based on the information provided in the transcripts, it is stated that an HTK training takes around six hours or more. The discussion includes testing the model's performance on three different types of noise conditions (matched, mismatched, and mismatched channel) using a script. However, there is no explicit mention of the time required to complete a training session with the specific configuration you mentioned. To provide an accurate answer, further details or specifications about the computational cost associated with this particular scenario are needed.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is mentioned by Professor F when discussing the analysis of test results.&#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus.&#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments.&#10;4. Computational cost: While combining noise conditions for testing doesn't require as much computation as initial training, there is still some computational overhead for computing the KL transformation. Additionally, there will be further computational costs for training and testing the HTK models with these combinations." target="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is because the model will have limited exposure and experience in handling diverse types of unseen noise, which could lead to suboptimal performance when dealing with real-world scenarios where various kinds of noises are present.&#10;   &#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus. By training with seen noise conditions, the model can become more robust in handling those specific types of noises, improving its overall performance in noisy environments that it has encountered before.&#10;   &#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments. While this doesn't directly address the issue of not being able to train on unseen noise conditions, combination testing can provide valuable insights into how the model performs under various noisy scenarios and potentially lead to improvements in the model's ability to generalize to new, unseen noise conditions.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is mentioned by Professor F when discussing the analysis of test results.&#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus.&#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments.&#10;4. Computational cost: While combining noise conditions for testing doesn't require as much computation as initial training, there is still some computational overhead for computing the KL transformation. Additionally, there will be further computational costs for training and testing the HTK models with these combinations." target="Based on the information provided in the transcripts, specifically the discussion about HTK training taking around six hours or more, and considering that there will be some computational overhead for computing the KL transformation when testing combinations of built-in training programs like PLP, MSG, and JRA, it can be estimated that conducting experiments on these combinations might take a significant amount of time. However, the exact duration is not explicitly mentioned in the transcripts.&#10;&#10;To provide an accurate answer, additional information about the computational resources available for running these experiments would be helpful. Factors such as the number of possible combinations, the complexity of each combination, and the computational power of the hardware being used would all impact the total time required to complete the experiments.&#10;&#10;In summary, while it is not possible to provide a precise estimate based on the given transcripts, conducting experiments on combinations of built-in training programs such as PLP, MSG, and JRA is likely to take a considerable amount of time, possibly extending beyond a few days, considering the estimated six hours or more for each HTK training session and additional computational overhead.">
      <data key="d0">1</data>
    </edge>
    <edge source=" all {disfmarker} they 're all gonna be test tested , um , with one training of the HTK system . Um , there 's a script that tests all three different types of noise conditions . Test - A is like a matched noise . Test - B is a {disfmarker} is a slightly mismatched . And test - C is a , um , mismatched channel .&#10;Speaker: Grad D&#10;Content: And do we do all our {pause} training on clean data ?&#10;Speaker: Grad C&#10;Content: Um , no , no ,&#10;Speaker: PhD E&#10;Content: Also , we can clean that .&#10;Speaker: Grad C&#10;Content: we 're {disfmarker} we 're gonna be , um , training on the noise files that we do have .&#10;Speaker: PhD G&#10;Content: No .&#10;Speaker: Professor F&#10;Content: So , um {disfmarker} Yeah , so I guess the question is how long does it take to do a {disfmarker} a training ? I mean , it 's not totally crazy t I mean , these are {disfmarker} a lot of these are built - in" target="1. HTK Training and Testing: The speakers discuss conducting tests using the Hidden Markov Model Toolkit (HTK) system. This involves training the HTK models with one set of data and testing it with another. The computational cost and time required for an HTK training, which is around six hours or more, are considered.&#10;2. Noise Condition Tests: Grad D brings up the topic of noise conditions, questioning whether they should include a column for it in their analysis. Three types of noise conditions (A, B, and C) will be used for testing the model's performance with different noisy environments. Type A is matched noise, Type B is slightly mismatched, and Type C is mismatched channel noise.&#10;3. Cross-Language Testing: PhD G mentions a cross-language testing scenario where results showed better performance. This raises an interesting question about whether they should leverage this method for their own testing purposes.&#10;4. Training Corpus Tests: The discussion also involves tests based on the training corpus, specifically for the ANN (Artificial Neural Network) model. The team considers various features and different combinations of the training corpus to optimize the ANN performance.&#10;5. Discriminant Neural Net Testing: Grad C mentions a test related to the discriminant neural net, where they analyze the first dimension of features used in the testing process.">
      <data key="d0">1</data>
    </edge>
    <edge source=" all {disfmarker} they 're all gonna be test tested , um , with one training of the HTK system . Um , there 's a script that tests all three different types of noise conditions . Test - A is like a matched noise . Test - B is a {disfmarker} is a slightly mismatched . And test - C is a , um , mismatched channel .&#10;Speaker: Grad D&#10;Content: And do we do all our {pause} training on clean data ?&#10;Speaker: Grad C&#10;Content: Um , no , no ,&#10;Speaker: PhD E&#10;Content: Also , we can clean that .&#10;Speaker: Grad C&#10;Content: we 're {disfmarker} we 're gonna be , um , training on the noise files that we do have .&#10;Speaker: PhD G&#10;Content: No .&#10;Speaker: Professor F&#10;Content: So , um {disfmarker} Yeah , so I guess the question is how long does it take to do a {disfmarker} a training ? I mean , it 's not totally crazy t I mean , these are {disfmarker} a lot of these are built - in" target="Based on the information provided in the transcripts, it is stated that an HTK training takes around six hours or more. The discussion includes testing the model's performance on three different types of noise conditions (matched, mismatched, and mismatched channel) using a script. However, there is no explicit mention of the time required to complete a training session with the specific configuration you mentioned. To provide an accurate answer, further details or specifications about the computational cost associated with this particular scenario are needed.">
      <data key="d0">1</data>
    </edge>
    <edge source=" deep South .&#10;Speaker: Professor F&#10;Content: So - s so it 's not really from the US either .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Is that {disfmarker} ? OK .&#10;Speaker: Grad C&#10;Content: Yeah . OK . And , um , with within the training corporas um , we 're , uh , thinking about , um , training with noise . So , incorporating the same kinds of noises that , um , Aurora is in incorporating in their , um {disfmarker} in their training corpus . Um , I don't think we we 're given the , uh {disfmarker} the unseen noise conditions , though , right ?&#10;Speaker: Professor F&#10;Content: I think what they were saying was that , um , for this next test there 's gonna be some of the cases where they have the same type of noise as you were given before hand and some cases where you 're not .&#10;Speaker: Grad C&#10;Content: Like {disfmarker} Mm - hmm . OK .&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker:" target="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is because the model will have limited exposure and experience in handling diverse types of unseen noise, which could lead to suboptimal performance when dealing with real-world scenarios where various kinds of noises are present.&#10;   &#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus. By training with seen noise conditions, the model can become more robust in handling those specific types of noises, improving its overall performance in noisy environments that it has encountered before.&#10;   &#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments. While this doesn't directly address the issue of not being able to train on unseen noise conditions, combination testing can provide valuable insights into how the model performs under various noisy scenarios and potentially lead to improvements in the model's ability to generalize to new, unseen noise conditions.">
      <data key="d0">1</data>
    </edge>
    <edge source=" C&#10;Content: Yeah . One hundred each , about .&#10;Speaker: Professor F&#10;Content: Uh , so that 's hundred and {disfmarker} {vocalsound} hundred and fourteen each .&#10;Speaker: Grad D&#10;Content: What a what about noise conditions ?&#10;Speaker: Professor F&#10;Content: What ?&#10;Speaker: Grad D&#10;Content: w Don't we need to put in the column for noise conditions ?&#10;Speaker: Professor F&#10;Content: Are you just trying to be difficult ?&#10;Speaker: Grad D&#10;Content: No , I just don't understand .&#10;Speaker: Grad C&#10;Content: Well , th uh , when {disfmarker} when I put these testings on there , I 'm assumi&#10;Speaker: Professor F&#10;Content: I 'm just kidding . Yeah .&#10;Speaker: Grad C&#10;Content: There - there 's three {disfmarker} three tests . Um , type - A , type - B , and type - C . And they 're all {disfmarker} they 're all gonna be test tested , um , with one training of the HTK system . Um , there 's" target="1. HTK Training and Testing: The speakers discuss conducting tests using the Hidden Markov Model Toolkit (HTK) system. This involves training the HTK models with one set of data and testing it with another. The computational cost and time required for an HTK training, which is around six hours or more, are considered.&#10;2. Noise Condition Tests: Grad D brings up the topic of noise conditions, questioning whether they should include a column for it in their analysis. Three types of noise conditions (A, B, and C) will be used for testing the model's performance with different noisy environments. Type A is matched noise, Type B is slightly mismatched, and Type C is mismatched channel noise.&#10;3. Cross-Language Testing: PhD G mentions a cross-language testing scenario where results showed better performance. This raises an interesting question about whether they should leverage this method for their own testing purposes.&#10;4. Training Corpus Tests: The discussion also involves tests based on the training corpus, specifically for the ANN (Artificial Neural Network) model. The team considers various features and different combinations of the training corpus to optimize the ANN performance.&#10;5. Discriminant Neural Net Testing: Grad C mentions a test related to the discriminant neural net, where they analyze the first dimension of features used in the testing process.">
      <data key="d0">1</data>
    </edge>
    <edge source=" C&#10;Content: Yeah . One hundred each , about .&#10;Speaker: Professor F&#10;Content: Uh , so that 's hundred and {disfmarker} {vocalsound} hundred and fourteen each .&#10;Speaker: Grad D&#10;Content: What a what about noise conditions ?&#10;Speaker: Professor F&#10;Content: What ?&#10;Speaker: Grad D&#10;Content: w Don't we need to put in the column for noise conditions ?&#10;Speaker: Professor F&#10;Content: Are you just trying to be difficult ?&#10;Speaker: Grad D&#10;Content: No , I just don't understand .&#10;Speaker: Grad C&#10;Content: Well , th uh , when {disfmarker} when I put these testings on there , I 'm assumi&#10;Speaker: Professor F&#10;Content: I 'm just kidding . Yeah .&#10;Speaker: Grad C&#10;Content: There - there 's three {disfmarker} three tests . Um , type - A , type - B , and type - C . And they 're all {disfmarker} they 're all gonna be test tested , um , with one training of the HTK system . Um , there 's" target="Based on the information provided in the transcripts, it is stated that an HTK training takes around six hours or more. The discussion includes testing the model's performance on three different types of noise conditions (matched, mismatched, and mismatched channel) using a script. However, there is no explicit mention of the time required to complete a training session with the specific configuration you mentioned. To provide an accurate answer, further details or specifications about the computational cost associated with this particular scenario are needed.">
      <data key="d0">1</data>
    </edge>
    <edge source=" long a {disfmarker} a training takes , if we can train up all these {disfmarker} these combinations , uh , then we can start working on testing of them individually , and in combination . Right ?&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: Because the putting them in combination , I think , is not as much computationally as the r training of the nets in the first place . Right ?&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: So y you do have to compute the KL transformation . Uh , which is a little bit , but it 's not too much .&#10;Speaker: PhD G&#10;Content: It 's not too much ,&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: no .&#10;Speaker: Professor F&#10;Content: So it 's {disfmarker}&#10;Speaker: PhD G&#10;Content: But {disfmarker} Yeah . But there is the testing also , which implies training , uh , the HTK models&#10;Speaker: PhD E&#10;Content: The {disfmarker}" target="Four hundred order hidden units are being used for nets trained on digits. The plan to increase the number of units for broader class nets, which actually means using more classes, is because these broader training corpus nets like TIMIT correspond to a larger number of phoneme classes compared to the digits nets that only correspond to about twenty phonemes.">
      <data key="d0">1</data>
    </edge>
    <edge source=" long a {disfmarker} a training takes , if we can train up all these {disfmarker} these combinations , uh , then we can start working on testing of them individually , and in combination . Right ?&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: Because the putting them in combination , I think , is not as much computationally as the r training of the nets in the first place . Right ?&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: So y you do have to compute the KL transformation . Uh , which is a little bit , but it 's not too much .&#10;Speaker: PhD G&#10;Content: It 's not too much ,&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: no .&#10;Speaker: Professor F&#10;Content: So it 's {disfmarker}&#10;Speaker: PhD G&#10;Content: But {disfmarker} Yeah . But there is the testing also , which implies training , uh , the HTK models&#10;Speaker: PhD E&#10;Content: The {disfmarker}" target="The transcript mentions that training a neural network on multiple languages is probably done using a single network, as suggested by PhD G's statement &quot;probably one net.&quot; The specifics of this method are not detailed in the conversation; however, it can be inferred that this would involve combining data from different languages during the training process.&#10;&#10;The speakers also discuss combining neural networks trained on individual languages with a more generic database, as stated by PhD G: &quot;training a net on the target languages but with a general database.&quot; It is important to note that the conversation does not explicitly confirm whether this method has been implemented or not.&#10;&#10;In summary, while the discussion does imply training a neural network using multiple languages in one network and combining individually trained networks, it does not provide explicit details about these methods.">
      <data key="d0">1</data>
    </edge>
    <edge source="The suggestion made by Professor F was to approach the task as a series of nested loops along the time axis, with a choice at each stage. This implies that instead of thinking of the process in stages, they should consider it as a single stream of data over time, with different choices to be made at various points. He also illustrates this concept by comparing it to having three nested loops with a choice for each one." target=" Yeah . Um , I {disfmarker} I was thinking two things . Uh , the first thing was , um {disfmarker} we {disfmarker} we actually had thought of this as sort of like , um {disfmarker} not {disfmarker} not in stages , {comment} but more along the {disfmarker} the time axis . Just kind of like one stream at a time ,&#10;Speaker: Professor F&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: je - je - je - je - je {comment} check out the results and {disfmarker} and go that way .&#10;Speaker: Professor F&#10;Content: Oh , yeah , yeah , sure . No , I 'm just saying , I 'm just thinking of it like loops ,&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: Professor F&#10;Content: right ? And so , y y y if you had three nested loops , that you have a choice for this , a choice for this , and a choice for that ,&#10;Speaker: Grad C&#10;Content: Yeah . Mm - hmm .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="The suggestion made by Professor F was to approach the task as a series of nested loops along the time axis, with a choice at each stage. This implies that instead of thinking of the process in stages, they should consider it as a single stream of data over time, with different choices to be made at various points. He also illustrates this concept by comparing it to having three nested loops with a choice for each one." target=" putting those together .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: Um . Yeah .&#10;Speaker: PhD A&#10;Content: When you do that , you 're increasing the size of the inputs to the net . Do you have to reduce the hidden layer , or something ?&#10;Speaker: Professor F&#10;Content: Well , so {disfmarker} I mean , so i it doesn't increase the number of trainings .&#10;Speaker: PhD A&#10;Content: No , no , I 'm {disfmarker} I 'm just wondering about number of parameters in the net . Do you have to worry about keeping that the same , or {disfmarker} ?&#10;Speaker: Professor F&#10;Content: Uh , I don't think so .&#10;Speaker: PhD B&#10;Content: There 's a computation limit , though , isn't there ?&#10;Speaker: Professor F&#10;Content: Yeah , I mean , it 's just more compu Excuse me ?&#10;Speaker: PhD B&#10;Content: Isn't there like a limit {pause} on the computation load , or d latency , or something like that for Aurora task">
      <data key="d0">1</data>
    </edge>
    <edge source="The suggestion made by Professor F was to approach the task as a series of nested loops along the time axis, with a choice at each stage. This implies that instead of thinking of the process in stages, they should consider it as a single stream of data over time, with different choices to be made at various points. He also illustrates this concept by comparing it to having three nested loops with a choice for each one." target="er} and the inputs are PLP and delta and that sort of thing ,&#10;Speaker: Grad C&#10;Content: Well , the inputs are one dimension of the cube ,&#10;Speaker: Professor F&#10;Content: or {disfmarker} ?&#10;Speaker: Grad C&#10;Content: which , um , we 've talked about it being , uh , PLP , um , M F C Cs , um , J - JRASTA , JRASTA - LDA {disfmarker}&#10;Speaker: PhD G&#10;Content: Hmm .&#10;Speaker: Professor F&#10;Content: Yeah , but your initial things you 're making one choice there ,&#10;Speaker: Grad C&#10;Content: Yeah ,&#10;Speaker: Professor F&#10;Content: right ?&#10;Speaker: Grad C&#10;Content: right .&#10;Speaker: Professor F&#10;Content: Which is PLP , or something ?&#10;Speaker: Grad C&#10;Content: Um , I {disfmarker} I haven't {disfmarker} I haven't decided on {disfmarker} on the initial thing .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Probably {">
      <data key="d0">1</data>
    </edge>
    <edge source="The suggestion made by Professor F was to approach the task as a series of nested loops along the time axis, with a choice at each stage. This implies that instead of thinking of the process in stages, they should consider it as a single stream of data over time, with different choices to be made at various points. He also illustrates this concept by comparing it to having three nested loops with a choice for each one." target="marker} and they 're han hand uh {disfmarker} hand - marked . Uh , I guess , actually , TIMIT was not entirely hand - marked . It was automatically first , and then hand {disfmarker} hand - corrected .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: Professor F&#10;Content: But {disfmarker} but , um , uh , it {disfmarker} it , um , it might be a better source . So , i it 's {disfmarker} you 're right . It would be another interesting scientific question to ask , &quot; Is it because it 's a broad source or because it was , you know , carefully ? &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: uh . And that 's something you could ask , but given limited time , I think the main thing is if it 's a better thing for going across languages on this training tandem system ,&#10;Speaker: PhD A&#10;Content: Yeah . Right .&#10;Speaker: Professor F&#10;Content: then it 's probably {disfmarker}&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="The suggestion made by Professor F was to approach the task as a series of nested loops along the time axis, with a choice at each stage. This implies that instead of thinking of the process in stages, they should consider it as a single stream of data over time, with different choices to be made at various points. He also illustrates this concept by comparing it to having three nested loops with a choice for each one." target="If they are able to successfully get all the specified features correct in an experiment, it would mean that they have defined and correctly identified all the phones (distinct units of sound) in the data. This is significant because phones are the building blocks of speech and language, and accurately identifying them is crucial for many applications such as speech recognition and synthesis. Professor F also mentions that this would be equivalent to saying that all the phones are correct. However, it's important to note that just getting all the features right may not necessarily guarantee perfect results in other aspects of their work, such as normalization or word discrimination, which are also discussed in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="The suggestion made by Professor F was to approach the task as a series of nested loops along the time axis, with a choice at each stage. This implies that instead of thinking of the process in stages, they should consider it as a single stream of data over time, with different choices to be made at various points. He also illustrates this concept by comparing it to having three nested loops with a choice for each one." target="Based on the transcript, there is no direct mention of Professor F's decision to schedule a 7:20 AM plane departure. However, it can be inferred that they plan to drive the group to the airport and want everyone to meet at 6 AM to ensure they arrive on time for this departure.&#10;&#10;As for accomplishing a same-day return visit, Professor F suggests driving the group to the airport and offers to pick up those who are not familiar with the area. This implies that they have considered the logistics of getting the group to the airport in time for their flight and making arrangements to facilitate this process. Additionally, since Professor F mentions that network upgrades are making it difficult to work due to slower network speeds compared to their increasingly faster computers and machines, it can be inferred that the purpose of the trip involves tasks that cannot be completed remotely or require access to specific resources at their destination.">
      <data key="d0">1</data>
    </edge>
    <edge source=" putting those together .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: Um . Yeah .&#10;Speaker: PhD A&#10;Content: When you do that , you 're increasing the size of the inputs to the net . Do you have to reduce the hidden layer , or something ?&#10;Speaker: Professor F&#10;Content: Well , so {disfmarker} I mean , so i it doesn't increase the number of trainings .&#10;Speaker: PhD A&#10;Content: No , no , I 'm {disfmarker} I 'm just wondering about number of parameters in the net . Do you have to worry about keeping that the same , or {disfmarker} ?&#10;Speaker: Professor F&#10;Content: Uh , I don't think so .&#10;Speaker: PhD B&#10;Content: There 's a computation limit , though , isn't there ?&#10;Speaker: Professor F&#10;Content: Yeah , I mean , it 's just more compu Excuse me ?&#10;Speaker: PhD B&#10;Content: Isn't there like a limit {pause} on the computation load , or d latency , or something like that for Aurora task" target="Four hundred order hidden units are being used for nets trained on digits. The plan to increase the number of units for broader class nets, which actually means using more classes, is because these broader training corpus nets like TIMIT correspond to a larger number of phoneme classes compared to the digits nets that only correspond to about twenty phonemes.">
      <data key="d0">1</data>
    </edge>
    <edge source=" putting those together .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: Um . Yeah .&#10;Speaker: PhD A&#10;Content: When you do that , you 're increasing the size of the inputs to the net . Do you have to reduce the hidden layer , or something ?&#10;Speaker: Professor F&#10;Content: Well , so {disfmarker} I mean , so i it doesn't increase the number of trainings .&#10;Speaker: PhD A&#10;Content: No , no , I 'm {disfmarker} I 'm just wondering about number of parameters in the net . Do you have to worry about keeping that the same , or {disfmarker} ?&#10;Speaker: Professor F&#10;Content: Uh , I don't think so .&#10;Speaker: PhD B&#10;Content: There 's a computation limit , though , isn't there ?&#10;Speaker: Professor F&#10;Content: Yeah , I mean , it 's just more compu Excuse me ?&#10;Speaker: PhD B&#10;Content: Isn't there like a limit {pause} on the computation load , or d latency , or something like that for Aurora task" target="The benefits of having multiple SPERT boards for running parallel jobs include:&#10;&#10;1. Increased processing speed and efficiency: As discussed in the meeting, the SPERT board is faster than other alternatives. Having multiple SPERT boards allows users to process and run tasks simultaneously, leading to time savings and increased productivity.&#10;2. Resource allocation: When running out of capacity on one SPERT board, having additional boards enables users to offload tasks and utilize their computer for other activities. This flexibility in resource allocation ensures optimal use of available hardware.&#10;3. Job management: Multiple SPERT boards allow for the setup of multiple jobs, enabling better organization and management of tasks. Users can run different jobs on separate boards and have several jobs running on various computers simultaneously, increasing overall throughput.&#10;4. Parallel processing: Running parallel jobs on multiple SPERT boards is particularly useful in machine learning applications where large data sets require significant processing power. This allows researchers to process massive amounts of data more efficiently than relying on a single board or computer.&#10;5. Improved collaboration: By dedicating specific sections within the cube for people to work on, as suggested during the discussion, multiple SPERT boards can foster better collaboration among team members working on different aspects of a project. This facilitates the division of labor and promotes a more efficient workflow.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker} and they 're han hand uh {disfmarker} hand - marked . Uh , I guess , actually , TIMIT was not entirely hand - marked . It was automatically first , and then hand {disfmarker} hand - corrected .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: Professor F&#10;Content: But {disfmarker} but , um , uh , it {disfmarker} it , um , it might be a better source . So , i it 's {disfmarker} you 're right . It would be another interesting scientific question to ask , &quot; Is it because it 's a broad source or because it was , you know , carefully ? &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: uh . And that 's something you could ask , but given limited time , I think the main thing is if it 's a better thing for going across languages on this training tandem system ,&#10;Speaker: PhD A&#10;Content: Yeah . Right .&#10;Speaker: Professor F&#10;Content: then it 's probably {disfmarker}&#10;Speaker:" target="If they are able to successfully get all the specified features correct in an experiment, it would mean that they have defined and correctly identified all the phones (distinct units of sound) in the data. This is significant because phones are the building blocks of speech and language, and accurately identifying them is crucial for many applications such as speech recognition and synthesis. Professor F also mentions that this would be equivalent to saying that all the phones are correct. However, it's important to note that just getting all the features right may not necessarily guarantee perfect results in other aspects of their work, such as normalization or word discrimination, which are also discussed in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker} and they 're han hand uh {disfmarker} hand - marked . Uh , I guess , actually , TIMIT was not entirely hand - marked . It was automatically first , and then hand {disfmarker} hand - corrected .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: Professor F&#10;Content: But {disfmarker} but , um , uh , it {disfmarker} it , um , it might be a better source . So , i it 's {disfmarker} you 're right . It would be another interesting scientific question to ask , &quot; Is it because it 's a broad source or because it was , you know , carefully ? &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: uh . And that 's something you could ask , but given limited time , I think the main thing is if it 's a better thing for going across languages on this training tandem system ,&#10;Speaker: PhD A&#10;Content: Yeah . Right .&#10;Speaker: Professor F&#10;Content: then it 's probably {disfmarker}&#10;Speaker:" target="Based on the discussion, it is suggested that using the TIMIT dataset as a source for training a tandem system across languages could be a better choice due to its broadness and careful hand-correction. However, given the limited time and resources, the speakers did not explicitly conclude that the TIMIT dataset's broadness or careful hand-correction is the sole reason for its potential superiority. Further investigation would be required to make a definitive conclusion.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are discussing the use of a neural network to classify phonemes using English neural networks that were trained with phonemes and seventeen broad classes. They mention the idea of creating a superset of broad phoneme classes, which could be used for cross-language handling. However, one speaker thinks there is something wrong with this approach, suggesting that using the same language for training the neural network and having a phonetically balanced database would be better.&#10;&#10;Later in the discussion, they talk about potential issues when discriminating words based on the output of the neural network. They mention that this may not provide enough information to distinguish between words due to the fact that most confusions occur within phone classes. This is because words can differ with only a single phoneme, which could be in the same broad class. The speakers note that this might have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. Additionally, they mention a glottal stop in the TIMIT dataset, which is a short period of silence, and this might also be relevant for the mapping process." target="aker: PhD G&#10;Content: I would say two days .&#10;Speaker: PhD A&#10;Content: Depends on the corpuses , right ?&#10;Speaker: PhD E&#10;Content: It depends .&#10;Speaker: PhD B&#10;Content: It s also depends on the net .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Depends on the corpus .&#10;Speaker: PhD B&#10;Content: How big is the net ?&#10;Speaker: PhD E&#10;Content: For Albayzin I trained on neural network , uh , was , um , one day also .&#10;Speaker: Professor F&#10;Content: Uh , but on what machine ?&#10;Speaker: Grad C&#10;Content: On a SPERT board .&#10;Speaker: PhD E&#10;Content: Uh . I {disfmarker} I think the neural net SPERT .&#10;Speaker: Grad C&#10;Content: Y you did a {disfmarker} you did it on a SPERT board .&#10;Speaker: PhD E&#10;Content: Yes .&#10;Speaker: Professor F&#10;Content: OK , again , we do have a bunch of">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are discussing the use of a neural network to classify phonemes using English neural networks that were trained with phonemes and seventeen broad classes. They mention the idea of creating a superset of broad phoneme classes, which could be used for cross-language handling. However, one speaker thinks there is something wrong with this approach, suggesting that using the same language for training the neural network and having a phonetically balanced database would be better.&#10;&#10;Later in the discussion, they talk about potential issues when discriminating words based on the output of the neural network. They mention that this may not provide enough information to distinguish between words due to the fact that most confusions occur within phone classes. This is because words can differ with only a single phoneme, which could be in the same broad class. The speakers note that this might have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. Additionally, they mention a glottal stop in the TIMIT dataset, which is a short period of silence, and this might also be relevant for the mapping process." target=" class .&#10;Speaker: Professor F&#10;Content: I see .&#10;Speaker: PhD G&#10;Content: Well . So .&#10;Speaker: Professor F&#10;Content: Right . Although , you are not using this for the {disfmarker}&#10;Speaker: PhD G&#10;Content: So , I 'm&#10;Speaker: Professor F&#10;Content: You 're using this for the feature generation , though , not the {disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah , but you will ask the net to put one for th th the phoneme class&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: and {disfmarker} So .&#10;Speaker: PhD A&#10;Content: So you 're saying that there may not be enough information coming out of the net to help you discriminate the words ?&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Well . Yeah , yeah . Mmm .&#10;Speaker: PhD B&#10;Content: Fact , most confusions are within the phone {disfmarker} phone classes , right ? I think , uh , Larry was saying like obstruents are">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are discussing the use of a neural network to classify phonemes using English neural networks that were trained with phonemes and seventeen broad classes. They mention the idea of creating a superset of broad phoneme classes, which could be used for cross-language handling. However, one speaker thinks there is something wrong with this approach, suggesting that using the same language for training the neural network and having a phonetically balanced database would be better.&#10;&#10;Later in the discussion, they talk about potential issues when discriminating words based on the output of the neural network. They mention that this may not provide enough information to distinguish between words due to the fact that most confusions occur within phone classes. This is because words can differ with only a single phoneme, which could be in the same broad class. The speakers note that this might have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. Additionally, they mention a glottal stop in the TIMIT dataset, which is a short period of silence, and this might also be relevant for the mapping process." target="One major criticism against current speech recognition systems is that they throw away all the harmonicity information and try to get spectral envelopes, discarding most of the information about the phonetic identity which is contained in the harmonics. Incorporating pitch or harmonicity information could address this issue by preserving some of this lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are discussing the use of a neural network to classify phonemes using English neural networks that were trained with phonemes and seventeen broad classes. They mention the idea of creating a superset of broad phoneme classes, which could be used for cross-language handling. However, one speaker thinks there is something wrong with this approach, suggesting that using the same language for training the neural network and having a phonetically balanced database would be better.&#10;&#10;Later in the discussion, they talk about potential issues when discriminating words based on the output of the neural network. They mention that this may not provide enough information to distinguish between words due to the fact that most confusions occur within phone classes. This is because words can differ with only a single phoneme, which could be in the same broad class. The speakers note that this might have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. Additionally, they mention a glottal stop in the TIMIT dataset, which is a short period of silence, and this might also be relevant for the mapping process." target="The &quot;unusual number&quot; the speakers refer to is the larger number of phone symbols used in their recognition system, which is 120 compared to the typical 39 in other systems. This larger set allows for a more nuanced identification of sounds, as it includes additional phonetic context beyond the basic sound itself. The use of this larger set enables the system to bring in context and better distinguish between similar sounds based on their surrounding phonetic environment.&#10;&#10;This approach aims to improve accuracy by considering how individual phones are influenced by neighboring sounds within words or phrases, which can help resolve ambiguities that may arise from using a smaller phone set. However, this increased level of detail also presents challenges in accurately training and fine-tuning the recognition system to effectively use this larger phone set for speech processing tasks.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are discussing the use of a neural network to classify phonemes using English neural networks that were trained with phonemes and seventeen broad classes. They mention the idea of creating a superset of broad phoneme classes, which could be used for cross-language handling. However, one speaker thinks there is something wrong with this approach, suggesting that using the same language for training the neural network and having a phonetically balanced database would be better.&#10;&#10;Later in the discussion, they talk about potential issues when discriminating words based on the output of the neural network. They mention that this may not provide enough information to distinguish between words due to the fact that most confusions occur within phone classes. This is because words can differ with only a single phoneme, which could be in the same broad class. The speakers note that this might have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. Additionally, they mention a glottal stop in the TIMIT dataset, which is a short period of silence, and this might also be relevant for the mapping process." target="1. The discussion was focused on the observation that most confusions in phone classification occur within the same broad classes, such as obstruents being confused with other obstruents. This led to the idea of examining articulatory types as a potential solution.&#10;2. The participants also discussed the possibility of using a multi-valued or multi-dimensional representation for articulatory features, allowing for the modeling of various combinations of articulatory gestures and a more nuanced understanding of speech sounds.&#10;3. Articulatory features were considered as targets for the neural network, which describe the position and movement of speech organs when producing sounds, typically used in speech synthesis and analysis.&#10;4. The challenge of addressing differences in phone sets between different databases and languages was mentioned, with a specific example of mapping from 61 phonemes in TIMIT to 56 phonemes in ICSI given.&#10;&#10;In summary, the discussion centered around using articulatory type information for classifying phones, considering multiple features being active at once, and addressing challenges posed by phone set differences between databases and languages.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are discussing the use of a neural network to classify phonemes using English neural networks that were trained with phonemes and seventeen broad classes. They mention the idea of creating a superset of broad phoneme classes, which could be used for cross-language handling. However, one speaker thinks there is something wrong with this approach, suggesting that using the same language for training the neural network and having a phonetically balanced database would be better.&#10;&#10;Later in the discussion, they talk about potential issues when discriminating words based on the output of the neural network. They mention that this may not provide enough information to distinguish between words due to the fact that most confusions occur within phone classes. This is because words can differ with only a single phoneme, which could be in the same broad class. The speakers note that this might have contributed to mapping down the system to fifty-six phones in the TIMIT dataset used in their discriminative system. Additionally, they mention a glottal stop in the TIMIT dataset, which is a short period of silence, and this might also be relevant for the mapping process." target="Based on the discussion, it is suggested that using the TIMIT dataset as a source for training a tandem system across languages could be a better choice due to its broadness and careful hand-correction. However, given the limited time and resources, the speakers did not explicitly conclude that the TIMIT dataset's broadness or careful hand-correction is the sole reason for its potential superiority. Further investigation would be required to make a definitive conclusion.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD G&#10;Content: I would say two days .&#10;Speaker: PhD A&#10;Content: Depends on the corpuses , right ?&#10;Speaker: PhD E&#10;Content: It depends .&#10;Speaker: PhD B&#10;Content: It s also depends on the net .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Depends on the corpus .&#10;Speaker: PhD B&#10;Content: How big is the net ?&#10;Speaker: PhD E&#10;Content: For Albayzin I trained on neural network , uh , was , um , one day also .&#10;Speaker: Professor F&#10;Content: Uh , but on what machine ?&#10;Speaker: Grad C&#10;Content: On a SPERT board .&#10;Speaker: PhD E&#10;Content: Uh . I {disfmarker} I think the neural net SPERT .&#10;Speaker: Grad C&#10;Content: Y you did a {disfmarker} you did it on a SPERT board .&#10;Speaker: PhD E&#10;Content: Yes .&#10;Speaker: Professor F&#10;Content: OK , again , we do have a bunch of" target="The benefits of having multiple SPERT boards for running parallel jobs include:&#10;&#10;1. Increased processing speed and efficiency: As discussed in the meeting, the SPERT board is faster than other alternatives. Having multiple SPERT boards allows users to process and run tasks simultaneously, leading to time savings and increased productivity.&#10;2. Resource allocation: When running out of capacity on one SPERT board, having additional boards enables users to offload tasks and utilize their computer for other activities. This flexibility in resource allocation ensures optimal use of available hardware.&#10;3. Job management: Multiple SPERT boards allow for the setup of multiple jobs, enabling better organization and management of tasks. Users can run different jobs on separate boards and have several jobs running on various computers simultaneously, increasing overall throughput.&#10;4. Parallel processing: Running parallel jobs on multiple SPERT boards is particularly useful in machine learning applications where large data sets require significant processing power. This allows researchers to process massive amounts of data more efficiently than relying on a single board or computer.&#10;5. Improved collaboration: By dedicating specific sections within the cube for people to work on, as suggested during the discussion, multiple SPERT boards can foster better collaboration among team members working on different aspects of a project. This facilitates the division of labor and promotes a more efficient workflow.">
      <data key="d0">1</data>
    </edge>
    <edge source=" class .&#10;Speaker: Professor F&#10;Content: I see .&#10;Speaker: PhD G&#10;Content: Well . So .&#10;Speaker: Professor F&#10;Content: Right . Although , you are not using this for the {disfmarker}&#10;Speaker: PhD G&#10;Content: So , I 'm&#10;Speaker: Professor F&#10;Content: You 're using this for the feature generation , though , not the {disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah , but you will ask the net to put one for th th the phoneme class&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: and {disfmarker} So .&#10;Speaker: PhD A&#10;Content: So you 're saying that there may not be enough information coming out of the net to help you discriminate the words ?&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Well . Yeah , yeah . Mmm .&#10;Speaker: PhD B&#10;Content: Fact , most confusions are within the phone {disfmarker} phone classes , right ? I think , uh , Larry was saying like obstruents are" target="If they are able to successfully get all the specified features correct in an experiment, it would mean that they have defined and correctly identified all the phones (distinct units of sound) in the data. This is significant because phones are the building blocks of speech and language, and accurately identifying them is crucial for many applications such as speech recognition and synthesis. Professor F also mentions that this would be equivalent to saying that all the phones are correct. However, it's important to note that just getting all the features right may not necessarily guarantee perfect results in other aspects of their work, such as normalization or word discrimination, which are also discussed in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source=" class .&#10;Speaker: Professor F&#10;Content: I see .&#10;Speaker: PhD G&#10;Content: Well . So .&#10;Speaker: Professor F&#10;Content: Right . Although , you are not using this for the {disfmarker}&#10;Speaker: PhD G&#10;Content: So , I 'm&#10;Speaker: Professor F&#10;Content: You 're using this for the feature generation , though , not the {disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah , but you will ask the net to put one for th th the phoneme class&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: and {disfmarker} So .&#10;Speaker: PhD A&#10;Content: So you 're saying that there may not be enough information coming out of the net to help you discriminate the words ?&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Well . Yeah , yeah . Mmm .&#10;Speaker: PhD B&#10;Content: Fact , most confusions are within the phone {disfmarker} phone classes , right ? I think , uh , Larry was saying like obstruents are" target="Professor F is trying to clarify that despite the upcoming upgrade of the networks in their place, which will improve the network speeds, currently they are still limited by pretty much all ten megabit lines. He mentions that this is becoming a significant drag on their work because the machines and computers they use are getting faster and faster, but the network speeds are unable to keep up with them.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. HTK Training and Testing: The speakers discuss conducting tests using the Hidden Markov Model Toolkit (HTK) system. This involves training the HTK models with one set of data and testing it with another. The computational cost and time required for an HTK training, which is around six hours or more, are considered.&#10;2. Noise Condition Tests: Grad D brings up the topic of noise conditions, questioning whether they should include a column for it in their analysis. Three types of noise conditions (A, B, and C) will be used for testing the model's performance with different noisy environments. Type A is matched noise, Type B is slightly mismatched, and Type C is mismatched channel noise.&#10;3. Cross-Language Testing: PhD G mentions a cross-language testing scenario where results showed better performance. This raises an interesting question about whether they should leverage this method for their own testing purposes.&#10;4. Training Corpus Tests: The discussion also involves tests based on the training corpus, specifically for the ANN (Artificial Neural Network) model. The team considers various features and different combinations of the training corpus to optimize the ANN performance.&#10;5. Discriminant Neural Net Testing: Grad C mentions a test related to the discriminant neural net, where they analyze the first dimension of features used in the testing process." target=" there is the testing also , which implies training , uh , the HTK models&#10;Speaker: PhD E&#10;Content: The {disfmarker} the model {disfmarker} the HTK model .&#10;Speaker: PhD G&#10;Content: and , well ,&#10;Speaker: Professor F&#10;Content: Uh , right .&#10;Speaker: PhD G&#10;Content: it 's {disfmarker}&#10;Speaker: Professor F&#10;Content: Right . So if you do have lots of combinations , it 's {disfmarker}&#10;Speaker: PhD G&#10;Content: yeah . But it 's {disfmarker} it 's {disfmarker} it 's not so long . It @ @ {disfmarker} Yeah .&#10;Speaker: Professor F&#10;Content: How long does it take for an , uh , HTK training ?&#10;Speaker: PhD G&#10;Content: It 's around six hours , I think .&#10;Speaker: PhD E&#10;Content: It depends on the {disfmarker}&#10;Speaker: PhD G&#10;Content: For training and testing , yeah .&#10;Speaker: PhD E&#10;Content: More than six hours">
      <data key="d0">1</data>
    </edge>
    <edge source="1. HTK Training and Testing: The speakers discuss conducting tests using the Hidden Markov Model Toolkit (HTK) system. This involves training the HTK models with one set of data and testing it with another. The computational cost and time required for an HTK training, which is around six hours or more, are considered.&#10;2. Noise Condition Tests: Grad D brings up the topic of noise conditions, questioning whether they should include a column for it in their analysis. Three types of noise conditions (A, B, and C) will be used for testing the model's performance with different noisy environments. Type A is matched noise, Type B is slightly mismatched, and Type C is mismatched channel noise.&#10;3. Cross-Language Testing: PhD G mentions a cross-language testing scenario where results showed better performance. This raises an interesting question about whether they should leverage this method for their own testing purposes.&#10;4. Training Corpus Tests: The discussion also involves tests based on the training corpus, specifically for the ANN (Artificial Neural Network) model. The team considers various features and different combinations of the training corpus to optimize the ANN performance.&#10;5. Discriminant Neural Net Testing: Grad C mentions a test related to the discriminant neural net, where they analyze the first dimension of features used in the testing process." target=" 's tethered .&#10;Speaker: Grad C&#10;Content: The first dimension is the {disfmarker} the features that we 're going to use . And the second dimension , um , is the training corpus . And that 's the training on the discriminant neural net . Um and the last dimension happens to be {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah and again {disfmarker} Yeah . So the {disfmarker} the training for HTK is always {disfmarker} that 's always set up for the individual test , right ? That there 's some training data and some test data . So that 's different than this .&#10;Speaker: Grad C&#10;Content: Right , right . This is {disfmarker} this is for {disfmarker} for ANN only . And , yeah , the training for the HTK models is always , uh , fixed for whatever language you 're testing on .&#10;Speaker: Professor F&#10;Content: Right .&#10;Speaker: Grad C&#10;Content: And then , there 's the testing corpus . So , then I think it 's probably instructive to go and {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. HTK Training and Testing: The speakers discuss conducting tests using the Hidden Markov Model Toolkit (HTK) system. This involves training the HTK models with one set of data and testing it with another. The computational cost and time required for an HTK training, which is around six hours or more, are considered.&#10;2. Noise Condition Tests: Grad D brings up the topic of noise conditions, questioning whether they should include a column for it in their analysis. Three types of noise conditions (A, B, and C) will be used for testing the model's performance with different noisy environments. Type A is matched noise, Type B is slightly mismatched, and Type C is mismatched channel noise.&#10;3. Cross-Language Testing: PhD G mentions a cross-language testing scenario where results showed better performance. This raises an interesting question about whether they should leverage this method for their own testing purposes.&#10;4. Training Corpus Tests: The discussion also involves tests based on the training corpus, specifically for the ANN (Artificial Neural Network) model. The team considers various features and different combinations of the training corpus to optimize the ANN performance.&#10;5. Discriminant Neural Net Testing: Grad C mentions a test related to the discriminant neural net, where they analyze the first dimension of features used in the testing process." target=" or {disfmarker} ?&#10;Speaker: PhD G&#10;Content: Yeah . Yeah , I think so .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD G&#10;Content: Uh , and , yeah . And the result was that apparently , when testing on cross - language it was better . I think so . But Hynek didn't add {disfmarker} didn't have all the results when he showed me that , so , well .&#10;Speaker: Professor F&#10;Content: So that does make an interesting question , though .&#10;Speaker: PhD G&#10;Content: But {disfmarker}&#10;Speaker: Professor F&#10;Content: Is there 's some way that we should tie into that with this . Um . Right ? I mean , if {disfmarker} if in fact that is a better thing to do , {pause} should we leverage that , rather than doing , {pause} um , our own . Right ? So , if i if {disfmarker} if they s I mean , we have {disfmarker} {pause} i we have the {disfmarker} the trainings with our own categories . And">
      <data key="d0">1</data>
    </edge>
    <edge source="1. HTK Training and Testing: The speakers discuss conducting tests using the Hidden Markov Model Toolkit (HTK) system. This involves training the HTK models with one set of data and testing it with another. The computational cost and time required for an HTK training, which is around six hours or more, are considered.&#10;2. Noise Condition Tests: Grad D brings up the topic of noise conditions, questioning whether they should include a column for it in their analysis. Three types of noise conditions (A, B, and C) will be used for testing the model's performance with different noisy environments. Type A is matched noise, Type B is slightly mismatched, and Type C is mismatched channel noise.&#10;3. Cross-Language Testing: PhD G mentions a cross-language testing scenario where results showed better performance. This raises an interesting question about whether they should leverage this method for their own testing purposes.&#10;4. Training Corpus Tests: The discussion also involves tests based on the training corpus, specifically for the ANN (Artificial Neural Network) model. The team considers various features and different combinations of the training corpus to optimize the ANN performance.&#10;5. Discriminant Neural Net Testing: Grad C mentions a test related to the discriminant neural net, where they analyze the first dimension of features used in the testing process." target="Based on the information provided in the transcripts, it is stated that an HTK training takes around six hours or more. The discussion includes testing the model's performance on three different types of noise conditions (matched, mismatched, and mismatched channel) using a script. However, there is no explicit mention of the time required to complete a training session with the specific configuration you mentioned. To provide an accurate answer, further details or specifications about the computational cost associated with this particular scenario are needed.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. HTK Training and Testing: The speakers discuss conducting tests using the Hidden Markov Model Toolkit (HTK) system. This involves training the HTK models with one set of data and testing it with another. The computational cost and time required for an HTK training, which is around six hours or more, are considered.&#10;2. Noise Condition Tests: Grad D brings up the topic of noise conditions, questioning whether they should include a column for it in their analysis. Three types of noise conditions (A, B, and C) will be used for testing the model's performance with different noisy environments. Type A is matched noise, Type B is slightly mismatched, and Type C is mismatched channel noise.&#10;3. Cross-Language Testing: PhD G mentions a cross-language testing scenario where results showed better performance. This raises an interesting question about whether they should leverage this method for their own testing purposes.&#10;4. Training Corpus Tests: The discussion also involves tests based on the training corpus, specifically for the ANN (Artificial Neural Network) model. The team considers various features and different combinations of the training corpus to optimize the ANN performance.&#10;5. Discriminant Neural Net Testing: Grad C mentions a test related to the discriminant neural net, where they analyze the first dimension of features used in the testing process." target="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is because the model will have limited exposure and experience in handling diverse types of unseen noise, which could lead to suboptimal performance when dealing with real-world scenarios where various kinds of noises are present.&#10;   &#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus. By training with seen noise conditions, the model can become more robust in handling those specific types of noises, improving its overall performance in noisy environments that it has encountered before.&#10;   &#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments. While this doesn't directly address the issue of not being able to train on unseen noise conditions, combination testing can provide valuable insights into how the model performs under various noisy scenarios and potentially lead to improvements in the model's ability to generalize to new, unseen noise conditions.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. HTK Training and Testing: The speakers discuss conducting tests using the Hidden Markov Model Toolkit (HTK) system. This involves training the HTK models with one set of data and testing it with another. The computational cost and time required for an HTK training, which is around six hours or more, are considered.&#10;2. Noise Condition Tests: Grad D brings up the topic of noise conditions, questioning whether they should include a column for it in their analysis. Three types of noise conditions (A, B, and C) will be used for testing the model's performance with different noisy environments. Type A is matched noise, Type B is slightly mismatched, and Type C is mismatched channel noise.&#10;3. Cross-Language Testing: PhD G mentions a cross-language testing scenario where results showed better performance. This raises an interesting question about whether they should leverage this method for their own testing purposes.&#10;4. Training Corpus Tests: The discussion also involves tests based on the training corpus, specifically for the ANN (Artificial Neural Network) model. The team considers various features and different combinations of the training corpus to optimize the ANN performance.&#10;5. Discriminant Neural Net Testing: Grad C mentions a test related to the discriminant neural net, where they analyze the first dimension of features used in the testing process." target="The transcript mentions that training a neural network on multiple languages is probably done using a single network, as suggested by PhD G's statement &quot;probably one net.&quot; The specifics of this method are not detailed in the conversation; however, it can be inferred that this would involve combining data from different languages during the training process.&#10;&#10;The speakers also discuss combining neural networks trained on individual languages with a more generic database, as stated by PhD G: &quot;training a net on the target languages but with a general database.&quot; It is important to note that the conversation does not explicitly confirm whether this method has been implemented or not.&#10;&#10;In summary, while the discussion does imply training a neural network using multiple languages in one network and combining individually trained networks, it does not provide explicit details about these methods.">
      <data key="d0">1</data>
    </edge>
    <edge source=" there is the testing also , which implies training , uh , the HTK models&#10;Speaker: PhD E&#10;Content: The {disfmarker} the model {disfmarker} the HTK model .&#10;Speaker: PhD G&#10;Content: and , well ,&#10;Speaker: Professor F&#10;Content: Uh , right .&#10;Speaker: PhD G&#10;Content: it 's {disfmarker}&#10;Speaker: Professor F&#10;Content: Right . So if you do have lots of combinations , it 's {disfmarker}&#10;Speaker: PhD G&#10;Content: yeah . But it 's {disfmarker} it 's {disfmarker} it 's not so long . It @ @ {disfmarker} Yeah .&#10;Speaker: Professor F&#10;Content: How long does it take for an , uh , HTK training ?&#10;Speaker: PhD G&#10;Content: It 's around six hours , I think .&#10;Speaker: PhD E&#10;Content: It depends on the {disfmarker}&#10;Speaker: PhD G&#10;Content: For training and testing , yeah .&#10;Speaker: PhD E&#10;Content: More than six hours" target="Based on the information provided in the transcripts, it is stated that an HTK training takes around six hours or more. The discussion includes testing the model's performance on three different types of noise conditions (matched, mismatched, and mismatched channel) using a script. However, there is no explicit mention of the time required to complete a training session with the specific configuration you mentioned. To provide an accurate answer, further details or specifications about the computational cost associated with this particular scenario are needed.">
      <data key="d0">1</data>
    </edge>
    <edge source=" there is the testing also , which implies training , uh , the HTK models&#10;Speaker: PhD E&#10;Content: The {disfmarker} the model {disfmarker} the HTK model .&#10;Speaker: PhD G&#10;Content: and , well ,&#10;Speaker: Professor F&#10;Content: Uh , right .&#10;Speaker: PhD G&#10;Content: it 's {disfmarker}&#10;Speaker: Professor F&#10;Content: Right . So if you do have lots of combinations , it 's {disfmarker}&#10;Speaker: PhD G&#10;Content: yeah . But it 's {disfmarker} it 's {disfmarker} it 's not so long . It @ @ {disfmarker} Yeah .&#10;Speaker: Professor F&#10;Content: How long does it take for an , uh , HTK training ?&#10;Speaker: PhD G&#10;Content: It 's around six hours , I think .&#10;Speaker: PhD E&#10;Content: It depends on the {disfmarker}&#10;Speaker: PhD G&#10;Content: For training and testing , yeah .&#10;Speaker: PhD E&#10;Content: More than six hours" target="Based on the information provided in the transcripts, specifically the discussion about HTK training taking around six hours or more, and considering that there will be some computational overhead for computing the KL transformation when testing combinations of built-in training programs like PLP, MSG, and JRA, it can be estimated that conducting experiments on these combinations might take a significant amount of time. However, the exact duration is not explicitly mentioned in the transcripts.&#10;&#10;To provide an accurate answer, additional information about the computational resources available for running these experiments would be helpful. Factors such as the number of possible combinations, the complexity of each combination, and the computational power of the hardware being used would all impact the total time required to complete the experiments.&#10;&#10;In summary, while it is not possible to provide a precise estimate based on the given transcripts, conducting experiments on combinations of built-in training programs such as PLP, MSG, and JRA is likely to take a considerable amount of time, possibly extending beyond a few days, considering the estimated six hours or more for each HTK training session and additional computational overhead.">
      <data key="d0">1</data>
    </edge>
    <edge source="If they are able to successfully get all the specified features correct in an experiment, it would mean that they have defined and correctly identified all the phones (distinct units of sound) in the data. This is significant because phones are the building blocks of speech and language, and accurately identifying them is crucial for many applications such as speech recognition and synthesis. Professor F also mentions that this would be equivalent to saying that all the phones are correct. However, it's important to note that just getting all the features right may not necessarily guarantee perfect results in other aspects of their work, such as normalization or word discrimination, which are also discussed in the conversation." target="disfmarker}&#10;Speaker: Professor F&#10;Content: He {disfmarker} he di he didn't mention that part .&#10;Speaker: PhD A&#10;Content: Well , Hynek said that {disfmarker} that , I guess before they had him work on this , they had done some experiment where if they could get that one feature right , it dramatically improved the result .&#10;Speaker: Professor F&#10;Content: But . I see . OK .&#10;Speaker: PhD A&#10;Content: So I was thinking , you know {disfmarker} it made me think about this , that if {disfmarker} it 'd be an interesting experiment just to see , you know , if you did get all of those right .&#10;Speaker: Professor F&#10;Content: Should be . Because if you get all of them in there , that defines all of the phones . So that 's {disfmarker} that 's equivalent to saying that you 've got {disfmarker} {vocalsound} got all the phones right .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: So , if that doesn't help ,">
      <data key="d0">1</data>
    </edge>
    <edge source="If they are able to successfully get all the specified features correct in an experiment, it would mean that they have defined and correctly identified all the phones (distinct units of sound) in the data. This is significant because phones are the building blocks of speech and language, and accurately identifying them is crucial for many applications such as speech recognition and synthesis. Professor F also mentions that this would be equivalent to saying that all the phones are correct. However, it's important to note that just getting all the features right may not necessarily guarantee perfect results in other aspects of their work, such as normalization or word discrimination, which are also discussed in the conversation." target=" an another question occurred to me is {disfmarker} is what were you folks planning to do about normalization ?&#10;Speaker: PhD G&#10;Content: Um . Well , we were thinking about using this systematically for all the experiments . Um .&#10;Speaker: Professor F&#10;Content: This being {disfmarker} ?&#10;Speaker: PhD G&#10;Content: So , but {disfmarker} Uh . So that this could be another dimension , but we think perhaps we can use the {disfmarker} the best , uh , um , uh , normalization scheme as OGI is using , so , with parameters that they use there , &#10;Speaker: Professor F&#10;Content: Yeah , I think that 's a good idea .&#10;Speaker: PhD G&#10;Content: u {vocalsound} u&#10;Speaker: Professor F&#10;Content: I mean it 's i i we {disfmarker} we seem to have enough dimensions as it is . So probably if we {vocalsound} sort of take their {disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah , yeah , yeah .&#10;Speaker: Professor F&#10;Content: probably the on -">
      <data key="d0">1</data>
    </edge>
    <edge source="If they are able to successfully get all the specified features correct in an experiment, it would mean that they have defined and correctly identified all the phones (distinct units of sound) in the data. This is significant because phones are the building blocks of speech and language, and accurately identifying them is crucial for many applications such as speech recognition and synthesis. Professor F also mentions that this would be equivalent to saying that all the phones are correct. However, it's important to note that just getting all the features right may not necessarily guarantee perfect results in other aspects of their work, such as normalization or word discrimination, which are also discussed in the conversation." target=" , with a general database {disfmarker} general databases . u So that th Well , the {disfmarker} the guy who has to develop an application with one language can use the net trained o on that language , or a generic net ,&#10;Speaker: Professor F&#10;Content: Uh , depen it depen it depends how you mean &quot; using the net &quot; .&#10;Speaker: PhD G&#10;Content: but not trained on a {disfmarker}&#10;Speaker: Professor F&#10;Content: So , if you 're talking about for producing these discriminative features {pause} that we 're talking about {pause} you can't do that .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Because {disfmarker} because the {disfmarker} what they 're asking for is {disfmarker} is a feature set . Right ? And so , uh , we 're the ones who have been weird by {disfmarker} by doing this training . But if we say , &quot; No , you have to have a different feature set for each language , &quot; I think this is ver gonna be very bad .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="If they are able to successfully get all the specified features correct in an experiment, it would mean that they have defined and correctly identified all the phones (distinct units of sound) in the data. This is significant because phones are the building blocks of speech and language, and accurately identifying them is crucial for many applications such as speech recognition and synthesis. Professor F also mentions that this would be equivalent to saying that all the phones are correct. However, it's important to note that just getting all the features right may not necessarily guarantee perfect results in other aspects of their work, such as normalization or word discrimination, which are also discussed in the conversation." target="1. The targets of the net, as discussed in this context, appear to be articulatory features. Articulatory features are a way to describe the position and movement of speech organs when producing sounds. They are typically used in speech synthesis and analysis.&#10;2. Yes, articulatory features imply that multiple features can be active at the same time in this discussion. Professor F confirms that having more than one articulatory feature active at a time is correct, indicating that they are considering a multi-valued or multi-dimensional representation for these features. This allows for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures.">
      <data key="d0">1</data>
    </edge>
    <edge source="If they are able to successfully get all the specified features correct in an experiment, it would mean that they have defined and correctly identified all the phones (distinct units of sound) in the data. This is significant because phones are the building blocks of speech and language, and accurately identifying them is crucial for many applications such as speech recognition and synthesis. Professor F also mentions that this would be equivalent to saying that all the phones are correct. However, it's important to note that just getting all the features right may not necessarily guarantee perfect results in other aspects of their work, such as normalization or word discrimination, which are also discussed in the conversation." target="One major criticism against current speech recognition systems is that they throw away all the harmonicity information and try to get spectral envelopes, discarding most of the information about the phonetic identity which is contained in the harmonics. Incorporating pitch or harmonicity information could address this issue by preserving some of this lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.">
      <data key="d0">1</data>
    </edge>
    <edge source="If they are able to successfully get all the specified features correct in an experiment, it would mean that they have defined and correctly identified all the phones (distinct units of sound) in the data. This is significant because phones are the building blocks of speech and language, and accurately identifying them is crucial for many applications such as speech recognition and synthesis. Professor F also mentions that this would be equivalent to saying that all the phones are correct. However, it's important to note that just getting all the features right may not necessarily guarantee perfect results in other aspects of their work, such as normalization or word discrimination, which are also discussed in the conversation." target="The goal of computing and sending features instead of actual speech in the development of new telephone standards, as discussed by the speakers, is to improve speech recognition and synthesis by accurately identifying phones (distinct units of sound) in the data. This would allow for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures. Additionally, incorporating pitch or harmonicity information could address current criticism of speech recognition systems by preserving some of the lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.&#10;&#10;One challenge in achieving this goal is the differences in phone sets between different databases and languages. In the conversation, they mention a mapping from the 61 phonemes in TIMIT to the 56 phonemes in ICSI and the differences in phone sets between the two databases. This shows that addressing the issue of phone set differences is an important step towards achieving the goal of accurately identifying phones and improving speech recognition and synthesis in new telephone standards.">
      <data key="d0">1</data>
    </edge>
    <edge source=" an another question occurred to me is {disfmarker} is what were you folks planning to do about normalization ?&#10;Speaker: PhD G&#10;Content: Um . Well , we were thinking about using this systematically for all the experiments . Um .&#10;Speaker: Professor F&#10;Content: This being {disfmarker} ?&#10;Speaker: PhD G&#10;Content: So , but {disfmarker} Uh . So that this could be another dimension , but we think perhaps we can use the {disfmarker} the best , uh , um , uh , normalization scheme as OGI is using , so , with parameters that they use there , &#10;Speaker: Professor F&#10;Content: Yeah , I think that 's a good idea .&#10;Speaker: PhD G&#10;Content: u {vocalsound} u&#10;Speaker: Professor F&#10;Content: I mean it 's i i we {disfmarker} we seem to have enough dimensions as it is . So probably if we {vocalsound} sort of take their {disfmarker}&#10;Speaker: PhD G&#10;Content: Yeah , yeah , yeah .&#10;Speaker: Professor F&#10;Content: probably the on -" target="The speakers discussed using the same normalization scheme as OGI is using for all their experiments, as this would add another dimension to their analysis. They acknowledged having enough dimensions already and thus considered adopting the normalization method from OGI to avoid increasing complexity unnecessarily.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , with a general database {disfmarker} general databases . u So that th Well , the {disfmarker} the guy who has to develop an application with one language can use the net trained o on that language , or a generic net ,&#10;Speaker: Professor F&#10;Content: Uh , depen it depen it depends how you mean &quot; using the net &quot; .&#10;Speaker: PhD G&#10;Content: but not trained on a {disfmarker}&#10;Speaker: Professor F&#10;Content: So , if you 're talking about for producing these discriminative features {pause} that we 're talking about {pause} you can't do that .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Because {disfmarker} because the {disfmarker} what they 're asking for is {disfmarker} is a feature set . Right ? And so , uh , we 're the ones who have been weird by {disfmarker} by doing this training . But if we say , &quot; No , you have to have a different feature set for each language , &quot; I think this is ver gonna be very bad .&#10;" target="1. The targets of the net, as discussed in this context, appear to be articulatory features. Articulatory features are a way to describe the position and movement of speech organs when producing sounds. They are typically used in speech synthesis and analysis.&#10;2. Yes, articulatory features imply that multiple features can be active at the same time in this discussion. Professor F confirms that having more than one articulatory feature active at a time is correct, indicating that they are considering a multi-valued or multi-dimensional representation for these features. This allows for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The targets of the net, as discussed in this context, appear to be articulatory features. Articulatory features are a way to describe the position and movement of speech organs when producing sounds. They are typically used in speech synthesis and analysis.&#10;2. Yes, articulatory features imply that multiple features can be active at the same time in this discussion. Professor F confirms that having more than one articulatory feature active at a time is correct, indicating that they are considering a multi-valued or multi-dimensional representation for these features. This allows for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures." target=" really mark articulatory features , you really wanna look at the acoustics and {disfmarker} and see where everything is , and we 're not gonna do that . So , uh , the second class way of doing it is {pause} to look at the , uh , phones that are labeled and translate them into acoustic {disfmarker} uh , uh {disfmarker} articulatory , uh , uh , features . So it won't really be right . You won't really have these overlapping {pause} things and so forth ,&#10;Speaker: PhD A&#10;Content: So the targets of the net {disfmarker} are these {disfmarker} ?&#10;Speaker: Professor F&#10;Content: but {disfmarker}&#10;Speaker: PhD A&#10;Content: Articulatory features .&#10;Speaker: Professor F&#10;Content: Articulatory feature .&#10;Speaker: PhD A&#10;Content: But that implies that you can have more than one on at a time ?&#10;Speaker: Professor F&#10;Content: Right . That 's right .&#10;Speaker: PhD A&#10;Content: Ah . OK .&#10;Speaker: Professor F&#10;Content: You either do that or">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The targets of the net, as discussed in this context, appear to be articulatory features. Articulatory features are a way to describe the position and movement of speech organs when producing sounds. They are typically used in speech synthesis and analysis.&#10;2. Yes, articulatory features imply that multiple features can be active at the same time in this discussion. Professor F confirms that having more than one articulatory feature active at a time is correct, indicating that they are considering a multi-valued or multi-dimensional representation for these features. This allows for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures." target=" be done {disfmarker}&#10;Speaker: PhD B&#10;Content: Um .&#10;Speaker: Professor F&#10;Content: is that we could {disfmarker} we could , uh , just translate {disfmarker} instead of translating to a superset , {pause} just translate to articulatory features , some set of articulatory features and train with that . Now the fact {disfmarker} even though it 's a smaller number , {pause} it 's still fine because you have the {disfmarker} the , uh , combinations . So , in fact , it has every , you know {disfmarker} it had {disfmarker} has {disfmarker} has every distinction in it that you would have the other way .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: But it should go across languages better .&#10;Speaker: PhD A&#10;Content: We could do an interesting cheating experiment with that too . We could {disfmarker} I don't know , if you had uh the phone labels , you could replace them by their articulatory features and then feed in a vector with those uh , things">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The targets of the net, as discussed in this context, appear to be articulatory features. Articulatory features are a way to describe the position and movement of speech organs when producing sounds. They are typically used in speech synthesis and analysis.&#10;2. Yes, articulatory features imply that multiple features can be active at the same time in this discussion. Professor F confirms that having more than one articulatory feature active at a time is correct, indicating that they are considering a multi-valued or multi-dimensional representation for these features. This allows for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures." target="marker} which I {disfmarker} I guess they 're starting to look at up there , {comment} training to something more like articulatory features . Uh , and if you have something that 's just good for distinguishing different articulatory features that should just be good across , you know , a wide range of languages .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Uh , but {disfmarker} Yeah , so I don't th I know {disfmarker} unfortunately I don't {disfmarker} I see what you 're comi where you 're coming from , I think , but I don't think we can ignore it .&#10;Speaker: PhD G&#10;Content: So we {disfmarker} we really have to do test with a real cross - language . I mean , tr for instance training on English and testing on Italian , or {disfmarker} Or we can train {disfmarker} or else , uh , can we train a net on , uh , a range of languages and {disfmarker} which can include the test {disfmarker} the test @ @ the target language ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The targets of the net, as discussed in this context, appear to be articulatory features. Articulatory features are a way to describe the position and movement of speech organs when producing sounds. They are typically used in speech synthesis and analysis.&#10;2. Yes, articulatory features imply that multiple features can be active at the same time in this discussion. Professor F confirms that having more than one articulatory feature active at a time is correct, indicating that they are considering a multi-valued or multi-dimensional representation for these features. This allows for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures." target="One major criticism against current speech recognition systems is that they throw away all the harmonicity information and try to get spectral envelopes, discarding most of the information about the phonetic identity which is contained in the harmonics. Incorporating pitch or harmonicity information could address this issue by preserving some of this lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The targets of the net, as discussed in this context, appear to be articulatory features. Articulatory features are a way to describe the position and movement of speech organs when producing sounds. They are typically used in speech synthesis and analysis.&#10;2. Yes, articulatory features imply that multiple features can be active at the same time in this discussion. Professor F confirms that having more than one articulatory feature active at a time is correct, indicating that they are considering a multi-valued or multi-dimensional representation for these features. This allows for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures." target="The goal of computing and sending features instead of actual speech in the development of new telephone standards, as discussed by the speakers, is to improve speech recognition and synthesis by accurately identifying phones (distinct units of sound) in the data. This would allow for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures. Additionally, incorporating pitch or harmonicity information could address current criticism of speech recognition systems by preserving some of the lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.&#10;&#10;One challenge in achieving this goal is the differences in phone sets between different databases and languages. In the conversation, they mention a mapping from the 61 phonemes in TIMIT to the 56 phonemes in ICSI and the differences in phone sets between the two databases. This shows that addressing the issue of phone set differences is an important step towards achieving the goal of accurately identifying phones and improving speech recognition and synthesis in new telephone standards.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The targets of the net, as discussed in this context, appear to be articulatory features. Articulatory features are a way to describe the position and movement of speech organs when producing sounds. They are typically used in speech synthesis and analysis.&#10;2. Yes, articulatory features imply that multiple features can be active at the same time in this discussion. Professor F confirms that having more than one articulatory feature active at a time is correct, indicating that they are considering a multi-valued or multi-dimensional representation for these features. This allows for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures." target="1. The discussion was focused on the observation that most confusions in phone classification occur within the same broad classes, such as obstruents being confused with other obstruents. This led to the idea of examining articulatory types as a potential solution.&#10;2. The participants also discussed the possibility of using a multi-valued or multi-dimensional representation for articulatory features, allowing for the modeling of various combinations of articulatory gestures and a more nuanced understanding of speech sounds.&#10;3. Articulatory features were considered as targets for the neural network, which describe the position and movement of speech organs when producing sounds, typically used in speech synthesis and analysis.&#10;4. The challenge of addressing differences in phone sets between different databases and languages was mentioned, with a specific example of mapping from 61 phonemes in TIMIT to 56 phonemes in ICSI given.&#10;&#10;In summary, the discussion centered around using articulatory type information for classifying phones, considering multiple features being active at once, and addressing challenges posed by phone set differences between databases and languages.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speaker, presumably Professor F, is arranging to meet with the group at 6 AM in front of an unspecified location. If there are any delays, Professor F mentioned that they will be fine and do not need to be woken up, but they offered to provide their phone number just in case." target="&#10;Speaker: Grad C&#10;Content: s So just {disfmarker}&#10;Speaker: Professor F&#10;Content: W you 're staying overnight . I figured you wouldn't need a great big suitcase , yeah .&#10;Speaker: PhD G&#10;Content: Oh yeah . Yeah .&#10;Speaker: Professor F&#10;Content: That 's sort of {pause} {vocalsound} one night . So . Anyway . OK .&#10;Speaker: Grad C&#10;Content: So , s six AM , in front .&#10;Speaker: Professor F&#10;Content: Six AM in front .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: Uh , I 'll be here . Uh {disfmarker} I 'll {disfmarker} I 'll {disfmarker} I 'll {disfmarker} I 'll give you my phone number , If I 'm not here for a few m after a few minutes then&#10;Speaker: Grad C&#10;Content: Wake you up .&#10;Speaker: Professor F&#10;Content: Nah , I 'll be fine . I just , uh {disfmarker} it {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="The speaker, presumably Professor F, is arranging to meet with the group at 6 AM in front of an unspecified location. If there are any delays, Professor F mentioned that they will be fine and do not need to be woken up, but they offered to provide their phone number just in case." target=" That 's right .&#10;Speaker: PhD A&#10;Content: Ah . OK .&#10;Speaker: Professor F&#10;Content: You either do that or you have multiple nets .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: Professor F&#10;Content:  Um . And , um I don't know if our software {disfmarker} this {disfmarker} if the qu versions of the Quicknet that we 're using allows for that . Do you know ?&#10;Speaker: Grad C&#10;Content: Allows for {disfmarker} ?&#10;Speaker: Professor F&#10;Content: Multiple targets being one ?&#10;Speaker: Grad C&#10;Content: Oh , um , we have gotten soft targets to {disfmarker} to work .&#10;Speaker: Professor F&#10;Content: OK . So that {disfmarker} that 'll work , yeah .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: OK . So , um , that 's another thing that could be done {disfmarker}&#10;Speaker: PhD B&#10;Content: Um .&#10;Speaker: Professor F&#10;Content: is that we could">
      <data key="d0">1</data>
    </edge>
    <edge source="The speaker, presumably Professor F, is arranging to meet with the group at 6 AM in front of an unspecified location. If there are any delays, Professor F mentioned that they will be fine and do not need to be woken up, but they offered to provide their phone number just in case." target=" , most confusions are within the phone {disfmarker} phone classes , right ? I think , uh , Larry was saying like obstruents are only confused with other obstruents , et cetera , et cetera .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor F&#10;Content: Yeah . Yeah .&#10;Speaker: PhD G&#10;Content: Yeah , this is another p yeah , another point .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: So {disfmarker} so , maybe we could look at articulatory type stuff ,&#10;Speaker: Professor F&#10;Content: But that 's what I thought they were gonna {disfmarker}&#10;Speaker: Grad C&#10;Content: right ?&#10;Speaker: Professor F&#10;Content: Did they not do that , or {disfmarker} ?&#10;Speaker: PhD G&#10;Content: I don't think so . Well ,&#10;Speaker: Professor F&#10;Content: So {disfmarker}&#10;Speaker: PhD G&#10;Content: they were talking about , perhaps , but they d&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The speaker, presumably Professor F, is arranging to meet with the group at 6 AM in front of an unspecified location. If there are any delays, Professor F mentioned that they will be fine and do not need to be woken up, but they offered to provide their phone number just in case." target="1. Purpose of the trip: The purpose of the trip is not explicitly stated in the transcript, but it involves Professor F, PhD G, and possibly others, conducting experiments and working on a project, which seems to involve analyzing audio recordings based on the context.&#10;2. Preparations for Dave's contribution: In response to Dave's offer to contribute to the project, they are discussing where his help would be most useful. PhD G suggests having Dave focus on research aspects like implementing speech noise detection or voiced-unvoiced sound detection algorithms in addition to the ongoing experiments. The team should consider how best to integrate Dave into their workflow and delegate tasks based on his skills and interests, as well as ensure he has access to necessary resources such as data and computational power.&#10;&#10;However, there is no information provided about specific preparations related to time or disk space mentioned in the transcripts. These considerations are discussed separately in the context of the project's progress but not directly linked to Dave's offer to help.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speaker, presumably Professor F, is arranging to meet with the group at 6 AM in front of an unspecified location. If there are any delays, Professor F mentioned that they will be fine and do not need to be woken up, but they offered to provide their phone number just in case." target="Professor F is trying to clarify that despite the upcoming upgrade of the networks in their place, which will improve the network speeds, currently they are still limited by pretty much all ten megabit lines. He mentions that this is becoming a significant drag on their work because the machines and computers they use are getting faster and faster, but the network speeds are unable to keep up with them.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speaker, presumably Professor F, is arranging to meet with the group at 6 AM in front of an unspecified location. If there are any delays, Professor F mentioned that they will be fine and do not need to be woken up, but they offered to provide their phone number just in case." target="Based on the transcript, there is no direct mention of Professor F's decision to schedule a 7:20 AM plane departure. However, it can be inferred that they plan to drive the group to the airport and want everyone to meet at 6 AM to ensure they arrive on time for this departure.&#10;&#10;As for accomplishing a same-day return visit, Professor F suggests driving the group to the airport and offers to pick up those who are not familiar with the area. This implies that they have considered the logistics of getting the group to the airport in time for their flight and making arrangements to facilitate this process. Additionally, since Professor F mentions that network upgrades are making it difficult to work due to slower network speeds compared to their increasingly faster computers and machines, it can be inferred that the purpose of the trip involves tasks that cannot be completed remotely or require access to specific resources at their destination.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , most confusions are within the phone {disfmarker} phone classes , right ? I think , uh , Larry was saying like obstruents are only confused with other obstruents , et cetera , et cetera .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor F&#10;Content: Yeah . Yeah .&#10;Speaker: PhD G&#10;Content: Yeah , this is another p yeah , another point .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: So {disfmarker} so , maybe we could look at articulatory type stuff ,&#10;Speaker: Professor F&#10;Content: But that 's what I thought they were gonna {disfmarker}&#10;Speaker: Grad C&#10;Content: right ?&#10;Speaker: Professor F&#10;Content: Did they not do that , or {disfmarker} ?&#10;Speaker: PhD G&#10;Content: I don't think so . Well ,&#10;Speaker: Professor F&#10;Content: So {disfmarker}&#10;Speaker: PhD G&#10;Content: they were talking about , perhaps , but they d&#10;" target="1. The discussion was focused on the observation that most confusions in phone classification occur within the same broad classes, such as obstruents being confused with other obstruents. This led to the idea of examining articulatory types as a potential solution.&#10;2. The participants also discussed the possibility of using a multi-valued or multi-dimensional representation for articulatory features, allowing for the modeling of various combinations of articulatory gestures and a more nuanced understanding of speech sounds.&#10;3. Articulatory features were considered as targets for the neural network, which describe the position and movement of speech organs when producing sounds, typically used in speech synthesis and analysis.&#10;4. The challenge of addressing differences in phone sets between different databases and languages was mentioned, with a specific example of mapping from 61 phonemes in TIMIT to 56 phonemes in ICSI given.&#10;&#10;In summary, the discussion centered around using articulatory type information for classifying phones, considering multiple features being active at once, and addressing challenges posed by phone set differences between databases and languages.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the information provided in the transcripts, it is stated that an HTK training takes around six hours or more. The discussion includes testing the model's performance on three different types of noise conditions (matched, mismatched, and mismatched channel) using a script. However, there is no explicit mention of the time required to complete a training session with the specific configuration you mentioned. To provide an accurate answer, further details or specifications about the computational cost associated with this particular scenario are needed." target="1. Purpose of the trip: The purpose of the trip is not explicitly stated in the transcript, but it involves Professor F, PhD G, and possibly others, conducting experiments and working on a project, which seems to involve analyzing audio recordings based on the context.&#10;2. Preparations for Dave's contribution: In response to Dave's offer to contribute to the project, they are discussing where his help would be most useful. PhD G suggests having Dave focus on research aspects like implementing speech noise detection or voiced-unvoiced sound detection algorithms in addition to the ongoing experiments. The team should consider how best to integrate Dave into their workflow and delegate tasks based on his skills and interests, as well as ensure he has access to necessary resources such as data and computational power.&#10;&#10;However, there is no information provided about specific preparations related to time or disk space mentioned in the transcripts. These considerations are discussed separately in the context of the project's progress but not directly linked to Dave's offer to help.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the information provided in the transcripts, it is stated that an HTK training takes around six hours or more. The discussion includes testing the model's performance on three different types of noise conditions (matched, mismatched, and mismatched channel) using a script. However, there is no explicit mention of the time required to complete a training session with the specific configuration you mentioned. To provide an accurate answer, further details or specifications about the computational cost associated with this particular scenario are needed." target="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is because the model will have limited exposure and experience in handling diverse types of unseen noise, which could lead to suboptimal performance when dealing with real-world scenarios where various kinds of noises are present.&#10;   &#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus. By training with seen noise conditions, the model can become more robust in handling those specific types of noises, improving its overall performance in noisy environments that it has encountered before.&#10;   &#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments. While this doesn't directly address the issue of not being able to train on unseen noise conditions, combination testing can provide valuable insights into how the model performs under various noisy scenarios and potentially lead to improvements in the model's ability to generalize to new, unseen noise conditions.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the information provided in the transcripts, it is stated that an HTK training takes around six hours or more. The discussion includes testing the model's performance on three different types of noise conditions (matched, mismatched, and mismatched channel) using a script. However, there is no explicit mention of the time required to complete a training session with the specific configuration you mentioned. To provide an accurate answer, further details or specifications about the computational cost associated with this particular scenario are needed." target="Based on the information provided in the transcripts, specifically the discussion about HTK training taking around six hours or more, and considering that there will be some computational overhead for computing the KL transformation when testing combinations of built-in training programs like PLP, MSG, and JRA, it can be estimated that conducting experiments on these combinations might take a significant amount of time. However, the exact duration is not explicitly mentioned in the transcripts.&#10;&#10;To provide an accurate answer, additional information about the computational resources available for running these experiments would be helpful. Factors such as the number of possible combinations, the complexity of each combination, and the computational power of the hardware being used would all impact the total time required to complete the experiments.&#10;&#10;In summary, while it is not possible to provide a precise estimate based on the given transcripts, conducting experiments on combinations of built-in training programs such as PLP, MSG, and JRA is likely to take a considerable amount of time, possibly extending beyond a few days, considering the estimated six hours or more for each HTK training session and additional computational overhead.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the information provided in the transcripts, it is stated that an HTK training takes around six hours or more. The discussion includes testing the model's performance on three different types of noise conditions (matched, mismatched, and mismatched channel) using a script. However, there is no explicit mention of the time required to complete a training session with the specific configuration you mentioned. To provide an accurate answer, further details or specifications about the computational cost associated with this particular scenario are needed." target="The transcript does not provide specific information about the available disk space on the Nutmeg and Mustard computational servers. However, it is mentioned that they are computational servers and there are discussions around checking for space in slash-scratch and other disks (slash-X-whatever) on Mustard and Nutmeg. It is recommended to use local storage if running experiments multiple times to avoid network overhead. To get the exact available disk space, it is advised to manually check by logging onto those servers and using commands like &quot;DF -K&quot; or checking the specific directories suggested in the transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="Four hundred order hidden units are being used for nets trained on digits. The plan to increase the number of units for broader class nets, which actually means using more classes, is because these broader training corpus nets like TIMIT correspond to a larger number of phoneme classes compared to the digits nets that only correspond to about twenty phonemes." target=" . Um , yeah , so I I 've been looking at , uh , uh , TIMIT stuff . Um , the {disfmarker} the stuff that we 've been working on with TIMIT , trying to get a , um {disfmarker} a labels file so we can , uh , train up a {disfmarker} train up a net on TIMIT and test , um , the difference between this net trained on TIMIT and a net trained on digits alone . Um , and seeing if {disfmarker} if it hurts or helps .&#10;Speaker: Professor F&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: Anyway .&#10;Speaker: Professor F&#10;Content: And again , when y just to clarify , when you 're talking about training up a net , you 're talking about training up a net for a tandem approach ?&#10;Speaker: Grad C&#10;Content: Yeah , yeah . Um . Mm - hmm .&#10;Speaker: Professor F&#10;Content: And {disfmarker} and the inputs are PLP and delta and that sort of thing ,&#10;Speaker: Grad C&#10;Content: Well , the inputs are one dimension">
      <data key="d0">1</data>
    </edge>
    <edge source="Four hundred order hidden units are being used for nets trained on digits. The plan to increase the number of units for broader class nets, which actually means using more classes, is because these broader training corpus nets like TIMIT correspond to a larger number of phoneme classes compared to the digits nets that only correspond to about twenty phonemes." target="The &quot;unusual number&quot; the speakers refer to is the larger number of phone symbols used in their recognition system, which is 120 compared to the typical 39 in other systems. This larger set allows for a more nuanced identification of sounds, as it includes additional phonetic context beyond the basic sound itself. The use of this larger set enables the system to bring in context and better distinguish between similar sounds based on their surrounding phonetic environment.&#10;&#10;This approach aims to improve accuracy by considering how individual phones are influenced by neighboring sounds within words or phrases, which can help resolve ambiguities that may arise from using a smaller phone set. However, this increased level of detail also presents challenges in accurately training and fine-tuning the recognition system to effectively use this larger phone set for speech processing tasks.">
      <data key="d0">1</data>
    </edge>
    <edge source="Four hundred order hidden units are being used for nets trained on digits. The plan to increase the number of units for broader class nets, which actually means using more classes, is because these broader training corpus nets like TIMIT correspond to a larger number of phoneme classes compared to the digits nets that only correspond to about twenty phonemes." target="Based on the discussion, it is suggested that using the TIMIT dataset as a source for training a tandem system across languages could be a better choice due to its broadness and careful hand-correction. However, given the limited time and resources, the speakers did not explicitly conclude that the TIMIT dataset's broadness or careful hand-correction is the sole reason for its potential superiority. Further investigation would be required to make a definitive conclusion.">
      <data key="d0">1</data>
    </edge>
    <edge source="One major criticism against current speech recognition systems is that they throw away all the harmonicity information and try to get spectral envelopes, discarding most of the information about the phonetic identity which is contained in the harmonics. Incorporating pitch or harmonicity information could address this issue by preserving some of this lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds." target=" So , um {disfmarker} And th th the other tricky thing is , since we are , uh , even though we 're not {disfmarker} we don't have a strict prohibition on memory size , and {disfmarker} and computational complexity , uh , clearly there 's some limitation to it . So if we have to {disfmarker} if we say we have to have a pitch detector , say , if we {disfmarker} if we 're trying to incorporate pitch information , or at least some kind of harmonic {disfmarker} harmonicity , or something , this is another whole thing , take a while to develop . Anyway , it 's a very very interesting topic . I mean , one {disfmarker} I think one of the {disfmarker} a lot of people would say , and I think Dan would also , uh , that one of the things wrong with current speech recognition is that we {disfmarker} we really do throw away all the harmonicity information . Uh , we try to get spectral envelopes . Reason for doing that is that most of the information about the phonetic identity is in the spectral envelopes are not in the harmonic">
      <data key="d0">1</data>
    </edge>
    <edge source="One major criticism against current speech recognition systems is that they throw away all the harmonicity information and try to get spectral envelopes, discarding most of the information about the phonetic identity which is contained in the harmonics. Incorporating pitch or harmonicity information could address this issue by preserving some of this lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds." target="The goal of computing and sending features instead of actual speech in the development of new telephone standards, as discussed by the speakers, is to improve speech recognition and synthesis by accurately identifying phones (distinct units of sound) in the data. This would allow for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures. Additionally, incorporating pitch or harmonicity information could address current criticism of speech recognition systems by preserving some of the lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.&#10;&#10;One challenge in achieving this goal is the differences in phone sets between different databases and languages. In the conversation, they mention a mapping from the 61 phonemes in TIMIT to the 56 phonemes in ICSI and the differences in phone sets between the two databases. This shows that addressing the issue of phone set differences is an important step towards achieving the goal of accurately identifying phones and improving speech recognition and synthesis in new telephone standards.">
      <data key="d0">1</data>
    </edge>
    <edge source="One major criticism against current speech recognition systems is that they throw away all the harmonicity information and try to get spectral envelopes, discarding most of the information about the phonetic identity which is contained in the harmonics. Incorporating pitch or harmonicity information could address this issue by preserving some of this lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds." target="The &quot;unusual number&quot; the speakers refer to is the larger number of phone symbols used in their recognition system, which is 120 compared to the typical 39 in other systems. This larger set allows for a more nuanced identification of sounds, as it includes additional phonetic context beyond the basic sound itself. The use of this larger set enables the system to bring in context and better distinguish between similar sounds based on their surrounding phonetic environment.&#10;&#10;This approach aims to improve accuracy by considering how individual phones are influenced by neighboring sounds within words or phrases, which can help resolve ambiguities that may arise from using a smaller phone set. However, this increased level of detail also presents challenges in accurately training and fine-tuning the recognition system to effectively use this larger phone set for speech processing tasks.">
      <data key="d0">1</data>
    </edge>
    <edge source="One major criticism against current speech recognition systems is that they throw away all the harmonicity information and try to get spectral envelopes, discarding most of the information about the phonetic identity which is contained in the harmonics. Incorporating pitch or harmonicity information could address this issue by preserving some of this lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds." target="1. The discussion was focused on the observation that most confusions in phone classification occur within the same broad classes, such as obstruents being confused with other obstruents. This led to the idea of examining articulatory types as a potential solution.&#10;2. The participants also discussed the possibility of using a multi-valued or multi-dimensional representation for articulatory features, allowing for the modeling of various combinations of articulatory gestures and a more nuanced understanding of speech sounds.&#10;3. Articulatory features were considered as targets for the neural network, which describe the position and movement of speech organs when producing sounds, typically used in speech synthesis and analysis.&#10;4. The challenge of addressing differences in phone sets between different databases and languages was mentioned, with a specific example of mapping from 61 phonemes in TIMIT to 56 phonemes in ICSI given.&#10;&#10;In summary, the discussion centered around using articulatory type information for classifying phones, considering multiple features being active at once, and addressing challenges posed by phone set differences between databases and languages.">
      <data key="d0">1</data>
    </edge>
    <edge source="The goal of computing and sending features instead of actual speech in the development of new telephone standards, as discussed by the speakers, is to improve speech recognition and synthesis by accurately identifying phones (distinct units of sound) in the data. This would allow for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures. Additionally, incorporating pitch or harmonicity information could address current criticism of speech recognition systems by preserving some of the lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.&#10;&#10;One challenge in achieving this goal is the differences in phone sets between different databases and languages. In the conversation, they mention a mapping from the 61 phonemes in TIMIT to the 56 phonemes in ICSI and the differences in phone sets between the two databases. This shows that addressing the issue of phone set differences is an important step towards achieving the goal of accurately identifying phones and improving speech recognition and synthesis in new telephone standards." target=" SpeechDat databases are not telephone speech . They are downsampled to eight kilohertz but {disfmarker} but they are not {vocalsound} uh with telephone bandwidth .&#10;Speaker: Professor F&#10;Content: Yeah . That 's really funny isn't it ? I mean cuz th this whole thing is for {pause} developing new standards for the telephone .&#10;Speaker: Grad C&#10;Content: Telephone .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Yeah , but the {disfmarker} the idea is to compute the feature before {pause} the {disfmarker} before sending them to the {disfmarker} Well , {pause} you don't {disfmarker} do not send speech , you send features , computed on th the {disfmarker} {pause} the device ,&#10;Speaker: Professor F&#10;Content: Mm - hmm . Yeah , I know , but the reason {disfmarker}&#10;Speaker: PhD G&#10;Content: or {disfmarker} Well .&#10;Speaker: Professor F&#10;Content: Oh I see , so your point is that it '">
      <data key="d0">1</data>
    </edge>
    <edge source="The goal of computing and sending features instead of actual speech in the development of new telephone standards, as discussed by the speakers, is to improve speech recognition and synthesis by accurately identifying phones (distinct units of sound) in the data. This would allow for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures. Additionally, incorporating pitch or harmonicity information could address current criticism of speech recognition systems by preserving some of the lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.&#10;&#10;One challenge in achieving this goal is the differences in phone sets between different databases and languages. In the conversation, they mention a mapping from the 61 phonemes in TIMIT to the 56 phonemes in ICSI and the differences in phone sets between the two databases. This shows that addressing the issue of phone set differences is an important step towards achieving the goal of accurately identifying phones and improving speech recognition and synthesis in new telephone standards." target="The &quot;unusual number&quot; the speakers refer to is the larger number of phone symbols used in their recognition system, which is 120 compared to the typical 39 in other systems. This larger set allows for a more nuanced identification of sounds, as it includes additional phonetic context beyond the basic sound itself. The use of this larger set enables the system to bring in context and better distinguish between similar sounds based on their surrounding phonetic environment.&#10;&#10;This approach aims to improve accuracy by considering how individual phones are influenced by neighboring sounds within words or phrases, which can help resolve ambiguities that may arise from using a smaller phone set. However, this increased level of detail also presents challenges in accurately training and fine-tuning the recognition system to effectively use this larger phone set for speech processing tasks.">
      <data key="d0">1</data>
    </edge>
    <edge source="The goal of computing and sending features instead of actual speech in the development of new telephone standards, as discussed by the speakers, is to improve speech recognition and synthesis by accurately identifying phones (distinct units of sound) in the data. This would allow for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures. Additionally, incorporating pitch or harmonicity information could address current criticism of speech recognition systems by preserving some of the lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.&#10;&#10;One challenge in achieving this goal is the differences in phone sets between different databases and languages. In the conversation, they mention a mapping from the 61 phonemes in TIMIT to the 56 phonemes in ICSI and the differences in phone sets between the two databases. This shows that addressing the issue of phone set differences is an important step towards achieving the goal of accurately identifying phones and improving speech recognition and synthesis in new telephone standards." target="1. The discussion was focused on the observation that most confusions in phone classification occur within the same broad classes, such as obstruents being confused with other obstruents. This led to the idea of examining articulatory types as a potential solution.&#10;2. The participants also discussed the possibility of using a multi-valued or multi-dimensional representation for articulatory features, allowing for the modeling of various combinations of articulatory gestures and a more nuanced understanding of speech sounds.&#10;3. Articulatory features were considered as targets for the neural network, which describe the position and movement of speech organs when producing sounds, typically used in speech synthesis and analysis.&#10;4. The challenge of addressing differences in phone sets between different databases and languages was mentioned, with a specific example of mapping from 61 phonemes in TIMIT to 56 phonemes in ICSI given.&#10;&#10;In summary, the discussion centered around using articulatory type information for classifying phones, considering multiple features being active at once, and addressing challenges posed by phone set differences between databases and languages.">
      <data key="d0">1</data>
    </edge>
    <edge source="The goal of computing and sending features instead of actual speech in the development of new telephone standards, as discussed by the speakers, is to improve speech recognition and synthesis by accurately identifying phones (distinct units of sound) in the data. This would allow for a more nuanced and accurate modeling of speech sounds, as they can be the result of various combinations of articulatory gestures. Additionally, incorporating pitch or harmonicity information could address current criticism of speech recognition systems by preserving some of the lost harmonic information, potentially improving the system's understanding and identification of various phonetic features and sounds.&#10;&#10;One challenge in achieving this goal is the differences in phone sets between different databases and languages. In the conversation, they mention a mapping from the 61 phonemes in TIMIT to the 56 phonemes in ICSI and the differences in phone sets between the two databases. This shows that addressing the issue of phone set differences is an important step towards achieving the goal of accurately identifying phones and improving speech recognition and synthesis in new telephone standards." target="Based on the discussion, it is suggested that using the TIMIT dataset as a source for training a tandem system across languages could be a better choice due to its broadness and careful hand-correction. However, given the limited time and resources, the speakers did not explicitly conclude that the TIMIT dataset's broadness or careful hand-correction is the sole reason for its potential superiority. Further investigation would be required to make a definitive conclusion.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Purpose of the trip: The purpose of the trip is not explicitly stated in the transcript, but it involves Professor F, PhD G, and possibly others, conducting experiments and working on a project, which seems to involve analyzing audio recordings based on the context.&#10;2. Preparations for Dave's contribution: In response to Dave's offer to contribute to the project, they are discussing where his help would be most useful. PhD G suggests having Dave focus on research aspects like implementing speech noise detection or voiced-unvoiced sound detection algorithms in addition to the ongoing experiments. The team should consider how best to integrate Dave into their workflow and delegate tasks based on his skills and interests, as well as ensure he has access to necessary resources such as data and computational power.&#10;&#10;However, there is no information provided about specific preparations related to time or disk space mentioned in the transcripts. These considerations are discussed separately in the context of the project's progress but not directly linked to Dave's offer to help." target=" trip started off with {disfmarker} with , uh , Stephane talking to Hynek , so you may have {disfmarker} you may have had other goals , uh , for going up , and any anything else you can think of would be {disfmarker} we should think about {pause} accomplishing ? I mean , I 'm just saying this because {pause} maybe there 's things we need to do in preparation .&#10;Speaker: PhD G&#10;Content: Oh , I think basically , this is {disfmarker} this is , uh , yeah .&#10;Speaker: Professor F&#10;Content: OK . OK . Uh . Alright . And uh {disfmarker} and the other {disfmarker} the {disfmarker} the last topic I had here was , um , uh d Dave 's fine offer to {disfmarker} to , uh , do something {pause} {vocalsound} on this . I mean he 's doing {disfmarker} {vocalsound} {disfmarker} he 's working on other things , but to {disfmarker} to do something on this project . So the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Purpose of the trip: The purpose of the trip is not explicitly stated in the transcript, but it involves Professor F, PhD G, and possibly others, conducting experiments and working on a project, which seems to involve analyzing audio recordings based on the context.&#10;2. Preparations for Dave's contribution: In response to Dave's offer to contribute to the project, they are discussing where his help would be most useful. PhD G suggests having Dave focus on research aspects like implementing speech noise detection or voiced-unvoiced sound detection algorithms in addition to the ongoing experiments. The team should consider how best to integrate Dave into their workflow and delegate tasks based on his skills and interests, as well as ensure he has access to necessary resources such as data and computational power.&#10;&#10;However, there is no information provided about specific preparations related to time or disk space mentioned in the transcripts. These considerations are discussed separately in the context of the project's progress but not directly linked to Dave's offer to help." target="} It 's gonna take us a couple weeks at least to get the , uh , uh , the amount of disk we 're gonna be getting . We 're actually gonna get , uh , I think four more , uh , thirty - six gigabyte drives and , uh , put them on another {disfmarker} another disk rack . We ran out of space on the disk rack that we had , so we 're getting another disk rack and {vocalsound} four more drives to share between , uh {disfmarker} primarily between this project and the Meetings {disfmarker} Meetings Project . Um . But , uh , we 've put another {disfmarker} I guess there 's another eighteen gigabytes that 's {disfmarker} that 's in there now to help us with the immediate crunch . But , uh , are you saying {disfmarker} So I don't know where {pause} you 're {disfmarker} Stephane , where you 're doing your computations . If {disfmarker} i so , you 're on an NT machine , so you 're using some external machine&#10;Speaker: PhD G&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Purpose of the trip: The purpose of the trip is not explicitly stated in the transcript, but it involves Professor F, PhD G, and possibly others, conducting experiments and working on a project, which seems to involve analyzing audio recordings based on the context.&#10;2. Preparations for Dave's contribution: In response to Dave's offer to contribute to the project, they are discussing where his help would be most useful. PhD G suggests having Dave focus on research aspects like implementing speech noise detection or voiced-unvoiced sound detection algorithms in addition to the ongoing experiments. The team should consider how best to integrate Dave into their workflow and delegate tasks based on his skills and interests, as well as ensure he has access to necessary resources such as data and computational power.&#10;&#10;However, there is no information provided about specific preparations related to time or disk space mentioned in the transcripts. These considerations are discussed separately in the context of the project's progress but not directly linked to Dave's offer to help." target=" have with it is exactly the same reason why you thought it 'd be a good thing to do . Um , I {disfmarker} I think that {disfmarker} Let 's fall back to that . But I think the first responsibility is sort of to figure out if there 's something {pause} that , uh , an {disfmarker} an additional {disfmarker} Uh , that 's a good thing you {disfmarker} remove the mike . Go ahead , good . Uh , uh . What an additional clever person could help with when we 're really in a crunch for time . Right ? Cuz Dave 's gonna be around for a long time ,&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: right ? He 's {disfmarker} he 's gonna be here for years . And so , um ,&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: over years , if he 's {disfmarker} if he 's interested in , you know , voiced - unvoiced - silence , he could do a lot . But if there {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Purpose of the trip: The purpose of the trip is not explicitly stated in the transcript, but it involves Professor F, PhD G, and possibly others, conducting experiments and working on a project, which seems to involve analyzing audio recordings based on the context.&#10;2. Preparations for Dave's contribution: In response to Dave's offer to contribute to the project, they are discussing where his help would be most useful. PhD G suggests having Dave focus on research aspects like implementing speech noise detection or voiced-unvoiced sound detection algorithms in addition to the ongoing experiments. The team should consider how best to integrate Dave into their workflow and delegate tasks based on his skills and interests, as well as ensure he has access to necessary resources such as data and computational power.&#10;&#10;However, there is no information provided about specific preparations related to time or disk space mentioned in the transcripts. These considerations are discussed separately in the context of the project's progress but not directly linked to Dave's offer to help." target="Based on the transcript, there is no direct mention of Professor F's decision to schedule a 7:20 AM plane departure. However, it can be inferred that they plan to drive the group to the airport and want everyone to meet at 6 AM to ensure they arrive on time for this departure.&#10;&#10;As for accomplishing a same-day return visit, Professor F suggests driving the group to the airport and offers to pick up those who are not familiar with the area. This implies that they have considered the logistics of getting the group to the airport in time for their flight and making arrangements to facilitate this process. Additionally, since Professor F mentions that network upgrades are making it difficult to work due to slower network speeds compared to their increasingly faster computers and machines, it can be inferred that the purpose of the trip involves tasks that cannot be completed remotely or require access to specific resources at their destination.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The model's ability to match noise may be compromised: Since they can't train on unseen noise conditions, the model's performance in matching noises that it hasn't encountered during training might suffer. This is because the model will have limited exposure and experience in handling diverse types of unseen noise, which could lead to suboptimal performance when dealing with real-world scenarios where various kinds of noises are present.&#10;   &#10;2. Training with seen noise conditions: As an alternative, they consider using noise conditions that were previously seen during training. This is what Aurora does, as mentioned by Professor F, and Grad C brings up the idea of incorporating similar noises in their own training corpus. By training with seen noise conditions, the model can become more robust in handling those specific types of noises, improving its overall performance in noisy environments that it has encountered before.&#10;   &#10;3. Combination testing: They plan to test the model with various combinations of noise types (Types A, B, and C) once they have trained all possible combinations. This may help improve the model's performance on unseen noise conditions by exposing it to a variety of noisy environments. While this doesn't directly address the issue of not being able to train on unseen noise conditions, combination testing can provide valuable insights into how the model performs under various noisy scenarios and potentially lead to improvements in the model's ability to generalize to new, unseen noise conditions." target="Based on the information provided in the transcripts, specifically the discussion about HTK training taking around six hours or more, and considering that there will be some computational overhead for computing the KL transformation when testing combinations of built-in training programs like PLP, MSG, and JRA, it can be estimated that conducting experiments on these combinations might take a significant amount of time. However, the exact duration is not explicitly mentioned in the transcripts.&#10;&#10;To provide an accurate answer, additional information about the computational resources available for running these experiments would be helpful. Factors such as the number of possible combinations, the complexity of each combination, and the computational power of the hardware being used would all impact the total time required to complete the experiments.&#10;&#10;In summary, while it is not possible to provide a precise estimate based on the given transcripts, conducting experiments on combinations of built-in training programs such as PLP, MSG, and JRA is likely to take a considerable amount of time, possibly extending beyond a few days, considering the estimated six hours or more for each HTK training session and additional computational overhead.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the information provided in the transcripts, specifically the discussion about HTK training taking around six hours or more, and considering that there will be some computational overhead for computing the KL transformation when testing combinations of built-in training programs like PLP, MSG, and JRA, it can be estimated that conducting experiments on these combinations might take a significant amount of time. However, the exact duration is not explicitly mentioned in the transcripts.&#10;&#10;To provide an accurate answer, additional information about the computational resources available for running these experiments would be helpful. Factors such as the number of possible combinations, the complexity of each combination, and the computational power of the hardware being used would all impact the total time required to complete the experiments.&#10;&#10;In summary, while it is not possible to provide a precise estimate based on the given transcripts, conducting experiments on combinations of built-in training programs such as PLP, MSG, and JRA is likely to take a considerable amount of time, possibly extending beyond a few days, considering the estimated six hours or more for each HTK training session and additional computational overhead." target=" a training ? I mean , it 's not totally crazy t I mean , these are {disfmarker} a lot of these are built - in things and we know {disfmarker} we have programs that compute PLP , we have MSG , we have JRA you know , a lot of these things will just kind of happen , won't take uh a huge amount of development , it 's just trying it out . So , we actually can do quite a few experiments .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: But how {disfmarker} how long does it take , do we think , for one of these {pause} {comment} trainings ?&#10;Speaker: Grad C&#10;Content: That 's a good question .&#10;Speaker: PhD A&#10;Content: What about combinations of things ?&#10;Speaker: Professor F&#10;Content: Oh yeah , that 's right . I mean , cuz , so , for instance , I think the major advantage of MSG {disfmarker}&#10;Speaker: Grad C&#10;Content: Oh !&#10;Speaker: Professor F&#10;Content: Yeah ,&#10;Speaker: Grad C">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the information provided in the transcripts, specifically the discussion about HTK training taking around six hours or more, and considering that there will be some computational overhead for computing the KL transformation when testing combinations of built-in training programs like PLP, MSG, and JRA, it can be estimated that conducting experiments on these combinations might take a significant amount of time. However, the exact duration is not explicitly mentioned in the transcripts.&#10;&#10;To provide an accurate answer, additional information about the computational resources available for running these experiments would be helpful. Factors such as the number of possible combinations, the complexity of each combination, and the computational power of the hardware being used would all impact the total time required to complete the experiments.&#10;&#10;In summary, while it is not possible to provide a precise estimate based on the given transcripts, conducting experiments on combinations of built-in training programs such as PLP, MSG, and JRA is likely to take a considerable amount of time, possibly extending beyond a few days, considering the estimated six hours or more for each HTK training session and additional computational overhead." target="The transcript does not provide specific information about the available disk space on the Nutmeg and Mustard computational servers. However, it is mentioned that they are computational servers and there are discussions around checking for space in slash-scratch and other disks (slash-X-whatever) on Mustard and Nutmeg. It is recommended to use local storage if running experiments multiple times to avoid network overhead. To get the exact available disk space, it is advised to manually check by logging onto those servers and using commands like &quot;DF -K&quot; or checking the specific directories suggested in the transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;slash-scratch&quot; directory in a Unix workstation is intended for temporary storage of files. It is not backed up and may refer to an internal disk of the machine. Therefore, it's not suitable for storing important files long-term because if the machine is replaced or disappears, any files stored in &quot;slash-scratch&quot; will be lost. Additionally, if a user stores large amounts of data in &quot;slash-scratch,&quot; it could potentially impact network performance as the networks may still be using ten megabit lines while machines are getting faster and faster. To check space availability in &quot;slash-scratch,&quot; users can use the command &quot;DF -K&quot; when they're on that machine." target=" have a {disfmarker} a {disfmarker} a Unix workstation , and they attach an external disk , {comment} it 'll be called &quot; slash - X - something &quot; uh , if it 's not backed up and it 'll be &quot; slash - D - something &quot; if it is backed up . And if it 's inside the machine on the desk , it 's called &quot; slash - scratch &quot; . But the problem is , if you ever get a new machine , they take your machine away . It 's easy to unhook the external disks , put them back on the new machine , but then your slash - scratch is gone . So , you don't wanna put anything in slash - scratch that you wanna keep around for a long period of time . But if it 's a copy of , say , some data that 's on a server , you can put it on slash - scratch because , um , first of all it 's not backed up , and second it doesn't matter if that machine disappears and you get a new machine because you just recopy it to slash - scratch . So tha that 's why I was saying you could check slash - scratch on those {disfmarker} on {">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;slash-scratch&quot; directory in a Unix workstation is intended for temporary storage of files. It is not backed up and may refer to an internal disk of the machine. Therefore, it's not suitable for storing important files long-term because if the machine is replaced or disappears, any files stored in &quot;slash-scratch&quot; will be lost. Additionally, if a user stores large amounts of data in &quot;slash-scratch,&quot; it could potentially impact network performance as the networks may still be using ten megabit lines while machines are getting faster and faster. To check space availability in &quot;slash-scratch,&quot; users can use the command &quot;DF -K&quot; when they're on that machine." target=": PhD A&#10;Content: Uh , and if there is then , uh {disfmarker}&#10;Speaker: Professor F&#10;Content: But wasn't it , uh {disfmarker} I think Dave was saying that he preferred that people didn't put stuff in slash - scratch . It 's more putting in d s XA or XB or ,&#10;Speaker: PhD A&#10;Content: Well , there 's different {disfmarker} there , um , there 's {disfmarker}&#10;Speaker: Professor F&#10;Content: right ?&#10;Speaker: PhD A&#10;Content: Right . So there 's the slash - X - whatever disks , and then there 's slash - scratch . And both of those two kinds are not backed up . And if it 's called &quot; slash - scratch &quot; , it means it 's probably an internal disk to the machine . Um . And so that 's the kind of thing where , like if {disfmarker} um , OK , if you don't have an NT , but you have a {disfmarker} a {disfmarker} a Unix workstation , and they attach an external disk , {comment} it '">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;slash-scratch&quot; directory in a Unix workstation is intended for temporary storage of files. It is not backed up and may refer to an internal disk of the machine. Therefore, it's not suitable for storing important files long-term because if the machine is replaced or disappears, any files stored in &quot;slash-scratch&quot; will be lost. Additionally, if a user stores large amounts of data in &quot;slash-scratch,&quot; it could potentially impact network performance as the networks may still be using ten megabit lines while machines are getting faster and faster. To check space availability in &quot;slash-scratch,&quot; users can use the command &quot;DF -K&quot; when they're on that machine." target="&#10;Speaker: PhD A&#10;Content: Well , I 'll check on that .&#10;Speaker: Professor F&#10;Content: Yeah . Yeah , so basically , uh , Chuck will be the one who will be sorting out what disk needs to be where , and so on , and I 'll be the one who says , &quot; OK , spend the money . &quot; So . {vocalsound} Which , I mean , n these days , uh , if you 're talking about scratch space , it doesn't increase the , uh , need for backup , and , uh , I think it 's not that big a d and the {disfmarker} the disks themselves are not that expensive . Right now it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: What you can do , when you 're on that machine , is , uh , just go to the slash - scratch directory , and do a DF minus K , and it 'll tell you if there 's space available .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh , and if there is then , uh {disfmarker}&#10;Speaker: Professor F&#10;Content: But wasn">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;slash-scratch&quot; directory in a Unix workstation is intended for temporary storage of files. It is not backed up and may refer to an internal disk of the machine. Therefore, it's not suitable for storing important files long-term because if the machine is replaced or disappears, any files stored in &quot;slash-scratch&quot; will be lost. Additionally, if a user stores large amounts of data in &quot;slash-scratch,&quot; it could potentially impact network performance as the networks may still be using ten megabit lines while machines are getting faster and faster. To check space availability in &quot;slash-scratch,&quot; users can use the command &quot;DF -K&quot; when they're on that machine." target="opy it to slash - scratch . So tha that 's why I was saying you could check slash - scratch on those {disfmarker} on {disfmarker} on , um , Mustard and {disfmarker} and Nutmeg to see if {disfmarker} if there 's space that you could use there .&#10;Speaker: Professor F&#10;Content: I see .&#10;Speaker: PhD A&#10;Content: You could also use slash - X - whatever disks on Mustard and Nutmeg .&#10;Speaker: PhD G&#10;Content: Yeah , yeah .&#10;Speaker: PhD A&#10;Content: Um . Yeah , and we do have {disfmarker} I mean , yeah , so {disfmarker} so you {disfmarker} yeah , it 's better to have things local if you 're gonna run over them lots of times so you don't have to go to the network .&#10;Speaker: Professor F&#10;Content: Right , so es so especially if you 're {disfmarker} right , if you 're {disfmarker} if you 're taking some piece of the training corpus , which usually resides in where Chuck is putting">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;slash-scratch&quot; directory in a Unix workstation is intended for temporary storage of files. It is not backed up and may refer to an internal disk of the machine. Therefore, it's not suitable for storing important files long-term because if the machine is replaced or disappears, any files stored in &quot;slash-scratch&quot; will be lost. Additionally, if a user stores large amounts of data in &quot;slash-scratch,&quot; it could potentially impact network performance as the networks may still be using ten megabit lines while machines are getting faster and faster. To check space availability in &quot;slash-scratch,&quot; users can use the command &quot;DF -K&quot; when they're on that machine." target=" e scratch space for {disfmarker} for people to work on . And I know that , uh , Stephane 's working from an NT machine , so his {disfmarker} his home directory exists somewhere else .&#10;Speaker: Professor F&#10;Content: His {disfmarker} his stuff is somewhere else , yeah . Yeah , I mean , my point I {disfmarker} I want to {disfmarker} Yeah , thanks for bring it back to that . My {disfmarker} th I want to clarify my point about that {disfmarker} that {disfmarker} that Chuck repeated in his note . Um . We 're {disfmarker} over the next year or two , we 're gonna be upgrading the networks in this place ,&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: but right now they 're still all te pretty much all ten megabit lines . And we have reached the {disfmarker} this {disfmarker} the machines are getting faster and faster . So , it actually has reached the point where it 's a significant drag on the time">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;slash-scratch&quot; directory in a Unix workstation is intended for temporary storage of files. It is not backed up and may refer to an internal disk of the machine. Therefore, it's not suitable for storing important files long-term because if the machine is replaced or disappears, any files stored in &quot;slash-scratch&quot; will be lost. Additionally, if a user stores large amounts of data in &quot;slash-scratch,&quot; it could potentially impact network performance as the networks may still be using ten megabit lines while machines are getting faster and faster. To check space availability in &quot;slash-scratch,&quot; users can use the command &quot;DF -K&quot; when they're on that machine." target=" new , um , drive onto Abbott , that 's an X disk , which means it 's not backed up . So , um , I 've been going through and copying data that is , you know , some kind of corpus stuff usually , that {disfmarker} that we 've got on a CD - ROM or something , onto that new disk to free up space {pause} on other disks . And , um , so far , um , I 've copied a couple of Carmen 's , um , databases over there . We haven't deleted them off of the slash - DC disk that they 're on right now in Abbott , um , uh , but we {disfmarker} I would like to go through {disfmarker} sit down with you about some of these other ones and see if we can move them onto , um , this new disk also . There 's {disfmarker} there 's a lot more space there ,&#10;Speaker: PhD G&#10;Content: Yeah , OK .&#10;Speaker: PhD A&#10;Content: and it 'll free up more space for doing the experiments and things . So , anything that {disfmarker} that you don't need backed up ,">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;slash-scratch&quot; directory in a Unix workstation is intended for temporary storage of files. It is not backed up and may refer to an internal disk of the machine. Therefore, it's not suitable for storing important files long-term because if the machine is replaced or disappears, any files stored in &quot;slash-scratch&quot; will be lost. Additionally, if a user stores large amounts of data in &quot;slash-scratch,&quot; it could potentially impact network performance as the networks may still be using ten megabit lines while machines are getting faster and faster. To check space availability in &quot;slash-scratch,&quot; users can use the command &quot;DF -K&quot; when they're on that machine." target="The transcript does not provide specific information about the available disk space on the Nutmeg and Mustard computational servers. However, it is mentioned that they are computational servers and there are discussions around checking for space in slash-scratch and other disks (slash-X-whatever) on Mustard and Nutmeg. It is recommended to use local storage if running experiments multiple times to avoid network overhead. To get the exact available disk space, it is advised to manually check by logging onto those servers and using commands like &quot;DF -K&quot; or checking the specific directories suggested in the transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source=": PhD A&#10;Content: Uh , and if there is then , uh {disfmarker}&#10;Speaker: Professor F&#10;Content: But wasn't it , uh {disfmarker} I think Dave was saying that he preferred that people didn't put stuff in slash - scratch . It 's more putting in d s XA or XB or ,&#10;Speaker: PhD A&#10;Content: Well , there 's different {disfmarker} there , um , there 's {disfmarker}&#10;Speaker: Professor F&#10;Content: right ?&#10;Speaker: PhD A&#10;Content: Right . So there 's the slash - X - whatever disks , and then there 's slash - scratch . And both of those two kinds are not backed up . And if it 's called &quot; slash - scratch &quot; , it means it 's probably an internal disk to the machine . Um . And so that 's the kind of thing where , like if {disfmarker} um , OK , if you don't have an NT , but you have a {disfmarker} a {disfmarker} a Unix workstation , and they attach an external disk , {comment} it '" target="The speakers discussed using the same normalization scheme as OGI is using for all their experiments, as this would add another dimension to their analysis. They acknowledged having enough dimensions already and thus considered adopting the normalization method from OGI to avoid increasing complexity unnecessarily.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: PhD A&#10;Content: Well , I 'll check on that .&#10;Speaker: Professor F&#10;Content: Yeah . Yeah , so basically , uh , Chuck will be the one who will be sorting out what disk needs to be where , and so on , and I 'll be the one who says , &quot; OK , spend the money . &quot; So . {vocalsound} Which , I mean , n these days , uh , if you 're talking about scratch space , it doesn't increase the , uh , need for backup , and , uh , I think it 's not that big a d and the {disfmarker} the disks themselves are not that expensive . Right now it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: What you can do , when you 're on that machine , is , uh , just go to the slash - scratch directory , and do a DF minus K , and it 'll tell you if there 's space available .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh , and if there is then , uh {disfmarker}&#10;Speaker: Professor F&#10;Content: But wasn" target="Professor F is trying to clarify that despite the upcoming upgrade of the networks in their place, which will improve the network speeds, currently they are still limited by pretty much all ten megabit lines. He mentions that this is becoming a significant drag on their work because the machines and computers they use are getting faster and faster, but the network speeds are unable to keep up with them.">
      <data key="d0">1</data>
    </edge>
    <edge source="opy it to slash - scratch . So tha that 's why I was saying you could check slash - scratch on those {disfmarker} on {disfmarker} on , um , Mustard and {disfmarker} and Nutmeg to see if {disfmarker} if there 's space that you could use there .&#10;Speaker: Professor F&#10;Content: I see .&#10;Speaker: PhD A&#10;Content: You could also use slash - X - whatever disks on Mustard and Nutmeg .&#10;Speaker: PhD G&#10;Content: Yeah , yeah .&#10;Speaker: PhD A&#10;Content: Um . Yeah , and we do have {disfmarker} I mean , yeah , so {disfmarker} so you {disfmarker} yeah , it 's better to have things local if you 're gonna run over them lots of times so you don't have to go to the network .&#10;Speaker: Professor F&#10;Content: Right , so es so especially if you 're {disfmarker} right , if you 're {disfmarker} if you 're taking some piece of the training corpus , which usually resides in where Chuck is putting" target="The transcript does not provide specific information about the available disk space on the Nutmeg and Mustard computational servers. However, it is mentioned that they are computational servers and there are discussions around checking for space in slash-scratch and other disks (slash-X-whatever) on Mustard and Nutmeg. It is recommended to use local storage if running experiments multiple times to avoid network overhead. To get the exact available disk space, it is advised to manually check by logging onto those servers and using commands like &quot;DF -K&quot; or checking the specific directories suggested in the transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source=" e scratch space for {disfmarker} for people to work on . And I know that , uh , Stephane 's working from an NT machine , so his {disfmarker} his home directory exists somewhere else .&#10;Speaker: Professor F&#10;Content: His {disfmarker} his stuff is somewhere else , yeah . Yeah , I mean , my point I {disfmarker} I want to {disfmarker} Yeah , thanks for bring it back to that . My {disfmarker} th I want to clarify my point about that {disfmarker} that {disfmarker} that Chuck repeated in his note . Um . We 're {disfmarker} over the next year or two , we 're gonna be upgrading the networks in this place ,&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor F&#10;Content: but right now they 're still all te pretty much all ten megabit lines . And we have reached the {disfmarker} this {disfmarker} the machines are getting faster and faster . So , it actually has reached the point where it 's a significant drag on the time" target="Professor F is trying to clarify that despite the upcoming upgrade of the networks in their place, which will improve the network speeds, currently they are still limited by pretty much all ten megabit lines. He mentions that this is becoming a significant drag on their work because the machines and computers they use are getting faster and faster, but the network speeds are unable to keep up with them.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor F is trying to clarify that despite the upcoming upgrade of the networks in their place, which will improve the network speeds, currently they are still limited by pretty much all ten megabit lines. He mentions that this is becoming a significant drag on their work because the machines and computers they use are getting faster and faster, but the network speeds are unable to keep up with them." target=" .&#10;Speaker: Professor F&#10;Content: So .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Uh , haven't been {disfmarker} haven't been doing much computing personally , so . Um . Yeah , so those are computational servers . So I guess the other question is what disk there i space there is there on the computational servers .&#10;Speaker: PhD A&#10;Content: Right . Yeah , I 'm not sure what 's available on {disfmarker} is it {disfmarker} you said Nutmeg and what was the other one ?&#10;Speaker: PhD G&#10;Content: Mustard .&#10;Speaker: PhD A&#10;Content: Mustard . OK .&#10;Speaker: PhD B&#10;Content: Huh .&#10;Speaker: Professor F&#10;Content: Yeah , Well , you 're the {disfmarker} you 're the disk czar now .&#10;Speaker: PhD A&#10;Content: Right , right .&#10;Speaker: Professor F&#10;Content: So&#10;Speaker: PhD A&#10;Content: Well , I 'll check on that .&#10;Speaker: Professor F&#10;Content: Yeah . Yeah , so">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor F is trying to clarify that despite the upcoming upgrade of the networks in their place, which will improve the network speeds, currently they are still limited by pretty much all ten megabit lines. He mentions that this is becoming a significant drag on their work because the machines and computers they use are getting faster and faster, but the network speeds are unable to keep up with them." target=" the phones right .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor F&#10;Content: So , if that doesn't help , there 's {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Although , yeah , it would be {disfmarker} make an interesting cheating experiment because we are using it in this funny way ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: where we 're converting it into features .&#10;Speaker: PhD A&#10;Content: And then you also don't know what error they 've got on the HTK side . You know ? It sort of gives you your {disfmarker} the best you could hope for , kind of .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: Mmm . Mmm , I see .&#10;Speaker: PhD B&#10;Content: The soft training of the nets still requires the vector to sum to one , though , right ?&#10;Speaker: Grad C&#10;Content: To sum up to one .&#10;Speaker: PhD B&#10;Content: So you can't">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor F is trying to clarify that despite the upcoming upgrade of the networks in their place, which will improve the network speeds, currently they are still limited by pretty much all ten megabit lines. He mentions that this is becoming a significant drag on their work because the machines and computers they use are getting faster and faster, but the network speeds are unable to keep up with them." target="Based on the transcript, there is no direct mention of Professor F's decision to schedule a 7:20 AM plane departure. However, it can be inferred that they plan to drive the group to the airport and want everyone to meet at 6 AM to ensure they arrive on time for this departure.&#10;&#10;As for accomplishing a same-day return visit, Professor F suggests driving the group to the airport and offers to pick up those who are not familiar with the area. This implies that they have considered the logistics of getting the group to the airport in time for their flight and making arrangements to facilitate this process. Additionally, since Professor F mentions that network upgrades are making it difficult to work due to slower network speeds compared to their increasingly faster computers and machines, it can be inferred that the purpose of the trip involves tasks that cannot be completed remotely or require access to specific resources at their destination.">
      <data key="d0">1</data>
    </edge>
    <edge source=" .&#10;Speaker: Professor F&#10;Content: So .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Uh , haven't been {disfmarker} haven't been doing much computing personally , so . Um . Yeah , so those are computational servers . So I guess the other question is what disk there i space there is there on the computational servers .&#10;Speaker: PhD A&#10;Content: Right . Yeah , I 'm not sure what 's available on {disfmarker} is it {disfmarker} you said Nutmeg and what was the other one ?&#10;Speaker: PhD G&#10;Content: Mustard .&#10;Speaker: PhD A&#10;Content: Mustard . OK .&#10;Speaker: PhD B&#10;Content: Huh .&#10;Speaker: Professor F&#10;Content: Yeah , Well , you 're the {disfmarker} you 're the disk czar now .&#10;Speaker: PhD A&#10;Content: Right , right .&#10;Speaker: Professor F&#10;Content: So&#10;Speaker: PhD A&#10;Content: Well , I 'll check on that .&#10;Speaker: Professor F&#10;Content: Yeah . Yeah , so" target="The transcript does not provide specific information about the available disk space on the Nutmeg and Mustard computational servers. However, it is mentioned that they are computational servers and there are discussions around checking for space in slash-scratch and other disks (slash-X-whatever) on Mustard and Nutmeg. It is recommended to use local storage if running experiments multiple times to avoid network overhead. To get the exact available disk space, it is advised to manually check by logging onto those servers and using commands like &quot;DF -K&quot; or checking the specific directories suggested in the transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source=" .&#10;Speaker: Professor F&#10;Content: So .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Uh , haven't been {disfmarker} haven't been doing much computing personally , so . Um . Yeah , so those are computational servers . So I guess the other question is what disk there i space there is there on the computational servers .&#10;Speaker: PhD A&#10;Content: Right . Yeah , I 'm not sure what 's available on {disfmarker} is it {disfmarker} you said Nutmeg and what was the other one ?&#10;Speaker: PhD G&#10;Content: Mustard .&#10;Speaker: PhD A&#10;Content: Mustard . OK .&#10;Speaker: PhD B&#10;Content: Huh .&#10;Speaker: Professor F&#10;Content: Yeah , Well , you 're the {disfmarker} you 're the disk czar now .&#10;Speaker: PhD A&#10;Content: Right , right .&#10;Speaker: Professor F&#10;Content: So&#10;Speaker: PhD A&#10;Content: Well , I 'll check on that .&#10;Speaker: Professor F&#10;Content: Yeah . Yeah , so" target="The speakers discussed using the same normalization scheme as OGI is using for all their experiments, as this would add another dimension to their analysis. They acknowledged having enough dimensions already and thus considered adopting the normalization method from OGI to avoid increasing complexity unnecessarily.">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcript mentions that training a neural network on multiple languages is probably done using a single network, as suggested by PhD G's statement &quot;probably one net.&quot; The specifics of this method are not detailed in the conversation; however, it can be inferred that this would involve combining data from different languages during the training process.&#10;&#10;The speakers also discuss combining neural networks trained on individual languages with a more generic database, as stated by PhD G: &quot;training a net on the target languages but with a general database.&quot; It is important to note that the conversation does not explicitly confirm whether this method has been implemented or not.&#10;&#10;In summary, while the discussion does imply training a neural network using multiple languages in one network and combining individually trained networks, it does not provide explicit details about these methods." target=" , so there 's plenty of stuff around . OK , so anyway , th the basic plan is to , uh , test this cube . Yes .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: To fill in the cube .&#10;Speaker: Professor F&#10;Content: To fill i fill it in , yeah . OK .&#10;Speaker: PhD G&#10;Content: Yeah , and perhaps , um {disfmarker} {pause} We were thinking that perhaps the cross - language issue is not , uh , so big of a issue . Well , w w we {disfmarker} perhaps we should not focus too much on that cross - language stuff . I mean , uh , training {disfmarker} training a net on a language and testing a for another language .&#10;Speaker: Professor F&#10;Content: Uh - huh . But that 's {disfmarker}&#10;Speaker: PhD G&#10;Content: Mmm . Perhaps the most important is to have neural networks trained on the target languages . But , uh , with a general database {disfmarker} general databases . u So that th Well , the {disfmarker} the guy who has to">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcript mentions that training a neural network on multiple languages is probably done using a single network, as suggested by PhD G's statement &quot;probably one net.&quot; The specifics of this method are not detailed in the conversation; however, it can be inferred that this would involve combining data from different languages during the training process.&#10;&#10;The speakers also discuss combining neural networks trained on individual languages with a more generic database, as stated by PhD G: &quot;training a net on the target languages but with a general database.&quot; It is important to note that the conversation does not explicitly confirm whether this method has been implemented or not.&#10;&#10;In summary, while the discussion does imply training a neural network using multiple languages in one network and combining individually trained networks, it does not provide explicit details about these methods." target="} and , uh {disfmarker} So , what it {disfmarker} it blows out is the number of uh testings . And , you know {disfmarker} and the number of times you do that last part . But that last part , I think , is so {disfmarker} has gotta be pretty quick , so . Uh . Right ? I mean , it 's just running the data through {disfmarker}&#10;Speaker: Grad C&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: But wh what about a net that 's trained on multiple languages , though ?&#10;Speaker: Professor F&#10;Content: Well , you gotta do the KL transformation ,&#10;Speaker: PhD G&#10;Content: Eight {disfmarker} y&#10;Speaker: Professor F&#10;Content: but {disfmarker}&#10;Speaker: PhD A&#10;Content: Is that just separate nets for each language then combined , or is that actually one net trained on ?&#10;Speaker: PhD E&#10;Content: Necessary to put in .&#10;Speaker: Professor F&#10;Content: Good question .&#10;Speaker: PhD G&#10;Content: Uh , probably one net .">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcript mentions that training a neural network on multiple languages is probably done using a single network, as suggested by PhD G's statement &quot;probably one net.&quot; The specifics of this method are not detailed in the conversation; however, it can be inferred that this would involve combining data from different languages during the training process.&#10;&#10;The speakers also discuss combining neural networks trained on individual languages with a more generic database, as stated by PhD G: &quot;training a net on the target languages but with a general database.&quot; It is important to note that the conversation does not explicitly confirm whether this method has been implemented or not.&#10;&#10;In summary, while the discussion does imply training a neural network using multiple languages in one network and combining individually trained networks, it does not provide explicit details about these methods." target="1. When evaluating a system's ability to handle a specific language like Portuguese within a multi-language context, it is important to compare the performance of the system with and without including the target language in the training set. This will help determine if the system can adequately process the specific language.&#10;   &#10;2. It may also be beneficial to train separate neural networks for each language and then combine them during the evaluation process, ensuring that the system has been exposed to the phonetic contexts and characteristics of the target language.&#10;&#10;3. Using a multi-valued or multi-dimensional representation for articulatory features can provide a more nuanced understanding of speech sounds, allowing the system to better model various combinations of articulatory gestures and reduce confusions between broad classes like obstruents.&#10;&#10;4. Examining articulatory types as targets for the neural network could help improve phone classification, especially when dealing with differences in phone sets between databases and languages. This might be particularly useful when addressing challenges posed by the specific phoneme set of Portuguese within a multi-language context.&#10;&#10;5. Consider using existing English, Spanish, and French databases as a starting point for the multi-lingual part of the system, as these resources are already available and can provide initial support for processing the target language (Portuguese) in a multi-language context.">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcript mentions that training a neural network on multiple languages is probably done using a single network, as suggested by PhD G's statement &quot;probably one net.&quot; The specifics of this method are not detailed in the conversation; however, it can be inferred that this would involve combining data from different languages during the training process.&#10;&#10;The speakers also discuss combining neural networks trained on individual languages with a more generic database, as stated by PhD G: &quot;training a net on the target languages but with a general database.&quot; It is important to note that the conversation does not explicitly confirm whether this method has been implemented or not.&#10;&#10;In summary, while the discussion does imply training a neural network using multiple languages in one network and combining individually trained networks, it does not provide explicit details about these methods." target="The speakers discussed using the same normalization scheme as OGI is using for all their experiments, as this would add another dimension to their analysis. They acknowledged having enough dimensions already and thus considered adopting the normalization method from OGI to avoid increasing complexity unnecessarily.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The discussion was focused on the observation that most confusions in phone classification occur within the same broad classes, such as obstruents being confused with other obstruents. This led to the idea of examining articulatory types as a potential solution.&#10;2. The participants also discussed the possibility of using a multi-valued or multi-dimensional representation for articulatory features, allowing for the modeling of various combinations of articulatory gestures and a more nuanced understanding of speech sounds.&#10;3. Articulatory features were considered as targets for the neural network, which describe the position and movement of speech organs when producing sounds, typically used in speech synthesis and analysis.&#10;4. The challenge of addressing differences in phone sets between different databases and languages was mentioned, with a specific example of mapping from 61 phonemes in TIMIT to 56 phonemes in ICSI given.&#10;&#10;In summary, the discussion centered around using articulatory type information for classifying phones, considering multiple features being active at once, and addressing challenges posed by phone set differences between databases and languages." target="1. When evaluating a system's ability to handle a specific language like Portuguese within a multi-language context, it is important to compare the performance of the system with and without including the target language in the training set. This will help determine if the system can adequately process the specific language.&#10;   &#10;2. It may also be beneficial to train separate neural networks for each language and then combine them during the evaluation process, ensuring that the system has been exposed to the phonetic contexts and characteristics of the target language.&#10;&#10;3. Using a multi-valued or multi-dimensional representation for articulatory features can provide a more nuanced understanding of speech sounds, allowing the system to better model various combinations of articulatory gestures and reduce confusions between broad classes like obstruents.&#10;&#10;4. Examining articulatory types as targets for the neural network could help improve phone classification, especially when dealing with differences in phone sets between databases and languages. This might be particularly useful when addressing challenges posed by the specific phoneme set of Portuguese within a multi-language context.&#10;&#10;5. Consider using existing English, Spanish, and French databases as a starting point for the multi-lingual part of the system, as these resources are already available and can provide initial support for processing the target language (Portuguese) in a multi-language context.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The discussion was focused on the observation that most confusions in phone classification occur within the same broad classes, such as obstruents being confused with other obstruents. This led to the idea of examining articulatory types as a potential solution.&#10;2. The participants also discussed the possibility of using a multi-valued or multi-dimensional representation for articulatory features, allowing for the modeling of various combinations of articulatory gestures and a more nuanced understanding of speech sounds.&#10;3. Articulatory features were considered as targets for the neural network, which describe the position and movement of speech organs when producing sounds, typically used in speech synthesis and analysis.&#10;4. The challenge of addressing differences in phone sets between different databases and languages was mentioned, with a specific example of mapping from 61 phonemes in TIMIT to 56 phonemes in ICSI given.&#10;&#10;In summary, the discussion centered around using articulatory type information for classifying phones, considering multiple features being active at once, and addressing challenges posed by phone set differences between databases and languages." target="Based on the discussion, it is suggested that using the TIMIT dataset as a source for training a tandem system across languages could be a better choice due to its broadness and careful hand-correction. However, given the limited time and resources, the speakers did not explicitly conclude that the TIMIT dataset's broadness or careful hand-correction is the sole reason for its potential superiority. Further investigation would be required to make a definitive conclusion.">
      <data key="d0">1</data>
    </edge>
    <edge source="The benefits of having multiple SPERT boards for running parallel jobs include:&#10;&#10;1. Increased processing speed and efficiency: As discussed in the meeting, the SPERT board is faster than other alternatives. Having multiple SPERT boards allows users to process and run tasks simultaneously, leading to time savings and increased productivity.&#10;2. Resource allocation: When running out of capacity on one SPERT board, having additional boards enables users to offload tasks and utilize their computer for other activities. This flexibility in resource allocation ensures optimal use of available hardware.&#10;3. Job management: Multiple SPERT boards allow for the setup of multiple jobs, enabling better organization and management of tasks. Users can run different jobs on separate boards and have several jobs running on various computers simultaneously, increasing overall throughput.&#10;4. Parallel processing: Running parallel jobs on multiple SPERT boards is particularly useful in machine learning applications where large data sets require significant processing power. This allows researchers to process massive amounts of data more efficiently than relying on a single board or computer.&#10;5. Improved collaboration: By dedicating specific sections within the cube for people to work on, as suggested during the discussion, multiple SPERT boards can foster better collaboration among team members working on different aspects of a project. This facilitates the division of labor and promotes a more efficient workflow." target="isfmarker} or Dan did some testing and they found that the SPERT board 's still {disfmarker} still faster .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: And the benefits is that , you know , you run out of SPERT and then you can do other things on your {disfmarker} your computer ,&#10;Speaker: Professor F&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: and you don't {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah . So you could be {disfmarker} we have quite a few SPERT boards . You could set up , uh , you know , ten different jobs , or something , to run on SPERT {disfmarker} different SPERT boards and {disfmarker} and have ten other jobs running on different computers . So , it 's got to take that sort of thing , or {disfmarker} or we 're not going to get through any significant number of these .&#10;Speaker: Grad C&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The benefits of having multiple SPERT boards for running parallel jobs include:&#10;&#10;1. Increased processing speed and efficiency: As discussed in the meeting, the SPERT board is faster than other alternatives. Having multiple SPERT boards allows users to process and run tasks simultaneously, leading to time savings and increased productivity.&#10;2. Resource allocation: When running out of capacity on one SPERT board, having additional boards enables users to offload tasks and utilize their computer for other activities. This flexibility in resource allocation ensures optimal use of available hardware.&#10;3. Job management: Multiple SPERT boards allow for the setup of multiple jobs, enabling better organization and management of tasks. Users can run different jobs on separate boards and have several jobs running on various computers simultaneously, increasing overall throughput.&#10;4. Parallel processing: Running parallel jobs on multiple SPERT boards is particularly useful in machine learning applications where large data sets require significant processing power. This allows researchers to process massive amounts of data more efficiently than relying on a single board or computer.&#10;5. Improved collaboration: By dedicating specific sections within the cube for people to work on, as suggested during the discussion, multiple SPERT boards can foster better collaboration among team members working on different aspects of a project. This facilitates the division of labor and promotes a more efficient workflow." target=" board .&#10;Speaker: PhD E&#10;Content: Yes .&#10;Speaker: Professor F&#10;Content: OK , again , we do have a bunch of SPERT boards .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: And I think there {disfmarker} there {disfmarker} there 's {disfmarker} I think you folks are probably go the ones using them right now .&#10;Speaker: PhD A&#10;Content: Is it faster to do it on the SPERT , or {disfmarker} ?&#10;Speaker: Professor F&#10;Content: Uh , don't know .&#10;Speaker: Grad C&#10;Content: It 's {disfmarker} it 's still a little faster on the&#10;Speaker: Professor F&#10;Content: Used to be .&#10;Speaker: PhD A&#10;Content: Is it ?&#10;Speaker: Grad C&#10;Content: Yeah , yeah . Ad - Adam {disfmarker} Adam did some testing . Or either Adam or {disfmarker} or Dan did some testing and they found that the SPERT board 's still {disfmarker} still faster .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="The benefits of having multiple SPERT boards for running parallel jobs include:&#10;&#10;1. Increased processing speed and efficiency: As discussed in the meeting, the SPERT board is faster than other alternatives. Having multiple SPERT boards allows users to process and run tasks simultaneously, leading to time savings and increased productivity.&#10;2. Resource allocation: When running out of capacity on one SPERT board, having additional boards enables users to offload tasks and utilize their computer for other activities. This flexibility in resource allocation ensures optimal use of available hardware.&#10;3. Job management: Multiple SPERT boards allow for the setup of multiple jobs, enabling better organization and management of tasks. Users can run different jobs on separate boards and have several jobs running on various computers simultaneously, increasing overall throughput.&#10;4. Parallel processing: Running parallel jobs on multiple SPERT boards is particularly useful in machine learning applications where large data sets require significant processing power. This allows researchers to process massive amounts of data more efficiently than relying on a single board or computer.&#10;5. Improved collaboration: By dedicating specific sections within the cube for people to work on, as suggested during the discussion, multiple SPERT boards can foster better collaboration among team members working on different aspects of a project. This facilitates the division of labor and promotes a more efficient workflow." target="er} now you 've got me sort of intrigued . What {disfmarker} there 's {disfmarker} Can you describe what {disfmarker} what 's on the cube ?&#10;Speaker: Grad C&#10;Content: Yeah , w I th I think that 's a good idea&#10;Speaker: Professor F&#10;Content: I mean {disfmarker}&#10;Speaker: Grad C&#10;Content: to {disfmarker} to talk about the whole cube&#10;Speaker: Professor F&#10;Content: Yeah , yeah .&#10;Speaker: PhD E&#10;Content: Yeah . Yeah .&#10;Speaker: Grad C&#10;Content: and maybe we could sections in the cube for people to work on .&#10;Speaker: Professor F&#10;Content: Yeah . Yeah .&#10;Speaker: Grad C&#10;Content: Um , OK . Uh , do you wanna do it ?&#10;Speaker: Professor F&#10;Content: OK , so even {disfmarker} even though the meeting recorder doesn't {disfmarker} doesn't , uh {disfmarker} and since you 're not running a video camera we won't get this , but if you use a board it 'll">
      <data key="d0">1</data>
    </edge>
    <edge source="The benefits of having multiple SPERT boards for running parallel jobs include:&#10;&#10;1. Increased processing speed and efficiency: As discussed in the meeting, the SPERT board is faster than other alternatives. Having multiple SPERT boards allows users to process and run tasks simultaneously, leading to time savings and increased productivity.&#10;2. Resource allocation: When running out of capacity on one SPERT board, having additional boards enables users to offload tasks and utilize their computer for other activities. This flexibility in resource allocation ensures optimal use of available hardware.&#10;3. Job management: Multiple SPERT boards allow for the setup of multiple jobs, enabling better organization and management of tasks. Users can run different jobs on separate boards and have several jobs running on various computers simultaneously, increasing overall throughput.&#10;4. Parallel processing: Running parallel jobs on multiple SPERT boards is particularly useful in machine learning applications where large data sets require significant processing power. This allows researchers to process massive amounts of data more efficiently than relying on a single board or computer.&#10;5. Improved collaboration: By dedicating specific sections within the cube for people to work on, as suggested during the discussion, multiple SPERT boards can foster better collaboration among team members working on different aspects of a project. This facilitates the division of labor and promotes a more efficient workflow." target=" this , a choice for this , and a choice for that ,&#10;Speaker: Grad C&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor F&#10;Content: right ? And you 're going through them all . That {disfmarker} that 's what I meant .&#10;Speaker: Grad C&#10;Content: Right , right .&#10;Speaker: Professor F&#10;Content: And , uh , the thing is that once you get a better handle on how much you can realistically do , uh , um , {vocalsound} concurrently on different machines , different SPERTs , and so forth , uh , and you see how long it takes on what machine and so forth , you can stand back from it and say , &quot; OK , if we look at all these combinations we 're talking about , and combinations of combinations , and so forth , &quot; you 'll probably find you can't do it all .&#10;Speaker: Grad C&#10;Content: Mm - hmm . OK .&#10;Speaker: Professor F&#10;Content: OK , so then at that point , uh , we should sort out which ones do we throw away .&#10;Speaker: Grad C&#10;Content: Mm - hmm .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Spain (Spanish accent)&#10;2. France (French accent, specifically from Paris)&#10;3. Texas, USA (TIMIT corpus is from Texas)&#10;4. Finland (Finnish language)&#10;5. Italy (Italian language)&#10;6. Mexican Spain (possibly a mixed or regional accent)&#10;7. Switzerland (Swiss German or French accent was mentioned as a possibility)" target=" Spanish from Spain .&#10;Speaker: Professor F&#10;Content: Yeah , OK .&#10;Speaker: Grad C&#10;Content: From Spain .&#10;Speaker: Professor F&#10;Content: Alright . Spanish from Spain . Yeah , we 're really covered there now . OK .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: And the French from France .&#10;Speaker: PhD G&#10;Content: Yeah , the {disfmarker} No , the French is f yeah , from , uh , Paris ,&#10;Speaker: Grad C&#10;Content: Oh , from Paris , OK .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: And TIMIT 's from {pause} lots of different places .&#10;Speaker: PhD G&#10;Content: OK .&#10;Speaker: Professor F&#10;Content: From TI . From {disfmarker} i It 's from Texas . So may maybe it 's {disfmarker}&#10;Speaker: PhD B&#10;Content: From the deep South .&#10;Speaker: Professor F&#10;Content: So - s so it 's not really from the US either .&#10;Speaker: Grad C">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Spain (Spanish accent)&#10;2. France (French accent, specifically from Paris)&#10;3. Texas, USA (TIMIT corpus is from Texas)&#10;4. Finland (Finnish language)&#10;5. Italy (Italian language)&#10;6. Mexican Spain (possibly a mixed or regional accent)&#10;7. Switzerland (Swiss German or French accent was mentioned as a possibility)" target=" ?&#10;Speaker: Professor F&#10;Content: The newer one .&#10;Speaker: PhD G&#10;Content: So English , uh , Finnish and Italian are Aurora .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: And Spanish and French is something that we can use in addition to Aurora . Uh , well .&#10;Speaker: Professor F&#10;Content: Yeah , so Carmen brought the Spanish , and Stephane brought the French .&#10;Speaker: Grad C&#10;Content: OK . And , um , oh yeah , and {disfmarker}&#10;Speaker: Professor F&#10;Content: Is it French French or Belgian French ? There 's a {disfmarker}&#10;Speaker: PhD G&#10;Content: It 's , uh , French French .&#10;Speaker: Grad C&#10;Content: French French .&#10;Speaker: PhD E&#10;Content: Like Mexican Spain and Spain .&#10;Speaker: Professor F&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Or Swiss .&#10;Speaker: PhD E&#10;Content: I think that is more important ,&#10;Speaker: PhD B&#10;Content: Swiss - German .&#10;Speaker: PhD E&#10;Content: Mexican">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Spain (Spanish accent)&#10;2. France (French accent, specifically from Paris)&#10;3. Texas, USA (TIMIT corpus is from Texas)&#10;4. Finland (Finnish language)&#10;5. Italy (Italian language)&#10;6. Mexican Spain (possibly a mixed or regional accent)&#10;7. Switzerland (Swiss German or French accent was mentioned as a possibility)" target="ound} from the various languages . Um , English Spanish um , French What else do we have ?&#10;Speaker: PhD G&#10;Content: And the {pause} Finnish .&#10;Speaker: Grad C&#10;Content: Finnish .&#10;Speaker: PhD A&#10;Content: Where did th where did that come from ?&#10;Speaker: PhD E&#10;Content: And Italian .&#10;Speaker: PhD A&#10;Content: Digits ?&#10;Speaker: PhD E&#10;Content: Uh , no , Italian no . Italian no .&#10;Speaker: PhD A&#10;Content: Oh .&#10;Speaker: Grad C&#10;Content: Oh . Italian .&#10;Speaker: PhD E&#10;Content: I Italian yes . Italian ?&#10;Speaker: Professor F&#10;Content: Italian .&#10;Speaker: PhD A&#10;Content: Is that {disfmarker} Was that distributed with Aurora , or {disfmarker} ?&#10;Speaker: Grad C&#10;Content: One L or two L 's ?&#10;Speaker: PhD A&#10;Content: Where did that {disfmarker} ?&#10;Speaker: Professor F&#10;Content: The newer one .&#10;Speaker: PhD G&#10;Content: So English , uh , Finnish and Italian are">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Spain (Spanish accent)&#10;2. France (French accent, specifically from Paris)&#10;3. Texas, USA (TIMIT corpus is from Texas)&#10;4. Finland (Finnish language)&#10;5. Italy (Italian language)&#10;6. Mexican Spain (possibly a mixed or regional accent)&#10;7. Switzerland (Swiss German or French accent was mentioned as a possibility)" target=": Ah , but which Dan ?&#10;Speaker: Grad C&#10;Content: Uh , Ellis . Right ?&#10;Speaker: Professor F&#10;Content: OK . OK .&#10;Speaker: Grad C&#10;Content: Yeah . So .&#10;Speaker: PhD A&#10;Content: I was just wondering because that test you 're t&#10;Speaker: Grad C&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: I {disfmarker} I think you 're doing this test because you want to determine whether or not , uh , having s general speech performs as well as having specific {pause} speech .&#10;Speaker: Grad C&#10;Content: That 's right .&#10;Speaker: Professor F&#10;Content: Well , especially when you go over the different languages again , because you 'd {disfmarker} the different languages have different words for the different digits ,&#10;Speaker: PhD A&#10;Content: Mm - hmm . And I was {disfmarker}&#10;Speaker: Professor F&#10;Content: so it 's {disfmarker}&#10;Speaker: PhD A&#10;Content: yeah , so I was just wondering if the fact that TIMIT {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When evaluating a system's ability to handle a specific language like Portuguese within a multi-language context, it is important to compare the performance of the system with and without including the target language in the training set. This will help determine if the system can adequately process the specific language.&#10;   &#10;2. It may also be beneficial to train separate neural networks for each language and then combine them during the evaluation process, ensuring that the system has been exposed to the phonetic contexts and characteristics of the target language.&#10;&#10;3. Using a multi-valued or multi-dimensional representation for articulatory features can provide a more nuanced understanding of speech sounds, allowing the system to better model various combinations of articulatory gestures and reduce confusions between broad classes like obstruents.&#10;&#10;4. Examining articulatory types as targets for the neural network could help improve phone classification, especially when dealing with differences in phone sets between databases and languages. This might be particularly useful when addressing challenges posed by the specific phoneme set of Portuguese within a multi-language context.&#10;&#10;5. Consider using existing English, Spanish, and French databases as a starting point for the multi-lingual part of the system, as these resources are already available and can provide initial support for processing the target language (Portuguese) in a multi-language context." target=" PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Uh , now , what do you do , uh , when somebody has Portuguese ? &quot; you know ? Um , and {disfmarker} Uh , however , you aren't {disfmarker} it isn't actually a constraint in this evaluation . So I would say if it looks like there 's a big difference to put it in , then we 'd make note of it , and then we probably put in the other , because we have so many other problems in trying to get things to work well here that {disfmarker} that , you know , it 's not so bad as long as we {disfmarker} we note it and say , &quot; Look , we did do this &quot; .&#10;Speaker: PhD G&#10;Content: Mmm ?&#10;Speaker: PhD A&#10;Content: And so , ideally , what you 'd wanna do is you 'd wanna run it with and without the target language and the training set for a wide range of languages .&#10;Speaker: Professor F&#10;Content: Uh . Yeah .&#10;Speaker: PhD G&#10;Content: Yeah , perhaps . Yeah .&#10;Speaker: PhD A&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When evaluating a system's ability to handle a specific language like Portuguese within a multi-language context, it is important to compare the performance of the system with and without including the target language in the training set. This will help determine if the system can adequately process the specific language.&#10;   &#10;2. It may also be beneficial to train separate neural networks for each language and then combine them during the evaluation process, ensuring that the system has been exposed to the phonetic contexts and characteristics of the target language.&#10;&#10;3. Using a multi-valued or multi-dimensional representation for articulatory features can provide a more nuanced understanding of speech sounds, allowing the system to better model various combinations of articulatory gestures and reduce confusions between broad classes like obstruents.&#10;&#10;4. Examining articulatory types as targets for the neural network could help improve phone classification, especially when dealing with differences in phone sets between databases and languages. This might be particularly useful when addressing challenges posed by the specific phoneme set of Portuguese within a multi-language context.&#10;&#10;5. Consider using existing English, Spanish, and French databases as a starting point for the multi-lingual part of the system, as these resources are already available and can provide initial support for processing the target language (Portuguese) in a multi-language context." target=" uh , a range of languages and {disfmarker} which can include the test {disfmarker} the test @ @ the target language ,&#10;Speaker: Grad C&#10;Content: Test on an unseen .&#10;Speaker: PhD G&#10;Content: or {disfmarker}&#10;Speaker: Professor F&#10;Content: Yeah , so , um , there 's {disfmarker} there 's , uh {disfmarker} This is complex . So , ultimately , uh , as I was saying , I think it doesn't fit within their image that you switch nets based on language . Now , can you include , uh , the {disfmarker} the target language ?&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor F&#10;Content: Um , from a purist 's standpoint it 'd be nice not to because then you can say when {disfmarker} because surely someone is going to say at some point , &quot; OK , so you put in the German and the Finnish .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: Uh , now , what do you do , uh , when somebody has Portuguese">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When evaluating a system's ability to handle a specific language like Portuguese within a multi-language context, it is important to compare the performance of the system with and without including the target language in the training set. This will help determine if the system can adequately process the specific language.&#10;   &#10;2. It may also be beneficial to train separate neural networks for each language and then combine them during the evaluation process, ensuring that the system has been exposed to the phonetic contexts and characteristics of the target language.&#10;&#10;3. Using a multi-valued or multi-dimensional representation for articulatory features can provide a more nuanced understanding of speech sounds, allowing the system to better model various combinations of articulatory gestures and reduce confusions between broad classes like obstruents.&#10;&#10;4. Examining articulatory types as targets for the neural network could help improve phone classification, especially when dealing with differences in phone sets between databases and languages. This might be particularly useful when addressing challenges posed by the specific phoneme set of Portuguese within a multi-language context.&#10;&#10;5. Consider using existing English, Spanish, and French databases as a starting point for the multi-lingual part of the system, as these resources are already available and can provide initial support for processing the target language (Portuguese) in a multi-language context." target=" question is how important is it to {disfmarker} for us to get multiple languages uh , in there .&#10;Speaker: PhD G&#10;Content: Yeah , but {disfmarker} Mm - hmm . {vocalsound} Um . Yeah . Well , actually , for the moment if we w do not want to use these phone databases , we {disfmarker} we already have uh {disfmarker} English , Spanish and French uh , with microphone speech . &#10;Speaker: Professor F&#10;Content: Mm - hmm . Yeah .&#10;Speaker: PhD G&#10;Content: So .&#10;Speaker: Professor F&#10;Content: So that 's what you 're thinking of using is sort of the multi the equivalent of the multiple ?&#10;Speaker: PhD G&#10;Content: Well . Yeah , for the multilingual part we were thinking of using these three databases .&#10;Speaker: Professor F&#10;Content: And for the difference in phonetic context {pause} that you {disfmarker} ? Provide that .&#10;Speaker: PhD G&#10;Content: Well , this {disfmarker} Uh , actually , these three databases are um generic databases .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcript does not provide specific information about the available disk space on the Nutmeg and Mustard computational servers. However, it is mentioned that they are computational servers and there are discussions around checking for space in slash-scratch and other disks (slash-X-whatever) on Mustard and Nutmeg. It is recommended to use local storage if running experiments multiple times to avoid network overhead. To get the exact available disk space, it is advised to manually check by logging onto those servers and using commands like &quot;DF -K&quot; or checking the specific directories suggested in the transcripts." target="disfmarker} i so , you 're on an NT machine , so you 're using some external machine&#10;Speaker: PhD G&#10;Content: Yeah , it , uh {disfmarker} Well , to {disfmarker} It 's Nutmeg and Mustard , I think , &#10;Speaker: Professor F&#10;Content: Do you know these yet ?&#10;Speaker: PhD G&#10;Content: I don't know what kind .&#10;Speaker: PhD A&#10;Content: Nuh - uh .&#10;Speaker: Professor F&#10;Content: Yeah , OK . Uh , are these {disfmarker} are these , uh , computational servers , or something ? I 'm {disfmarker} I 've been kind of out of it .&#10;Speaker: PhD G&#10;Content: Yeah , I think , yeah . I think so .&#10;Speaker: Professor F&#10;Content: Unfortunately , these days my idea of running comput of computa doing computation is running a spread sheet .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content: So .&#10;Speaker: PhD G&#10;Content: Mmm .&#10;Speaker: Professor F&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcript does not provide specific information about the available disk space on the Nutmeg and Mustard computational servers. However, it is mentioned that they are computational servers and there are discussions around checking for space in slash-scratch and other disks (slash-X-whatever) on Mustard and Nutmeg. It is recommended to use local storage if running experiments multiple times to avoid network overhead. To get the exact available disk space, it is advised to manually check by logging onto those servers and using commands like &quot;DF -K&quot; or checking the specific directories suggested in the transcripts." target="The speakers discussed using the same normalization scheme as OGI is using for all their experiments, as this would add another dimension to their analysis. They acknowledged having enough dimensions already and thus considered adopting the normalization method from OGI to avoid increasing complexity unnecessarily.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers discussed using the same normalization scheme as OGI is using for all their experiments, as this would add another dimension to their analysis. They acknowledged having enough dimensions already and thus considered adopting the normalization method from OGI to avoid increasing complexity unnecessarily." target=" it 'll free up more space for doing the experiments and things . So , anything that {disfmarker} that you don't need backed up , we can put on this new disk . Um , but if it 's experiments and you 're creating files and things that you 're gonna need , you probably wanna have those on a disk that 's backed up , just in case something {comment} goes wrong . So . Um So far I 've {disfmarker} I 've copied a couple of things , but I haven't deleted anything off of the old disk to make room yet . Um , and I haven't looked at the {disfmarker} any of the Aurora stuff , except for the Spanish . So I {disfmarker} I guess I 'll need to get together with you and see what data we can move onto the new disk .&#10;Speaker: PhD G&#10;Content: Yeah , OK .&#10;Speaker: Professor F&#10;Content: Um , yeah , I {disfmarker} I just {disfmarker} an another question occurred to me is {disfmarker} is what were you folks planning to do about normalization ?&#10;Speaker: PhD G&#10;">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
