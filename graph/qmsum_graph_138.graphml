<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="1. The method for phone recognition mentioned in the transcript involves calculating the likelihood of an event (such as onset or offset of a phonological feature like voicing) given a phone. This is done by counting co-occurrences of the event and the phone, and dividing it by the number of occurrences of the phone. This gives the probability of the event occurring given the phone.&#10;2. According to Professor C, this method would be effective in recognizing phones if onset and end of voicing events are accurately marked. They mentioned an example where, if these events are correctly identified, counting their co-occurrences with specific phones should result in accurate phone recognition.&#10;3. However, it is important to note that this method relies on the accurate identification and marking of phonological events, which can be challenging and prone to errors. Nonetheless, when done accurately, this simple HMM (Hidden Markov Model) based approach can be effective for phone recognition tasks." />
    <node id=" the {disfmarker} the uh {disfmarker} uh uh occ uh count the co - occurrences between the event and the phone and divide them by the number of occurrences of the phone , and that would give you the likelihood of the {disfmarker} of the event given the phone . And um then just use that in a very simple HMM and uh you could uh do phone recognition then and uh wouldn't have any of the issues of the uh training of the net or {disfmarker} I mean , it 'd be on the simple side , but&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: uh um you know , if {disfmarker} uh uh the example I was giving was that if {disfmarker} if you had um onset of voicing and {disfmarker} and end of voicing as being two kinds of events , then if you had those a all marked correctly , and you counted co - occurrences , you should get it completely right .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So . um {disfmarker} But" />
    <node id=" phone ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Uh , is this the class project , or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: OK . um&#10;Speaker: PhD E&#10;Content: Is that what you 're {disfmarker}&#10;Speaker: Grad A&#10;Content: Right , {comment} Right , right f so for every phone there is {disfmarker} there is a um {disfmarker} a vector of ones and zeros {vocalsound} f uh corresponding to whether it exhibits a particular phonological feature or not .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm . And so when you do your wh I 'm {disfmarker} what is the task for the class project ? To come up with the phones ?&#10;Speaker: Grad A&#10;Content: Um&#10;Speaker: PhD E&#10;Content: or to come up with these vectors to see how closely they match the phones ,&#10;Speaker: Grad A&#10;Content: Oh . Right , um to come up with a mapping from um MFCC" />
    <node id=" have uh {vocalsound} um a chosen set of features , or acoustic events , and we train up a hybrid {vocalsound} um system to do phone recognition on TIMIT . So i i the idea is if we get good phone recognition results , {vocalsound} using um these set of acoustic events , {vocalsound} then {vocalsound} um that {disfmarker} that says that these acoustic events are g sufficient to cover {vocalsound} a set of phones , at least found in TIMIT . Um so i it would be a {disfmarker} {vocalsound} a measure of &quot; are we on the right track with {disfmarker} with the {disfmarker} the choices of our acoustic events &quot; . Um , {vocalsound} So that 's going on . And {vocalsound} also , just uh working on my {vocalsound} uh final project for Jordan 's class , uh which is {disfmarker}&#10;Speaker: Professor C&#10;Content: Actually , let me {disfmarker}&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Hold that thought" />
    <node id=" see how closely they match the phones ,&#10;Speaker: Grad A&#10;Content: Oh . Right , um to come up with a mapping from um MFCC 's or s some feature set , {vocalsound} um to {vocalsound} uh w to whether there 's existence of a particular phonological feature .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ? Mm - hmm .&#10;Speaker: Grad A&#10;Content: And um yeah , basically it 's to learn a mapping {vocalsound} from {disfmarker} {vocalsound} from the MFCC 's to {vocalsound} uh phonological features . Is it {disfmarker} did that answer your question ?&#10;Speaker: PhD E&#10;Content: I think so .&#10;Speaker: Grad A&#10;Content: OK . C&#10;Speaker: PhD E&#10;Content: I guess {disfmarker} I mean , uh {disfmarker} I 'm not sure what you {disfmarker} what you 're {disfmarker} what you get out of your system . Do you get out a uh {disfmarker} a vector" />
    <node id=" . Then uh I talked a little bit about {vocalsound} um continuing with these dynamic ev um acoustic events , and um {vocalsound} {vocalsound} we 're {disfmarker} we 're {disfmarker} we 're {vocalsound} thinking about a way to test the completeness of a {disfmarker} a set of um dynamic uh events . Uh , completeness in the {disfmarker} in the sense that {vocalsound} um if we {disfmarker} if we pick these X number of acoustic events , {vocalsound} do they provide sufficient coverage {vocalsound} for the phones that we 're trying to recognize {vocalsound} or {disfmarker} or the f the words that we 're gonna try to recognize later on . And so Morgan and I were uh discussing {vocalsound} um s uh s a form of a cheating experiment {vocalsound} where we get {disfmarker} {vocalsound} um we have uh {vocalsound} um a chosen set of features , or acoustic events , and we train up a hybrid {vocalsound} um system to" />
    <node id="aker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So . um {disfmarker} But you 'd get all the other distinctions , you know , randomly wrong . I mean there 'd be nothing to tell you that . So um {vocalsound} uh If you just do this by counting , then you should be able to find out in a pretty straightforward way whether you have a sufficient uh set of events to {disfmarker} to do the kind of level of {disfmarker} {vocalsound} of uh classification of phones that you 'd like . So that was {disfmarker} that was the idea . And then the other thing that we were discussing was {disfmarker} was um {vocalsound} OK , how do you get the {disfmarker} your training data .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Cuz uh the {vocalsound} Switchboard transcription project uh uh you know was half a dozen people , or so working off and on over a couple years , and uh similar {disfmarker} {voc" />
    <node id="1. The new feature that PhD D has been working on this week to detect voice-unvoice is a feature related to the Aurora system with a new filter, referred to as VAD (Voice Activity Detection) or something similar to it.&#10;2. PhD D is trying to use multiple Multi-Layer Perceptrons (MLPs) with the new feature and fifteen features from the bus base system in the Mel Cepstrum. One MLP has three outputs for voice, unvoice, and silence, while the other MLP has fifty-six outputs for the probabilities of allophones. He conducted some recognition experiments with these and found that the results were a little better but more or less similar when using the MLP with three outputs." />
    <node id="Speaker: Professor C&#10;Content: OK . So uh , he 's not here ,&#10;Speaker: PhD D&#10;Content: So .&#10;Speaker: Professor C&#10;Content: so you get to {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah , I will try to explain the thing that I did this {disfmarker} this week {disfmarker} during this week .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Well eh you know that I work {disfmarker} I begin to work with a new feature to detect voice - unvoice .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: What I trying two MLP to {disfmarker} to the {disfmarker} with this new feature and the fifteen feature uh from the eh bus base system&#10;Speaker: PhD E&#10;Content: The {disfmarker} the mel cepstrum ?&#10;Speaker: PhD D&#10;Content: No , satly the mes the Mel Cepstrum , the new base system {disfmarker} the new base system" />
    <node id=" PhD D&#10;Content: No , satly the mes the Mel Cepstrum , the new base system {disfmarker} the new base system .&#10;Speaker: PhD E&#10;Content: Oh the {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah , we {disfmarker}&#10;Speaker: PhD E&#10;Content: OK , the Aurora system .&#10;Speaker: PhD D&#10;Content: yeah the Aurora system with the new filter , VAD or something like that .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: And I 'm trying two MLP , one one that only have t three output , voice , unvoice , and silence ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: and other one that have fifty - six output . The probabilities of the allophone . And I tried to do some experiment of recognition with that and only have result with {disfmarker} with the MLP with the three output . And I put together the fifteen features and the three MLP output . And , well , the result are li a little bit better , but more or" />
    <node id=" the {disfmarker} {vocalsound} {comment} S R I system looks like it works is that it reads the wavefiles directly , uh and does all of the cepstral computation stuff on the fly .&#10;Speaker: Professor C&#10;Content: Right . Right .&#10;Speaker: PhD E&#10;Content: And , so there 's no place where these {disfmarker} where the cepstral files are stored , anywhere that I can go look at and compare to the PLP ones , so whereas with our features , he 's actually storing the cepstrum on disk , and he reads those in .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: But it looked like he had to give it {disfmarker} uh even though the cepstrum is already computed , he has to give it uh a front - end parameter file . Which talks about the kind of uh com computation that his mel cepstrum thing does ,&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: so i I {disfmarker} I don't know if that {disfmark" />
    <node id=" output . And I put together the fifteen features and the three MLP output . And , well , the result are li a little bit better , but more or less similar .&#10;Speaker: Professor C&#10;Content: Uh , I {disfmarker} I 'm {disfmarker} I 'm slightly confused .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: What {disfmarker} what feeds the uh {disfmarker} the three - output net ?&#10;Speaker: PhD D&#10;Content: Voice , unvoice , and si&#10;Speaker: Professor C&#10;Content: No no , what feeds it ? What features does it see ?&#10;Speaker: PhD D&#10;Content: The feature {disfmarker} the input ? The inputs are the fifteen {disfmarker} the fifteen uh bases feature .&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: the {disfmarker} with the new code . And the other three features are R , the variance of the difference between the two spectrum ,&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD" />
    <node id="s not that critical . I mean there 's {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: You can {disfmarker} You can throw away stuff below a hundred hertz or so and it 's just not going to affect phonetic classification at all .&#10;Speaker: PhD E&#10;Content: Another thing I was thinking about was um is there a {disfmarker} I was wondering if there 's maybe um {vocalsound} certain settings of the parameters when you compute PLP which would basically cause it to output mel cepstrum . So that , in effect , what I could do is use our code but produce mel cepstrum and compare that directly to {disfmarker}&#10;Speaker: Professor C&#10;Content: Well , it 's not precisely . Yeah . I mean ,&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: um , {vocalsound} um what you can do is um you can definitely change the {disfmarker} the filter bank from being uh a uh trapezoidal integration to a {disfmarker} a {d" />
    <node id=" that I did notice was that the um log likelihoods coming out of the log recognizer from the PLP data were much lower , much smaller , than for the mel cepstral stuff , and that the average amount of pruning that was happening was therefore a little bit higher for the PLP features .&#10;Speaker: Professor C&#10;Content: Oh - huh !&#10;Speaker: PhD E&#10;Content: So , since he used the same exact pruning thresholds for both , I was wondering if it could be that we 're getting more pruning .&#10;Speaker: Professor C&#10;Content: Oh ! He {disfmarker} he {disfmarker} {vocalsound} He used the identical pruning thresholds even though the s the range of p of the likeli&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Oh well that 's {disfmarker} {vocalsound} That 's a pretty good {comment} point right there .&#10;Speaker: PhD E&#10;Content: Right . Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Yeah ,&#10;Speaker: Professor C&#10;Content" />
    <node id="Based on the transcript, the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated. However, it is mentioned that the error rate for a certain setup was reduced from around 80% to 4% with the use of a specific method. It can be inferred that the remaining error rate of 4% corresponds to the voiced-unvoiced frame error measurement accuracy rate of approximately 96%. However, this conclusion is based on indirect information and may not accurately reflect the true value discussed in the conversation." />
    <node id=" PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: Yeah , it 's probably worth it .&#10;Speaker: Grad A&#10;Content: What are {disfmarker} what are your f uh frame error rates for {disfmarker} for this ?&#10;Speaker: PhD D&#10;Content: Eh fifty - f six uh no , the frame error rate ?&#10;Speaker: Grad A&#10;Content: O&#10;Speaker: PhD D&#10;Content: Fifty - six I think .&#10;Speaker: Professor C&#10;Content: Is that {disfmarker} maybe that 's accuracy ?&#10;Speaker: PhD D&#10;Content: Percent .&#10;Speaker: Grad A&#10;Content: Fif - fifty - six percent accurate for v voice - unvoice&#10;Speaker: PhD D&#10;Content: The accuracy . Mm - hmm . No for , yes f I don't remember for voice - unvoice ,&#10;Speaker: Grad A&#10;Content: Oh , OK .&#10;Speaker: PhD D&#10;Content: maybe for the other one .&#10;Speaker: Grad A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Yeah , voiced - unvoiced hopefully would be a lot" />
    <node id="} {vocalsound} I take that frame and four f the four {disfmarker} I take {disfmarker} Sorry , I take the current frame and the four past frames and the {vocalsound} four future frames and that adds up to six seconds of speech . And I calculate um {vocalsound} the spectral mean , {vocalsound} of the log magnitude spectrum {pause} over that N . I use that to normalize the s the current center frame {vocalsound} by mean subtraction . And I then {disfmarker} then I move to the next frame and I {disfmarker} {vocalsound} I do it again . Well , actually I calculate all the means first and then I do the subtraction . And um {vocalsound} the {disfmarker} I tried that with HDK , the Aurora setup of HDK training on clean TI - digits , and um {vocalsound} it {disfmarker} it helped um in a phony reverberation case um {vocalsound} where I just used the simulated impulse response um {vocalsound} the error rate went from something like eighty it was from something like eighteen percent" />
    <node id="aker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: but then you have something like spectral slope , which is you get like R - one ov over R - zero or something like that .&#10;Speaker: PhD D&#10;Content: Uh yeah .&#10;Speaker: PhD E&#10;Content: What are the R 's ?&#10;Speaker: Professor C&#10;Content: R correlations .&#10;Speaker: PhD E&#10;Content: I 'm sorry I missed it .&#10;Speaker: PhD D&#10;Content: No , R c No .&#10;Speaker: PhD E&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: Auto - correlation ? Yes , yes , the variance of the auto - correlation function that uses that&#10;Speaker: Professor C&#10;Content: Ye - Well that 's the variance , but if you just say &quot; what is {disfmarker} &quot; I mean , to first order , um yeah one of the differences between voiced , unvoiced and silence is energy . Another one is {disfmarker} but the other one is the spectral shape .&#10;Speaker: PhD D&#10;Content: Yeah , I I 'll {disfmarker} The spectral shape ," />
    <node id="alsound} where I just used the simulated impulse response um {vocalsound} the error rate went from something like eighty it was from something like eighteen percent {vocalsound} to um four percent . And on meeting rec recorder far mike digits , mike {disfmarker} on channel F , it went from um {vocalsound} {vocalsound} forty - one percent error to eight percent error .&#10;Speaker: PhD E&#10;Content: On {disfmarker} on the real data , not with artificial reverb ?&#10;Speaker: Grad B&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Grad B&#10;Content: And that {disfmarker} that was um {vocalsound} trained on clean speech only , which I 'm guessing is the reason why the baseline was so bad . And {disfmarker}&#10;Speaker: Professor C&#10;Content: That 's ac actually a little side point is I think that 's the first results that we have uh uh uh of any sort on the far field uh {disfmarker} on {disfmarker} on the far field data uh for {disf" />
    <node id="&#10;Speaker: Grad A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Yeah , voiced - unvoiced hopefully would be a lot better .&#10;Speaker: PhD D&#10;Content: for voiced . I don't reme&#10;Speaker: Grad A&#10;Content: Should be in nineties somewhere .&#10;Speaker: PhD D&#10;Content: Better . Maybe for voice - unvoice .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: This is for the other one . I should {disfmarker} I can't show that .&#10;Speaker: Grad A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: But I think that fifty - five was for the {disfmarker} when the output are the fifty - six phone .&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: That I look in the {disfmarker} with the other {disfmarker} nnn the other MLP that we have are more or less the same number . Silence will be better but more or less the same .&#10;Speaker: Professor C&#10;Content: I think at" />
    <node id=" have are more or less the same number . Silence will be better but more or less the same .&#10;Speaker: Professor C&#10;Content: I think at the frame level for fifty - six that was the kind of number we were getting for {disfmarker} for uh um reduced band width uh stuff .&#10;Speaker: PhD D&#10;Content: I think that {disfmarker} I {disfmarker} I {disfmarker} I think that for the other one , for the three output , is sixty sixty - two , sixty three more or less .&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's all ?&#10;Speaker: PhD D&#10;Content: It 's {disfmarker} Yeah .&#10;Speaker: Professor C&#10;Content: That 's pretty bad .&#10;Speaker: PhD D&#10;Content: Yeah , because it 's noise also .&#10;Speaker: Grad A&#10;Content: Oh yeah .&#10;Speaker: Professor C&#10;Content: Aha !&#10;Speaker: PhD D&#10;Content: And we have&#10;Speaker: Professor C&#10;Content: Aha ! Yeah . Yeah . OK" />
    <node id="The output result of the reduced &quot;vocalsound&quot; parameterization in the model is either a zero or a one. This is because the model has been simplified to keep only certain selected examples, resulting in a binary output. The original parameterization might have produced a number between zero and one, but due to the reduction in complexity, it no longer does so." />
    <node id="disfmarker} a reduced um {vocalsound} parameterization of {disfmarker} of the {disfmarker} the model by just keeping {vocalsound} certain selected examples .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Yeah . So .&#10;Speaker: Professor C&#10;Content: But I don't know if people have done sort of careful comparisons of this on large tasks or anything . Maybe {disfmarker} maybe they have . I don't know .&#10;Speaker: Grad A&#10;Content: Yeah , I don't know either .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad B&#10;Content: S do you get some kind of number between zero and one at the output ?&#10;Speaker: Grad A&#10;Content: Actually you don't get a {disfmarker} you don't get a nice number between zero and one . You get {disfmarker} you get either a zero or a one . Um , uh there are {disfmarker} there are pap Well , basically , it 's {disfmarker} it 's um {vocalsound} you {d" />
    <node id="Content: Hmm .&#10;Speaker: Professor C&#10;Content: And th the You know , um neural net approach uh or Gaussian mixtures for that matter are sort of {disfmarker} fairly brute force kinds of things , where you sort of {disfmarker} {vocalsound} you predefine that there is this big bunch of parameters and then you {disfmarker} you place them as you best can to define the boundaries , and in fact , as you know , {vocalsound} these things do take a lot of parameters and {disfmarker} and uh {vocalsound} if you have uh only a modest amount of data , you have trouble {vocalsound} uh learning them . Um , so I {disfmarker} I guess the idea to this is that it {disfmarker} it is reputed to uh be somewhat better in that regard .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: Right . I it can be a {disfmarker} a reduced um {vocalsound} parameterization of {disfmarker} of the {disfmarker} the model" />
    <node id=": Yeah .&#10;Speaker: Professor C&#10;Content: OK , and you were saying something {disfmarker} starting to say something else about your {disfmarker} your class project , or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Oh . Yeah th Um .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: So for my class project I 'm {vocalsound} um {vocalsound} {vocalsound} I 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . And so I 'm gonna apply that to {vocalsound} um compare it with the results by um King and Taylor who did {vocalsound} um these um using recurrent neural nets , they recognized {vocalsound} um {vocalsound} a set of phonological features um and made a mapping from the MFCC 's to these phonological features , so I 'm gonna {vocalsound} do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker" />
    <node id=" what you 're {disfmarker} what you get out of your system . Do you get out a uh {disfmarker} a vector of these ones and zeros and then try to find the closest matching phoneme to that vector ,&#10;Speaker: Grad A&#10;Content: Mm - hmm . Oh .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: No , no . I 'm not {disfmarker} I 'm not planning to do any {disfmarker} any phoneme mapping yet . Just {disfmarker} {vocalsound} it 's {disfmarker} it 's basically {disfmarker} it 's {disfmarker} it 's really simple , basically a detection {vocalsound} of phonological features .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Grad A&#10;Content: Yeah ,&#10;Speaker: PhD E&#10;Content: I see .&#10;Speaker: Grad A&#10;Content: and um {vocalsound} {vocalsound} cuz the uh {disfmarker} So" />
    <node id="1. Better results with triangular filters in RASTA: The speakers mention that some people have claimed better performance using triangular filters in RASTA instead of trapezoidal ones. This could be due to the fact that typical mel cepstral filter banks use triangular filters, which might lead to better results when applied to this problem.&#10;2. Training data issues: The speakers discuss the possibility of training data being &quot;messed up,&quot; causing undesirable behavior in the system. This could mean that there are errors or inconsistencies in the data used for training the models, leading to poor performance. One possible issue mentioned is a difference in normalization, which might not account for all discrepancies but could be contributing to the problem.&#10;3. Compression issues: Another possibility raised by PhD E is the presence of certain types of compression or other unnoticed issues within a subset of the training data. This would cause the computed features for this subset to be incorrect, leading to poor performance and models that are &quot;all messed up.&quot;" />
    <node id=" hmm .&#10;Speaker: Professor C&#10;Content: but we did {disfmarker} eh we did hear this comment from people at some point , that {vocalsound} um it uh they got some better results with the triangular filters rather than the trapezoidal . So that is an option in RASTA .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: Uh and you can certainly play with that . But I think you 're probably doing the right thing to look for bugs first . I don't know .&#10;Speaker: PhD E&#10;Content: Yeah just {disfmarker} it just seems like this kind of behavior could be caused by you know s some of the training data being messed up .&#10;Speaker: Professor C&#10;Content: Could be .&#10;Speaker: PhD E&#10;Content: You know , you 're sort of getting most of the way there , but there 's a {disfmarker} So I started going through and looking {disfmarker} One of the things that I did notice was that the um log likelihoods coming out of the log recognizer from the PLP data were much lower , much smaller , than for" />
    <node id=" definitely change the {disfmarker} the filter bank from being uh a uh trapezoidal integration to a {disfmarker} a {disfmarker} a triangular one ,&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: which is what the typical mel {disfmarker} mel cepstral uh filter bank does .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And some people have claimed that they got some better performance doing that , so you certainly could do that easily . But the fundamental difference , I mean , there 's other small differences {disfmarker}&#10;Speaker: PhD E&#10;Content: There 's a cubic root that happens , right ?&#10;Speaker: Professor C&#10;Content: Yeah , but , you know , as opposed to the log in the other case . I mean {vocalsound} the fundamental d d difference that we 've seen any kind of difference from before , which is actually an advantage for the P L P i uh , I think , is that the {disfmarker} the smoothing at the end is auto -" />
    <node id=" {disfmarker}&#10;Speaker: Professor C&#10;Content: But see {disfmarker} see my point ? If you had {disfmarker} {vocalsound} If you had ten filters , {vocalsound} then you would be throwing away a lot at the two ends .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And if you had {disfmarker} if you had fifty filters , you 'd be throwing away hardly anything .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Um , I don't remember there being an independent way of saying &quot; we 're just gonna make them from here to here &quot; .&#10;Speaker: PhD E&#10;Content: Use this analysis bandwidth or something .&#10;Speaker: Professor C&#10;Content: But I {disfmarker} I {disfmarker} I don't know , it 's actually been awhile since I 've looked at it .&#10;Speaker: PhD E&#10;Content: Yeah , I went through the Feacalc code and then looked at you know just calling the RASTA" />
    <node id=" I agree , but I thought that the normalization difference was one of the possibilities ,&#10;Speaker: PhD E&#10;Content: and {disfmarker} Yeah , but I don't {disfmarker} I 'm not {disfmarker}&#10;Speaker: Professor C&#10;Content: right ?&#10;Speaker: PhD E&#10;Content: Yeah , I guess I don't think that the normalization difference is gonna account for everything .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: So what I was working on is um just going through and checking the headers of the wavefiles , to see if maybe there was a um {disfmarker} a certain type of compression or something that was done that my script wasn't catching . So that for some subset of the training data , uh the {disfmarker} the {disfmarker} the features I was computing were junk .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: Which would you know cause it to perform OK , but uh , you know , the {disfmarker} the models would be all messed up . So I was going through and just" />
    <node id=" do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker: PhD E&#10;Content: So what 's the advantage of support vector machines ? What {disfmarker}&#10;Speaker: Grad A&#10;Content: Um . So , support vector machines are {disfmarker} are good with dealing with a less amount of data&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: and um so if you {disfmarker} if you give it less data it still does a reasonable job {vocalsound} in learning the {disfmarker} the patterns .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Um and {vocalsound} um&#10;Speaker: Professor C&#10;Content: I guess it {disfmarker} yeah , they 're sort of succinct , and {disfmarker} and they {vocalsound} uh&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Does there some kind of a distance metric that they use or how do they {d" />
    <node id="1. Using a &quot;net&quot; or machine learning model: The speakers suggest using a neural network or &quot;net&quot; to learn and implement measures over time. They propose feeding the log magnitude spectrum into the neural net, as it is good at figuring out what it most needs from various inputs.&#10;2. When not to use the net: It might be more beneficial to not use the net when you have exactly the right thing or a clear understanding of how to implement the measures without relying on the net. This would prevent asking the net to learn something that is already understood, which could lead to better results. However, this approach assumes that the data being used is correct and well-normalized, as any issues in the training data can still cause problems in the models." />
    <node id="aker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Then you have this over time , and {disfmarker} and both kinds of measures and uh you might get uh something better .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Um .&#10;Speaker: PhD E&#10;Content: So {disfmarker} so don't uh {disfmarker} don't do the division , but let the net have everything .&#10;Speaker: Professor C&#10;Content: That 's another thing you could do yeah . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Um . I mean , it seems to me , if you have exactly the right thing then it 's better to do it without the net because otherwise you 're asking the net to learn this {disfmarker} you know , say if you wanted to learn how to do multiplication .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: I mean you could feed it a bunch of s you could feed two numbers that you" />
    <node id=" that {disfmarker} that one could use , but I think that , for me , the thing that {disfmarker} that struck me was that uh you wanna get something back here , so here 's {disfmarker} here 's an idea . uh What about it you skip all the {disfmarker} all the really clever things , and just fed the log magnitude spectrum into this ?&#10;Speaker: PhD D&#10;Content: Ah {disfmarker} I 'm sorry .&#10;Speaker: Professor C&#10;Content: This is f You have the log magnitude spectrum , and you were looking at that and the difference between the filter bank and {disfmarker} and c c computing the variance .&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's a clever thing to do .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: What if you stopped being clever ? And you just took this thing in here because it 's a neural net and neural nets are wonderful&#10;Speaker: PhD D&#10;Content: Mm - hmm ." />
    <node id=" took this thing in here because it 's a neural net and neural nets are wonderful&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: and figure out what they can {disfmarker} what they most need from things , and I mean that 's what they 're good at .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So I mean you 're {disfmarker} you 're {disfmarker} you 're trying to be clever and say what 's the statistic that should {disfmarker} we should get about this difference but uh in fact , you know maybe just feeding this in or {disfmarker} or feeding both of them in&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: you know , another way , saying let it figure out what 's the {disfmarker} what is the interaction , especially if you do this over multiple frames ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Then you have this over time , and {d" />
    <node id="The particular measure chosen by the speaker to integrate out the effects of pitch is the variance of the difference between the log FFT power spectrum and the filter bank. The choice of this statistic is not explicitly stated in the transcript, but it is mentioned that it was chosen because it might be a useful measure to compare the two.&#10;&#10;The reason why the variance of this difference stood out as a potential useful statistic is not explicitly explained in the transcript either, but it can be inferred that it has something to do with the fact that it allows for a comparison between the original signal (or FFT) and the filter bank. The speakers mention that using the variance, they were able to observe certain differences between the two, which is why it was considered as a potential useful statistic.&#10;&#10;Additionally, it's mentioned that there might be other measures or energy measures that could be used instead of the variance, but the speakers do not go into detail about what those measures might be. They also mention that in the past, people have used zero crossing counts for this purpose." />
    <node id=" you feed it something different . And something different in some fundamental way . And so the kind of thing that {disfmarker} that she was talking about before , was looking at something uh ab um {disfmarker} something uh about the difference between the {disfmarker} the uh um log FFT uh log power uh and the log magnitude uh F F - spectrum uh and the um uh filter bank .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: And so the filter bank is chosen in fact to sort of integrate out the effects of pitch and she 's saying you know trying {disfmarker} So the particular measure that she chose was the variance of this m of this difference , but that might not be the right number .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Maybe .&#10;Speaker: Professor C&#10;Content: Right ? I mean maybe there 's something about the variance that 's {disfmarker} that 's not enough or maybe there 's something else that {disfmarker} that one could use , but I think that , for me , the thing that {disfmarker} that struck me" />
    <node id=" .&#10;Speaker: Professor C&#10;Content: or i i you know you 'd have some other energy measure and like in the old days people did like uh zero crossing counts .&#10;Speaker: PhD D&#10;Content: Yeah , yeah .&#10;Speaker: Professor C&#10;Content: Right . S S&#10;Speaker: PhD D&#10;Content: Well , I can also th use this .&#10;Speaker: Professor C&#10;Content: Yeah . Um ,&#10;Speaker: PhD D&#10;Content: Bec - because the result are a little bit better but we have in a point that everything is more or less the similar {disfmarker} more or less similar .&#10;Speaker: Professor C&#10;Content: Yeah . But um&#10;Speaker: PhD D&#10;Content: It 's not quite better .&#10;Speaker: Professor C&#10;Content: Right , but it seemed to me that what you were what you were getting at before was that there is something about the difference between the original signal or the original FFT and with the filter which is what {disfmarker} and the variance was one take uh on it .&#10;Speaker: PhD D&#10;Content: Yeah , I used this too .&#10;Speaker: Professor C&#10;" />
    <node id=" {disfmarker}&#10;Speaker: PhD E&#10;Content: At least I couldn't see any kind of control for that .&#10;Speaker: Professor C&#10;Content: Yeah , I don't think it 's in there , I think it 's in the uh uh uh the filters . So , the F F T is on everything , but the filters um , for instance , ignore the {disfmarker} the lowest bins and the highest bins . And what it does is it {disfmarker} it copies&#10;Speaker: PhD E&#10;Content: The {disfmarker} the filters ? Which filters ?&#10;Speaker: Professor C&#10;Content: um The filter bank which is created by integrating over F F T bins .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: um&#10;Speaker: PhD E&#10;Content: When you get the mel {disfmarker} When you go to the mel scale .&#10;Speaker: Professor C&#10;Content: Right . Yeah , it 's bark scale , and it 's {disfmarker} it {disfmarker} it um {disfmarker} it actually copies" />
    <node id="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals." />
    <node id="&#10;Speaker: Grad A&#10;Content: and um {vocalsound} {vocalsound} cuz the uh {disfmarker} So King and {disfmarker} and Taylor {vocalsound} um did this with uh recurrent neural nets ,&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: and this i their {disfmarker} their idea was to first find {vocalsound} a mapping from MFCC 's to {vocalsound} uh phonological features&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: and then later on , once you have these {vocalsound} phonological features , {vocalsound} then uh map that to phones .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: So I 'm {disfmarker} I 'm sort of reproducing phase one of their stuff .&#10;Speaker: PhD E&#10;Content: Mmm . So they had one recurrent net for each particular feature ?&#10;Speaker: Grad A&#10;Content: Right . Right . Right . Right .&#10;" />
    <node id=" Grad B&#10;Content: Oh . OK .&#10;Speaker: Grad A&#10;Content: So uh for example , {vocalsound} this {disfmarker} this uh feature set called the uh sound patterns of English {vocalsound} um is just a bunch of {vocalsound} um {vocalsound} binary valued features . Let 's say , is this voicing , or is this not voicing , is this {vocalsound} sonorants , not sonorants , and {vocalsound} stuff like that .&#10;Speaker: Grad B&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: So .&#10;Speaker: PhD E&#10;Content: Did you find any more mistakes in their tables ?&#10;Speaker: Grad A&#10;Content: Oh ! Uh I haven't gone through the entire table , {pause} yet . Yeah , yesterday I brought Chuck {vocalsound} the table and I was like , &quot; wait , this {disfmarker} is {disfmarker} Is the mapping from N to {disfmarker} to this phonological feature called um &quot; coronal &quot; , is {disfmarker} is {disfmarker} should it be" />
    <node id="1. Relevant meetings for computational speech processing include Interspeech Computeational Seminar in Speech Research (ICSLP) and Eurospeech. These conferences are more targeted for &quot;dyed-in-the-wool&quot; speech professionals, as they focus specifically on speech processing topics.&#10;2. One might ignore these meetings due to being too busy or not finding the time to attend. Additionally, if a professional has a clear understanding of how to implement specific measures without needing a net, they might choose to not use the net and instead focus on their existing knowledge. This decision could lead to better results, as long as the data being used is correct and well-normalized.&#10;&#10;Relevant sources from the transcript:&#10;- Speaker: PhD E, Content: for sort of dyed - in - the wool speech people , um I think that ICSLP and Eurospeech are much more targeted .&#10;- Speaker: Professor C, Content: So . I mean , I mostly just ignored it because I was too busy..." />
    <node id="disfmarker} it 's if you haven't been there much , it 's good to go to , uh to get a feel for things , a range of things , not just speech . Uh . But I think for {disfmarker} for sort of dyed - in - the - wool speech people , um I think that ICSLP and Eurospeech are much more targeted .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Uh . And then there 's these other meetings , like HLT and {disfmarker} and uh ASRU {disfmarker}&#10;Speaker: PhD E&#10;Content: &#10;Speaker: Professor C&#10;Content: so there 's {disfmarker} there 's actually plenty of meetings that are really relevant to {disfmarker} to uh computational uh speech processing of one sort or another .&#10;Speaker: PhD E&#10;Content: Mm - hmm . &#10;Speaker: Professor C&#10;Content: Um . So . I mean , I mostly just ignored it because I was too busy and {vocalsound} didn't get to it . So uh Wanna talk a" />
    <node id="1. The feature set called &quot;the sound patterns of English&quot; consists of binary-valued features that represent various phonological aspects of English sounds. Examples of these features include voicing (voiced or voiceless) and sonorance (sonorant or not). These features are used to describe phones in the English language.&#10;2. During the discussion, Grad A mentioned that they found potential mistakes in the tables associated with &quot;the sound patterns of English&quot; feature set, specifically related to the mapping from phones to a phonological feature called &quot;coronal.&quot; They had identified inconsistencies in how the feature was labeled and were planning to review the tables further to find more possible errors." />
    <node id="marker} to this phonological feature called um &quot; coronal &quot; , is {disfmarker} is {disfmarker} should it be {disfmarker} shouldn't it be a one ? or should it {disfmarker} should it be you know coronal instead of not coronal as it was labelled in the paper ? &quot; So I ha haven't hunted down all the {disfmarker} all the mistakes yet ,&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: Grad A&#10;Content: but {disfmarker}&#10;Speaker: Professor C&#10;Content: But a as I was saying , people do get probabilities from these things ,&#10;Speaker: Grad B&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: and {disfmarker} and uh we were just trying to remember how they do , but people have used it for speech recognition , and they have gotten probabilities . So they have some conversion from these distances to probabilities .&#10;Speaker: Grad B&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Right , yeah .&#10;Speaker: Professor C&#10;Content: There 's {disfmarker} you" />
    <node id="Based on the transcript, PhD E had been working with Jeremy on his project, but the specific details of their collaboration are not provided. Later in the conversation, PhD E says &quot;You had the same answer anyway&quot; when referring to a response given to Professor C's question, suggesting that they both arrived at the same answer or understanding. However, the transcript does not include the context of the question or the specific answer, so it is not possible to definitively determine what caused them to have the same answer. It can be inferred that their collaboration on Jeremy's project may have led to a shared understanding or perspective, but without more information, a precise explanation cannot be given." />
    <node id=" Uh - huh .&#10;Speaker: PhD E&#10;Content: so i I {disfmarker} I don't know if that {disfmarker} it probably doesn't mess it up , it probably just ignores it if it determines that it 's already in the right format or something but {disfmarker} the {disfmarker} the {disfmarker} the two processes that happen are a little different .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: So .&#10;Speaker: Professor C&#10;Content: So anyway , there 's stuff there to sort out .&#10;Speaker: PhD E&#10;Content: Yeah . Yeah .&#10;Speaker: Professor C&#10;Content: So , OK . Let 's go back to what you thought I was asking you .&#10;Speaker: PhD E&#10;Content: Yeah no and I didn't have a chance to do that .&#10;Speaker: Professor C&#10;Content: Ha ! Oh ! You had the sa same answer anyway .&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . I 've been um , {disfmarker} I 've been working with um Jeremy on his project and then" />
    <node id="&#10;Content: Yeah . Yeah . I 've been um , {disfmarker} I 've been working with um Jeremy on his project and then I 've been trying to track down this bug in uh the ICSI front - end features .&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: So one thing that I did notice , yesterday I was studying the um {disfmarker} the uh RASTA code&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: and it looks like we don't have any way to um control the frequency range that we use in our analysis . We basically {disfmarker} it looks to me like we do the FFT , um and then we just take all the bins and we use everything . We don't have any set of parameters where we can say you know , &quot; only process from you know a hundred and ten hertz to thirty - seven - fifty &quot; .&#10;Speaker: Professor C&#10;Content: Um {disfmarker}&#10;Speaker: PhD E&#10;Content: At least I couldn't see any kind of control for that .&#10;Speaker:" />
    <node id=" to perform OK , but uh , you know , the {disfmarker} the models would be all messed up . So I was going through and just double - checking that kind of think first , to see if there was just some kind of obvious bug in the way that I was computing the features .&#10;Speaker: Professor C&#10;Content: Mm - hmm . I see . OK .&#10;Speaker: PhD E&#10;Content: Looking at all the sampling rates to make sure all the sampling rates were what {disfmarker} eight K , what I was assuming they were ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: um {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah , that makes sense , to check all that .&#10;Speaker: PhD E&#10;Content: Yeah . So I was doing that first , before I did these other things , just to make sure there wasn't something {disfmarker}&#10;Speaker: Professor C&#10;Content: Although really , uh uh , a couple three percent uh difference in word error rate uh {comment} could easily come from some difference in normalization , I would think . But&#10;Speaker:" />
    <node id=" Mmm . So they had one recurrent net for each particular feature ?&#10;Speaker: Grad A&#10;Content: Right . Right . Right . Right .&#10;Speaker: PhD E&#10;Content: I see . I wo did they compare that {disfmarker} I mean , what if you just did phone recognition and did the reverse lookup .&#10;Speaker: Grad A&#10;Content: Uh .&#10;Speaker: PhD E&#10;Content: So you recognize a phone and which ever phone was recognized , you spit out it 's vector of ones and zeros .&#10;Speaker: Grad A&#10;Content: Mm - hmm . Uh .&#10;Speaker: Professor C&#10;Content: I expect you could do that .&#10;Speaker: PhD E&#10;Content: I mean uh {disfmarker}&#10;Speaker: Professor C&#10;Content: That 's probably not what he 's going to do on his class project . Yeah .&#10;Speaker: PhD E&#10;Content: Yeah . No .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So um have you had a chance to do this um thing we talked about yet with the uh {disfmarker} um&#10;Spe" />
    <node id=": Yeah . I think , given at the level you 're doing things in floating point on the computer , I don't think it matters , would be my guess ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: but .&#10;Speaker: PhD D&#10;Content: I {disfmarker} this more or less anything&#10;Speaker: Professor C&#10;Content: Yeah . OK , and wh when did Stephane take off ? He took off {disfmarker}&#10;Speaker: PhD D&#10;Content: I think that Stephane will arrive today or tomorrow .&#10;Speaker: Professor C&#10;Content: Oh , he was gone these first few days , and then he 's here for a couple days before he goes to Salt Lake City .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: He 's {disfmarker} I think that he is in Las Vegas or something like that .&#10;Speaker: Professor C&#10;Content: Yeah . Yeah . So he 's {disfmarker} he 's going to ICAS" />
    <node id=" my microphone on ?&#10;Speaker: PhD E&#10;Content: Uh , yeah .&#10;Speaker: Professor C&#10;Content: Yeah . Thank you .&#10;Speaker: PhD E&#10;Content: Yep . Yeah . That 'll work .&#10;Speaker: Professor C&#10;Content:  I can be out of here quickly . {comment} {comment} {vocalsound} {vocalsound} That 's I just have to run for another appointment . OK ,  I t Yeah . I left it on . OK ." />
    <node id="The high error rate, such as forty-one, in the SRI system's identification of digits could be due to the training data. The SRI system was trained on &quot;this TV broadcast type stuff&quot; which has a much wider range of channels and noise compared to the clean TI-digits training data. This discrepancy between the training data and the test data (digits in this study) causes the system to perform poorly, resulting in a high error rate." />
    <node id=": OK , so it 's then {disfmarker} then it 's {disfmarker} it 's {disfmarker} it 's reasonable to expect it would be helpful if we used it with the SRI system and&#10;Speaker: Professor C&#10;Content: Yeah , I mean , as helpful {disfmarker} I mean , so that 's the question . Yeah , w we 're often asked this when we work with a system that {disfmarker} that isn't {disfmarker} isn't sort of industry {disfmarker} industry standard great ,&#10;Speaker: Grad B&#10;Content: Uh - huh .&#10;Speaker: Professor C&#10;Content: uh and we see some reduction in error using some clever method , then , you know , will it work on a {disfmarker} {vocalsound} on a {disfmarker} on a good system . So uh you know , this other one 's {disfmarker} it was a pretty good system . I think , you know , one {disfmarker} one percent word error rate on digits is {disfmarker} uh digit strings is not {" />
    <node id=" why were you getting forty - one here ? Is this {disfmarker}&#10;Speaker: Grad B&#10;Content: Um . I {disfmarker} I 'm g I 'm guessing it was the {disfmarker} the training data . Uh , clean TI - digits is , like , pretty pristine {vocalsound} training data , and if they trained {vocalsound} the SRI system on this TV broadcast type stuff , I think it 's a much wider range of channels and it {disfmarker}&#10;Speaker: Professor C&#10;Content: No , but wait a minute . I {disfmarker} I {disfmarker} I th {disfmarker} I think he {disfmarker} What am I saying here ? Yeah , so that was the SRI system . Maybe you 're right . Yeah . Cuz it was getting like one percent {disfmarker} {vocalsound} So it 's still this kind of ratio . It was {disfmarker} it was getting one percent or something on the near field . Wasn't it ?&#10;Speaker: PhD E&#10;Content: Mm - hmm , or" />
    <node id=" use th use it for the SRI system .&#10;Speaker: Grad B&#10;Content: b You me you mean um ta&#10;Speaker: Professor C&#10;Content: So you 're {disfmarker} so you have a system which for one reason or another is relatively poor ,&#10;Speaker: Grad B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and {disfmarker} and uh you have something like forty - one percent error uh and then you transform it to eight by doing {disfmarker} doing this {disfmarker} this work . Um . So here 's this other system , which is a lot better , but there 's still this kind of ratio . It 's something like five percent error {vocalsound} with the {disfmarker} the distant mike , and one percent with the close mike .&#10;Speaker: Grad B&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: So the question is {vocalsound} how close to that one can you get {vocalsound} if you transform the data using that system .&#10;Speaker: Grad B&#10;Content: r Right , so {disfmarker}" />
    <node id=" C&#10;Content: So um have you had a chance to do this um thing we talked about yet with the uh {disfmarker} um&#10;Speaker: PhD E&#10;Content: Insertion penalty ?&#10;Speaker: Professor C&#10;Content: Uh . No actually I was going a different {disfmarker} That 's a good question , too , but I was gonna ask about the {disfmarker} {vocalsound} the um {vocalsound} changes to the data in comparing PLP and mel cepstrum for the SRI system .&#10;Speaker: PhD E&#10;Content: Uh . Well what I 've been {disfmarker} &quot; Changes to the data &quot; , I 'm not sure I {disfmarker}&#10;Speaker: Professor C&#10;Content: Right . So we talked on the phone about this , that {disfmarker} that there was still a difference of a {disfmarker} of a few percent&#10;Speaker: PhD E&#10;Content: Yeah . Right .&#10;Speaker: Professor C&#10;Content: and {vocalsound} you told me that there was a difference in how the normalization was done . And I was asking" />
    <node id=" was getting one percent or something on the near field . Wasn't it ?&#10;Speaker: PhD E&#10;Content: Mm - hmm , or it wa a it was around one .&#10;Speaker: Professor C&#10;Content: Yeah . Yeah . I think it was getting around one percent for the near {disfmarker} for the n for the close mike .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad B&#10;Content: Huh ? OK .&#10;Speaker: Professor C&#10;Content: So it was like one to five {disfmarker} So it 's still this kind of ratio . It 's just {disfmarker} yeah , it 's a lot more training data . So So probably it should be something we should try then is to {disfmarker} is to see if {disfmarker} is {vocalsound} at some point just to take {disfmarker} i to transform the data and then {disfmarker} {vocalsound} and then uh use th use it for the SRI system .&#10;Speaker: Grad B&#10;Content: b You me you mean um ta&#10;Speaker: Professor C" />
    <node id="vocalsound} if you transform the data using that system .&#10;Speaker: Grad B&#10;Content: r Right , so {disfmarker} so I guess this SRI system is trained on a lot of s Broadcast News or Switchboard data . Is that right ?&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad B&#10;Content: Do you know which one it is ?&#10;Speaker: PhD E&#10;Content: It 's trained on a lot of different things . Um . It 's trained on uh a lot of Switchboard , Call Home ,&#10;Speaker: Grad B&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: um a bunch of different sources , some digits , there 's some digits training in there .&#10;Speaker: Grad B&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Hmm .&#10;Speaker: Grad B&#10;Content: O one thing I 'm wondering about is what this mean subtraction method {vocalsound} um will do if it 's faced with additive noise . Cuz I {disfmarker} I {disfmarker} it 's cuz I don't know what log magnitude spectral subt" />
    <node id="The suggested approach that is reputed to be better than neural nets or Gaussian mixtures in terms of requiring fewer parameters and performing well even with a modest amount of data is not explicitly mentioned in the provided transcript. However, the speakers discuss an idea where they simplify the model and directly feed the log magnitude spectrum into it, without applying any clever techniques. This could potentially be a more parameter-efficient approach, but it's not specified if this method has any particular name or reputation for better performance with less data." />
    <node id="1. The speakers mention changing the filter bank from a trapezoidal integration to a triangular one because some people have reported better performance using triangular filters in RASTA. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;&#10;2. The fundamental difference in using a cubic root for smoothing is contrasted with the log used in another case. This is an advantage for the PLP (Perceptual Linear Prediction) method, as it provides smoothing at the end through an auto-process." />
    <node id="The support vectors, or the critical examples, are saved during the process of recreating the boundary for the test set in a Support Vector Machine (SVM) because they provide several benefits compared to just saving the boundary itself. The support vectors represent the data points that lie closest to the decision boundary, and they effectively define the boundary. By storing these critical examples, SVMs can:&#10;&#10;1. Handle new classes: If new classes appear in the test set that were not present during training, the current boundary might not be useful. However, having the support vectors allows the algorithm to adapt and create a new boundary that better suits the updated data distribution.&#10;2. Adapt to changes in the data distribution: In real-world scenarios, data distributions can change over time. By saving the support vectors, SVMs can efficiently update the decision boundary when needed, without requiring access to the entire original dataset.&#10;3. Reduce computational complexity: Classifying new examples involves calculating their distance to the decision boundary. By using support vectors instead of the whole boundary, the computational cost is significantly reduced because there are far fewer support vectors than potential points on the boundary.&#10;4. Improve interpretability: Keeping the support vectors can provide more insights into the dataset and the classification problem, making it easier for analysts to understand why certain decisions are made.&#10;&#10;In summary, saving support vectors instead of just the decision boundary allows SVMs to be more flexible, adaptable, efficient, and interpretable when dealing with new or changing data distributions." />
    <node id=" and remembers those&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: and {disfmarker} {vocalsound} and uses them to recreate the boundary for the test set . So , given these {vocalsound} um these features , or {disfmarker} or these {disfmarker} these examples , {pause} um , {pause} critical examples , {vocalsound} which they call support f support vectors , {vocalsound} then um {vocalsound} given a new example , {vocalsound} if the new example falls {vocalsound} um away from the boundary in one direction then it 's classified as being a part of this particular class&#10;Speaker: PhD E&#10;Content: Oh .&#10;Speaker: Grad A&#10;Content: and otherwise it 's the other class .&#10;Speaker: PhD E&#10;Content: So why save the examples ? Why not just save what the boundary itself is ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Um . Hmm . Let 's see . Uh . Yeah , that 's a good question . I {disfmarker} yeah" />
    <node id=" A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Does there some kind of a distance metric that they use or how do they {disfmarker} for cla what do they do for classification ?&#10;Speaker: Grad A&#10;Content: Um . Right . So , {vocalsound} the {disfmarker} the simple idea behind a support vector machine is {vocalsound} um , {vocalsound} you have {disfmarker} you have this feature space , right ? and then it finds the optimal separating plane , um between these two different um classes ,&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm . Mm - hmm .&#10;Speaker: Grad A&#10;Content: and um {vocalsound} and so {vocalsound} um , what it {disfmarker} i at the end of the day , what it actually does is {vocalsound} it picks {vocalsound} those examples of the features that are closest to the separating boundary , and remembers those&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: and {disfmark" />
    <node id=" then it can take a while to {disfmarker} to use nearest - neighbor . There 's lots of look ups . So a long time ago people talked about things where you would have uh a condensed nearest - neighbor , where you would {disfmarker} you would {disfmarker} you would pick out uh some representative examples which would uh be sufficient to represent {disfmarker} to {disfmarker} to correctly classify everything that came in .&#10;Speaker: PhD E&#10;Content: Oh . Mm - hmm .&#10;Speaker: Professor C&#10;Content: I {disfmarker} I think s I think support vector stuff sort of goes back to {disfmarker} {vocalsound} to that kind of thing . Um .&#10;Speaker: PhD E&#10;Content: I see . So rather than doing nearest neighbor where you compare to every single one , you just pick a few critical ones , and {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: And th the You know , um neural net approach uh or Gaussian mixtures for that matter" />
    <node id="1. The method for adjusting the bandwidth of the first filters involves copying the configuration of the second filters over to the first filters. This is done in the RASTA (Relative Spectral) processing technique, where some people have reported better performance using triangular filters instead of trapezoidal ones. Typical mel cepstral filter banks use triangular filters, which might be why they work better in this context.&#10;&#10;2. Specifying a different number of filters causes the last ones to be ignored because the system assumes that the same number of filters should be applied across all frequency bins. If a different number is specified, the excess filters will not have any corresponding input and thus will be ignored. This behavior allows for controlling the bandwidth of the filters in the early stages of processing." />
    <node id="s bark scale , and it 's {disfmarker} it {disfmarker} it um {disfmarker} it actually copies the uh um {disfmarker} the second filters over to the first . So the first filters are always {disfmarker} and you can s you can specify a different number of {vocalsound} uh features {disfmarker} different number of filters , I think , as I recall . So you can specify a different number of filters , and whatever {vocalsound} um uh you specify , the last ones are gonna be ignored . So that {disfmarker} that 's a way that you sort of change what the {disfmarker} what the bandwidth is . Y you can't do it without I think changing the number of filters , but {disfmarker}&#10;Speaker: PhD E&#10;Content: I saw something about uh {disfmarker} that looked like it was doing something like that , but I didn't quite understand it . So maybe {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah , so the idea is that the very lowest frequencies and {disfmarker} and typically the" />
    <node id="Based on the transcript provided, it is not explicitly stated that Professor C confirmed she left the microphone on before leaving for another appointment. However, she did mention &quot;I left it on&quot; in her speech. It's unclear whether she was referring to the microphone or something else, but given the context of the conversation, it's possible that she was indeed confirming that the microphone was left on.&#10;&#10;As for whether this was agreeable to PhD E, there is no explicit response from him regarding this matter in the transcript. Therefore, it cannot be definitively determined if he agreed or disagreed with leaving the microphone on." />
    <node id=" sum of them , or {disfmarker} ? Oh it {disfmarker} i Oh , so it is a sigmoidal .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: OK . Alright .&#10;Speaker: PhD E&#10;Content: Did the {disfmarker} did they get good results with that ?&#10;Speaker: Professor C&#10;Content: I mean , they 're OK , I {disfmarker} I don't {disfmarker} I don't think they were earth {disfmarker} earth shattering , but I think that {vocalsound} uh this was a couple years ago ,&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: I remember them doing it at some meeting , and {disfmarker} and um I don't think people were very critical because it was interesting just to {disfmarker} to try this and you know , it was the first time they tried it , so {disfmarker} {vocalsound} so the {disfmarker} you know , the numbers were not incredibly good&#10;Speaker: PhD E&#10;" />
    <node id="marker} {vocalsound} so the {disfmarker} you know , the numbers were not incredibly good&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: but there 's you know , it was th reasonable .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: I {disfmarker} I don't remember anymore . I don't even remember what the task was , it {comment} was Broadcast News , or {vocalsound} something . I don't know .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Grad B&#10;Content: Uh s So Barry , if you just have zero and ones , how are you doing the speech recognition ?&#10;Speaker: Grad A&#10;Content: Oh I 'm not do I 'm not planning on doing speech recognition with it . I 'm just doing {vocalsound} detection of phonological features .&#10;Speaker: Grad B&#10;Content: Oh . OK .&#10;Speaker: Grad A&#10;Content: So uh for example , {vocalsound} this {disf" />
    <node id="1. The concept of condensed nearest neighbor (CNN) refers to a modification of the traditional nearest neighbor classification algorithm. In the standard nearest neighbor method, you compare a query point to all training samples to find the closest one(s). This can be computationally expensive and time-consuming, especially with large datasets.&#10;2. To address this issue, CNN was proposed to reduce the number of comparisons needed. The idea is to select a subset of representative examples from the original dataset that can accurately classify any new input. By comparing the query point only to these condensed representatives instead of the entire dataset, you significantly decrease computation time without compromising classification accuracy.&#10;3. CNN can be related to support vector machines (SVM) and neural nets approaches in terms of dimensionality reduction and model simplicity. SVMs aim to find a hyperplane that maximally separates classes with a margin, focusing on critical support vectors instead of all training samples. Similarly, condensed nearest neighbor selects key examples, which can be seen as a form of data compression or dimensionality reduction.&#10;4. Neural nets and Gaussian mixtures, on the other hand, typically require more parameters and data to achieve good performance compared to SVMs and CNN. However, they can model complex decision boundaries and are better suited for multi-class problems and cases where relationships between features are not easily captured by a linear boundary or distance metric.&#10;5. In summary, condensed nearest neighbor is a more efficient variant of traditional nearest neighbor classification that reduces computation time by selecting representative examples from the original dataset. This approach shares similarities with support vector machines, focusing on key data points, while differing from neural nets and Gaussian mixtures, which often require more parameters and data to perform well." />
    <node id="Professor C is trying to convey the idea that the number of filters used in an analysis can affect the resultant output. He suggests that using a larger number of filters would result in less information being thrown away, as compared to using fewer filters which would discard more data from the lower and higher frequency bins.&#10;&#10;Professor C also mentions that there may not be an independent way to specify a particular range for filter creation, but rather, it might be determined by the analysis bandwidth used in the integration over FFT bins.&#10;&#10;PhD E responds by referring to the Feacalc code and checking for potential bugs or inconsistencies in feature computation. They also mention the need to ensure that all sampling rates are as expected.&#10;&#10;However, there is no explicit mention of using analysis bandwidth in PhD E's response." />
    <node id="er}&#10;Speaker: Professor C&#10;Content: Yeah , so the idea is that the very lowest frequencies and {disfmarker} and typically the veriest {comment} highest frequencies are kind of junk .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Professor C&#10;Content: And so um you just {disfmarker} for continuity you just approximate them by {disfmarker} {vocalsound} by the second to highest and second to lowest . It 's just a simple thing we put in .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And {disfmarker} and so if you h&#10;Speaker: PhD E&#10;Content: But {disfmarker} so the {disfmarker} but that 's a fixed uh thing ?&#10;Speaker: Professor C&#10;Content: Yeah , {comment} I think that 's a fixed thing .&#10;Speaker: PhD E&#10;Content: There 's nothing that lets you {disfmarker}&#10;Speaker: Professor C&#10;Content: But see {disfmarker} see my point ? If you had {d" />
    <node id="1. The filters being used in the process of integrating over FFT bins to create a filter bank are referred to as the mel cepstral filter bank, which typically uses triangular filters. These filters ignore the lowest and highest bins by copying the configuration of the second filters over to the first filters, as done in the RASTA (Relative Spectral) processing technique. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;2. The mel or bark scale refers to a psychoacoustic scale of pitches, where the width of each filter is determined by the analysis bandwidth used in the integration over FFT bins. It's mentioned that there may not be an independent way to specify a particular range for filter creation. Instead, the filter bank is created by integrating over FFT bins, and the filters are defined by the mel or bark scale." />
    <node id="In the context of this discussion, a reasonable word error rate on digit strings is around 1%, while a subpar performance would be an error rate of 4-5% or higher. This system, which wasn't trained on the task and used as a comparison, has a word error rate of about 1% on digits, which is considered reasonable. However, an error rate of 4-5% or more would be considered poor performance for this task." />
    <node id=" think , you know , one {disfmarker} one percent word error rate on digits is {disfmarker} uh digit strings is not {vocalsound} uh you know stellar , but {disfmarker} but given that this is real {vocalsound} digits , as opposed to uh sort of laboratory {disfmarker}&#10;Speaker: Grad B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Well .&#10;Speaker: PhD E&#10;Content: And it wasn't trained on this task either .&#10;Speaker: Professor C&#10;Content: And it wasn't trained on this task . Actually one percent is sort of {disfmarker} you know , sort of in a reasonable range .&#10;Speaker: Grad B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: People would say &quot; yeah , I could {disfmarker} I can imagine getting that &quot; . And uh so the {disfmarker} the four or five percent or something is {disfmarker} is {disfmarker} is quite poor .&#10;Speaker: Grad B&#10;Content: Mm - hmm .&#10;Spe" />
    <node id=" Mm - hmm .&#10;Speaker: Professor C&#10;Content: I mean you could feed it a bunch of s you could feed two numbers that you wanted to multiply into a net and have a bunch of nonlinearities in the middle and train it to get the product of the output and it would work . But , it 's kind of crazy , cuz we know how to multiply and you {disfmarker} you 'd be you know much lower error usually {vocalsound} if you just multiplied it out . But suppose you don't really know what the right thing is . And that 's what these sort of dumb machine learning methods are good at . So . Um . Anyway . It 's just a thought .&#10;Speaker: PhD E&#10;Content: How long does it take , Carmen , to train up one of these nets ?&#10;Speaker: PhD D&#10;Content: Oh , not too much .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Mmm , one day or less .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: Yeah , it 's probably worth it .&#10;Speaker: Grad A&#10;" />
    <node id="Professor C is trying to convey that the number of filters used in an analysis can affect the resultant output. She suggests that using a larger number of filters would result in less information being discarded, as compared to using fewer filters which would throw away more data from the lower and higher frequency bins. This is because when there are fewer filters, more data at the extremes of the frequency range will be lost.&#10;&#10;PhD E mentions the use of analysis bandwidth or checking for potential bugs or inconsistencies in feature computation in the context of the Feacalc code because they want to ensure that all sampling rates are as expected and that there are no errors in the calculation of features. This is an important step to take before making any conclusions about the data analysis. However, PhD E does not explicitly mention using analysis bandwidth in their response, so it is unclear if they are directly addressing Professor C's point about the range of filter creation." />
    <node id="1. The SRI system reads wavefiles directly and performs cepstral computation on the fly, but it doesn't store the cepstral files anywhere for comparison with PLP features. This is in contrast to the PLP system, which stores the cepstrum on disk and reads those in during feature extraction.&#10;&#10;2. The SRI system requires a front-end parameter file even after the cepstrum is computed because it's used to specify the kind of computation that the mel cepstrum uses. This may be related to the fact that some people have reported better performance using triangular filters in RASTA, instead of trapezoidal ones, which are typically used in mel cepstral filter banks.&#10;&#10;In summary, the main differences between the SRI and PLP systems lie in file storage (with the former reading wavefiles directly and computing cepstra on the fly and the latter storing computed cepstra on disk) and configuration parameters (where the SRI system requires a front-end parameter file even after the cepstrum is computed). The reason for this difference may be related to the type of filter bank used in each system." />
    <node id="1. Grad A has made progress in securing committee members for their qualifications. So far, they have confirmed Morgan, Hynek, Mike Jordan, and John Ohala.&#10;2. The remaining committee member that Grad A needs to ask is Malek." />
    <node id=" . I mean , I mostly just ignored it because I was too busy and {vocalsound} didn't get to it . So uh Wanna talk a little bit about what we were talking about this morning ?&#10;Speaker: Grad A&#10;Content: Oh ! um {pause} uh {pause} Yeah .&#10;Speaker: Professor C&#10;Content: Just briefly , or {pause} Or anything else ?&#10;Speaker: Grad A&#10;Content: So . I {disfmarker} I guess some of the progress , I {disfmarker} I 've been getting a {disfmarker} getting my committee members for the quals . And um so far I have Morgan and Hynek , {vocalsound} Mike Jordan , and I asked John Ohala and he agreed . Yeah . Yeah .&#10;Speaker: PhD E&#10;Content: Cool .&#10;Speaker: Grad A&#10;Content: So I 'm {disfmarker} I {disfmarker} I just need to ask um Malek . One more . Um . Tsk . Then uh I talked a little bit about {vocalsound} um continuing with these dynamic ev um acoustic events , and um {vocalsound} {" />
    <node id="&#10;Speaker: Grad A&#10;Content: Right , yeah .&#10;Speaker: Professor C&#10;Content: There 's {disfmarker} you have {disfmarker} you have the paper , right ? The Mississippi State paper ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: Yeah , if you 're interested y you could look ,&#10;Speaker: Grad B&#10;Content: And {disfmarker} OK . OK .&#10;Speaker: Grad A&#10;Content: Yeah , I can {disfmarker} I can show you {disfmarker} I {disfmarker}&#10;Speaker: Professor C&#10;Content: yeah .&#10;Speaker: Grad A&#10;Content: yeah , our {disfmarker}&#10;Speaker: PhD E&#10;Content: So in your {disfmarker} in {disfmarker} in the thing that you 're doing , uh you have a vector of ones and zeros for each phone ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Uh , is this the class project , or {disfmarker}" />
    <node id="The system learns a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals, to binary vectors representing the presence or absence of specific phonological features in corresponding phones. The output of the system is a vector of ones and zeros for each phone, where each element corresponds to whether the phone exhibits a particular phonological feature or not. This information can be further used to classify phones based on their audio signals." />
    <node id="1. The concept being discussed involves calculating a distance measure from MFCC features to phonological features, which is then translated into a binary value (0 or 1) for classification purposes. This approach is used for binary classification where each feature is evaluated as either present (1) or absent (0).&#10;2. In the context of speech recognition, this distance measure is calculated between the MFCC features derived from audio signals and the phonological features corresponding to phones. The distances are then translated into binary values representing whether specific phonological features are present in the given phone.&#10;3. This method utilizes distances by converting them into probabilities for phonological feature occurrences, which can be used for recognizing phones based on their audio signals. Mississippi State researchers have successfully applied this technique using support vector machines to estimate probabilities for speech recognition tasks. The accuracy of the phone recognition depends on the precise identification and marking of the phonological events within the audio signal, as mentioned by Professor C." />
    <node id="isfmarker} there are pap Well , basically , it 's {disfmarker} it 's um {vocalsound} you {disfmarker} you get a distance measure at the end of the day , and then that distance measure is {disfmarker} is um {disfmarker} {vocalsound} is translated to a zero or one . Um .&#10;Speaker: Professor C&#10;Content: But that 's looking at it for {disfmarker} for classification {disfmarker} for binary classification ,&#10;Speaker: Grad A&#10;Content: That 's for classification , right .&#10;Speaker: Professor C&#10;Content: right ?&#10;Speaker: PhD E&#10;Content: And you get that for each class , you get a zero or a one .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: But you have the distances to work with .&#10;Speaker: Grad A&#10;Content: You have the distances to work with ,&#10;Speaker: Professor C&#10;Content: Cuz actually Mississippi State people did use support vector machines for uh uh speech recognition and they were using it to estimate probabilities .&#10;Speaker: Grad" />
    <node id="To allow the Perceptual Linear Prediction (PLP) system to output mel cepstrum, one of the changes that could be made is to alter the filter bank from a trapezoidal integration to a triangular one. This change might lead to better results since typical mel cepstral filter banks use triangular filters. By doing this, the PLP system's output could be compared directly with the mel cepstrum produced by the SRI system.&#10;&#10;Comparing the outputs would be useful for phonetic classification as it can provide insights into the performance of each system and help identify which approach might be more suitable for specific applications. If both systems produce similar results, it could indicate that either method can be used interchangeably. However, if there are significant differences, it might suggest that one method is more appropriate for certain tasks than the other. Ultimately, this comparison can lead to a better understanding of the strengths and weaknesses of each system, informing future research and development in the field of phonetic classification." />
    <node id="Based on the information provided in the transcript, the error rate reduction in speech recognition achieved by using a simulated impulse response and training on clean speech only was from approximately 80% to 4%. This conclusion is based on indirect information since the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated.&#10;&#10;The transcript mentions that when using a simulated impulse response, the error rate went from something like eighteen percent to four percent for vocalsound and from forty-one percent to eight percent for mike far digits on channel F. It can be inferred that these reductions in error rates were due to using a simulated impulse response and training on clean speech only.&#10;&#10;However, the transcript does not provide information about the error rate when not using artificial reverb or if the system was trained on noisy speech. The comparison is made between using artificial reverb and clean speech versus an unspecified setup that may or may not have used artificial reverb and could have been trained on noisy speech.&#10;&#10;Therefore, while there was a significant reduction in error rates for speech recognition when using a simulated impulse response and training on clean speech only, the exact difference in error rates between these setups cannot be determined from the information provided in the transcript." />
    <node id="Support Vector Machines (SVM) are a type of classification algorithm that use a feature space to find the optimal separating plane between two different classes. The goal is to pick those examples of features that are closest to the separating boundary, which are referred to as support vectors. These critical examples effectively define the boundary, providing several benefits compared to just saving the boundary itself:&#10;&#10;1. Handling new classes: By storing support vectors, SVMs can adapt and create a new boundary when new classes appear in the test set that were not present during training.&#10;2. Adapting to changes in data distribution: Support vectors allow SVMs to efficiently update the decision boundary when data distributions change over time without requiring access to the entire original dataset.&#10;3. Reducing computational complexity: Classifying new examples involves calculating their distance to the decision boundary. Using support vectors significantly reduces computation cost since there are far fewer support vectors than potential points on the boundary.&#10;4. Improving interpretability: Keeping the support vectors provides insights into the dataset and classification problem, making it easier for analysts to understand decision-making processes.&#10;&#10;In summary, SVMs use a feature space and pick examples closest to the separating boundary (support vectors) because this approach offers flexibility, adaptability, efficiency, and interpretability when dealing with new or changing data distributions." />
    <node id="In the project, the speakers are using a vector of ones and zeros to represent each phone where each element in the vector corresponds to whether the phone exhibits a particular phonological feature or not. This vector is obtained by learning a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals. The output of the system is a vector of ones and zeros for each phone, where each element in the vector represents the presence or absence of a specific phonological feature." />
    <edge source="1. The method for phone recognition mentioned in the transcript involves calculating the likelihood of an event (such as onset or offset of a phonological feature like voicing) given a phone. This is done by counting co-occurrences of the event and the phone, and dividing it by the number of occurrences of the phone. This gives the probability of the event occurring given the phone.&#10;2. According to Professor C, this method would be effective in recognizing phones if onset and end of voicing events are accurately marked. They mentioned an example where, if these events are correctly identified, counting their co-occurrences with specific phones should result in accurate phone recognition.&#10;3. However, it is important to note that this method relies on the accurate identification and marking of phonological events, which can be challenging and prone to errors. Nonetheless, when done accurately, this simple HMM (Hidden Markov Model) based approach can be effective for phone recognition tasks." target=" the {disfmarker} the uh {disfmarker} uh uh occ uh count the co - occurrences between the event and the phone and divide them by the number of occurrences of the phone , and that would give you the likelihood of the {disfmarker} of the event given the phone . And um then just use that in a very simple HMM and uh you could uh do phone recognition then and uh wouldn't have any of the issues of the uh training of the net or {disfmarker} I mean , it 'd be on the simple side , but&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: uh um you know , if {disfmarker} uh uh the example I was giving was that if {disfmarker} if you had um onset of voicing and {disfmarker} and end of voicing as being two kinds of events , then if you had those a all marked correctly , and you counted co - occurrences , you should get it completely right .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So . um {disfmarker} But">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The method for phone recognition mentioned in the transcript involves calculating the likelihood of an event (such as onset or offset of a phonological feature like voicing) given a phone. This is done by counting co-occurrences of the event and the phone, and dividing it by the number of occurrences of the phone. This gives the probability of the event occurring given the phone.&#10;2. According to Professor C, this method would be effective in recognizing phones if onset and end of voicing events are accurately marked. They mentioned an example where, if these events are correctly identified, counting their co-occurrences with specific phones should result in accurate phone recognition.&#10;3. However, it is important to note that this method relies on the accurate identification and marking of phonological events, which can be challenging and prone to errors. Nonetheless, when done accurately, this simple HMM (Hidden Markov Model) based approach can be effective for phone recognition tasks." target=" phone ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Uh , is this the class project , or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: OK . um&#10;Speaker: PhD E&#10;Content: Is that what you 're {disfmarker}&#10;Speaker: Grad A&#10;Content: Right , {comment} Right , right f so for every phone there is {disfmarker} there is a um {disfmarker} a vector of ones and zeros {vocalsound} f uh corresponding to whether it exhibits a particular phonological feature or not .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm . And so when you do your wh I 'm {disfmarker} what is the task for the class project ? To come up with the phones ?&#10;Speaker: Grad A&#10;Content: Um&#10;Speaker: PhD E&#10;Content: or to come up with these vectors to see how closely they match the phones ,&#10;Speaker: Grad A&#10;Content: Oh . Right , um to come up with a mapping from um MFCC">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The method for phone recognition mentioned in the transcript involves calculating the likelihood of an event (such as onset or offset of a phonological feature like voicing) given a phone. This is done by counting co-occurrences of the event and the phone, and dividing it by the number of occurrences of the phone. This gives the probability of the event occurring given the phone.&#10;2. According to Professor C, this method would be effective in recognizing phones if onset and end of voicing events are accurately marked. They mentioned an example where, if these events are correctly identified, counting their co-occurrences with specific phones should result in accurate phone recognition.&#10;3. However, it is important to note that this method relies on the accurate identification and marking of phonological events, which can be challenging and prone to errors. Nonetheless, when done accurately, this simple HMM (Hidden Markov Model) based approach can be effective for phone recognition tasks." target=" have uh {vocalsound} um a chosen set of features , or acoustic events , and we train up a hybrid {vocalsound} um system to do phone recognition on TIMIT . So i i the idea is if we get good phone recognition results , {vocalsound} using um these set of acoustic events , {vocalsound} then {vocalsound} um that {disfmarker} that says that these acoustic events are g sufficient to cover {vocalsound} a set of phones , at least found in TIMIT . Um so i it would be a {disfmarker} {vocalsound} a measure of &quot; are we on the right track with {disfmarker} with the {disfmarker} the choices of our acoustic events &quot; . Um , {vocalsound} So that 's going on . And {vocalsound} also , just uh working on my {vocalsound} uh final project for Jordan 's class , uh which is {disfmarker}&#10;Speaker: Professor C&#10;Content: Actually , let me {disfmarker}&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Hold that thought">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The method for phone recognition mentioned in the transcript involves calculating the likelihood of an event (such as onset or offset of a phonological feature like voicing) given a phone. This is done by counting co-occurrences of the event and the phone, and dividing it by the number of occurrences of the phone. This gives the probability of the event occurring given the phone.&#10;2. According to Professor C, this method would be effective in recognizing phones if onset and end of voicing events are accurately marked. They mentioned an example where, if these events are correctly identified, counting their co-occurrences with specific phones should result in accurate phone recognition.&#10;3. However, it is important to note that this method relies on the accurate identification and marking of phonological events, which can be challenging and prone to errors. Nonetheless, when done accurately, this simple HMM (Hidden Markov Model) based approach can be effective for phone recognition tasks." target=" see how closely they match the phones ,&#10;Speaker: Grad A&#10;Content: Oh . Right , um to come up with a mapping from um MFCC 's or s some feature set , {vocalsound} um to {vocalsound} uh w to whether there 's existence of a particular phonological feature .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ? Mm - hmm .&#10;Speaker: Grad A&#10;Content: And um yeah , basically it 's to learn a mapping {vocalsound} from {disfmarker} {vocalsound} from the MFCC 's to {vocalsound} uh phonological features . Is it {disfmarker} did that answer your question ?&#10;Speaker: PhD E&#10;Content: I think so .&#10;Speaker: Grad A&#10;Content: OK . C&#10;Speaker: PhD E&#10;Content: I guess {disfmarker} I mean , uh {disfmarker} I 'm not sure what you {disfmarker} what you 're {disfmarker} what you get out of your system . Do you get out a uh {disfmarker} a vector">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The method for phone recognition mentioned in the transcript involves calculating the likelihood of an event (such as onset or offset of a phonological feature like voicing) given a phone. This is done by counting co-occurrences of the event and the phone, and dividing it by the number of occurrences of the phone. This gives the probability of the event occurring given the phone.&#10;2. According to Professor C, this method would be effective in recognizing phones if onset and end of voicing events are accurately marked. They mentioned an example where, if these events are correctly identified, counting their co-occurrences with specific phones should result in accurate phone recognition.&#10;3. However, it is important to note that this method relies on the accurate identification and marking of phonological events, which can be challenging and prone to errors. Nonetheless, when done accurately, this simple HMM (Hidden Markov Model) based approach can be effective for phone recognition tasks." target=" . Then uh I talked a little bit about {vocalsound} um continuing with these dynamic ev um acoustic events , and um {vocalsound} {vocalsound} we 're {disfmarker} we 're {disfmarker} we 're {vocalsound} thinking about a way to test the completeness of a {disfmarker} a set of um dynamic uh events . Uh , completeness in the {disfmarker} in the sense that {vocalsound} um if we {disfmarker} if we pick these X number of acoustic events , {vocalsound} do they provide sufficient coverage {vocalsound} for the phones that we 're trying to recognize {vocalsound} or {disfmarker} or the f the words that we 're gonna try to recognize later on . And so Morgan and I were uh discussing {vocalsound} um s uh s a form of a cheating experiment {vocalsound} where we get {disfmarker} {vocalsound} um we have uh {vocalsound} um a chosen set of features , or acoustic events , and we train up a hybrid {vocalsound} um system to">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The method for phone recognition mentioned in the transcript involves calculating the likelihood of an event (such as onset or offset of a phonological feature like voicing) given a phone. This is done by counting co-occurrences of the event and the phone, and dividing it by the number of occurrences of the phone. This gives the probability of the event occurring given the phone.&#10;2. According to Professor C, this method would be effective in recognizing phones if onset and end of voicing events are accurately marked. They mentioned an example where, if these events are correctly identified, counting their co-occurrences with specific phones should result in accurate phone recognition.&#10;3. However, it is important to note that this method relies on the accurate identification and marking of phonological events, which can be challenging and prone to errors. Nonetheless, when done accurately, this simple HMM (Hidden Markov Model) based approach can be effective for phone recognition tasks." target="aker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So . um {disfmarker} But you 'd get all the other distinctions , you know , randomly wrong . I mean there 'd be nothing to tell you that . So um {vocalsound} uh If you just do this by counting , then you should be able to find out in a pretty straightforward way whether you have a sufficient uh set of events to {disfmarker} to do the kind of level of {disfmarker} {vocalsound} of uh classification of phones that you 'd like . So that was {disfmarker} that was the idea . And then the other thing that we were discussing was {disfmarker} was um {vocalsound} OK , how do you get the {disfmarker} your training data .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Cuz uh the {vocalsound} Switchboard transcription project uh uh you know was half a dozen people , or so working off and on over a couple years , and uh similar {disfmarker} {voc">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The method for phone recognition mentioned in the transcript involves calculating the likelihood of an event (such as onset or offset of a phonological feature like voicing) given a phone. This is done by counting co-occurrences of the event and the phone, and dividing it by the number of occurrences of the phone. This gives the probability of the event occurring given the phone.&#10;2. According to Professor C, this method would be effective in recognizing phones if onset and end of voicing events are accurately marked. They mentioned an example where, if these events are correctly identified, counting their co-occurrences with specific phones should result in accurate phone recognition.&#10;3. However, it is important to note that this method relies on the accurate identification and marking of phonological events, which can be challenging and prone to errors. Nonetheless, when done accurately, this simple HMM (Hidden Markov Model) based approach can be effective for phone recognition tasks." target="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The method for phone recognition mentioned in the transcript involves calculating the likelihood of an event (such as onset or offset of a phonological feature like voicing) given a phone. This is done by counting co-occurrences of the event and the phone, and dividing it by the number of occurrences of the phone. This gives the probability of the event occurring given the phone.&#10;2. According to Professor C, this method would be effective in recognizing phones if onset and end of voicing events are accurately marked. They mentioned an example where, if these events are correctly identified, counting their co-occurrences with specific phones should result in accurate phone recognition.&#10;3. However, it is important to note that this method relies on the accurate identification and marking of phonological events, which can be challenging and prone to errors. Nonetheless, when done accurately, this simple HMM (Hidden Markov Model) based approach can be effective for phone recognition tasks." target="1. Relevant meetings for computational speech processing include Interspeech Computeational Seminar in Speech Research (ICSLP) and Eurospeech. These conferences are more targeted for &quot;dyed-in-the-wool&quot; speech professionals, as they focus specifically on speech processing topics.&#10;2. One might ignore these meetings due to being too busy or not finding the time to attend. Additionally, if a professional has a clear understanding of how to implement specific measures without needing a net, they might choose to not use the net and instead focus on their existing knowledge. This decision could lead to better results, as long as the data being used is correct and well-normalized.&#10;&#10;Relevant sources from the transcript:&#10;- Speaker: PhD E, Content: for sort of dyed - in - the wool speech people , um I think that ICSLP and Eurospeech are much more targeted .&#10;- Speaker: Professor C, Content: So . I mean , I mostly just ignored it because I was too busy...">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The method for phone recognition mentioned in the transcript involves calculating the likelihood of an event (such as onset or offset of a phonological feature like voicing) given a phone. This is done by counting co-occurrences of the event and the phone, and dividing it by the number of occurrences of the phone. This gives the probability of the event occurring given the phone.&#10;2. According to Professor C, this method would be effective in recognizing phones if onset and end of voicing events are accurately marked. They mentioned an example where, if these events are correctly identified, counting their co-occurrences with specific phones should result in accurate phone recognition.&#10;3. However, it is important to note that this method relies on the accurate identification and marking of phonological events, which can be challenging and prone to errors. Nonetheless, when done accurately, this simple HMM (Hidden Markov Model) based approach can be effective for phone recognition tasks." target="1. The concept being discussed involves calculating a distance measure from MFCC features to phonological features, which is then translated into a binary value (0 or 1) for classification purposes. This approach is used for binary classification where each feature is evaluated as either present (1) or absent (0).&#10;2. In the context of speech recognition, this distance measure is calculated between the MFCC features derived from audio signals and the phonological features corresponding to phones. The distances are then translated into binary values representing whether specific phonological features are present in the given phone.&#10;3. This method utilizes distances by converting them into probabilities for phonological feature occurrences, which can be used for recognizing phones based on their audio signals. Mississippi State researchers have successfully applied this technique using support vector machines to estimate probabilities for speech recognition tasks. The accuracy of the phone recognition depends on the precise identification and marking of the phonological events within the audio signal, as mentioned by Professor C.">
      <data key="d0">1</data>
    </edge>
    <edge source=" phone ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Uh , is this the class project , or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: OK . um&#10;Speaker: PhD E&#10;Content: Is that what you 're {disfmarker}&#10;Speaker: Grad A&#10;Content: Right , {comment} Right , right f so for every phone there is {disfmarker} there is a um {disfmarker} a vector of ones and zeros {vocalsound} f uh corresponding to whether it exhibits a particular phonological feature or not .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm . And so when you do your wh I 'm {disfmarker} what is the task for the class project ? To come up with the phones ?&#10;Speaker: Grad A&#10;Content: Um&#10;Speaker: PhD E&#10;Content: or to come up with these vectors to see how closely they match the phones ,&#10;Speaker: Grad A&#10;Content: Oh . Right , um to come up with a mapping from um MFCC" target="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals.">
      <data key="d0">1</data>
    </edge>
    <edge source=" phone ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Uh , is this the class project , or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: OK . um&#10;Speaker: PhD E&#10;Content: Is that what you 're {disfmarker}&#10;Speaker: Grad A&#10;Content: Right , {comment} Right , right f so for every phone there is {disfmarker} there is a um {disfmarker} a vector of ones and zeros {vocalsound} f uh corresponding to whether it exhibits a particular phonological feature or not .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm . And so when you do your wh I 'm {disfmarker} what is the task for the class project ? To come up with the phones ?&#10;Speaker: Grad A&#10;Content: Um&#10;Speaker: PhD E&#10;Content: or to come up with these vectors to see how closely they match the phones ,&#10;Speaker: Grad A&#10;Content: Oh . Right , um to come up with a mapping from um MFCC" target="1. Grad A has made progress in securing committee members for their qualifications. So far, they have confirmed Morgan, Hynek, Mike Jordan, and John Ohala.&#10;2. The remaining committee member that Grad A needs to ask is Malek.">
      <data key="d0">1</data>
    </edge>
    <edge source=" phone ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Uh , is this the class project , or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: OK . um&#10;Speaker: PhD E&#10;Content: Is that what you 're {disfmarker}&#10;Speaker: Grad A&#10;Content: Right , {comment} Right , right f so for every phone there is {disfmarker} there is a um {disfmarker} a vector of ones and zeros {vocalsound} f uh corresponding to whether it exhibits a particular phonological feature or not .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm . And so when you do your wh I 'm {disfmarker} what is the task for the class project ? To come up with the phones ?&#10;Speaker: Grad A&#10;Content: Um&#10;Speaker: PhD E&#10;Content: or to come up with these vectors to see how closely they match the phones ,&#10;Speaker: Grad A&#10;Content: Oh . Right , um to come up with a mapping from um MFCC" target="The system learns a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals, to binary vectors representing the presence or absence of specific phonological features in corresponding phones. The output of the system is a vector of ones and zeros for each phone, where each element corresponds to whether the phone exhibits a particular phonological feature or not. This information can be further used to classify phones based on their audio signals.">
      <data key="d0">1</data>
    </edge>
    <edge source=" phone ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Uh , is this the class project , or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: OK . um&#10;Speaker: PhD E&#10;Content: Is that what you 're {disfmarker}&#10;Speaker: Grad A&#10;Content: Right , {comment} Right , right f so for every phone there is {disfmarker} there is a um {disfmarker} a vector of ones and zeros {vocalsound} f uh corresponding to whether it exhibits a particular phonological feature or not .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm . And so when you do your wh I 'm {disfmarker} what is the task for the class project ? To come up with the phones ?&#10;Speaker: Grad A&#10;Content: Um&#10;Speaker: PhD E&#10;Content: or to come up with these vectors to see how closely they match the phones ,&#10;Speaker: Grad A&#10;Content: Oh . Right , um to come up with a mapping from um MFCC" target="In the project, the speakers are using a vector of ones and zeros to represent each phone where each element in the vector corresponds to whether the phone exhibits a particular phonological feature or not. This vector is obtained by learning a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals. The output of the system is a vector of ones and zeros for each phone, where each element in the vector represents the presence or absence of a specific phonological feature.">
      <data key="d0">1</data>
    </edge>
    <edge source=" see how closely they match the phones ,&#10;Speaker: Grad A&#10;Content: Oh . Right , um to come up with a mapping from um MFCC 's or s some feature set , {vocalsound} um to {vocalsound} uh w to whether there 's existence of a particular phonological feature .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ? Mm - hmm .&#10;Speaker: Grad A&#10;Content: And um yeah , basically it 's to learn a mapping {vocalsound} from {disfmarker} {vocalsound} from the MFCC 's to {vocalsound} uh phonological features . Is it {disfmarker} did that answer your question ?&#10;Speaker: PhD E&#10;Content: I think so .&#10;Speaker: Grad A&#10;Content: OK . C&#10;Speaker: PhD E&#10;Content: I guess {disfmarker} I mean , uh {disfmarker} I 'm not sure what you {disfmarker} what you 're {disfmarker} what you get out of your system . Do you get out a uh {disfmarker} a vector" target="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals.">
      <data key="d0">1</data>
    </edge>
    <edge source=" see how closely they match the phones ,&#10;Speaker: Grad A&#10;Content: Oh . Right , um to come up with a mapping from um MFCC 's or s some feature set , {vocalsound} um to {vocalsound} uh w to whether there 's existence of a particular phonological feature .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ? Mm - hmm .&#10;Speaker: Grad A&#10;Content: And um yeah , basically it 's to learn a mapping {vocalsound} from {disfmarker} {vocalsound} from the MFCC 's to {vocalsound} uh phonological features . Is it {disfmarker} did that answer your question ?&#10;Speaker: PhD E&#10;Content: I think so .&#10;Speaker: Grad A&#10;Content: OK . C&#10;Speaker: PhD E&#10;Content: I guess {disfmarker} I mean , uh {disfmarker} I 'm not sure what you {disfmarker} what you 're {disfmarker} what you get out of your system . Do you get out a uh {disfmarker} a vector" target="1. The feature set called &quot;the sound patterns of English&quot; consists of binary-valued features that represent various phonological aspects of English sounds. Examples of these features include voicing (voiced or voiceless) and sonorance (sonorant or not). These features are used to describe phones in the English language.&#10;2. During the discussion, Grad A mentioned that they found potential mistakes in the tables associated with &quot;the sound patterns of English&quot; feature set, specifically related to the mapping from phones to a phonological feature called &quot;coronal.&quot; They had identified inconsistencies in how the feature was labeled and were planning to review the tables further to find more possible errors.">
      <data key="d0">1</data>
    </edge>
    <edge source=" see how closely they match the phones ,&#10;Speaker: Grad A&#10;Content: Oh . Right , um to come up with a mapping from um MFCC 's or s some feature set , {vocalsound} um to {vocalsound} uh w to whether there 's existence of a particular phonological feature .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ? Mm - hmm .&#10;Speaker: Grad A&#10;Content: And um yeah , basically it 's to learn a mapping {vocalsound} from {disfmarker} {vocalsound} from the MFCC 's to {vocalsound} uh phonological features . Is it {disfmarker} did that answer your question ?&#10;Speaker: PhD E&#10;Content: I think so .&#10;Speaker: Grad A&#10;Content: OK . C&#10;Speaker: PhD E&#10;Content: I guess {disfmarker} I mean , uh {disfmarker} I 'm not sure what you {disfmarker} what you 're {disfmarker} what you get out of your system . Do you get out a uh {disfmarker} a vector" target="The system learns a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals, to binary vectors representing the presence or absence of specific phonological features in corresponding phones. The output of the system is a vector of ones and zeros for each phone, where each element corresponds to whether the phone exhibits a particular phonological feature or not. This information can be further used to classify phones based on their audio signals.">
      <data key="d0">1</data>
    </edge>
    <edge source=" see how closely they match the phones ,&#10;Speaker: Grad A&#10;Content: Oh . Right , um to come up with a mapping from um MFCC 's or s some feature set , {vocalsound} um to {vocalsound} uh w to whether there 's existence of a particular phonological feature .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ? Mm - hmm .&#10;Speaker: Grad A&#10;Content: And um yeah , basically it 's to learn a mapping {vocalsound} from {disfmarker} {vocalsound} from the MFCC 's to {vocalsound} uh phonological features . Is it {disfmarker} did that answer your question ?&#10;Speaker: PhD E&#10;Content: I think so .&#10;Speaker: Grad A&#10;Content: OK . C&#10;Speaker: PhD E&#10;Content: I guess {disfmarker} I mean , uh {disfmarker} I 'm not sure what you {disfmarker} what you 're {disfmarker} what you get out of your system . Do you get out a uh {disfmarker} a vector" target="In the project, the speakers are using a vector of ones and zeros to represent each phone where each element in the vector corresponds to whether the phone exhibits a particular phonological feature or not. This vector is obtained by learning a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals. The output of the system is a vector of ones and zeros for each phone, where each element in the vector represents the presence or absence of a specific phonological feature.">
      <data key="d0">1</data>
    </edge>
    <edge source=" . Then uh I talked a little bit about {vocalsound} um continuing with these dynamic ev um acoustic events , and um {vocalsound} {vocalsound} we 're {disfmarker} we 're {disfmarker} we 're {vocalsound} thinking about a way to test the completeness of a {disfmarker} a set of um dynamic uh events . Uh , completeness in the {disfmarker} in the sense that {vocalsound} um if we {disfmarker} if we pick these X number of acoustic events , {vocalsound} do they provide sufficient coverage {vocalsound} for the phones that we 're trying to recognize {vocalsound} or {disfmarker} or the f the words that we 're gonna try to recognize later on . And so Morgan and I were uh discussing {vocalsound} um s uh s a form of a cheating experiment {vocalsound} where we get {disfmarker} {vocalsound} um we have uh {vocalsound} um a chosen set of features , or acoustic events , and we train up a hybrid {vocalsound} um system to" target="The output result of the reduced &quot;vocalsound&quot; parameterization in the model is either a zero or a one. This is because the model has been simplified to keep only certain selected examples, resulting in a binary output. The original parameterization might have produced a number between zero and one, but due to the reduction in complexity, it no longer does so.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The new feature that PhD D has been working on this week to detect voice-unvoice is a feature related to the Aurora system with a new filter, referred to as VAD (Voice Activity Detection) or something similar to it.&#10;2. PhD D is trying to use multiple Multi-Layer Perceptrons (MLPs) with the new feature and fifteen features from the bus base system in the Mel Cepstrum. One MLP has three outputs for voice, unvoice, and silence, while the other MLP has fifty-six outputs for the probabilities of allophones. He conducted some recognition experiments with these and found that the results were a little better but more or less similar when using the MLP with three outputs." target="Speaker: Professor C&#10;Content: OK . So uh , he 's not here ,&#10;Speaker: PhD D&#10;Content: So .&#10;Speaker: Professor C&#10;Content: so you get to {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah , I will try to explain the thing that I did this {disfmarker} this week {disfmarker} during this week .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Well eh you know that I work {disfmarker} I begin to work with a new feature to detect voice - unvoice .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: What I trying two MLP to {disfmarker} to the {disfmarker} with this new feature and the fifteen feature uh from the eh bus base system&#10;Speaker: PhD E&#10;Content: The {disfmarker} the mel cepstrum ?&#10;Speaker: PhD D&#10;Content: No , satly the mes the Mel Cepstrum , the new base system {disfmarker} the new base system">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The new feature that PhD D has been working on this week to detect voice-unvoice is a feature related to the Aurora system with a new filter, referred to as VAD (Voice Activity Detection) or something similar to it.&#10;2. PhD D is trying to use multiple Multi-Layer Perceptrons (MLPs) with the new feature and fifteen features from the bus base system in the Mel Cepstrum. One MLP has three outputs for voice, unvoice, and silence, while the other MLP has fifty-six outputs for the probabilities of allophones. He conducted some recognition experiments with these and found that the results were a little better but more or less similar when using the MLP with three outputs." target=" PhD D&#10;Content: No , satly the mes the Mel Cepstrum , the new base system {disfmarker} the new base system .&#10;Speaker: PhD E&#10;Content: Oh the {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah , we {disfmarker}&#10;Speaker: PhD E&#10;Content: OK , the Aurora system .&#10;Speaker: PhD D&#10;Content: yeah the Aurora system with the new filter , VAD or something like that .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: And I 'm trying two MLP , one one that only have t three output , voice , unvoice , and silence ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: and other one that have fifty - six output . The probabilities of the allophone . And I tried to do some experiment of recognition with that and only have result with {disfmarker} with the MLP with the three output . And I put together the fifteen features and the three MLP output . And , well , the result are li a little bit better , but more or">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The new feature that PhD D has been working on this week to detect voice-unvoice is a feature related to the Aurora system with a new filter, referred to as VAD (Voice Activity Detection) or something similar to it.&#10;2. PhD D is trying to use multiple Multi-Layer Perceptrons (MLPs) with the new feature and fifteen features from the bus base system in the Mel Cepstrum. One MLP has three outputs for voice, unvoice, and silence, while the other MLP has fifty-six outputs for the probabilities of allophones. He conducted some recognition experiments with these and found that the results were a little better but more or less similar when using the MLP with three outputs." target=" the {disfmarker} {vocalsound} {comment} S R I system looks like it works is that it reads the wavefiles directly , uh and does all of the cepstral computation stuff on the fly .&#10;Speaker: Professor C&#10;Content: Right . Right .&#10;Speaker: PhD E&#10;Content: And , so there 's no place where these {disfmarker} where the cepstral files are stored , anywhere that I can go look at and compare to the PLP ones , so whereas with our features , he 's actually storing the cepstrum on disk , and he reads those in .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: But it looked like he had to give it {disfmarker} uh even though the cepstrum is already computed , he has to give it uh a front - end parameter file . Which talks about the kind of uh com computation that his mel cepstrum thing does ,&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: so i I {disfmarker} I don't know if that {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The new feature that PhD D has been working on this week to detect voice-unvoice is a feature related to the Aurora system with a new filter, referred to as VAD (Voice Activity Detection) or something similar to it.&#10;2. PhD D is trying to use multiple Multi-Layer Perceptrons (MLPs) with the new feature and fifteen features from the bus base system in the Mel Cepstrum. One MLP has three outputs for voice, unvoice, and silence, while the other MLP has fifty-six outputs for the probabilities of allophones. He conducted some recognition experiments with these and found that the results were a little better but more or less similar when using the MLP with three outputs." target=" output . And I put together the fifteen features and the three MLP output . And , well , the result are li a little bit better , but more or less similar .&#10;Speaker: Professor C&#10;Content: Uh , I {disfmarker} I 'm {disfmarker} I 'm slightly confused .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: What {disfmarker} what feeds the uh {disfmarker} the three - output net ?&#10;Speaker: PhD D&#10;Content: Voice , unvoice , and si&#10;Speaker: Professor C&#10;Content: No no , what feeds it ? What features does it see ?&#10;Speaker: PhD D&#10;Content: The feature {disfmarker} the input ? The inputs are the fifteen {disfmarker} the fifteen uh bases feature .&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: the {disfmarker} with the new code . And the other three features are R , the variance of the difference between the two spectrum ,&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The new feature that PhD D has been working on this week to detect voice-unvoice is a feature related to the Aurora system with a new filter, referred to as VAD (Voice Activity Detection) or something similar to it.&#10;2. PhD D is trying to use multiple Multi-Layer Perceptrons (MLPs) with the new feature and fifteen features from the bus base system in the Mel Cepstrum. One MLP has three outputs for voice, unvoice, and silence, while the other MLP has fifty-six outputs for the probabilities of allophones. He conducted some recognition experiments with these and found that the results were a little better but more or less similar when using the MLP with three outputs." target="s not that critical . I mean there 's {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: You can {disfmarker} You can throw away stuff below a hundred hertz or so and it 's just not going to affect phonetic classification at all .&#10;Speaker: PhD E&#10;Content: Another thing I was thinking about was um is there a {disfmarker} I was wondering if there 's maybe um {vocalsound} certain settings of the parameters when you compute PLP which would basically cause it to output mel cepstrum . So that , in effect , what I could do is use our code but produce mel cepstrum and compare that directly to {disfmarker}&#10;Speaker: Professor C&#10;Content: Well , it 's not precisely . Yeah . I mean ,&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: um , {vocalsound} um what you can do is um you can definitely change the {disfmarker} the filter bank from being uh a uh trapezoidal integration to a {disfmarker} a {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The new feature that PhD D has been working on this week to detect voice-unvoice is a feature related to the Aurora system with a new filter, referred to as VAD (Voice Activity Detection) or something similar to it.&#10;2. PhD D is trying to use multiple Multi-Layer Perceptrons (MLPs) with the new feature and fifteen features from the bus base system in the Mel Cepstrum. One MLP has three outputs for voice, unvoice, and silence, while the other MLP has fifty-six outputs for the probabilities of allophones. He conducted some recognition experiments with these and found that the results were a little better but more or less similar when using the MLP with three outputs." target=" that I did notice was that the um log likelihoods coming out of the log recognizer from the PLP data were much lower , much smaller , than for the mel cepstral stuff , and that the average amount of pruning that was happening was therefore a little bit higher for the PLP features .&#10;Speaker: Professor C&#10;Content: Oh - huh !&#10;Speaker: PhD E&#10;Content: So , since he used the same exact pruning thresholds for both , I was wondering if it could be that we 're getting more pruning .&#10;Speaker: Professor C&#10;Content: Oh ! He {disfmarker} he {disfmarker} {vocalsound} He used the identical pruning thresholds even though the s the range of p of the likeli&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Oh well that 's {disfmarker} {vocalsound} That 's a pretty good {comment} point right there .&#10;Speaker: PhD E&#10;Content: Right . Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Yeah ,&#10;Speaker: Professor C&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The new feature that PhD D has been working on this week to detect voice-unvoice is a feature related to the Aurora system with a new filter, referred to as VAD (Voice Activity Detection) or something similar to it.&#10;2. PhD D is trying to use multiple Multi-Layer Perceptrons (MLPs) with the new feature and fifteen features from the bus base system in the Mel Cepstrum. One MLP has three outputs for voice, unvoice, and silence, while the other MLP has fifty-six outputs for the probabilities of allophones. He conducted some recognition experiments with these and found that the results were a little better but more or less similar when using the MLP with three outputs." target="To allow the Perceptual Linear Prediction (PLP) system to output mel cepstrum, one of the changes that could be made is to alter the filter bank from a trapezoidal integration to a triangular one. This change might lead to better results since typical mel cepstral filter banks use triangular filters. By doing this, the PLP system's output could be compared directly with the mel cepstrum produced by the SRI system.&#10;&#10;Comparing the outputs would be useful for phonetic classification as it can provide insights into the performance of each system and help identify which approach might be more suitable for specific applications. If both systems produce similar results, it could indicate that either method can be used interchangeably. However, if there are significant differences, it might suggest that one method is more appropriate for certain tasks than the other. Ultimately, this comparison can lead to a better understanding of the strengths and weaknesses of each system, informing future research and development in the field of phonetic classification.">
      <data key="d0">1</data>
    </edge>
    <edge source=" the {disfmarker} {vocalsound} {comment} S R I system looks like it works is that it reads the wavefiles directly , uh and does all of the cepstral computation stuff on the fly .&#10;Speaker: Professor C&#10;Content: Right . Right .&#10;Speaker: PhD E&#10;Content: And , so there 's no place where these {disfmarker} where the cepstral files are stored , anywhere that I can go look at and compare to the PLP ones , so whereas with our features , he 's actually storing the cepstrum on disk , and he reads those in .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: But it looked like he had to give it {disfmarker} uh even though the cepstrum is already computed , he has to give it uh a front - end parameter file . Which talks about the kind of uh com computation that his mel cepstrum thing does ,&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: so i I {disfmarker} I don't know if that {disfmark" target="1. The SRI system reads wavefiles directly and performs cepstral computation on the fly, but it doesn't store the cepstral files anywhere for comparison with PLP features. This is in contrast to the PLP system, which stores the cepstrum on disk and reads those in during feature extraction.&#10;&#10;2. The SRI system requires a front-end parameter file even after the cepstrum is computed because it's used to specify the kind of computation that the mel cepstrum uses. This may be related to the fact that some people have reported better performance using triangular filters in RASTA, instead of trapezoidal ones, which are typically used in mel cepstral filter banks.&#10;&#10;In summary, the main differences between the SRI and PLP systems lie in file storage (with the former reading wavefiles directly and computing cepstra on the fly and the latter storing computed cepstra on disk) and configuration parameters (where the SRI system requires a front-end parameter file even after the cepstrum is computed). The reason for this difference may be related to the type of filter bank used in each system.">
      <data key="d0">1</data>
    </edge>
    <edge source=" the {disfmarker} {vocalsound} {comment} S R I system looks like it works is that it reads the wavefiles directly , uh and does all of the cepstral computation stuff on the fly .&#10;Speaker: Professor C&#10;Content: Right . Right .&#10;Speaker: PhD E&#10;Content: And , so there 's no place where these {disfmarker} where the cepstral files are stored , anywhere that I can go look at and compare to the PLP ones , so whereas with our features , he 's actually storing the cepstrum on disk , and he reads those in .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: But it looked like he had to give it {disfmarker} uh even though the cepstrum is already computed , he has to give it uh a front - end parameter file . Which talks about the kind of uh com computation that his mel cepstrum thing does ,&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: so i I {disfmarker} I don't know if that {disfmark" target="To allow the Perceptual Linear Prediction (PLP) system to output mel cepstrum, one of the changes that could be made is to alter the filter bank from a trapezoidal integration to a triangular one. This change might lead to better results since typical mel cepstral filter banks use triangular filters. By doing this, the PLP system's output could be compared directly with the mel cepstrum produced by the SRI system.&#10;&#10;Comparing the outputs would be useful for phonetic classification as it can provide insights into the performance of each system and help identify which approach might be more suitable for specific applications. If both systems produce similar results, it could indicate that either method can be used interchangeably. However, if there are significant differences, it might suggest that one method is more appropriate for certain tasks than the other. Ultimately, this comparison can lead to a better understanding of the strengths and weaknesses of each system, informing future research and development in the field of phonetic classification.">
      <data key="d0">1</data>
    </edge>
    <edge source=" output . And I put together the fifteen features and the three MLP output . And , well , the result are li a little bit better , but more or less similar .&#10;Speaker: Professor C&#10;Content: Uh , I {disfmarker} I 'm {disfmarker} I 'm slightly confused .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: What {disfmarker} what feeds the uh {disfmarker} the three - output net ?&#10;Speaker: PhD D&#10;Content: Voice , unvoice , and si&#10;Speaker: Professor C&#10;Content: No no , what feeds it ? What features does it see ?&#10;Speaker: PhD D&#10;Content: The feature {disfmarker} the input ? The inputs are the fifteen {disfmarker} the fifteen uh bases feature .&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: the {disfmarker} with the new code . And the other three features are R , the variance of the difference between the two spectrum ,&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD" target="1. The feature set called &quot;the sound patterns of English&quot; consists of binary-valued features that represent various phonological aspects of English sounds. Examples of these features include voicing (voiced or voiceless) and sonorance (sonorant or not). These features are used to describe phones in the English language.&#10;2. During the discussion, Grad A mentioned that they found potential mistakes in the tables associated with &quot;the sound patterns of English&quot; feature set, specifically related to the mapping from phones to a phonological feature called &quot;coronal.&quot; They had identified inconsistencies in how the feature was labeled and were planning to review the tables further to find more possible errors.">
      <data key="d0">1</data>
    </edge>
    <edge source="s not that critical . I mean there 's {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: You can {disfmarker} You can throw away stuff below a hundred hertz or so and it 's just not going to affect phonetic classification at all .&#10;Speaker: PhD E&#10;Content: Another thing I was thinking about was um is there a {disfmarker} I was wondering if there 's maybe um {vocalsound} certain settings of the parameters when you compute PLP which would basically cause it to output mel cepstrum . So that , in effect , what I could do is use our code but produce mel cepstrum and compare that directly to {disfmarker}&#10;Speaker: Professor C&#10;Content: Well , it 's not precisely . Yeah . I mean ,&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: um , {vocalsound} um what you can do is um you can definitely change the {disfmarker} the filter bank from being uh a uh trapezoidal integration to a {disfmarker} a {d" target="1. The SRI system reads wavefiles directly and performs cepstral computation on the fly, but it doesn't store the cepstral files anywhere for comparison with PLP features. This is in contrast to the PLP system, which stores the cepstrum on disk and reads those in during feature extraction.&#10;&#10;2. The SRI system requires a front-end parameter file even after the cepstrum is computed because it's used to specify the kind of computation that the mel cepstrum uses. This may be related to the fact that some people have reported better performance using triangular filters in RASTA, instead of trapezoidal ones, which are typically used in mel cepstral filter banks.&#10;&#10;In summary, the main differences between the SRI and PLP systems lie in file storage (with the former reading wavefiles directly and computing cepstra on the fly and the latter storing computed cepstra on disk) and configuration parameters (where the SRI system requires a front-end parameter file even after the cepstrum is computed). The reason for this difference may be related to the type of filter bank used in each system.">
      <data key="d0">1</data>
    </edge>
    <edge source="s not that critical . I mean there 's {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: You can {disfmarker} You can throw away stuff below a hundred hertz or so and it 's just not going to affect phonetic classification at all .&#10;Speaker: PhD E&#10;Content: Another thing I was thinking about was um is there a {disfmarker} I was wondering if there 's maybe um {vocalsound} certain settings of the parameters when you compute PLP which would basically cause it to output mel cepstrum . So that , in effect , what I could do is use our code but produce mel cepstrum and compare that directly to {disfmarker}&#10;Speaker: Professor C&#10;Content: Well , it 's not precisely . Yeah . I mean ,&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: um , {vocalsound} um what you can do is um you can definitely change the {disfmarker} the filter bank from being uh a uh trapezoidal integration to a {disfmarker} a {d" target="To allow the Perceptual Linear Prediction (PLP) system to output mel cepstrum, one of the changes that could be made is to alter the filter bank from a trapezoidal integration to a triangular one. This change might lead to better results since typical mel cepstral filter banks use triangular filters. By doing this, the PLP system's output could be compared directly with the mel cepstrum produced by the SRI system.&#10;&#10;Comparing the outputs would be useful for phonetic classification as it can provide insights into the performance of each system and help identify which approach might be more suitable for specific applications. If both systems produce similar results, it could indicate that either method can be used interchangeably. However, if there are significant differences, it might suggest that one method is more appropriate for certain tasks than the other. Ultimately, this comparison can lead to a better understanding of the strengths and weaknesses of each system, informing future research and development in the field of phonetic classification.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that I did notice was that the um log likelihoods coming out of the log recognizer from the PLP data were much lower , much smaller , than for the mel cepstral stuff , and that the average amount of pruning that was happening was therefore a little bit higher for the PLP features .&#10;Speaker: Professor C&#10;Content: Oh - huh !&#10;Speaker: PhD E&#10;Content: So , since he used the same exact pruning thresholds for both , I was wondering if it could be that we 're getting more pruning .&#10;Speaker: Professor C&#10;Content: Oh ! He {disfmarker} he {disfmarker} {vocalsound} He used the identical pruning thresholds even though the s the range of p of the likeli&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Oh well that 's {disfmarker} {vocalsound} That 's a pretty good {comment} point right there .&#10;Speaker: PhD E&#10;Content: Right . Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Yeah ,&#10;Speaker: Professor C&#10;Content" target="The particular measure chosen by the speaker to integrate out the effects of pitch is the variance of the difference between the log FFT power spectrum and the filter bank. The choice of this statistic is not explicitly stated in the transcript, but it is mentioned that it was chosen because it might be a useful measure to compare the two.&#10;&#10;The reason why the variance of this difference stood out as a potential useful statistic is not explicitly explained in the transcript either, but it can be inferred that it has something to do with the fact that it allows for a comparison between the original signal (or FFT) and the filter bank. The speakers mention that using the variance, they were able to observe certain differences between the two, which is why it was considered as a potential useful statistic.&#10;&#10;Additionally, it's mentioned that there might be other measures or energy measures that could be used instead of the variance, but the speakers do not go into detail about what those measures might be. They also mention that in the past, people have used zero crossing counts for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated. However, it is mentioned that the error rate for a certain setup was reduced from around 80% to 4% with the use of a specific method. It can be inferred that the remaining error rate of 4% corresponds to the voiced-unvoiced frame error measurement accuracy rate of approximately 96%. However, this conclusion is based on indirect information and may not accurately reflect the true value discussed in the conversation." target=" PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: Yeah , it 's probably worth it .&#10;Speaker: Grad A&#10;Content: What are {disfmarker} what are your f uh frame error rates for {disfmarker} for this ?&#10;Speaker: PhD D&#10;Content: Eh fifty - f six uh no , the frame error rate ?&#10;Speaker: Grad A&#10;Content: O&#10;Speaker: PhD D&#10;Content: Fifty - six I think .&#10;Speaker: Professor C&#10;Content: Is that {disfmarker} maybe that 's accuracy ?&#10;Speaker: PhD D&#10;Content: Percent .&#10;Speaker: Grad A&#10;Content: Fif - fifty - six percent accurate for v voice - unvoice&#10;Speaker: PhD D&#10;Content: The accuracy . Mm - hmm . No for , yes f I don't remember for voice - unvoice ,&#10;Speaker: Grad A&#10;Content: Oh , OK .&#10;Speaker: PhD D&#10;Content: maybe for the other one .&#10;Speaker: Grad A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Yeah , voiced - unvoiced hopefully would be a lot">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated. However, it is mentioned that the error rate for a certain setup was reduced from around 80% to 4% with the use of a specific method. It can be inferred that the remaining error rate of 4% corresponds to the voiced-unvoiced frame error measurement accuracy rate of approximately 96%. However, this conclusion is based on indirect information and may not accurately reflect the true value discussed in the conversation." target="} {vocalsound} I take that frame and four f the four {disfmarker} I take {disfmarker} Sorry , I take the current frame and the four past frames and the {vocalsound} four future frames and that adds up to six seconds of speech . And I calculate um {vocalsound} the spectral mean , {vocalsound} of the log magnitude spectrum {pause} over that N . I use that to normalize the s the current center frame {vocalsound} by mean subtraction . And I then {disfmarker} then I move to the next frame and I {disfmarker} {vocalsound} I do it again . Well , actually I calculate all the means first and then I do the subtraction . And um {vocalsound} the {disfmarker} I tried that with HDK , the Aurora setup of HDK training on clean TI - digits , and um {vocalsound} it {disfmarker} it helped um in a phony reverberation case um {vocalsound} where I just used the simulated impulse response um {vocalsound} the error rate went from something like eighty it was from something like eighteen percent">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated. However, it is mentioned that the error rate for a certain setup was reduced from around 80% to 4% with the use of a specific method. It can be inferred that the remaining error rate of 4% corresponds to the voiced-unvoiced frame error measurement accuracy rate of approximately 96%. However, this conclusion is based on indirect information and may not accurately reflect the true value discussed in the conversation." target="aker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: but then you have something like spectral slope , which is you get like R - one ov over R - zero or something like that .&#10;Speaker: PhD D&#10;Content: Uh yeah .&#10;Speaker: PhD E&#10;Content: What are the R 's ?&#10;Speaker: Professor C&#10;Content: R correlations .&#10;Speaker: PhD E&#10;Content: I 'm sorry I missed it .&#10;Speaker: PhD D&#10;Content: No , R c No .&#10;Speaker: PhD E&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: Auto - correlation ? Yes , yes , the variance of the auto - correlation function that uses that&#10;Speaker: Professor C&#10;Content: Ye - Well that 's the variance , but if you just say &quot; what is {disfmarker} &quot; I mean , to first order , um yeah one of the differences between voiced , unvoiced and silence is energy . Another one is {disfmarker} but the other one is the spectral shape .&#10;Speaker: PhD D&#10;Content: Yeah , I I 'll {disfmarker} The spectral shape ,">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated. However, it is mentioned that the error rate for a certain setup was reduced from around 80% to 4% with the use of a specific method. It can be inferred that the remaining error rate of 4% corresponds to the voiced-unvoiced frame error measurement accuracy rate of approximately 96%. However, this conclusion is based on indirect information and may not accurately reflect the true value discussed in the conversation." target="alsound} where I just used the simulated impulse response um {vocalsound} the error rate went from something like eighty it was from something like eighteen percent {vocalsound} to um four percent . And on meeting rec recorder far mike digits , mike {disfmarker} on channel F , it went from um {vocalsound} {vocalsound} forty - one percent error to eight percent error .&#10;Speaker: PhD E&#10;Content: On {disfmarker} on the real data , not with artificial reverb ?&#10;Speaker: Grad B&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Grad B&#10;Content: And that {disfmarker} that was um {vocalsound} trained on clean speech only , which I 'm guessing is the reason why the baseline was so bad . And {disfmarker}&#10;Speaker: Professor C&#10;Content: That 's ac actually a little side point is I think that 's the first results that we have uh uh uh of any sort on the far field uh {disfmarker} on {disfmarker} on the far field data uh for {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated. However, it is mentioned that the error rate for a certain setup was reduced from around 80% to 4% with the use of a specific method. It can be inferred that the remaining error rate of 4% corresponds to the voiced-unvoiced frame error measurement accuracy rate of approximately 96%. However, this conclusion is based on indirect information and may not accurately reflect the true value discussed in the conversation." target="&#10;Speaker: Grad A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Yeah , voiced - unvoiced hopefully would be a lot better .&#10;Speaker: PhD D&#10;Content: for voiced . I don't reme&#10;Speaker: Grad A&#10;Content: Should be in nineties somewhere .&#10;Speaker: PhD D&#10;Content: Better . Maybe for voice - unvoice .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: This is for the other one . I should {disfmarker} I can't show that .&#10;Speaker: Grad A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: But I think that fifty - five was for the {disfmarker} when the output are the fifty - six phone .&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: That I look in the {disfmarker} with the other {disfmarker} nnn the other MLP that we have are more or less the same number . Silence will be better but more or less the same .&#10;Speaker: Professor C&#10;Content: I think at">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated. However, it is mentioned that the error rate for a certain setup was reduced from around 80% to 4% with the use of a specific method. It can be inferred that the remaining error rate of 4% corresponds to the voiced-unvoiced frame error measurement accuracy rate of approximately 96%. However, this conclusion is based on indirect information and may not accurately reflect the true value discussed in the conversation." target=" have are more or less the same number . Silence will be better but more or less the same .&#10;Speaker: Professor C&#10;Content: I think at the frame level for fifty - six that was the kind of number we were getting for {disfmarker} for uh um reduced band width uh stuff .&#10;Speaker: PhD D&#10;Content: I think that {disfmarker} I {disfmarker} I {disfmarker} I think that for the other one , for the three output , is sixty sixty - two , sixty three more or less .&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's all ?&#10;Speaker: PhD D&#10;Content: It 's {disfmarker} Yeah .&#10;Speaker: Professor C&#10;Content: That 's pretty bad .&#10;Speaker: PhD D&#10;Content: Yeah , because it 's noise also .&#10;Speaker: Grad A&#10;Content: Oh yeah .&#10;Speaker: Professor C&#10;Content: Aha !&#10;Speaker: PhD D&#10;Content: And we have&#10;Speaker: Professor C&#10;Content: Aha ! Yeah . Yeah . OK">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated. However, it is mentioned that the error rate for a certain setup was reduced from around 80% to 4% with the use of a specific method. It can be inferred that the remaining error rate of 4% corresponds to the voiced-unvoiced frame error measurement accuracy rate of approximately 96%. However, this conclusion is based on indirect information and may not accurately reflect the true value discussed in the conversation." target="Based on the information provided in the transcript, the error rate reduction in speech recognition achieved by using a simulated impulse response and training on clean speech only was from approximately 80% to 4%. This conclusion is based on indirect information since the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated.&#10;&#10;The transcript mentions that when using a simulated impulse response, the error rate went from something like eighteen percent to four percent for vocalsound and from forty-one percent to eight percent for mike far digits on channel F. It can be inferred that these reductions in error rates were due to using a simulated impulse response and training on clean speech only.&#10;&#10;However, the transcript does not provide information about the error rate when not using artificial reverb or if the system was trained on noisy speech. The comparison is made between using artificial reverb and clean speech versus an unspecified setup that may or may not have used artificial reverb and could have been trained on noisy speech.&#10;&#10;Therefore, while there was a significant reduction in error rates for speech recognition when using a simulated impulse response and training on clean speech only, the exact difference in error rates between these setups cannot be determined from the information provided in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source=" PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: Yeah , it 's probably worth it .&#10;Speaker: Grad A&#10;Content: What are {disfmarker} what are your f uh frame error rates for {disfmarker} for this ?&#10;Speaker: PhD D&#10;Content: Eh fifty - f six uh no , the frame error rate ?&#10;Speaker: Grad A&#10;Content: O&#10;Speaker: PhD D&#10;Content: Fifty - six I think .&#10;Speaker: Professor C&#10;Content: Is that {disfmarker} maybe that 's accuracy ?&#10;Speaker: PhD D&#10;Content: Percent .&#10;Speaker: Grad A&#10;Content: Fif - fifty - six percent accurate for v voice - unvoice&#10;Speaker: PhD D&#10;Content: The accuracy . Mm - hmm . No for , yes f I don't remember for voice - unvoice ,&#10;Speaker: Grad A&#10;Content: Oh , OK .&#10;Speaker: PhD D&#10;Content: maybe for the other one .&#10;Speaker: Grad A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Yeah , voiced - unvoiced hopefully would be a lot" target="Based on the transcript provided, it is not explicitly stated that Professor C confirmed she left the microphone on before leaving for another appointment. However, she did mention &quot;I left it on&quot; in her speech. It's unclear whether she was referring to the microphone or something else, but given the context of the conversation, it's possible that she was indeed confirming that the microphone was left on.&#10;&#10;As for whether this was agreeable to PhD E, there is no explicit response from him regarding this matter in the transcript. Therefore, it cannot be definitively determined if he agreed or disagreed with leaving the microphone on.">
      <data key="d0">1</data>
    </edge>
    <edge source="} {vocalsound} I take that frame and four f the four {disfmarker} I take {disfmarker} Sorry , I take the current frame and the four past frames and the {vocalsound} four future frames and that adds up to six seconds of speech . And I calculate um {vocalsound} the spectral mean , {vocalsound} of the log magnitude spectrum {pause} over that N . I use that to normalize the s the current center frame {vocalsound} by mean subtraction . And I then {disfmarker} then I move to the next frame and I {disfmarker} {vocalsound} I do it again . Well , actually I calculate all the means first and then I do the subtraction . And um {vocalsound} the {disfmarker} I tried that with HDK , the Aurora setup of HDK training on clean TI - digits , and um {vocalsound} it {disfmarker} it helped um in a phony reverberation case um {vocalsound} where I just used the simulated impulse response um {vocalsound} the error rate went from something like eighty it was from something like eighteen percent" target="The output result of the reduced &quot;vocalsound&quot; parameterization in the model is either a zero or a one. This is because the model has been simplified to keep only certain selected examples, resulting in a binary output. The original parameterization might have produced a number between zero and one, but due to the reduction in complexity, it no longer does so.">
      <data key="d0">1</data>
    </edge>
    <edge source="alsound} where I just used the simulated impulse response um {vocalsound} the error rate went from something like eighty it was from something like eighteen percent {vocalsound} to um four percent . And on meeting rec recorder far mike digits , mike {disfmarker} on channel F , it went from um {vocalsound} {vocalsound} forty - one percent error to eight percent error .&#10;Speaker: PhD E&#10;Content: On {disfmarker} on the real data , not with artificial reverb ?&#10;Speaker: Grad B&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Grad B&#10;Content: And that {disfmarker} that was um {vocalsound} trained on clean speech only , which I 'm guessing is the reason why the baseline was so bad . And {disfmarker}&#10;Speaker: Professor C&#10;Content: That 's ac actually a little side point is I think that 's the first results that we have uh uh uh of any sort on the far field uh {disfmarker} on {disfmarker} on the far field data uh for {disf" target="Based on the information provided in the transcript, the error rate reduction in speech recognition achieved by using a simulated impulse response and training on clean speech only was from approximately 80% to 4%. This conclusion is based on indirect information since the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated.&#10;&#10;The transcript mentions that when using a simulated impulse response, the error rate went from something like eighteen percent to four percent for vocalsound and from forty-one percent to eight percent for mike far digits on channel F. It can be inferred that these reductions in error rates were due to using a simulated impulse response and training on clean speech only.&#10;&#10;However, the transcript does not provide information about the error rate when not using artificial reverb or if the system was trained on noisy speech. The comparison is made between using artificial reverb and clean speech versus an unspecified setup that may or may not have used artificial reverb and could have been trained on noisy speech.&#10;&#10;Therefore, while there was a significant reduction in error rates for speech recognition when using a simulated impulse response and training on clean speech only, the exact difference in error rates between these setups cannot be determined from the information provided in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="The output result of the reduced &quot;vocalsound&quot; parameterization in the model is either a zero or a one. This is because the model has been simplified to keep only certain selected examples, resulting in a binary output. The original parameterization might have produced a number between zero and one, but due to the reduction in complexity, it no longer does so." target="disfmarker} a reduced um {vocalsound} parameterization of {disfmarker} of the {disfmarker} the model by just keeping {vocalsound} certain selected examples .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Yeah . So .&#10;Speaker: Professor C&#10;Content: But I don't know if people have done sort of careful comparisons of this on large tasks or anything . Maybe {disfmarker} maybe they have . I don't know .&#10;Speaker: Grad A&#10;Content: Yeah , I don't know either .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad B&#10;Content: S do you get some kind of number between zero and one at the output ?&#10;Speaker: Grad A&#10;Content: Actually you don't get a {disfmarker} you don't get a nice number between zero and one . You get {disfmarker} you get either a zero or a one . Um , uh there are {disfmarker} there are pap Well , basically , it 's {disfmarker} it 's um {vocalsound} you {d">
      <data key="d0">1</data>
    </edge>
    <edge source="The output result of the reduced &quot;vocalsound&quot; parameterization in the model is either a zero or a one. This is because the model has been simplified to keep only certain selected examples, resulting in a binary output. The original parameterization might have produced a number between zero and one, but due to the reduction in complexity, it no longer does so." target="Content: Hmm .&#10;Speaker: Professor C&#10;Content: And th the You know , um neural net approach uh or Gaussian mixtures for that matter are sort of {disfmarker} fairly brute force kinds of things , where you sort of {disfmarker} {vocalsound} you predefine that there is this big bunch of parameters and then you {disfmarker} you place them as you best can to define the boundaries , and in fact , as you know , {vocalsound} these things do take a lot of parameters and {disfmarker} and uh {vocalsound} if you have uh only a modest amount of data , you have trouble {vocalsound} uh learning them . Um , so I {disfmarker} I guess the idea to this is that it {disfmarker} it is reputed to uh be somewhat better in that regard .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: Right . I it can be a {disfmarker} a reduced um {vocalsound} parameterization of {disfmarker} of the {disfmarker} the model">
      <data key="d0">1</data>
    </edge>
    <edge source="The output result of the reduced &quot;vocalsound&quot; parameterization in the model is either a zero or a one. This is because the model has been simplified to keep only certain selected examples, resulting in a binary output. The original parameterization might have produced a number between zero and one, but due to the reduction in complexity, it no longer does so." target=": Yeah .&#10;Speaker: Professor C&#10;Content: OK , and you were saying something {disfmarker} starting to say something else about your {disfmarker} your class project , or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Oh . Yeah th Um .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: So for my class project I 'm {vocalsound} um {vocalsound} {vocalsound} I 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . And so I 'm gonna apply that to {vocalsound} um compare it with the results by um King and Taylor who did {vocalsound} um these um using recurrent neural nets , they recognized {vocalsound} um {vocalsound} a set of phonological features um and made a mapping from the MFCC 's to these phonological features , so I 'm gonna {vocalsound} do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="The output result of the reduced &quot;vocalsound&quot; parameterization in the model is either a zero or a one. This is because the model has been simplified to keep only certain selected examples, resulting in a binary output. The original parameterization might have produced a number between zero and one, but due to the reduction in complexity, it no longer does so." target=" what you 're {disfmarker} what you get out of your system . Do you get out a uh {disfmarker} a vector of these ones and zeros and then try to find the closest matching phoneme to that vector ,&#10;Speaker: Grad A&#10;Content: Mm - hmm . Oh .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: No , no . I 'm not {disfmarker} I 'm not planning to do any {disfmarker} any phoneme mapping yet . Just {disfmarker} {vocalsound} it 's {disfmarker} it 's basically {disfmarker} it 's {disfmarker} it 's really simple , basically a detection {vocalsound} of phonological features .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Grad A&#10;Content: Yeah ,&#10;Speaker: PhD E&#10;Content: I see .&#10;Speaker: Grad A&#10;Content: and um {vocalsound} {vocalsound} cuz the uh {disfmarker} So">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Hmm .&#10;Speaker: Professor C&#10;Content: And th the You know , um neural net approach uh or Gaussian mixtures for that matter are sort of {disfmarker} fairly brute force kinds of things , where you sort of {disfmarker} {vocalsound} you predefine that there is this big bunch of parameters and then you {disfmarker} you place them as you best can to define the boundaries , and in fact , as you know , {vocalsound} these things do take a lot of parameters and {disfmarker} and uh {vocalsound} if you have uh only a modest amount of data , you have trouble {vocalsound} uh learning them . Um , so I {disfmarker} I guess the idea to this is that it {disfmarker} it is reputed to uh be somewhat better in that regard .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: Right . I it can be a {disfmarker} a reduced um {vocalsound} parameterization of {disfmarker} of the {disfmarker} the model" target="The suggested approach that is reputed to be better than neural nets or Gaussian mixtures in terms of requiring fewer parameters and performing well even with a modest amount of data is not explicitly mentioned in the provided transcript. However, the speakers discuss an idea where they simplify the model and directly feed the log magnitude spectrum into it, without applying any clever techniques. This could potentially be a more parameter-efficient approach, but it's not specified if this method has any particular name or reputation for better performance with less data.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: Professor C&#10;Content: OK , and you were saying something {disfmarker} starting to say something else about your {disfmarker} your class project , or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Oh . Yeah th Um .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: So for my class project I 'm {vocalsound} um {vocalsound} {vocalsound} I 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . And so I 'm gonna apply that to {vocalsound} um compare it with the results by um King and Taylor who did {vocalsound} um these um using recurrent neural nets , they recognized {vocalsound} um {vocalsound} a set of phonological features um and made a mapping from the MFCC 's to these phonological features , so I 'm gonna {vocalsound} do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker" target="1. Better results with triangular filters in RASTA: The speakers mention that some people have claimed better performance using triangular filters in RASTA instead of trapezoidal ones. This could be due to the fact that typical mel cepstral filter banks use triangular filters, which might lead to better results when applied to this problem.&#10;2. Training data issues: The speakers discuss the possibility of training data being &quot;messed up,&quot; causing undesirable behavior in the system. This could mean that there are errors or inconsistencies in the data used for training the models, leading to poor performance. One possible issue mentioned is a difference in normalization, which might not account for all discrepancies but could be contributing to the problem.&#10;3. Compression issues: Another possibility raised by PhD E is the presence of certain types of compression or other unnoticed issues within a subset of the training data. This would cause the computed features for this subset to be incorrect, leading to poor performance and models that are &quot;all messed up.&quot;">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: Professor C&#10;Content: OK , and you were saying something {disfmarker} starting to say something else about your {disfmarker} your class project , or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Oh . Yeah th Um .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: So for my class project I 'm {vocalsound} um {vocalsound} {vocalsound} I 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . And so I 'm gonna apply that to {vocalsound} um compare it with the results by um King and Taylor who did {vocalsound} um these um using recurrent neural nets , they recognized {vocalsound} um {vocalsound} a set of phonological features um and made a mapping from the MFCC 's to these phonological features , so I 'm gonna {vocalsound} do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker" target="1. Using a &quot;net&quot; or machine learning model: The speakers suggest using a neural network or &quot;net&quot; to learn and implement measures over time. They propose feeding the log magnitude spectrum into the neural net, as it is good at figuring out what it most needs from various inputs.&#10;2. When not to use the net: It might be more beneficial to not use the net when you have exactly the right thing or a clear understanding of how to implement the measures without relying on the net. This would prevent asking the net to learn something that is already understood, which could lead to better results. However, this approach assumes that the data being used is correct and well-normalized, as any issues in the training data can still cause problems in the models.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: Professor C&#10;Content: OK , and you were saying something {disfmarker} starting to say something else about your {disfmarker} your class project , or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Oh . Yeah th Um .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: So for my class project I 'm {vocalsound} um {vocalsound} {vocalsound} I 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . And so I 'm gonna apply that to {vocalsound} um compare it with the results by um King and Taylor who did {vocalsound} um these um using recurrent neural nets , they recognized {vocalsound} um {vocalsound} a set of phonological features um and made a mapping from the MFCC 's to these phonological features , so I 'm gonna {vocalsound} do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker" target="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: Professor C&#10;Content: OK , and you were saying something {disfmarker} starting to say something else about your {disfmarker} your class project , or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Oh . Yeah th Um .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: So for my class project I 'm {vocalsound} um {vocalsound} {vocalsound} I 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . And so I 'm gonna apply that to {vocalsound} um compare it with the results by um King and Taylor who did {vocalsound} um these um using recurrent neural nets , they recognized {vocalsound} um {vocalsound} a set of phonological features um and made a mapping from the MFCC 's to these phonological features , so I 'm gonna {vocalsound} do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker" target="1. Relevant meetings for computational speech processing include Interspeech Computeational Seminar in Speech Research (ICSLP) and Eurospeech. These conferences are more targeted for &quot;dyed-in-the-wool&quot; speech professionals, as they focus specifically on speech processing topics.&#10;2. One might ignore these meetings due to being too busy or not finding the time to attend. Additionally, if a professional has a clear understanding of how to implement specific measures without needing a net, they might choose to not use the net and instead focus on their existing knowledge. This decision could lead to better results, as long as the data being used is correct and well-normalized.&#10;&#10;Relevant sources from the transcript:&#10;- Speaker: PhD E, Content: for sort of dyed - in - the wool speech people , um I think that ICSLP and Eurospeech are much more targeted .&#10;- Speaker: Professor C, Content: So . I mean , I mostly just ignored it because I was too busy...">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: Professor C&#10;Content: OK , and you were saying something {disfmarker} starting to say something else about your {disfmarker} your class project , or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Oh . Yeah th Um .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: So for my class project I 'm {vocalsound} um {vocalsound} {vocalsound} I 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . And so I 'm gonna apply that to {vocalsound} um compare it with the results by um King and Taylor who did {vocalsound} um these um using recurrent neural nets , they recognized {vocalsound} um {vocalsound} a set of phonological features um and made a mapping from the MFCC 's to these phonological features , so I 'm gonna {vocalsound} do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker" target="1. The feature set called &quot;the sound patterns of English&quot; consists of binary-valued features that represent various phonological aspects of English sounds. Examples of these features include voicing (voiced or voiceless) and sonorance (sonorant or not). These features are used to describe phones in the English language.&#10;2. During the discussion, Grad A mentioned that they found potential mistakes in the tables associated with &quot;the sound patterns of English&quot; feature set, specifically related to the mapping from phones to a phonological feature called &quot;coronal.&quot; They had identified inconsistencies in how the feature was labeled and were planning to review the tables further to find more possible errors.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: Professor C&#10;Content: OK , and you were saying something {disfmarker} starting to say something else about your {disfmarker} your class project , or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Oh . Yeah th Um .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: So for my class project I 'm {vocalsound} um {vocalsound} {vocalsound} I 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . And so I 'm gonna apply that to {vocalsound} um compare it with the results by um King and Taylor who did {vocalsound} um these um using recurrent neural nets , they recognized {vocalsound} um {vocalsound} a set of phonological features um and made a mapping from the MFCC 's to these phonological features , so I 'm gonna {vocalsound} do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker" target="The support vectors, or the critical examples, are saved during the process of recreating the boundary for the test set in a Support Vector Machine (SVM) because they provide several benefits compared to just saving the boundary itself. The support vectors represent the data points that lie closest to the decision boundary, and they effectively define the boundary. By storing these critical examples, SVMs can:&#10;&#10;1. Handle new classes: If new classes appear in the test set that were not present during training, the current boundary might not be useful. However, having the support vectors allows the algorithm to adapt and create a new boundary that better suits the updated data distribution.&#10;2. Adapt to changes in the data distribution: In real-world scenarios, data distributions can change over time. By saving the support vectors, SVMs can efficiently update the decision boundary when needed, without requiring access to the entire original dataset.&#10;3. Reduce computational complexity: Classifying new examples involves calculating their distance to the decision boundary. By using support vectors instead of the whole boundary, the computational cost is significantly reduced because there are far fewer support vectors than potential points on the boundary.&#10;4. Improve interpretability: Keeping the support vectors can provide more insights into the dataset and the classification problem, making it easier for analysts to understand why certain decisions are made.&#10;&#10;In summary, saving support vectors instead of just the decision boundary allows SVMs to be more flexible, adaptable, efficient, and interpretable when dealing with new or changing data distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: Professor C&#10;Content: OK , and you were saying something {disfmarker} starting to say something else about your {disfmarker} your class project , or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Oh . Yeah th Um .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: So for my class project I 'm {vocalsound} um {vocalsound} {vocalsound} I 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . And so I 'm gonna apply that to {vocalsound} um compare it with the results by um King and Taylor who did {vocalsound} um these um using recurrent neural nets , they recognized {vocalsound} um {vocalsound} a set of phonological features um and made a mapping from the MFCC 's to these phonological features , so I 'm gonna {vocalsound} do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker" target="1. The concept of condensed nearest neighbor (CNN) refers to a modification of the traditional nearest neighbor classification algorithm. In the standard nearest neighbor method, you compare a query point to all training samples to find the closest one(s). This can be computationally expensive and time-consuming, especially with large datasets.&#10;2. To address this issue, CNN was proposed to reduce the number of comparisons needed. The idea is to select a subset of representative examples from the original dataset that can accurately classify any new input. By comparing the query point only to these condensed representatives instead of the entire dataset, you significantly decrease computation time without compromising classification accuracy.&#10;3. CNN can be related to support vector machines (SVM) and neural nets approaches in terms of dimensionality reduction and model simplicity. SVMs aim to find a hyperplane that maximally separates classes with a margin, focusing on critical support vectors instead of all training samples. Similarly, condensed nearest neighbor selects key examples, which can be seen as a form of data compression or dimensionality reduction.&#10;4. Neural nets and Gaussian mixtures, on the other hand, typically require more parameters and data to achieve good performance compared to SVMs and CNN. However, they can model complex decision boundaries and are better suited for multi-class problems and cases where relationships between features are not easily captured by a linear boundary or distance metric.&#10;5. In summary, condensed nearest neighbor is a more efficient variant of traditional nearest neighbor classification that reduces computation time by selecting representative examples from the original dataset. This approach shares similarities with support vector machines, focusing on key data points, while differing from neural nets and Gaussian mixtures, which often require more parameters and data to perform well.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: Professor C&#10;Content: OK , and you were saying something {disfmarker} starting to say something else about your {disfmarker} your class project , or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Oh . Yeah th Um .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: So for my class project I 'm {vocalsound} um {vocalsound} {vocalsound} I 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . And so I 'm gonna apply that to {vocalsound} um compare it with the results by um King and Taylor who did {vocalsound} um these um using recurrent neural nets , they recognized {vocalsound} um {vocalsound} a set of phonological features um and made a mapping from the MFCC 's to these phonological features , so I 'm gonna {vocalsound} do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker" target="The system learns a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals, to binary vectors representing the presence or absence of specific phonological features in corresponding phones. The output of the system is a vector of ones and zeros for each phone, where each element corresponds to whether the phone exhibits a particular phonological feature or not. This information can be further used to classify phones based on their audio signals.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: Professor C&#10;Content: OK , and you were saying something {disfmarker} starting to say something else about your {disfmarker} your class project , or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Oh . Yeah th Um .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: So for my class project I 'm {vocalsound} um {vocalsound} {vocalsound} I 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . And so I 'm gonna apply that to {vocalsound} um compare it with the results by um King and Taylor who did {vocalsound} um these um using recurrent neural nets , they recognized {vocalsound} um {vocalsound} a set of phonological features um and made a mapping from the MFCC 's to these phonological features , so I 'm gonna {vocalsound} do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker" target="1. The concept being discussed involves calculating a distance measure from MFCC features to phonological features, which is then translated into a binary value (0 or 1) for classification purposes. This approach is used for binary classification where each feature is evaluated as either present (1) or absent (0).&#10;2. In the context of speech recognition, this distance measure is calculated between the MFCC features derived from audio signals and the phonological features corresponding to phones. The distances are then translated into binary values representing whether specific phonological features are present in the given phone.&#10;3. This method utilizes distances by converting them into probabilities for phonological feature occurrences, which can be used for recognizing phones based on their audio signals. Mississippi State researchers have successfully applied this technique using support vector machines to estimate probabilities for speech recognition tasks. The accuracy of the phone recognition depends on the precise identification and marking of the phonological events within the audio signal, as mentioned by Professor C.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: Professor C&#10;Content: OK , and you were saying something {disfmarker} starting to say something else about your {disfmarker} your class project , or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Oh . Yeah th Um .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: So for my class project I 'm {vocalsound} um {vocalsound} {vocalsound} I 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . And so I 'm gonna apply that to {vocalsound} um compare it with the results by um King and Taylor who did {vocalsound} um these um using recurrent neural nets , they recognized {vocalsound} um {vocalsound} a set of phonological features um and made a mapping from the MFCC 's to these phonological features , so I 'm gonna {vocalsound} do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker" target="Support Vector Machines (SVM) are a type of classification algorithm that use a feature space to find the optimal separating plane between two different classes. The goal is to pick those examples of features that are closest to the separating boundary, which are referred to as support vectors. These critical examples effectively define the boundary, providing several benefits compared to just saving the boundary itself:&#10;&#10;1. Handling new classes: By storing support vectors, SVMs can adapt and create a new boundary when new classes appear in the test set that were not present during training.&#10;2. Adapting to changes in data distribution: Support vectors allow SVMs to efficiently update the decision boundary when data distributions change over time without requiring access to the entire original dataset.&#10;3. Reducing computational complexity: Classifying new examples involves calculating their distance to the decision boundary. Using support vectors significantly reduces computation cost since there are far fewer support vectors than potential points on the boundary.&#10;4. Improving interpretability: Keeping the support vectors provides insights into the dataset and classification problem, making it easier for analysts to understand decision-making processes.&#10;&#10;In summary, SVMs use a feature space and pick examples closest to the separating boundary (support vectors) because this approach offers flexibility, adaptability, efficiency, and interpretability when dealing with new or changing data distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source=" what you 're {disfmarker} what you get out of your system . Do you get out a uh {disfmarker} a vector of these ones and zeros and then try to find the closest matching phoneme to that vector ,&#10;Speaker: Grad A&#10;Content: Mm - hmm . Oh .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ?&#10;Speaker: Grad A&#10;Content: No , no . I 'm not {disfmarker} I 'm not planning to do any {disfmarker} any phoneme mapping yet . Just {disfmarker} {vocalsound} it 's {disfmarker} it 's basically {disfmarker} it 's {disfmarker} it 's really simple , basically a detection {vocalsound} of phonological features .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Grad A&#10;Content: Yeah ,&#10;Speaker: PhD E&#10;Content: I see .&#10;Speaker: Grad A&#10;Content: and um {vocalsound} {vocalsound} cuz the uh {disfmarker} So" target="In the project, the speakers are using a vector of ones and zeros to represent each phone where each element in the vector corresponds to whether the phone exhibits a particular phonological feature or not. This vector is obtained by learning a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals. The output of the system is a vector of ones and zeros for each phone, where each element in the vector represents the presence or absence of a specific phonological feature.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better results with triangular filters in RASTA: The speakers mention that some people have claimed better performance using triangular filters in RASTA instead of trapezoidal ones. This could be due to the fact that typical mel cepstral filter banks use triangular filters, which might lead to better results when applied to this problem.&#10;2. Training data issues: The speakers discuss the possibility of training data being &quot;messed up,&quot; causing undesirable behavior in the system. This could mean that there are errors or inconsistencies in the data used for training the models, leading to poor performance. One possible issue mentioned is a difference in normalization, which might not account for all discrepancies but could be contributing to the problem.&#10;3. Compression issues: Another possibility raised by PhD E is the presence of certain types of compression or other unnoticed issues within a subset of the training data. This would cause the computed features for this subset to be incorrect, leading to poor performance and models that are &quot;all messed up.&quot;" target=" hmm .&#10;Speaker: Professor C&#10;Content: but we did {disfmarker} eh we did hear this comment from people at some point , that {vocalsound} um it uh they got some better results with the triangular filters rather than the trapezoidal . So that is an option in RASTA .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: Uh and you can certainly play with that . But I think you 're probably doing the right thing to look for bugs first . I don't know .&#10;Speaker: PhD E&#10;Content: Yeah just {disfmarker} it just seems like this kind of behavior could be caused by you know s some of the training data being messed up .&#10;Speaker: Professor C&#10;Content: Could be .&#10;Speaker: PhD E&#10;Content: You know , you 're sort of getting most of the way there , but there 's a {disfmarker} So I started going through and looking {disfmarker} One of the things that I did notice was that the um log likelihoods coming out of the log recognizer from the PLP data were much lower , much smaller , than for">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better results with triangular filters in RASTA: The speakers mention that some people have claimed better performance using triangular filters in RASTA instead of trapezoidal ones. This could be due to the fact that typical mel cepstral filter banks use triangular filters, which might lead to better results when applied to this problem.&#10;2. Training data issues: The speakers discuss the possibility of training data being &quot;messed up,&quot; causing undesirable behavior in the system. This could mean that there are errors or inconsistencies in the data used for training the models, leading to poor performance. One possible issue mentioned is a difference in normalization, which might not account for all discrepancies but could be contributing to the problem.&#10;3. Compression issues: Another possibility raised by PhD E is the presence of certain types of compression or other unnoticed issues within a subset of the training data. This would cause the computed features for this subset to be incorrect, leading to poor performance and models that are &quot;all messed up.&quot;" target=" definitely change the {disfmarker} the filter bank from being uh a uh trapezoidal integration to a {disfmarker} a {disfmarker} a triangular one ,&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: which is what the typical mel {disfmarker} mel cepstral uh filter bank does .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And some people have claimed that they got some better performance doing that , so you certainly could do that easily . But the fundamental difference , I mean , there 's other small differences {disfmarker}&#10;Speaker: PhD E&#10;Content: There 's a cubic root that happens , right ?&#10;Speaker: Professor C&#10;Content: Yeah , but , you know , as opposed to the log in the other case . I mean {vocalsound} the fundamental d d difference that we 've seen any kind of difference from before , which is actually an advantage for the P L P i uh , I think , is that the {disfmarker} the smoothing at the end is auto -">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better results with triangular filters in RASTA: The speakers mention that some people have claimed better performance using triangular filters in RASTA instead of trapezoidal ones. This could be due to the fact that typical mel cepstral filter banks use triangular filters, which might lead to better results when applied to this problem.&#10;2. Training data issues: The speakers discuss the possibility of training data being &quot;messed up,&quot; causing undesirable behavior in the system. This could mean that there are errors or inconsistencies in the data used for training the models, leading to poor performance. One possible issue mentioned is a difference in normalization, which might not account for all discrepancies but could be contributing to the problem.&#10;3. Compression issues: Another possibility raised by PhD E is the presence of certain types of compression or other unnoticed issues within a subset of the training data. This would cause the computed features for this subset to be incorrect, leading to poor performance and models that are &quot;all messed up.&quot;" target=" {disfmarker}&#10;Speaker: Professor C&#10;Content: But see {disfmarker} see my point ? If you had {disfmarker} {vocalsound} If you had ten filters , {vocalsound} then you would be throwing away a lot at the two ends .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And if you had {disfmarker} if you had fifty filters , you 'd be throwing away hardly anything .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Um , I don't remember there being an independent way of saying &quot; we 're just gonna make them from here to here &quot; .&#10;Speaker: PhD E&#10;Content: Use this analysis bandwidth or something .&#10;Speaker: Professor C&#10;Content: But I {disfmarker} I {disfmarker} I don't know , it 's actually been awhile since I 've looked at it .&#10;Speaker: PhD E&#10;Content: Yeah , I went through the Feacalc code and then looked at you know just calling the RASTA">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better results with triangular filters in RASTA: The speakers mention that some people have claimed better performance using triangular filters in RASTA instead of trapezoidal ones. This could be due to the fact that typical mel cepstral filter banks use triangular filters, which might lead to better results when applied to this problem.&#10;2. Training data issues: The speakers discuss the possibility of training data being &quot;messed up,&quot; causing undesirable behavior in the system. This could mean that there are errors or inconsistencies in the data used for training the models, leading to poor performance. One possible issue mentioned is a difference in normalization, which might not account for all discrepancies but could be contributing to the problem.&#10;3. Compression issues: Another possibility raised by PhD E is the presence of certain types of compression or other unnoticed issues within a subset of the training data. This would cause the computed features for this subset to be incorrect, leading to poor performance and models that are &quot;all messed up.&quot;" target=" I agree , but I thought that the normalization difference was one of the possibilities ,&#10;Speaker: PhD E&#10;Content: and {disfmarker} Yeah , but I don't {disfmarker} I 'm not {disfmarker}&#10;Speaker: Professor C&#10;Content: right ?&#10;Speaker: PhD E&#10;Content: Yeah , I guess I don't think that the normalization difference is gonna account for everything .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: So what I was working on is um just going through and checking the headers of the wavefiles , to see if maybe there was a um {disfmarker} a certain type of compression or something that was done that my script wasn't catching . So that for some subset of the training data , uh the {disfmarker} the {disfmarker} the features I was computing were junk .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: Which would you know cause it to perform OK , but uh , you know , the {disfmarker} the models would be all messed up . So I was going through and just">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better results with triangular filters in RASTA: The speakers mention that some people have claimed better performance using triangular filters in RASTA instead of trapezoidal ones. This could be due to the fact that typical mel cepstral filter banks use triangular filters, which might lead to better results when applied to this problem.&#10;2. Training data issues: The speakers discuss the possibility of training data being &quot;messed up,&quot; causing undesirable behavior in the system. This could mean that there are errors or inconsistencies in the data used for training the models, leading to poor performance. One possible issue mentioned is a difference in normalization, which might not account for all discrepancies but could be contributing to the problem.&#10;3. Compression issues: Another possibility raised by PhD E is the presence of certain types of compression or other unnoticed issues within a subset of the training data. This would cause the computed features for this subset to be incorrect, leading to poor performance and models that are &quot;all messed up.&quot;" target=" do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker: PhD E&#10;Content: So what 's the advantage of support vector machines ? What {disfmarker}&#10;Speaker: Grad A&#10;Content: Um . So , support vector machines are {disfmarker} are good with dealing with a less amount of data&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: and um so if you {disfmarker} if you give it less data it still does a reasonable job {vocalsound} in learning the {disfmarker} the patterns .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Um and {vocalsound} um&#10;Speaker: Professor C&#10;Content: I guess it {disfmarker} yeah , they 're sort of succinct , and {disfmarker} and they {vocalsound} uh&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Does there some kind of a distance metric that they use or how do they {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better results with triangular filters in RASTA: The speakers mention that some people have claimed better performance using triangular filters in RASTA instead of trapezoidal ones. This could be due to the fact that typical mel cepstral filter banks use triangular filters, which might lead to better results when applied to this problem.&#10;2. Training data issues: The speakers discuss the possibility of training data being &quot;messed up,&quot; causing undesirable behavior in the system. This could mean that there are errors or inconsistencies in the data used for training the models, leading to poor performance. One possible issue mentioned is a difference in normalization, which might not account for all discrepancies but could be contributing to the problem.&#10;3. Compression issues: Another possibility raised by PhD E is the presence of certain types of compression or other unnoticed issues within a subset of the training data. This would cause the computed features for this subset to be incorrect, leading to poor performance and models that are &quot;all messed up.&quot;" target="1. Using a &quot;net&quot; or machine learning model: The speakers suggest using a neural network or &quot;net&quot; to learn and implement measures over time. They propose feeding the log magnitude spectrum into the neural net, as it is good at figuring out what it most needs from various inputs.&#10;2. When not to use the net: It might be more beneficial to not use the net when you have exactly the right thing or a clear understanding of how to implement the measures without relying on the net. This would prevent asking the net to learn something that is already understood, which could lead to better results. However, this approach assumes that the data being used is correct and well-normalized, as any issues in the training data can still cause problems in the models.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better results with triangular filters in RASTA: The speakers mention that some people have claimed better performance using triangular filters in RASTA instead of trapezoidal ones. This could be due to the fact that typical mel cepstral filter banks use triangular filters, which might lead to better results when applied to this problem.&#10;2. Training data issues: The speakers discuss the possibility of training data being &quot;messed up,&quot; causing undesirable behavior in the system. This could mean that there are errors or inconsistencies in the data used for training the models, leading to poor performance. One possible issue mentioned is a difference in normalization, which might not account for all discrepancies but could be contributing to the problem.&#10;3. Compression issues: Another possibility raised by PhD E is the presence of certain types of compression or other unnoticed issues within a subset of the training data. This would cause the computed features for this subset to be incorrect, leading to poor performance and models that are &quot;all messed up.&quot;" target="1. Relevant meetings for computational speech processing include Interspeech Computeational Seminar in Speech Research (ICSLP) and Eurospeech. These conferences are more targeted for &quot;dyed-in-the-wool&quot; speech professionals, as they focus specifically on speech processing topics.&#10;2. One might ignore these meetings due to being too busy or not finding the time to attend. Additionally, if a professional has a clear understanding of how to implement specific measures without needing a net, they might choose to not use the net and instead focus on their existing knowledge. This decision could lead to better results, as long as the data being used is correct and well-normalized.&#10;&#10;Relevant sources from the transcript:&#10;- Speaker: PhD E, Content: for sort of dyed - in - the wool speech people , um I think that ICSLP and Eurospeech are much more targeted .&#10;- Speaker: Professor C, Content: So . I mean , I mostly just ignored it because I was too busy...">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better results with triangular filters in RASTA: The speakers mention that some people have claimed better performance using triangular filters in RASTA instead of trapezoidal ones. This could be due to the fact that typical mel cepstral filter banks use triangular filters, which might lead to better results when applied to this problem.&#10;2. Training data issues: The speakers discuss the possibility of training data being &quot;messed up,&quot; causing undesirable behavior in the system. This could mean that there are errors or inconsistencies in the data used for training the models, leading to poor performance. One possible issue mentioned is a difference in normalization, which might not account for all discrepancies but could be contributing to the problem.&#10;3. Compression issues: Another possibility raised by PhD E is the presence of certain types of compression or other unnoticed issues within a subset of the training data. This would cause the computed features for this subset to be incorrect, leading to poor performance and models that are &quot;all messed up.&quot;" target="The suggested approach that is reputed to be better than neural nets or Gaussian mixtures in terms of requiring fewer parameters and performing well even with a modest amount of data is not explicitly mentioned in the provided transcript. However, the speakers discuss an idea where they simplify the model and directly feed the log magnitude spectrum into it, without applying any clever techniques. This could potentially be a more parameter-efficient approach, but it's not specified if this method has any particular name or reputation for better performance with less data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better results with triangular filters in RASTA: The speakers mention that some people have claimed better performance using triangular filters in RASTA instead of trapezoidal ones. This could be due to the fact that typical mel cepstral filter banks use triangular filters, which might lead to better results when applied to this problem.&#10;2. Training data issues: The speakers discuss the possibility of training data being &quot;messed up,&quot; causing undesirable behavior in the system. This could mean that there are errors or inconsistencies in the data used for training the models, leading to poor performance. One possible issue mentioned is a difference in normalization, which might not account for all discrepancies but could be contributing to the problem.&#10;3. Compression issues: Another possibility raised by PhD E is the presence of certain types of compression or other unnoticed issues within a subset of the training data. This would cause the computed features for this subset to be incorrect, leading to poor performance and models that are &quot;all messed up.&quot;" target="1. The speakers mention changing the filter bank from a trapezoidal integration to a triangular one because some people have reported better performance using triangular filters in RASTA. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;&#10;2. The fundamental difference in using a cubic root for smoothing is contrasted with the log used in another case. This is an advantage for the PLP (Perceptual Linear Prediction) method, as it provides smoothing at the end through an auto-process.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better results with triangular filters in RASTA: The speakers mention that some people have claimed better performance using triangular filters in RASTA instead of trapezoidal ones. This could be due to the fact that typical mel cepstral filter banks use triangular filters, which might lead to better results when applied to this problem.&#10;2. Training data issues: The speakers discuss the possibility of training data being &quot;messed up,&quot; causing undesirable behavior in the system. This could mean that there are errors or inconsistencies in the data used for training the models, leading to poor performance. One possible issue mentioned is a difference in normalization, which might not account for all discrepancies but could be contributing to the problem.&#10;3. Compression issues: Another possibility raised by PhD E is the presence of certain types of compression or other unnoticed issues within a subset of the training data. This would cause the computed features for this subset to be incorrect, leading to poor performance and models that are &quot;all messed up.&quot;" target="1. The method for adjusting the bandwidth of the first filters involves copying the configuration of the second filters over to the first filters. This is done in the RASTA (Relative Spectral) processing technique, where some people have reported better performance using triangular filters instead of trapezoidal ones. Typical mel cepstral filter banks use triangular filters, which might be why they work better in this context.&#10;&#10;2. Specifying a different number of filters causes the last ones to be ignored because the system assumes that the same number of filters should be applied across all frequency bins. If a different number is specified, the excess filters will not have any corresponding input and thus will be ignored. This behavior allows for controlling the bandwidth of the filters in the early stages of processing.">
      <data key="d0">1</data>
    </edge>
    <edge source=" definitely change the {disfmarker} the filter bank from being uh a uh trapezoidal integration to a {disfmarker} a {disfmarker} a triangular one ,&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: which is what the typical mel {disfmarker} mel cepstral uh filter bank does .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And some people have claimed that they got some better performance doing that , so you certainly could do that easily . But the fundamental difference , I mean , there 's other small differences {disfmarker}&#10;Speaker: PhD E&#10;Content: There 's a cubic root that happens , right ?&#10;Speaker: Professor C&#10;Content: Yeah , but , you know , as opposed to the log in the other case . I mean {vocalsound} the fundamental d d difference that we 've seen any kind of difference from before , which is actually an advantage for the P L P i uh , I think , is that the {disfmarker} the smoothing at the end is auto -" target="The particular measure chosen by the speaker to integrate out the effects of pitch is the variance of the difference between the log FFT power spectrum and the filter bank. The choice of this statistic is not explicitly stated in the transcript, but it is mentioned that it was chosen because it might be a useful measure to compare the two.&#10;&#10;The reason why the variance of this difference stood out as a potential useful statistic is not explicitly explained in the transcript either, but it can be inferred that it has something to do with the fact that it allows for a comparison between the original signal (or FFT) and the filter bank. The speakers mention that using the variance, they were able to observe certain differences between the two, which is why it was considered as a potential useful statistic.&#10;&#10;Additionally, it's mentioned that there might be other measures or energy measures that could be used instead of the variance, but the speakers do not go into detail about what those measures might be. They also mention that in the past, people have used zero crossing counts for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source=" definitely change the {disfmarker} the filter bank from being uh a uh trapezoidal integration to a {disfmarker} a {disfmarker} a triangular one ,&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: which is what the typical mel {disfmarker} mel cepstral uh filter bank does .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And some people have claimed that they got some better performance doing that , so you certainly could do that easily . But the fundamental difference , I mean , there 's other small differences {disfmarker}&#10;Speaker: PhD E&#10;Content: There 's a cubic root that happens , right ?&#10;Speaker: Professor C&#10;Content: Yeah , but , you know , as opposed to the log in the other case . I mean {vocalsound} the fundamental d d difference that we 've seen any kind of difference from before , which is actually an advantage for the P L P i uh , I think , is that the {disfmarker} the smoothing at the end is auto -" target="1. The speakers mention changing the filter bank from a trapezoidal integration to a triangular one because some people have reported better performance using triangular filters in RASTA. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;&#10;2. The fundamental difference in using a cubic root for smoothing is contrasted with the log used in another case. This is an advantage for the PLP (Perceptual Linear Prediction) method, as it provides smoothing at the end through an auto-process.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker}&#10;Speaker: Professor C&#10;Content: But see {disfmarker} see my point ? If you had {disfmarker} {vocalsound} If you had ten filters , {vocalsound} then you would be throwing away a lot at the two ends .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And if you had {disfmarker} if you had fifty filters , you 'd be throwing away hardly anything .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Um , I don't remember there being an independent way of saying &quot; we 're just gonna make them from here to here &quot; .&#10;Speaker: PhD E&#10;Content: Use this analysis bandwidth or something .&#10;Speaker: Professor C&#10;Content: But I {disfmarker} I {disfmarker} I don't know , it 's actually been awhile since I 've looked at it .&#10;Speaker: PhD E&#10;Content: Yeah , I went through the Feacalc code and then looked at you know just calling the RASTA" target="Based on the transcript provided, it is not explicitly stated that Professor C confirmed she left the microphone on before leaving for another appointment. However, she did mention &quot;I left it on&quot; in her speech. It's unclear whether she was referring to the microphone or something else, but given the context of the conversation, it's possible that she was indeed confirming that the microphone was left on.&#10;&#10;As for whether this was agreeable to PhD E, there is no explicit response from him regarding this matter in the transcript. Therefore, it cannot be definitively determined if he agreed or disagreed with leaving the microphone on.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker}&#10;Speaker: Professor C&#10;Content: But see {disfmarker} see my point ? If you had {disfmarker} {vocalsound} If you had ten filters , {vocalsound} then you would be throwing away a lot at the two ends .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And if you had {disfmarker} if you had fifty filters , you 'd be throwing away hardly anything .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Um , I don't remember there being an independent way of saying &quot; we 're just gonna make them from here to here &quot; .&#10;Speaker: PhD E&#10;Content: Use this analysis bandwidth or something .&#10;Speaker: Professor C&#10;Content: But I {disfmarker} I {disfmarker} I don't know , it 's actually been awhile since I 've looked at it .&#10;Speaker: PhD E&#10;Content: Yeah , I went through the Feacalc code and then looked at you know just calling the RASTA" target="Professor C is trying to convey the idea that the number of filters used in an analysis can affect the resultant output. He suggests that using a larger number of filters would result in less information being thrown away, as compared to using fewer filters which would discard more data from the lower and higher frequency bins.&#10;&#10;Professor C also mentions that there may not be an independent way to specify a particular range for filter creation, but rather, it might be determined by the analysis bandwidth used in the integration over FFT bins.&#10;&#10;PhD E responds by referring to the Feacalc code and checking for potential bugs or inconsistencies in feature computation. They also mention the need to ensure that all sampling rates are as expected.&#10;&#10;However, there is no explicit mention of using analysis bandwidth in PhD E's response.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker}&#10;Speaker: Professor C&#10;Content: But see {disfmarker} see my point ? If you had {disfmarker} {vocalsound} If you had ten filters , {vocalsound} then you would be throwing away a lot at the two ends .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And if you had {disfmarker} if you had fifty filters , you 'd be throwing away hardly anything .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Um , I don't remember there being an independent way of saying &quot; we 're just gonna make them from here to here &quot; .&#10;Speaker: PhD E&#10;Content: Use this analysis bandwidth or something .&#10;Speaker: Professor C&#10;Content: But I {disfmarker} I {disfmarker} I don't know , it 's actually been awhile since I 've looked at it .&#10;Speaker: PhD E&#10;Content: Yeah , I went through the Feacalc code and then looked at you know just calling the RASTA" target="Professor C is trying to convey that the number of filters used in an analysis can affect the resultant output. She suggests that using a larger number of filters would result in less information being discarded, as compared to using fewer filters which would throw away more data from the lower and higher frequency bins. This is because when there are fewer filters, more data at the extremes of the frequency range will be lost.&#10;&#10;PhD E mentions the use of analysis bandwidth or checking for potential bugs or inconsistencies in feature computation in the context of the Feacalc code because they want to ensure that all sampling rates are as expected and that there are no errors in the calculation of features. This is an important step to take before making any conclusions about the data analysis. However, PhD E does not explicitly mention using analysis bandwidth in their response, so it is unclear if they are directly addressing Professor C's point about the range of filter creation.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I agree , but I thought that the normalization difference was one of the possibilities ,&#10;Speaker: PhD E&#10;Content: and {disfmarker} Yeah , but I don't {disfmarker} I 'm not {disfmarker}&#10;Speaker: Professor C&#10;Content: right ?&#10;Speaker: PhD E&#10;Content: Yeah , I guess I don't think that the normalization difference is gonna account for everything .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: So what I was working on is um just going through and checking the headers of the wavefiles , to see if maybe there was a um {disfmarker} a certain type of compression or something that was done that my script wasn't catching . So that for some subset of the training data , uh the {disfmarker} the {disfmarker} the features I was computing were junk .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: Which would you know cause it to perform OK , but uh , you know , the {disfmarker} the models would be all messed up . So I was going through and just" target="The support vectors, or the critical examples, are saved during the process of recreating the boundary for the test set in a Support Vector Machine (SVM) because they provide several benefits compared to just saving the boundary itself. The support vectors represent the data points that lie closest to the decision boundary, and they effectively define the boundary. By storing these critical examples, SVMs can:&#10;&#10;1. Handle new classes: If new classes appear in the test set that were not present during training, the current boundary might not be useful. However, having the support vectors allows the algorithm to adapt and create a new boundary that better suits the updated data distribution.&#10;2. Adapt to changes in the data distribution: In real-world scenarios, data distributions can change over time. By saving the support vectors, SVMs can efficiently update the decision boundary when needed, without requiring access to the entire original dataset.&#10;3. Reduce computational complexity: Classifying new examples involves calculating their distance to the decision boundary. By using support vectors instead of the whole boundary, the computational cost is significantly reduced because there are far fewer support vectors than potential points on the boundary.&#10;4. Improve interpretability: Keeping the support vectors can provide more insights into the dataset and the classification problem, making it easier for analysts to understand why certain decisions are made.&#10;&#10;In summary, saving support vectors instead of just the decision boundary allows SVMs to be more flexible, adaptable, efficient, and interpretable when dealing with new or changing data distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker: PhD E&#10;Content: So what 's the advantage of support vector machines ? What {disfmarker}&#10;Speaker: Grad A&#10;Content: Um . So , support vector machines are {disfmarker} are good with dealing with a less amount of data&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: and um so if you {disfmarker} if you give it less data it still does a reasonable job {vocalsound} in learning the {disfmarker} the patterns .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Um and {vocalsound} um&#10;Speaker: Professor C&#10;Content: I guess it {disfmarker} yeah , they 're sort of succinct , and {disfmarker} and they {vocalsound} uh&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Does there some kind of a distance metric that they use or how do they {d" target="1. Using a &quot;net&quot; or machine learning model: The speakers suggest using a neural network or &quot;net&quot; to learn and implement measures over time. They propose feeding the log magnitude spectrum into the neural net, as it is good at figuring out what it most needs from various inputs.&#10;2. When not to use the net: It might be more beneficial to not use the net when you have exactly the right thing or a clear understanding of how to implement the measures without relying on the net. This would prevent asking the net to learn something that is already understood, which could lead to better results. However, this approach assumes that the data being used is correct and well-normalized, as any issues in the training data can still cause problems in the models.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker: PhD E&#10;Content: So what 's the advantage of support vector machines ? What {disfmarker}&#10;Speaker: Grad A&#10;Content: Um . So , support vector machines are {disfmarker} are good with dealing with a less amount of data&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: and um so if you {disfmarker} if you give it less data it still does a reasonable job {vocalsound} in learning the {disfmarker} the patterns .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Um and {vocalsound} um&#10;Speaker: Professor C&#10;Content: I guess it {disfmarker} yeah , they 're sort of succinct , and {disfmarker} and they {vocalsound} uh&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Does there some kind of a distance metric that they use or how do they {d" target="The suggested approach that is reputed to be better than neural nets or Gaussian mixtures in terms of requiring fewer parameters and performing well even with a modest amount of data is not explicitly mentioned in the provided transcript. However, the speakers discuss an idea where they simplify the model and directly feed the log magnitude spectrum into it, without applying any clever techniques. This could potentially be a more parameter-efficient approach, but it's not specified if this method has any particular name or reputation for better performance with less data.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker: PhD E&#10;Content: So what 's the advantage of support vector machines ? What {disfmarker}&#10;Speaker: Grad A&#10;Content: Um . So , support vector machines are {disfmarker} are good with dealing with a less amount of data&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: and um so if you {disfmarker} if you give it less data it still does a reasonable job {vocalsound} in learning the {disfmarker} the patterns .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Um and {vocalsound} um&#10;Speaker: Professor C&#10;Content: I guess it {disfmarker} yeah , they 're sort of succinct , and {disfmarker} and they {vocalsound} uh&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Does there some kind of a distance metric that they use or how do they {d" target="1. The concept of condensed nearest neighbor (CNN) refers to a modification of the traditional nearest neighbor classification algorithm. In the standard nearest neighbor method, you compare a query point to all training samples to find the closest one(s). This can be computationally expensive and time-consuming, especially with large datasets.&#10;2. To address this issue, CNN was proposed to reduce the number of comparisons needed. The idea is to select a subset of representative examples from the original dataset that can accurately classify any new input. By comparing the query point only to these condensed representatives instead of the entire dataset, you significantly decrease computation time without compromising classification accuracy.&#10;3. CNN can be related to support vector machines (SVM) and neural nets approaches in terms of dimensionality reduction and model simplicity. SVMs aim to find a hyperplane that maximally separates classes with a margin, focusing on critical support vectors instead of all training samples. Similarly, condensed nearest neighbor selects key examples, which can be seen as a form of data compression or dimensionality reduction.&#10;4. Neural nets and Gaussian mixtures, on the other hand, typically require more parameters and data to achieve good performance compared to SVMs and CNN. However, they can model complex decision boundaries and are better suited for multi-class problems and cases where relationships between features are not easily captured by a linear boundary or distance metric.&#10;5. In summary, condensed nearest neighbor is a more efficient variant of traditional nearest neighbor classification that reduces computation time by selecting representative examples from the original dataset. This approach shares similarities with support vector machines, focusing on key data points, while differing from neural nets and Gaussian mixtures, which often require more parameters and data to perform well.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do a similar thing with {disfmarker} {vocalsound} with support vector machines and see if {disfmarker}&#10;Speaker: PhD E&#10;Content: So what 's the advantage of support vector machines ? What {disfmarker}&#10;Speaker: Grad A&#10;Content: Um . So , support vector machines are {disfmarker} are good with dealing with a less amount of data&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: and um so if you {disfmarker} if you give it less data it still does a reasonable job {vocalsound} in learning the {disfmarker} the patterns .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Um and {vocalsound} um&#10;Speaker: Professor C&#10;Content: I guess it {disfmarker} yeah , they 're sort of succinct , and {disfmarker} and they {vocalsound} uh&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Does there some kind of a distance metric that they use or how do they {d" target="Support Vector Machines (SVM) are a type of classification algorithm that use a feature space to find the optimal separating plane between two different classes. The goal is to pick those examples of features that are closest to the separating boundary, which are referred to as support vectors. These critical examples effectively define the boundary, providing several benefits compared to just saving the boundary itself:&#10;&#10;1. Handling new classes: By storing support vectors, SVMs can adapt and create a new boundary when new classes appear in the test set that were not present during training.&#10;2. Adapting to changes in data distribution: Support vectors allow SVMs to efficiently update the decision boundary when data distributions change over time without requiring access to the entire original dataset.&#10;3. Reducing computational complexity: Classifying new examples involves calculating their distance to the decision boundary. Using support vectors significantly reduces computation cost since there are far fewer support vectors than potential points on the boundary.&#10;4. Improving interpretability: Keeping the support vectors provides insights into the dataset and classification problem, making it easier for analysts to understand decision-making processes.&#10;&#10;In summary, SVMs use a feature space and pick examples closest to the separating boundary (support vectors) because this approach offers flexibility, adaptability, efficiency, and interpretability when dealing with new or changing data distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using a &quot;net&quot; or machine learning model: The speakers suggest using a neural network or &quot;net&quot; to learn and implement measures over time. They propose feeding the log magnitude spectrum into the neural net, as it is good at figuring out what it most needs from various inputs.&#10;2. When not to use the net: It might be more beneficial to not use the net when you have exactly the right thing or a clear understanding of how to implement the measures without relying on the net. This would prevent asking the net to learn something that is already understood, which could lead to better results. However, this approach assumes that the data being used is correct and well-normalized, as any issues in the training data can still cause problems in the models." target="aker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Then you have this over time , and {disfmarker} and both kinds of measures and uh you might get uh something better .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Um .&#10;Speaker: PhD E&#10;Content: So {disfmarker} so don't uh {disfmarker} don't do the division , but let the net have everything .&#10;Speaker: Professor C&#10;Content: That 's another thing you could do yeah . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Um . I mean , it seems to me , if you have exactly the right thing then it 's better to do it without the net because otherwise you 're asking the net to learn this {disfmarker} you know , say if you wanted to learn how to do multiplication .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: I mean you could feed it a bunch of s you could feed two numbers that you">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using a &quot;net&quot; or machine learning model: The speakers suggest using a neural network or &quot;net&quot; to learn and implement measures over time. They propose feeding the log magnitude spectrum into the neural net, as it is good at figuring out what it most needs from various inputs.&#10;2. When not to use the net: It might be more beneficial to not use the net when you have exactly the right thing or a clear understanding of how to implement the measures without relying on the net. This would prevent asking the net to learn something that is already understood, which could lead to better results. However, this approach assumes that the data being used is correct and well-normalized, as any issues in the training data can still cause problems in the models." target=" that {disfmarker} that one could use , but I think that , for me , the thing that {disfmarker} that struck me was that uh you wanna get something back here , so here 's {disfmarker} here 's an idea . uh What about it you skip all the {disfmarker} all the really clever things , and just fed the log magnitude spectrum into this ?&#10;Speaker: PhD D&#10;Content: Ah {disfmarker} I 'm sorry .&#10;Speaker: Professor C&#10;Content: This is f You have the log magnitude spectrum , and you were looking at that and the difference between the filter bank and {disfmarker} and c c computing the variance .&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's a clever thing to do .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: What if you stopped being clever ? And you just took this thing in here because it 's a neural net and neural nets are wonderful&#10;Speaker: PhD D&#10;Content: Mm - hmm .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using a &quot;net&quot; or machine learning model: The speakers suggest using a neural network or &quot;net&quot; to learn and implement measures over time. They propose feeding the log magnitude spectrum into the neural net, as it is good at figuring out what it most needs from various inputs.&#10;2. When not to use the net: It might be more beneficial to not use the net when you have exactly the right thing or a clear understanding of how to implement the measures without relying on the net. This would prevent asking the net to learn something that is already understood, which could lead to better results. However, this approach assumes that the data being used is correct and well-normalized, as any issues in the training data can still cause problems in the models." target=" took this thing in here because it 's a neural net and neural nets are wonderful&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: and figure out what they can {disfmarker} what they most need from things , and I mean that 's what they 're good at .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So I mean you 're {disfmarker} you 're {disfmarker} you 're trying to be clever and say what 's the statistic that should {disfmarker} we should get about this difference but uh in fact , you know maybe just feeding this in or {disfmarker} or feeding both of them in&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: you know , another way , saying let it figure out what 's the {disfmarker} what is the interaction , especially if you do this over multiple frames ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Then you have this over time , and {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using a &quot;net&quot; or machine learning model: The speakers suggest using a neural network or &quot;net&quot; to learn and implement measures over time. They propose feeding the log magnitude spectrum into the neural net, as it is good at figuring out what it most needs from various inputs.&#10;2. When not to use the net: It might be more beneficial to not use the net when you have exactly the right thing or a clear understanding of how to implement the measures without relying on the net. This would prevent asking the net to learn something that is already understood, which could lead to better results. However, this approach assumes that the data being used is correct and well-normalized, as any issues in the training data can still cause problems in the models." target="1. Relevant meetings for computational speech processing include Interspeech Computeational Seminar in Speech Research (ICSLP) and Eurospeech. These conferences are more targeted for &quot;dyed-in-the-wool&quot; speech professionals, as they focus specifically on speech processing topics.&#10;2. One might ignore these meetings due to being too busy or not finding the time to attend. Additionally, if a professional has a clear understanding of how to implement specific measures without needing a net, they might choose to not use the net and instead focus on their existing knowledge. This decision could lead to better results, as long as the data being used is correct and well-normalized.&#10;&#10;Relevant sources from the transcript:&#10;- Speaker: PhD E, Content: for sort of dyed - in - the wool speech people , um I think that ICSLP and Eurospeech are much more targeted .&#10;- Speaker: Professor C, Content: So . I mean , I mostly just ignored it because I was too busy...">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using a &quot;net&quot; or machine learning model: The speakers suggest using a neural network or &quot;net&quot; to learn and implement measures over time. They propose feeding the log magnitude spectrum into the neural net, as it is good at figuring out what it most needs from various inputs.&#10;2. When not to use the net: It might be more beneficial to not use the net when you have exactly the right thing or a clear understanding of how to implement the measures without relying on the net. This would prevent asking the net to learn something that is already understood, which could lead to better results. However, this approach assumes that the data being used is correct and well-normalized, as any issues in the training data can still cause problems in the models." target="The suggested approach that is reputed to be better than neural nets or Gaussian mixtures in terms of requiring fewer parameters and performing well even with a modest amount of data is not explicitly mentioned in the provided transcript. However, the speakers discuss an idea where they simplify the model and directly feed the log magnitude spectrum into it, without applying any clever techniques. This could potentially be a more parameter-efficient approach, but it's not specified if this method has any particular name or reputation for better performance with less data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using a &quot;net&quot; or machine learning model: The speakers suggest using a neural network or &quot;net&quot; to learn and implement measures over time. They propose feeding the log magnitude spectrum into the neural net, as it is good at figuring out what it most needs from various inputs.&#10;2. When not to use the net: It might be more beneficial to not use the net when you have exactly the right thing or a clear understanding of how to implement the measures without relying on the net. This would prevent asking the net to learn something that is already understood, which could lead to better results. However, this approach assumes that the data being used is correct and well-normalized, as any issues in the training data can still cause problems in the models." target="1. The concept of condensed nearest neighbor (CNN) refers to a modification of the traditional nearest neighbor classification algorithm. In the standard nearest neighbor method, you compare a query point to all training samples to find the closest one(s). This can be computationally expensive and time-consuming, especially with large datasets.&#10;2. To address this issue, CNN was proposed to reduce the number of comparisons needed. The idea is to select a subset of representative examples from the original dataset that can accurately classify any new input. By comparing the query point only to these condensed representatives instead of the entire dataset, you significantly decrease computation time without compromising classification accuracy.&#10;3. CNN can be related to support vector machines (SVM) and neural nets approaches in terms of dimensionality reduction and model simplicity. SVMs aim to find a hyperplane that maximally separates classes with a margin, focusing on critical support vectors instead of all training samples. Similarly, condensed nearest neighbor selects key examples, which can be seen as a form of data compression or dimensionality reduction.&#10;4. Neural nets and Gaussian mixtures, on the other hand, typically require more parameters and data to achieve good performance compared to SVMs and CNN. However, they can model complex decision boundaries and are better suited for multi-class problems and cases where relationships between features are not easily captured by a linear boundary or distance metric.&#10;5. In summary, condensed nearest neighbor is a more efficient variant of traditional nearest neighbor classification that reduces computation time by selecting representative examples from the original dataset. This approach shares similarities with support vector machines, focusing on key data points, while differing from neural nets and Gaussian mixtures, which often require more parameters and data to perform well.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that {disfmarker} that one could use , but I think that , for me , the thing that {disfmarker} that struck me was that uh you wanna get something back here , so here 's {disfmarker} here 's an idea . uh What about it you skip all the {disfmarker} all the really clever things , and just fed the log magnitude spectrum into this ?&#10;Speaker: PhD D&#10;Content: Ah {disfmarker} I 'm sorry .&#10;Speaker: Professor C&#10;Content: This is f You have the log magnitude spectrum , and you were looking at that and the difference between the filter bank and {disfmarker} and c c computing the variance .&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's a clever thing to do .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: What if you stopped being clever ? And you just took this thing in here because it 's a neural net and neural nets are wonderful&#10;Speaker: PhD D&#10;Content: Mm - hmm ." target="The particular measure chosen by the speaker to integrate out the effects of pitch is the variance of the difference between the log FFT power spectrum and the filter bank. The choice of this statistic is not explicitly stated in the transcript, but it is mentioned that it was chosen because it might be a useful measure to compare the two.&#10;&#10;The reason why the variance of this difference stood out as a potential useful statistic is not explicitly explained in the transcript either, but it can be inferred that it has something to do with the fact that it allows for a comparison between the original signal (or FFT) and the filter bank. The speakers mention that using the variance, they were able to observe certain differences between the two, which is why it was considered as a potential useful statistic.&#10;&#10;Additionally, it's mentioned that there might be other measures or energy measures that could be used instead of the variance, but the speakers do not go into detail about what those measures might be. They also mention that in the past, people have used zero crossing counts for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that {disfmarker} that one could use , but I think that , for me , the thing that {disfmarker} that struck me was that uh you wanna get something back here , so here 's {disfmarker} here 's an idea . uh What about it you skip all the {disfmarker} all the really clever things , and just fed the log magnitude spectrum into this ?&#10;Speaker: PhD D&#10;Content: Ah {disfmarker} I 'm sorry .&#10;Speaker: Professor C&#10;Content: This is f You have the log magnitude spectrum , and you were looking at that and the difference between the filter bank and {disfmarker} and c c computing the variance .&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's a clever thing to do .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: What if you stopped being clever ? And you just took this thing in here because it 's a neural net and neural nets are wonderful&#10;Speaker: PhD D&#10;Content: Mm - hmm ." target="The suggested approach that is reputed to be better than neural nets or Gaussian mixtures in terms of requiring fewer parameters and performing well even with a modest amount of data is not explicitly mentioned in the provided transcript. However, the speakers discuss an idea where they simplify the model and directly feed the log magnitude spectrum into it, without applying any clever techniques. This could potentially be a more parameter-efficient approach, but it's not specified if this method has any particular name or reputation for better performance with less data.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that {disfmarker} that one could use , but I think that , for me , the thing that {disfmarker} that struck me was that uh you wanna get something back here , so here 's {disfmarker} here 's an idea . uh What about it you skip all the {disfmarker} all the really clever things , and just fed the log magnitude spectrum into this ?&#10;Speaker: PhD D&#10;Content: Ah {disfmarker} I 'm sorry .&#10;Speaker: Professor C&#10;Content: This is f You have the log magnitude spectrum , and you were looking at that and the difference between the filter bank and {disfmarker} and c c computing the variance .&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's a clever thing to do .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: What if you stopped being clever ? And you just took this thing in here because it 's a neural net and neural nets are wonderful&#10;Speaker: PhD D&#10;Content: Mm - hmm ." target="1. The speakers mention changing the filter bank from a trapezoidal integration to a triangular one because some people have reported better performance using triangular filters in RASTA. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;&#10;2. The fundamental difference in using a cubic root for smoothing is contrasted with the log used in another case. This is an advantage for the PLP (Perceptual Linear Prediction) method, as it provides smoothing at the end through an auto-process.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that {disfmarker} that one could use , but I think that , for me , the thing that {disfmarker} that struck me was that uh you wanna get something back here , so here 's {disfmarker} here 's an idea . uh What about it you skip all the {disfmarker} all the really clever things , and just fed the log magnitude spectrum into this ?&#10;Speaker: PhD D&#10;Content: Ah {disfmarker} I 'm sorry .&#10;Speaker: Professor C&#10;Content: This is f You have the log magnitude spectrum , and you were looking at that and the difference between the filter bank and {disfmarker} and c c computing the variance .&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's a clever thing to do .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: What if you stopped being clever ? And you just took this thing in here because it 's a neural net and neural nets are wonderful&#10;Speaker: PhD D&#10;Content: Mm - hmm ." target="Professor C is trying to convey the idea that the number of filters used in an analysis can affect the resultant output. He suggests that using a larger number of filters would result in less information being thrown away, as compared to using fewer filters which would discard more data from the lower and higher frequency bins.&#10;&#10;Professor C also mentions that there may not be an independent way to specify a particular range for filter creation, but rather, it might be determined by the analysis bandwidth used in the integration over FFT bins.&#10;&#10;PhD E responds by referring to the Feacalc code and checking for potential bugs or inconsistencies in feature computation. They also mention the need to ensure that all sampling rates are as expected.&#10;&#10;However, there is no explicit mention of using analysis bandwidth in PhD E's response.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that {disfmarker} that one could use , but I think that , for me , the thing that {disfmarker} that struck me was that uh you wanna get something back here , so here 's {disfmarker} here 's an idea . uh What about it you skip all the {disfmarker} all the really clever things , and just fed the log magnitude spectrum into this ?&#10;Speaker: PhD D&#10;Content: Ah {disfmarker} I 'm sorry .&#10;Speaker: Professor C&#10;Content: This is f You have the log magnitude spectrum , and you were looking at that and the difference between the filter bank and {disfmarker} and c c computing the variance .&#10;Speaker: PhD D&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's a clever thing to do .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: What if you stopped being clever ? And you just took this thing in here because it 's a neural net and neural nets are wonderful&#10;Speaker: PhD D&#10;Content: Mm - hmm ." target="Professor C is trying to convey that the number of filters used in an analysis can affect the resultant output. She suggests that using a larger number of filters would result in less information being discarded, as compared to using fewer filters which would throw away more data from the lower and higher frequency bins. This is because when there are fewer filters, more data at the extremes of the frequency range will be lost.&#10;&#10;PhD E mentions the use of analysis bandwidth or checking for potential bugs or inconsistencies in feature computation in the context of the Feacalc code because they want to ensure that all sampling rates are as expected and that there are no errors in the calculation of features. This is an important step to take before making any conclusions about the data analysis. However, PhD E does not explicitly mention using analysis bandwidth in their response, so it is unclear if they are directly addressing Professor C's point about the range of filter creation.">
      <data key="d0">1</data>
    </edge>
    <edge source="The particular measure chosen by the speaker to integrate out the effects of pitch is the variance of the difference between the log FFT power spectrum and the filter bank. The choice of this statistic is not explicitly stated in the transcript, but it is mentioned that it was chosen because it might be a useful measure to compare the two.&#10;&#10;The reason why the variance of this difference stood out as a potential useful statistic is not explicitly explained in the transcript either, but it can be inferred that it has something to do with the fact that it allows for a comparison between the original signal (or FFT) and the filter bank. The speakers mention that using the variance, they were able to observe certain differences between the two, which is why it was considered as a potential useful statistic.&#10;&#10;Additionally, it's mentioned that there might be other measures or energy measures that could be used instead of the variance, but the speakers do not go into detail about what those measures might be. They also mention that in the past, people have used zero crossing counts for this purpose." target=" you feed it something different . And something different in some fundamental way . And so the kind of thing that {disfmarker} that she was talking about before , was looking at something uh ab um {disfmarker} something uh about the difference between the {disfmarker} the uh um log FFT uh log power uh and the log magnitude uh F F - spectrum uh and the um uh filter bank .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: And so the filter bank is chosen in fact to sort of integrate out the effects of pitch and she 's saying you know trying {disfmarker} So the particular measure that she chose was the variance of this m of this difference , but that might not be the right number .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Maybe .&#10;Speaker: Professor C&#10;Content: Right ? I mean maybe there 's something about the variance that 's {disfmarker} that 's not enough or maybe there 's something else that {disfmarker} that one could use , but I think that , for me , the thing that {disfmarker} that struck me">
      <data key="d0">1</data>
    </edge>
    <edge source="The particular measure chosen by the speaker to integrate out the effects of pitch is the variance of the difference between the log FFT power spectrum and the filter bank. The choice of this statistic is not explicitly stated in the transcript, but it is mentioned that it was chosen because it might be a useful measure to compare the two.&#10;&#10;The reason why the variance of this difference stood out as a potential useful statistic is not explicitly explained in the transcript either, but it can be inferred that it has something to do with the fact that it allows for a comparison between the original signal (or FFT) and the filter bank. The speakers mention that using the variance, they were able to observe certain differences between the two, which is why it was considered as a potential useful statistic.&#10;&#10;Additionally, it's mentioned that there might be other measures or energy measures that could be used instead of the variance, but the speakers do not go into detail about what those measures might be. They also mention that in the past, people have used zero crossing counts for this purpose." target=" .&#10;Speaker: Professor C&#10;Content: or i i you know you 'd have some other energy measure and like in the old days people did like uh zero crossing counts .&#10;Speaker: PhD D&#10;Content: Yeah , yeah .&#10;Speaker: Professor C&#10;Content: Right . S S&#10;Speaker: PhD D&#10;Content: Well , I can also th use this .&#10;Speaker: Professor C&#10;Content: Yeah . Um ,&#10;Speaker: PhD D&#10;Content: Bec - because the result are a little bit better but we have in a point that everything is more or less the similar {disfmarker} more or less similar .&#10;Speaker: Professor C&#10;Content: Yeah . But um&#10;Speaker: PhD D&#10;Content: It 's not quite better .&#10;Speaker: Professor C&#10;Content: Right , but it seemed to me that what you were what you were getting at before was that there is something about the difference between the original signal or the original FFT and with the filter which is what {disfmarker} and the variance was one take uh on it .&#10;Speaker: PhD D&#10;Content: Yeah , I used this too .&#10;Speaker: Professor C&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The particular measure chosen by the speaker to integrate out the effects of pitch is the variance of the difference between the log FFT power spectrum and the filter bank. The choice of this statistic is not explicitly stated in the transcript, but it is mentioned that it was chosen because it might be a useful measure to compare the two.&#10;&#10;The reason why the variance of this difference stood out as a potential useful statistic is not explicitly explained in the transcript either, but it can be inferred that it has something to do with the fact that it allows for a comparison between the original signal (or FFT) and the filter bank. The speakers mention that using the variance, they were able to observe certain differences between the two, which is why it was considered as a potential useful statistic.&#10;&#10;Additionally, it's mentioned that there might be other measures or energy measures that could be used instead of the variance, but the speakers do not go into detail about what those measures might be. They also mention that in the past, people have used zero crossing counts for this purpose." target=" {disfmarker}&#10;Speaker: PhD E&#10;Content: At least I couldn't see any kind of control for that .&#10;Speaker: Professor C&#10;Content: Yeah , I don't think it 's in there , I think it 's in the uh uh uh the filters . So , the F F T is on everything , but the filters um , for instance , ignore the {disfmarker} the lowest bins and the highest bins . And what it does is it {disfmarker} it copies&#10;Speaker: PhD E&#10;Content: The {disfmarker} the filters ? Which filters ?&#10;Speaker: Professor C&#10;Content: um The filter bank which is created by integrating over F F T bins .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: um&#10;Speaker: PhD E&#10;Content: When you get the mel {disfmarker} When you go to the mel scale .&#10;Speaker: Professor C&#10;Content: Right . Yeah , it 's bark scale , and it 's {disfmarker} it {disfmarker} it um {disfmarker} it actually copies">
      <data key="d0">1</data>
    </edge>
    <edge source="The particular measure chosen by the speaker to integrate out the effects of pitch is the variance of the difference between the log FFT power spectrum and the filter bank. The choice of this statistic is not explicitly stated in the transcript, but it is mentioned that it was chosen because it might be a useful measure to compare the two.&#10;&#10;The reason why the variance of this difference stood out as a potential useful statistic is not explicitly explained in the transcript either, but it can be inferred that it has something to do with the fact that it allows for a comparison between the original signal (or FFT) and the filter bank. The speakers mention that using the variance, they were able to observe certain differences between the two, which is why it was considered as a potential useful statistic.&#10;&#10;Additionally, it's mentioned that there might be other measures or energy measures that could be used instead of the variance, but the speakers do not go into detail about what those measures might be. They also mention that in the past, people have used zero crossing counts for this purpose." target="1. The speakers mention changing the filter bank from a trapezoidal integration to a triangular one because some people have reported better performance using triangular filters in RASTA. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;&#10;2. The fundamental difference in using a cubic root for smoothing is contrasted with the log used in another case. This is an advantage for the PLP (Perceptual Linear Prediction) method, as it provides smoothing at the end through an auto-process.">
      <data key="d0">1</data>
    </edge>
    <edge source="The particular measure chosen by the speaker to integrate out the effects of pitch is the variance of the difference between the log FFT power spectrum and the filter bank. The choice of this statistic is not explicitly stated in the transcript, but it is mentioned that it was chosen because it might be a useful measure to compare the two.&#10;&#10;The reason why the variance of this difference stood out as a potential useful statistic is not explicitly explained in the transcript either, but it can be inferred that it has something to do with the fact that it allows for a comparison between the original signal (or FFT) and the filter bank. The speakers mention that using the variance, they were able to observe certain differences between the two, which is why it was considered as a potential useful statistic.&#10;&#10;Additionally, it's mentioned that there might be other measures or energy measures that could be used instead of the variance, but the speakers do not go into detail about what those measures might be. They also mention that in the past, people have used zero crossing counts for this purpose." target="1. The filters being used in the process of integrating over FFT bins to create a filter bank are referred to as the mel cepstral filter bank, which typically uses triangular filters. These filters ignore the lowest and highest bins by copying the configuration of the second filters over to the first filters, as done in the RASTA (Relative Spectral) processing technique. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;2. The mel or bark scale refers to a psychoacoustic scale of pitches, where the width of each filter is determined by the analysis bandwidth used in the integration over FFT bins. It's mentioned that there may not be an independent way to specify a particular range for filter creation. Instead, the filter bank is created by integrating over FFT bins, and the filters are defined by the mel or bark scale.">
      <data key="d0">1</data>
    </edge>
    <edge source=" .&#10;Speaker: Professor C&#10;Content: or i i you know you 'd have some other energy measure and like in the old days people did like uh zero crossing counts .&#10;Speaker: PhD D&#10;Content: Yeah , yeah .&#10;Speaker: Professor C&#10;Content: Right . S S&#10;Speaker: PhD D&#10;Content: Well , I can also th use this .&#10;Speaker: Professor C&#10;Content: Yeah . Um ,&#10;Speaker: PhD D&#10;Content: Bec - because the result are a little bit better but we have in a point that everything is more or less the similar {disfmarker} more or less similar .&#10;Speaker: Professor C&#10;Content: Yeah . But um&#10;Speaker: PhD D&#10;Content: It 's not quite better .&#10;Speaker: Professor C&#10;Content: Right , but it seemed to me that what you were what you were getting at before was that there is something about the difference between the original signal or the original FFT and with the filter which is what {disfmarker} and the variance was one take uh on it .&#10;Speaker: PhD D&#10;Content: Yeah , I used this too .&#10;Speaker: Professor C&#10;" target="Professor C is trying to convey the idea that the number of filters used in an analysis can affect the resultant output. He suggests that using a larger number of filters would result in less information being thrown away, as compared to using fewer filters which would discard more data from the lower and higher frequency bins.&#10;&#10;Professor C also mentions that there may not be an independent way to specify a particular range for filter creation, but rather, it might be determined by the analysis bandwidth used in the integration over FFT bins.&#10;&#10;PhD E responds by referring to the Feacalc code and checking for potential bugs or inconsistencies in feature computation. They also mention the need to ensure that all sampling rates are as expected.&#10;&#10;However, there is no explicit mention of using analysis bandwidth in PhD E's response.">
      <data key="d0">1</data>
    </edge>
    <edge source=" .&#10;Speaker: Professor C&#10;Content: or i i you know you 'd have some other energy measure and like in the old days people did like uh zero crossing counts .&#10;Speaker: PhD D&#10;Content: Yeah , yeah .&#10;Speaker: Professor C&#10;Content: Right . S S&#10;Speaker: PhD D&#10;Content: Well , I can also th use this .&#10;Speaker: Professor C&#10;Content: Yeah . Um ,&#10;Speaker: PhD D&#10;Content: Bec - because the result are a little bit better but we have in a point that everything is more or less the similar {disfmarker} more or less similar .&#10;Speaker: Professor C&#10;Content: Yeah . But um&#10;Speaker: PhD D&#10;Content: It 's not quite better .&#10;Speaker: Professor C&#10;Content: Right , but it seemed to me that what you were what you were getting at before was that there is something about the difference between the original signal or the original FFT and with the filter which is what {disfmarker} and the variance was one take uh on it .&#10;Speaker: PhD D&#10;Content: Yeah , I used this too .&#10;Speaker: Professor C&#10;" target="Professor C is trying to convey that the number of filters used in an analysis can affect the resultant output. She suggests that using a larger number of filters would result in less information being discarded, as compared to using fewer filters which would throw away more data from the lower and higher frequency bins. This is because when there are fewer filters, more data at the extremes of the frequency range will be lost.&#10;&#10;PhD E mentions the use of analysis bandwidth or checking for potential bugs or inconsistencies in feature computation in the context of the Feacalc code because they want to ensure that all sampling rates are as expected and that there are no errors in the calculation of features. This is an important step to take before making any conclusions about the data analysis. However, PhD E does not explicitly mention using analysis bandwidth in their response, so it is unclear if they are directly addressing Professor C's point about the range of filter creation.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker}&#10;Speaker: PhD E&#10;Content: At least I couldn't see any kind of control for that .&#10;Speaker: Professor C&#10;Content: Yeah , I don't think it 's in there , I think it 's in the uh uh uh the filters . So , the F F T is on everything , but the filters um , for instance , ignore the {disfmarker} the lowest bins and the highest bins . And what it does is it {disfmarker} it copies&#10;Speaker: PhD E&#10;Content: The {disfmarker} the filters ? Which filters ?&#10;Speaker: Professor C&#10;Content: um The filter bank which is created by integrating over F F T bins .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: um&#10;Speaker: PhD E&#10;Content: When you get the mel {disfmarker} When you go to the mel scale .&#10;Speaker: Professor C&#10;Content: Right . Yeah , it 's bark scale , and it 's {disfmarker} it {disfmarker} it um {disfmarker} it actually copies" target="1. The speakers mention changing the filter bank from a trapezoidal integration to a triangular one because some people have reported better performance using triangular filters in RASTA. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;&#10;2. The fundamental difference in using a cubic root for smoothing is contrasted with the log used in another case. This is an advantage for the PLP (Perceptual Linear Prediction) method, as it provides smoothing at the end through an auto-process.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker}&#10;Speaker: PhD E&#10;Content: At least I couldn't see any kind of control for that .&#10;Speaker: Professor C&#10;Content: Yeah , I don't think it 's in there , I think it 's in the uh uh uh the filters . So , the F F T is on everything , but the filters um , for instance , ignore the {disfmarker} the lowest bins and the highest bins . And what it does is it {disfmarker} it copies&#10;Speaker: PhD E&#10;Content: The {disfmarker} the filters ? Which filters ?&#10;Speaker: Professor C&#10;Content: um The filter bank which is created by integrating over F F T bins .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: um&#10;Speaker: PhD E&#10;Content: When you get the mel {disfmarker} When you go to the mel scale .&#10;Speaker: Professor C&#10;Content: Right . Yeah , it 's bark scale , and it 's {disfmarker} it {disfmarker} it um {disfmarker} it actually copies" target="1. The method for adjusting the bandwidth of the first filters involves copying the configuration of the second filters over to the first filters. This is done in the RASTA (Relative Spectral) processing technique, where some people have reported better performance using triangular filters instead of trapezoidal ones. Typical mel cepstral filter banks use triangular filters, which might be why they work better in this context.&#10;&#10;2. Specifying a different number of filters causes the last ones to be ignored because the system assumes that the same number of filters should be applied across all frequency bins. If a different number is specified, the excess filters will not have any corresponding input and thus will be ignored. This behavior allows for controlling the bandwidth of the filters in the early stages of processing.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker}&#10;Speaker: PhD E&#10;Content: At least I couldn't see any kind of control for that .&#10;Speaker: Professor C&#10;Content: Yeah , I don't think it 's in there , I think it 's in the uh uh uh the filters . So , the F F T is on everything , but the filters um , for instance , ignore the {disfmarker} the lowest bins and the highest bins . And what it does is it {disfmarker} it copies&#10;Speaker: PhD E&#10;Content: The {disfmarker} the filters ? Which filters ?&#10;Speaker: Professor C&#10;Content: um The filter bank which is created by integrating over F F T bins .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: um&#10;Speaker: PhD E&#10;Content: When you get the mel {disfmarker} When you go to the mel scale .&#10;Speaker: Professor C&#10;Content: Right . Yeah , it 's bark scale , and it 's {disfmarker} it {disfmarker} it um {disfmarker} it actually copies" target="Professor C is trying to convey the idea that the number of filters used in an analysis can affect the resultant output. He suggests that using a larger number of filters would result in less information being thrown away, as compared to using fewer filters which would discard more data from the lower and higher frequency bins.&#10;&#10;Professor C also mentions that there may not be an independent way to specify a particular range for filter creation, but rather, it might be determined by the analysis bandwidth used in the integration over FFT bins.&#10;&#10;PhD E responds by referring to the Feacalc code and checking for potential bugs or inconsistencies in feature computation. They also mention the need to ensure that all sampling rates are as expected.&#10;&#10;However, there is no explicit mention of using analysis bandwidth in PhD E's response.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker}&#10;Speaker: PhD E&#10;Content: At least I couldn't see any kind of control for that .&#10;Speaker: Professor C&#10;Content: Yeah , I don't think it 's in there , I think it 's in the uh uh uh the filters . So , the F F T is on everything , but the filters um , for instance , ignore the {disfmarker} the lowest bins and the highest bins . And what it does is it {disfmarker} it copies&#10;Speaker: PhD E&#10;Content: The {disfmarker} the filters ? Which filters ?&#10;Speaker: Professor C&#10;Content: um The filter bank which is created by integrating over F F T bins .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: um&#10;Speaker: PhD E&#10;Content: When you get the mel {disfmarker} When you go to the mel scale .&#10;Speaker: Professor C&#10;Content: Right . Yeah , it 's bark scale , and it 's {disfmarker} it {disfmarker} it um {disfmarker} it actually copies" target="1. The filters being used in the process of integrating over FFT bins to create a filter bank are referred to as the mel cepstral filter bank, which typically uses triangular filters. These filters ignore the lowest and highest bins by copying the configuration of the second filters over to the first filters, as done in the RASTA (Relative Spectral) processing technique. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;2. The mel or bark scale refers to a psychoacoustic scale of pitches, where the width of each filter is determined by the analysis bandwidth used in the integration over FFT bins. It's mentioned that there may not be an independent way to specify a particular range for filter creation. Instead, the filter bank is created by integrating over FFT bins, and the filters are defined by the mel or bark scale.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker}&#10;Speaker: PhD E&#10;Content: At least I couldn't see any kind of control for that .&#10;Speaker: Professor C&#10;Content: Yeah , I don't think it 's in there , I think it 's in the uh uh uh the filters . So , the F F T is on everything , but the filters um , for instance , ignore the {disfmarker} the lowest bins and the highest bins . And what it does is it {disfmarker} it copies&#10;Speaker: PhD E&#10;Content: The {disfmarker} the filters ? Which filters ?&#10;Speaker: Professor C&#10;Content: um The filter bank which is created by integrating over F F T bins .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: um&#10;Speaker: PhD E&#10;Content: When you get the mel {disfmarker} When you go to the mel scale .&#10;Speaker: Professor C&#10;Content: Right . Yeah , it 's bark scale , and it 's {disfmarker} it {disfmarker} it um {disfmarker} it actually copies" target="Professor C is trying to convey that the number of filters used in an analysis can affect the resultant output. She suggests that using a larger number of filters would result in less information being discarded, as compared to using fewer filters which would throw away more data from the lower and higher frequency bins. This is because when there are fewer filters, more data at the extremes of the frequency range will be lost.&#10;&#10;PhD E mentions the use of analysis bandwidth or checking for potential bugs or inconsistencies in feature computation in the context of the Feacalc code because they want to ensure that all sampling rates are as expected and that there are no errors in the calculation of features. This is an important step to take before making any conclusions about the data analysis. However, PhD E does not explicitly mention using analysis bandwidth in their response, so it is unclear if they are directly addressing Professor C's point about the range of filter creation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals." target="&#10;Speaker: Grad A&#10;Content: and um {vocalsound} {vocalsound} cuz the uh {disfmarker} So King and {disfmarker} and Taylor {vocalsound} um did this with uh recurrent neural nets ,&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: and this i their {disfmarker} their idea was to first find {vocalsound} a mapping from MFCC 's to {vocalsound} uh phonological features&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: and then later on , once you have these {vocalsound} phonological features , {vocalsound} then uh map that to phones .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: So I 'm {disfmarker} I 'm sort of reproducing phase one of their stuff .&#10;Speaker: PhD E&#10;Content: Mmm . So they had one recurrent net for each particular feature ?&#10;Speaker: Grad A&#10;Content: Right . Right . Right . Right .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals." target=" Grad B&#10;Content: Oh . OK .&#10;Speaker: Grad A&#10;Content: So uh for example , {vocalsound} this {disfmarker} this uh feature set called the uh sound patterns of English {vocalsound} um is just a bunch of {vocalsound} um {vocalsound} binary valued features . Let 's say , is this voicing , or is this not voicing , is this {vocalsound} sonorants , not sonorants , and {vocalsound} stuff like that .&#10;Speaker: Grad B&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: So .&#10;Speaker: PhD E&#10;Content: Did you find any more mistakes in their tables ?&#10;Speaker: Grad A&#10;Content: Oh ! Uh I haven't gone through the entire table , {pause} yet . Yeah , yesterday I brought Chuck {vocalsound} the table and I was like , &quot; wait , this {disfmarker} is {disfmarker} Is the mapping from N to {disfmarker} to this phonological feature called um &quot; coronal &quot; , is {disfmarker} is {disfmarker} should it be">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals." target="1. Relevant meetings for computational speech processing include Interspeech Computeational Seminar in Speech Research (ICSLP) and Eurospeech. These conferences are more targeted for &quot;dyed-in-the-wool&quot; speech professionals, as they focus specifically on speech processing topics.&#10;2. One might ignore these meetings due to being too busy or not finding the time to attend. Additionally, if a professional has a clear understanding of how to implement specific measures without needing a net, they might choose to not use the net and instead focus on their existing knowledge. This decision could lead to better results, as long as the data being used is correct and well-normalized.&#10;&#10;Relevant sources from the transcript:&#10;- Speaker: PhD E, Content: for sort of dyed - in - the wool speech people , um I think that ICSLP and Eurospeech are much more targeted .&#10;- Speaker: Professor C, Content: So . I mean , I mostly just ignored it because I was too busy...">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals." target="1. The feature set called &quot;the sound patterns of English&quot; consists of binary-valued features that represent various phonological aspects of English sounds. Examples of these features include voicing (voiced or voiceless) and sonorance (sonorant or not). These features are used to describe phones in the English language.&#10;2. During the discussion, Grad A mentioned that they found potential mistakes in the tables associated with &quot;the sound patterns of English&quot; feature set, specifically related to the mapping from phones to a phonological feature called &quot;coronal.&quot; They had identified inconsistencies in how the feature was labeled and were planning to review the tables further to find more possible errors.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals." target="1. The method for adjusting the bandwidth of the first filters involves copying the configuration of the second filters over to the first filters. This is done in the RASTA (Relative Spectral) processing technique, where some people have reported better performance using triangular filters instead of trapezoidal ones. Typical mel cepstral filter banks use triangular filters, which might be why they work better in this context.&#10;&#10;2. Specifying a different number of filters causes the last ones to be ignored because the system assumes that the same number of filters should be applied across all frequency bins. If a different number is specified, the excess filters will not have any corresponding input and thus will be ignored. This behavior allows for controlling the bandwidth of the filters in the early stages of processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals." target="1. The concept of condensed nearest neighbor (CNN) refers to a modification of the traditional nearest neighbor classification algorithm. In the standard nearest neighbor method, you compare a query point to all training samples to find the closest one(s). This can be computationally expensive and time-consuming, especially with large datasets.&#10;2. To address this issue, CNN was proposed to reduce the number of comparisons needed. The idea is to select a subset of representative examples from the original dataset that can accurately classify any new input. By comparing the query point only to these condensed representatives instead of the entire dataset, you significantly decrease computation time without compromising classification accuracy.&#10;3. CNN can be related to support vector machines (SVM) and neural nets approaches in terms of dimensionality reduction and model simplicity. SVMs aim to find a hyperplane that maximally separates classes with a margin, focusing on critical support vectors instead of all training samples. Similarly, condensed nearest neighbor selects key examples, which can be seen as a form of data compression or dimensionality reduction.&#10;4. Neural nets and Gaussian mixtures, on the other hand, typically require more parameters and data to achieve good performance compared to SVMs and CNN. However, they can model complex decision boundaries and are better suited for multi-class problems and cases where relationships between features are not easily captured by a linear boundary or distance metric.&#10;5. In summary, condensed nearest neighbor is a more efficient variant of traditional nearest neighbor classification that reduces computation time by selecting representative examples from the original dataset. This approach shares similarities with support vector machines, focusing on key data points, while differing from neural nets and Gaussian mixtures, which often require more parameters and data to perform well.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals." target="1. The SRI system reads wavefiles directly and performs cepstral computation on the fly, but it doesn't store the cepstral files anywhere for comparison with PLP features. This is in contrast to the PLP system, which stores the cepstrum on disk and reads those in during feature extraction.&#10;&#10;2. The SRI system requires a front-end parameter file even after the cepstrum is computed because it's used to specify the kind of computation that the mel cepstrum uses. This may be related to the fact that some people have reported better performance using triangular filters in RASTA, instead of trapezoidal ones, which are typically used in mel cepstral filter banks.&#10;&#10;In summary, the main differences between the SRI and PLP systems lie in file storage (with the former reading wavefiles directly and computing cepstra on the fly and the latter storing computed cepstra on disk) and configuration parameters (where the SRI system requires a front-end parameter file even after the cepstrum is computed). The reason for this difference may be related to the type of filter bank used in each system.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals." target="The system learns a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals, to binary vectors representing the presence or absence of specific phonological features in corresponding phones. The output of the system is a vector of ones and zeros for each phone, where each element corresponds to whether the phone exhibits a particular phonological feature or not. This information can be further used to classify phones based on their audio signals.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals." target="1. The concept being discussed involves calculating a distance measure from MFCC features to phonological features, which is then translated into a binary value (0 or 1) for classification purposes. This approach is used for binary classification where each feature is evaluated as either present (1) or absent (0).&#10;2. In the context of speech recognition, this distance measure is calculated between the MFCC features derived from audio signals and the phonological features corresponding to phones. The distances are then translated into binary values representing whether specific phonological features are present in the given phone.&#10;3. This method utilizes distances by converting them into probabilities for phonological feature occurrences, which can be used for recognizing phones based on their audio signals. Mississippi State researchers have successfully applied this technique using support vector machines to estimate probabilities for speech recognition tasks. The accuracy of the phone recognition depends on the precise identification and marking of the phonological events within the audio signal, as mentioned by Professor C.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals." target="Based on the information provided in the transcript, the error rate reduction in speech recognition achieved by using a simulated impulse response and training on clean speech only was from approximately 80% to 4%. This conclusion is based on indirect information since the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated.&#10;&#10;The transcript mentions that when using a simulated impulse response, the error rate went from something like eighteen percent to four percent for vocalsound and from forty-one percent to eight percent for mike far digits on channel F. It can be inferred that these reductions in error rates were due to using a simulated impulse response and training on clean speech only.&#10;&#10;However, the transcript does not provide information about the error rate when not using artificial reverb or if the system was trained on noisy speech. The comparison is made between using artificial reverb and clean speech versus an unspecified setup that may or may not have used artificial reverb and could have been trained on noisy speech.&#10;&#10;Therefore, while there was a significant reduction in error rates for speech recognition when using a simulated impulse response and training on clean speech only, the exact difference in error rates between these setups cannot be determined from the information provided in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The system being discussed in the transcript is learning a mapping from MFCC (Mel-frequency cepstral coefficients) features to phonological features. More specifically, it aims to learn a mapping from a set of features extracted from audio signals to binary vectors that represent whether certain phonological features, such as voicing or sonorance, are present or not in the corresponding phones.&#10;2. This is being done using support vector machines (SVM) for classification, which is one method among others like recurrent neural nets used for phone recognition tasks. The end goal is to create a mapping from MFCC features to phonological features and then potentially use this information to classify phones based on their audio signals." target="Support Vector Machines (SVM) are a type of classification algorithm that use a feature space to find the optimal separating plane between two different classes. The goal is to pick those examples of features that are closest to the separating boundary, which are referred to as support vectors. These critical examples effectively define the boundary, providing several benefits compared to just saving the boundary itself:&#10;&#10;1. Handling new classes: By storing support vectors, SVMs can adapt and create a new boundary when new classes appear in the test set that were not present during training.&#10;2. Adapting to changes in data distribution: Support vectors allow SVMs to efficiently update the decision boundary when data distributions change over time without requiring access to the entire original dataset.&#10;3. Reducing computational complexity: Classifying new examples involves calculating their distance to the decision boundary. Using support vectors significantly reduces computation cost since there are far fewer support vectors than potential points on the boundary.&#10;4. Improving interpretability: Keeping the support vectors provides insights into the dataset and classification problem, making it easier for analysts to understand decision-making processes.&#10;&#10;In summary, SVMs use a feature space and pick examples closest to the separating boundary (support vectors) because this approach offers flexibility, adaptability, efficiency, and interpretability when dealing with new or changing data distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: Grad A&#10;Content: and um {vocalsound} {vocalsound} cuz the uh {disfmarker} So King and {disfmarker} and Taylor {vocalsound} um did this with uh recurrent neural nets ,&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: and this i their {disfmarker} their idea was to first find {vocalsound} a mapping from MFCC 's to {vocalsound} uh phonological features&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: and then later on , once you have these {vocalsound} phonological features , {vocalsound} then uh map that to phones .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: So I 'm {disfmarker} I 'm sort of reproducing phase one of their stuff .&#10;Speaker: PhD E&#10;Content: Mmm . So they had one recurrent net for each particular feature ?&#10;Speaker: Grad A&#10;Content: Right . Right . Right . Right .&#10;" target="The system learns a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals, to binary vectors representing the presence or absence of specific phonological features in corresponding phones. The output of the system is a vector of ones and zeros for each phone, where each element corresponds to whether the phone exhibits a particular phonological feature or not. This information can be further used to classify phones based on their audio signals.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Grad B&#10;Content: Oh . OK .&#10;Speaker: Grad A&#10;Content: So uh for example , {vocalsound} this {disfmarker} this uh feature set called the uh sound patterns of English {vocalsound} um is just a bunch of {vocalsound} um {vocalsound} binary valued features . Let 's say , is this voicing , or is this not voicing , is this {vocalsound} sonorants , not sonorants , and {vocalsound} stuff like that .&#10;Speaker: Grad B&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: So .&#10;Speaker: PhD E&#10;Content: Did you find any more mistakes in their tables ?&#10;Speaker: Grad A&#10;Content: Oh ! Uh I haven't gone through the entire table , {pause} yet . Yeah , yesterday I brought Chuck {vocalsound} the table and I was like , &quot; wait , this {disfmarker} is {disfmarker} Is the mapping from N to {disfmarker} to this phonological feature called um &quot; coronal &quot; , is {disfmarker} is {disfmarker} should it be" target="1. The feature set called &quot;the sound patterns of English&quot; consists of binary-valued features that represent various phonological aspects of English sounds. Examples of these features include voicing (voiced or voiceless) and sonorance (sonorant or not). These features are used to describe phones in the English language.&#10;2. During the discussion, Grad A mentioned that they found potential mistakes in the tables associated with &quot;the sound patterns of English&quot; feature set, specifically related to the mapping from phones to a phonological feature called &quot;coronal.&quot; They had identified inconsistencies in how the feature was labeled and were planning to review the tables further to find more possible errors.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Grad B&#10;Content: Oh . OK .&#10;Speaker: Grad A&#10;Content: So uh for example , {vocalsound} this {disfmarker} this uh feature set called the uh sound patterns of English {vocalsound} um is just a bunch of {vocalsound} um {vocalsound} binary valued features . Let 's say , is this voicing , or is this not voicing , is this {vocalsound} sonorants , not sonorants , and {vocalsound} stuff like that .&#10;Speaker: Grad B&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: So .&#10;Speaker: PhD E&#10;Content: Did you find any more mistakes in their tables ?&#10;Speaker: Grad A&#10;Content: Oh ! Uh I haven't gone through the entire table , {pause} yet . Yeah , yesterday I brought Chuck {vocalsound} the table and I was like , &quot; wait , this {disfmarker} is {disfmarker} Is the mapping from N to {disfmarker} to this phonological feature called um &quot; coronal &quot; , is {disfmarker} is {disfmarker} should it be" target="1. Grad A has made progress in securing committee members for their qualifications. So far, they have confirmed Morgan, Hynek, Mike Jordan, and John Ohala.&#10;2. The remaining committee member that Grad A needs to ask is Malek.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Relevant meetings for computational speech processing include Interspeech Computeational Seminar in Speech Research (ICSLP) and Eurospeech. These conferences are more targeted for &quot;dyed-in-the-wool&quot; speech professionals, as they focus specifically on speech processing topics.&#10;2. One might ignore these meetings due to being too busy or not finding the time to attend. Additionally, if a professional has a clear understanding of how to implement specific measures without needing a net, they might choose to not use the net and instead focus on their existing knowledge. This decision could lead to better results, as long as the data being used is correct and well-normalized.&#10;&#10;Relevant sources from the transcript:&#10;- Speaker: PhD E, Content: for sort of dyed - in - the wool speech people , um I think that ICSLP and Eurospeech are much more targeted .&#10;- Speaker: Professor C, Content: So . I mean , I mostly just ignored it because I was too busy..." target="disfmarker} it 's if you haven't been there much , it 's good to go to , uh to get a feel for things , a range of things , not just speech . Uh . But I think for {disfmarker} for sort of dyed - in - the - wool speech people , um I think that ICSLP and Eurospeech are much more targeted .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Uh . And then there 's these other meetings , like HLT and {disfmarker} and uh ASRU {disfmarker}&#10;Speaker: PhD E&#10;Content: &#10;Speaker: Professor C&#10;Content: so there 's {disfmarker} there 's actually plenty of meetings that are really relevant to {disfmarker} to uh computational uh speech processing of one sort or another .&#10;Speaker: PhD E&#10;Content: Mm - hmm . &#10;Speaker: Professor C&#10;Content: Um . So . I mean , I mostly just ignored it because I was too busy and {vocalsound} didn't get to it . So uh Wanna talk a">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Relevant meetings for computational speech processing include Interspeech Computeational Seminar in Speech Research (ICSLP) and Eurospeech. These conferences are more targeted for &quot;dyed-in-the-wool&quot; speech professionals, as they focus specifically on speech processing topics.&#10;2. One might ignore these meetings due to being too busy or not finding the time to attend. Additionally, if a professional has a clear understanding of how to implement specific measures without needing a net, they might choose to not use the net and instead focus on their existing knowledge. This decision could lead to better results, as long as the data being used is correct and well-normalized.&#10;&#10;Relevant sources from the transcript:&#10;- Speaker: PhD E, Content: for sort of dyed - in - the wool speech people , um I think that ICSLP and Eurospeech are much more targeted .&#10;- Speaker: Professor C, Content: So . I mean , I mostly just ignored it because I was too busy..." target="The suggested approach that is reputed to be better than neural nets or Gaussian mixtures in terms of requiring fewer parameters and performing well even with a modest amount of data is not explicitly mentioned in the provided transcript. However, the speakers discuss an idea where they simplify the model and directly feed the log magnitude spectrum into it, without applying any clever techniques. This could potentially be a more parameter-efficient approach, but it's not specified if this method has any particular name or reputation for better performance with less data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The feature set called &quot;the sound patterns of English&quot; consists of binary-valued features that represent various phonological aspects of English sounds. Examples of these features include voicing (voiced or voiceless) and sonorance (sonorant or not). These features are used to describe phones in the English language.&#10;2. During the discussion, Grad A mentioned that they found potential mistakes in the tables associated with &quot;the sound patterns of English&quot; feature set, specifically related to the mapping from phones to a phonological feature called &quot;coronal.&quot; They had identified inconsistencies in how the feature was labeled and were planning to review the tables further to find more possible errors." target="marker} to this phonological feature called um &quot; coronal &quot; , is {disfmarker} is {disfmarker} should it be {disfmarker} shouldn't it be a one ? or should it {disfmarker} should it be you know coronal instead of not coronal as it was labelled in the paper ? &quot; So I ha haven't hunted down all the {disfmarker} all the mistakes yet ,&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: Grad A&#10;Content: but {disfmarker}&#10;Speaker: Professor C&#10;Content: But a as I was saying , people do get probabilities from these things ,&#10;Speaker: Grad B&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: and {disfmarker} and uh we were just trying to remember how they do , but people have used it for speech recognition , and they have gotten probabilities . So they have some conversion from these distances to probabilities .&#10;Speaker: Grad B&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Right , yeah .&#10;Speaker: Professor C&#10;Content: There 's {disfmarker} you">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The feature set called &quot;the sound patterns of English&quot; consists of binary-valued features that represent various phonological aspects of English sounds. Examples of these features include voicing (voiced or voiceless) and sonorance (sonorant or not). These features are used to describe phones in the English language.&#10;2. During the discussion, Grad A mentioned that they found potential mistakes in the tables associated with &quot;the sound patterns of English&quot; feature set, specifically related to the mapping from phones to a phonological feature called &quot;coronal.&quot; They had identified inconsistencies in how the feature was labeled and were planning to review the tables further to find more possible errors." target="1. Grad A has made progress in securing committee members for their qualifications. So far, they have confirmed Morgan, Hynek, Mike Jordan, and John Ohala.&#10;2. The remaining committee member that Grad A needs to ask is Malek.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The feature set called &quot;the sound patterns of English&quot; consists of binary-valued features that represent various phonological aspects of English sounds. Examples of these features include voicing (voiced or voiceless) and sonorance (sonorant or not). These features are used to describe phones in the English language.&#10;2. During the discussion, Grad A mentioned that they found potential mistakes in the tables associated with &quot;the sound patterns of English&quot; feature set, specifically related to the mapping from phones to a phonological feature called &quot;coronal.&quot; They had identified inconsistencies in how the feature was labeled and were planning to review the tables further to find more possible errors." target="The system learns a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals, to binary vectors representing the presence or absence of specific phonological features in corresponding phones. The output of the system is a vector of ones and zeros for each phone, where each element corresponds to whether the phone exhibits a particular phonological feature or not. This information can be further used to classify phones based on their audio signals.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD E had been working with Jeremy on his project, but the specific details of their collaboration are not provided. Later in the conversation, PhD E says &quot;You had the same answer anyway&quot; when referring to a response given to Professor C's question, suggesting that they both arrived at the same answer or understanding. However, the transcript does not include the context of the question or the specific answer, so it is not possible to definitively determine what caused them to have the same answer. It can be inferred that their collaboration on Jeremy's project may have led to a shared understanding or perspective, but without more information, a precise explanation cannot be given." target=" Uh - huh .&#10;Speaker: PhD E&#10;Content: so i I {disfmarker} I don't know if that {disfmarker} it probably doesn't mess it up , it probably just ignores it if it determines that it 's already in the right format or something but {disfmarker} the {disfmarker} the {disfmarker} the two processes that happen are a little different .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: So .&#10;Speaker: Professor C&#10;Content: So anyway , there 's stuff there to sort out .&#10;Speaker: PhD E&#10;Content: Yeah . Yeah .&#10;Speaker: Professor C&#10;Content: So , OK . Let 's go back to what you thought I was asking you .&#10;Speaker: PhD E&#10;Content: Yeah no and I didn't have a chance to do that .&#10;Speaker: Professor C&#10;Content: Ha ! Oh ! You had the sa same answer anyway .&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . I 've been um , {disfmarker} I 've been working with um Jeremy on his project and then">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD E had been working with Jeremy on his project, but the specific details of their collaboration are not provided. Later in the conversation, PhD E says &quot;You had the same answer anyway&quot; when referring to a response given to Professor C's question, suggesting that they both arrived at the same answer or understanding. However, the transcript does not include the context of the question or the specific answer, so it is not possible to definitively determine what caused them to have the same answer. It can be inferred that their collaboration on Jeremy's project may have led to a shared understanding or perspective, but without more information, a precise explanation cannot be given." target="&#10;Content: Yeah . Yeah . I 've been um , {disfmarker} I 've been working with um Jeremy on his project and then I 've been trying to track down this bug in uh the ICSI front - end features .&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: So one thing that I did notice , yesterday I was studying the um {disfmarker} the uh RASTA code&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: and it looks like we don't have any way to um control the frequency range that we use in our analysis . We basically {disfmarker} it looks to me like we do the FFT , um and then we just take all the bins and we use everything . We don't have any set of parameters where we can say you know , &quot; only process from you know a hundred and ten hertz to thirty - seven - fifty &quot; .&#10;Speaker: Professor C&#10;Content: Um {disfmarker}&#10;Speaker: PhD E&#10;Content: At least I couldn't see any kind of control for that .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD E had been working with Jeremy on his project, but the specific details of their collaboration are not provided. Later in the conversation, PhD E says &quot;You had the same answer anyway&quot; when referring to a response given to Professor C's question, suggesting that they both arrived at the same answer or understanding. However, the transcript does not include the context of the question or the specific answer, so it is not possible to definitively determine what caused them to have the same answer. It can be inferred that their collaboration on Jeremy's project may have led to a shared understanding or perspective, but without more information, a precise explanation cannot be given." target=" to perform OK , but uh , you know , the {disfmarker} the models would be all messed up . So I was going through and just double - checking that kind of think first , to see if there was just some kind of obvious bug in the way that I was computing the features .&#10;Speaker: Professor C&#10;Content: Mm - hmm . I see . OK .&#10;Speaker: PhD E&#10;Content: Looking at all the sampling rates to make sure all the sampling rates were what {disfmarker} eight K , what I was assuming they were ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: um {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah , that makes sense , to check all that .&#10;Speaker: PhD E&#10;Content: Yeah . So I was doing that first , before I did these other things , just to make sure there wasn't something {disfmarker}&#10;Speaker: Professor C&#10;Content: Although really , uh uh , a couple three percent uh difference in word error rate uh {comment} could easily come from some difference in normalization , I would think . But&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD E had been working with Jeremy on his project, but the specific details of their collaboration are not provided. Later in the conversation, PhD E says &quot;You had the same answer anyway&quot; when referring to a response given to Professor C's question, suggesting that they both arrived at the same answer or understanding. However, the transcript does not include the context of the question or the specific answer, so it is not possible to definitively determine what caused them to have the same answer. It can be inferred that their collaboration on Jeremy's project may have led to a shared understanding or perspective, but without more information, a precise explanation cannot be given." target=" Mmm . So they had one recurrent net for each particular feature ?&#10;Speaker: Grad A&#10;Content: Right . Right . Right . Right .&#10;Speaker: PhD E&#10;Content: I see . I wo did they compare that {disfmarker} I mean , what if you just did phone recognition and did the reverse lookup .&#10;Speaker: Grad A&#10;Content: Uh .&#10;Speaker: PhD E&#10;Content: So you recognize a phone and which ever phone was recognized , you spit out it 's vector of ones and zeros .&#10;Speaker: Grad A&#10;Content: Mm - hmm . Uh .&#10;Speaker: Professor C&#10;Content: I expect you could do that .&#10;Speaker: PhD E&#10;Content: I mean uh {disfmarker}&#10;Speaker: Professor C&#10;Content: That 's probably not what he 's going to do on his class project . Yeah .&#10;Speaker: PhD E&#10;Content: Yeah . No .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So um have you had a chance to do this um thing we talked about yet with the uh {disfmarker} um&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD E had been working with Jeremy on his project, but the specific details of their collaboration are not provided. Later in the conversation, PhD E says &quot;You had the same answer anyway&quot; when referring to a response given to Professor C's question, suggesting that they both arrived at the same answer or understanding. However, the transcript does not include the context of the question or the specific answer, so it is not possible to definitively determine what caused them to have the same answer. It can be inferred that their collaboration on Jeremy's project may have led to a shared understanding or perspective, but without more information, a precise explanation cannot be given." target=": Yeah . I think , given at the level you 're doing things in floating point on the computer , I don't think it matters , would be my guess ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: but .&#10;Speaker: PhD D&#10;Content: I {disfmarker} this more or less anything&#10;Speaker: Professor C&#10;Content: Yeah . OK , and wh when did Stephane take off ? He took off {disfmarker}&#10;Speaker: PhD D&#10;Content: I think that Stephane will arrive today or tomorrow .&#10;Speaker: Professor C&#10;Content: Oh , he was gone these first few days , and then he 's here for a couple days before he goes to Salt Lake City .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: He 's {disfmarker} I think that he is in Las Vegas or something like that .&#10;Speaker: Professor C&#10;Content: Yeah . Yeah . So he 's {disfmarker} he 's going to ICAS">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD E had been working with Jeremy on his project, but the specific details of their collaboration are not provided. Later in the conversation, PhD E says &quot;You had the same answer anyway&quot; when referring to a response given to Professor C's question, suggesting that they both arrived at the same answer or understanding. However, the transcript does not include the context of the question or the specific answer, so it is not possible to definitively determine what caused them to have the same answer. It can be inferred that their collaboration on Jeremy's project may have led to a shared understanding or perspective, but without more information, a precise explanation cannot be given." target=" my microphone on ?&#10;Speaker: PhD E&#10;Content: Uh , yeah .&#10;Speaker: Professor C&#10;Content: Yeah . Thank you .&#10;Speaker: PhD E&#10;Content: Yep . Yeah . That 'll work .&#10;Speaker: Professor C&#10;Content:  I can be out of here quickly . {comment} {comment} {vocalsound} {vocalsound} That 's I just have to run for another appointment . OK ,  I t Yeah . I left it on . OK .">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD E had been working with Jeremy on his project, but the specific details of their collaboration are not provided. Later in the conversation, PhD E says &quot;You had the same answer anyway&quot; when referring to a response given to Professor C's question, suggesting that they both arrived at the same answer or understanding. However, the transcript does not include the context of the question or the specific answer, so it is not possible to definitively determine what caused them to have the same answer. It can be inferred that their collaboration on Jeremy's project may have led to a shared understanding or perspective, but without more information, a precise explanation cannot be given." target="Based on the transcript provided, it is not explicitly stated that Professor C confirmed she left the microphone on before leaving for another appointment. However, she did mention &quot;I left it on&quot; in her speech. It's unclear whether she was referring to the microphone or something else, but given the context of the conversation, it's possible that she was indeed confirming that the microphone was left on.&#10;&#10;As for whether this was agreeable to PhD E, there is no explicit response from him regarding this matter in the transcript. Therefore, it cannot be definitively determined if he agreed or disagreed with leaving the microphone on.">
      <data key="d0">1</data>
    </edge>
    <edge source=" to perform OK , but uh , you know , the {disfmarker} the models would be all messed up . So I was going through and just double - checking that kind of think first , to see if there was just some kind of obvious bug in the way that I was computing the features .&#10;Speaker: Professor C&#10;Content: Mm - hmm . I see . OK .&#10;Speaker: PhD E&#10;Content: Looking at all the sampling rates to make sure all the sampling rates were what {disfmarker} eight K , what I was assuming they were ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: um {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah , that makes sense , to check all that .&#10;Speaker: PhD E&#10;Content: Yeah . So I was doing that first , before I did these other things , just to make sure there wasn't something {disfmarker}&#10;Speaker: Professor C&#10;Content: Although really , uh uh , a couple three percent uh difference in word error rate uh {comment} could easily come from some difference in normalization , I would think . But&#10;Speaker:" target="The support vectors, or the critical examples, are saved during the process of recreating the boundary for the test set in a Support Vector Machine (SVM) because they provide several benefits compared to just saving the boundary itself. The support vectors represent the data points that lie closest to the decision boundary, and they effectively define the boundary. By storing these critical examples, SVMs can:&#10;&#10;1. Handle new classes: If new classes appear in the test set that were not present during training, the current boundary might not be useful. However, having the support vectors allows the algorithm to adapt and create a new boundary that better suits the updated data distribution.&#10;2. Adapt to changes in the data distribution: In real-world scenarios, data distributions can change over time. By saving the support vectors, SVMs can efficiently update the decision boundary when needed, without requiring access to the entire original dataset.&#10;3. Reduce computational complexity: Classifying new examples involves calculating their distance to the decision boundary. By using support vectors instead of the whole boundary, the computational cost is significantly reduced because there are far fewer support vectors than potential points on the boundary.&#10;4. Improve interpretability: Keeping the support vectors can provide more insights into the dataset and the classification problem, making it easier for analysts to understand why certain decisions are made.&#10;&#10;In summary, saving support vectors instead of just the decision boundary allows SVMs to be more flexible, adaptable, efficient, and interpretable when dealing with new or changing data distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source=" to perform OK , but uh , you know , the {disfmarker} the models would be all messed up . So I was going through and just double - checking that kind of think first , to see if there was just some kind of obvious bug in the way that I was computing the features .&#10;Speaker: Professor C&#10;Content: Mm - hmm . I see . OK .&#10;Speaker: PhD E&#10;Content: Looking at all the sampling rates to make sure all the sampling rates were what {disfmarker} eight K , what I was assuming they were ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: um {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah , that makes sense , to check all that .&#10;Speaker: PhD E&#10;Content: Yeah . So I was doing that first , before I did these other things , just to make sure there wasn't something {disfmarker}&#10;Speaker: Professor C&#10;Content: Although really , uh uh , a couple three percent uh difference in word error rate uh {comment} could easily come from some difference in normalization , I would think . But&#10;Speaker:" target="Professor C is trying to convey the idea that the number of filters used in an analysis can affect the resultant output. He suggests that using a larger number of filters would result in less information being thrown away, as compared to using fewer filters which would discard more data from the lower and higher frequency bins.&#10;&#10;Professor C also mentions that there may not be an independent way to specify a particular range for filter creation, but rather, it might be determined by the analysis bandwidth used in the integration over FFT bins.&#10;&#10;PhD E responds by referring to the Feacalc code and checking for potential bugs or inconsistencies in feature computation. They also mention the need to ensure that all sampling rates are as expected.&#10;&#10;However, there is no explicit mention of using analysis bandwidth in PhD E's response.">
      <data key="d0">1</data>
    </edge>
    <edge source=" to perform OK , but uh , you know , the {disfmarker} the models would be all messed up . So I was going through and just double - checking that kind of think first , to see if there was just some kind of obvious bug in the way that I was computing the features .&#10;Speaker: Professor C&#10;Content: Mm - hmm . I see . OK .&#10;Speaker: PhD E&#10;Content: Looking at all the sampling rates to make sure all the sampling rates were what {disfmarker} eight K , what I was assuming they were ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: um {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah , that makes sense , to check all that .&#10;Speaker: PhD E&#10;Content: Yeah . So I was doing that first , before I did these other things , just to make sure there wasn't something {disfmarker}&#10;Speaker: Professor C&#10;Content: Although really , uh uh , a couple three percent uh difference in word error rate uh {comment} could easily come from some difference in normalization , I would think . But&#10;Speaker:" target="In the context of this discussion, a reasonable word error rate on digit strings is around 1%, while a subpar performance would be an error rate of 4-5% or higher. This system, which wasn't trained on the task and used as a comparison, has a word error rate of about 1% on digits, which is considered reasonable. However, an error rate of 4-5% or more would be considered poor performance for this task.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Mmm . So they had one recurrent net for each particular feature ?&#10;Speaker: Grad A&#10;Content: Right . Right . Right . Right .&#10;Speaker: PhD E&#10;Content: I see . I wo did they compare that {disfmarker} I mean , what if you just did phone recognition and did the reverse lookup .&#10;Speaker: Grad A&#10;Content: Uh .&#10;Speaker: PhD E&#10;Content: So you recognize a phone and which ever phone was recognized , you spit out it 's vector of ones and zeros .&#10;Speaker: Grad A&#10;Content: Mm - hmm . Uh .&#10;Speaker: Professor C&#10;Content: I expect you could do that .&#10;Speaker: PhD E&#10;Content: I mean uh {disfmarker}&#10;Speaker: Professor C&#10;Content: That 's probably not what he 's going to do on his class project . Yeah .&#10;Speaker: PhD E&#10;Content: Yeah . No .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So um have you had a chance to do this um thing we talked about yet with the uh {disfmarker} um&#10;Spe" target="In the project, the speakers are using a vector of ones and zeros to represent each phone where each element in the vector corresponds to whether the phone exhibits a particular phonological feature or not. This vector is obtained by learning a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals. The output of the system is a vector of ones and zeros for each phone, where each element in the vector represents the presence or absence of a specific phonological feature.">
      <data key="d0">1</data>
    </edge>
    <edge source=" my microphone on ?&#10;Speaker: PhD E&#10;Content: Uh , yeah .&#10;Speaker: Professor C&#10;Content: Yeah . Thank you .&#10;Speaker: PhD E&#10;Content: Yep . Yeah . That 'll work .&#10;Speaker: Professor C&#10;Content:  I can be out of here quickly . {comment} {comment} {vocalsound} {vocalsound} That 's I just have to run for another appointment . OK ,  I t Yeah . I left it on . OK ." target="Based on the transcript provided, it is not explicitly stated that Professor C confirmed she left the microphone on before leaving for another appointment. However, she did mention &quot;I left it on&quot; in her speech. It's unclear whether she was referring to the microphone or something else, but given the context of the conversation, it's possible that she was indeed confirming that the microphone was left on.&#10;&#10;As for whether this was agreeable to PhD E, there is no explicit response from him regarding this matter in the transcript. Therefore, it cannot be definitively determined if he agreed or disagreed with leaving the microphone on.">
      <data key="d0">1</data>
    </edge>
    <edge source="The high error rate, such as forty-one, in the SRI system's identification of digits could be due to the training data. The SRI system was trained on &quot;this TV broadcast type stuff&quot; which has a much wider range of channels and noise compared to the clean TI-digits training data. This discrepancy between the training data and the test data (digits in this study) causes the system to perform poorly, resulting in a high error rate." target=": OK , so it 's then {disfmarker} then it 's {disfmarker} it 's {disfmarker} it 's reasonable to expect it would be helpful if we used it with the SRI system and&#10;Speaker: Professor C&#10;Content: Yeah , I mean , as helpful {disfmarker} I mean , so that 's the question . Yeah , w we 're often asked this when we work with a system that {disfmarker} that isn't {disfmarker} isn't sort of industry {disfmarker} industry standard great ,&#10;Speaker: Grad B&#10;Content: Uh - huh .&#10;Speaker: Professor C&#10;Content: uh and we see some reduction in error using some clever method , then , you know , will it work on a {disfmarker} {vocalsound} on a {disfmarker} on a good system . So uh you know , this other one 's {disfmarker} it was a pretty good system . I think , you know , one {disfmarker} one percent word error rate on digits is {disfmarker} uh digit strings is not {">
      <data key="d0">1</data>
    </edge>
    <edge source="The high error rate, such as forty-one, in the SRI system's identification of digits could be due to the training data. The SRI system was trained on &quot;this TV broadcast type stuff&quot; which has a much wider range of channels and noise compared to the clean TI-digits training data. This discrepancy between the training data and the test data (digits in this study) causes the system to perform poorly, resulting in a high error rate." target=" why were you getting forty - one here ? Is this {disfmarker}&#10;Speaker: Grad B&#10;Content: Um . I {disfmarker} I 'm g I 'm guessing it was the {disfmarker} the training data . Uh , clean TI - digits is , like , pretty pristine {vocalsound} training data , and if they trained {vocalsound} the SRI system on this TV broadcast type stuff , I think it 's a much wider range of channels and it {disfmarker}&#10;Speaker: Professor C&#10;Content: No , but wait a minute . I {disfmarker} I {disfmarker} I th {disfmarker} I think he {disfmarker} What am I saying here ? Yeah , so that was the SRI system . Maybe you 're right . Yeah . Cuz it was getting like one percent {disfmarker} {vocalsound} So it 's still this kind of ratio . It was {disfmarker} it was getting one percent or something on the near field . Wasn't it ?&#10;Speaker: PhD E&#10;Content: Mm - hmm , or">
      <data key="d0">1</data>
    </edge>
    <edge source="The high error rate, such as forty-one, in the SRI system's identification of digits could be due to the training data. The SRI system was trained on &quot;this TV broadcast type stuff&quot; which has a much wider range of channels and noise compared to the clean TI-digits training data. This discrepancy between the training data and the test data (digits in this study) causes the system to perform poorly, resulting in a high error rate." target=" use th use it for the SRI system .&#10;Speaker: Grad B&#10;Content: b You me you mean um ta&#10;Speaker: Professor C&#10;Content: So you 're {disfmarker} so you have a system which for one reason or another is relatively poor ,&#10;Speaker: Grad B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and {disfmarker} and uh you have something like forty - one percent error uh and then you transform it to eight by doing {disfmarker} doing this {disfmarker} this work . Um . So here 's this other system , which is a lot better , but there 's still this kind of ratio . It 's something like five percent error {vocalsound} with the {disfmarker} the distant mike , and one percent with the close mike .&#10;Speaker: Grad B&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: So the question is {vocalsound} how close to that one can you get {vocalsound} if you transform the data using that system .&#10;Speaker: Grad B&#10;Content: r Right , so {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="The high error rate, such as forty-one, in the SRI system's identification of digits could be due to the training data. The SRI system was trained on &quot;this TV broadcast type stuff&quot; which has a much wider range of channels and noise compared to the clean TI-digits training data. This discrepancy between the training data and the test data (digits in this study) causes the system to perform poorly, resulting in a high error rate." target=" C&#10;Content: So um have you had a chance to do this um thing we talked about yet with the uh {disfmarker} um&#10;Speaker: PhD E&#10;Content: Insertion penalty ?&#10;Speaker: Professor C&#10;Content: Uh . No actually I was going a different {disfmarker} That 's a good question , too , but I was gonna ask about the {disfmarker} {vocalsound} the um {vocalsound} changes to the data in comparing PLP and mel cepstrum for the SRI system .&#10;Speaker: PhD E&#10;Content: Uh . Well what I 've been {disfmarker} &quot; Changes to the data &quot; , I 'm not sure I {disfmarker}&#10;Speaker: Professor C&#10;Content: Right . So we talked on the phone about this , that {disfmarker} that there was still a difference of a {disfmarker} of a few percent&#10;Speaker: PhD E&#10;Content: Yeah . Right .&#10;Speaker: Professor C&#10;Content: and {vocalsound} you told me that there was a difference in how the normalization was done . And I was asking">
      <data key="d0">1</data>
    </edge>
    <edge source="The high error rate, such as forty-one, in the SRI system's identification of digits could be due to the training data. The SRI system was trained on &quot;this TV broadcast type stuff&quot; which has a much wider range of channels and noise compared to the clean TI-digits training data. This discrepancy between the training data and the test data (digits in this study) causes the system to perform poorly, resulting in a high error rate." target=" was getting one percent or something on the near field . Wasn't it ?&#10;Speaker: PhD E&#10;Content: Mm - hmm , or it wa a it was around one .&#10;Speaker: Professor C&#10;Content: Yeah . Yeah . I think it was getting around one percent for the near {disfmarker} for the n for the close mike .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Grad B&#10;Content: Huh ? OK .&#10;Speaker: Professor C&#10;Content: So it was like one to five {disfmarker} So it 's still this kind of ratio . It 's just {disfmarker} yeah , it 's a lot more training data . So So probably it should be something we should try then is to {disfmarker} is to see if {disfmarker} is {vocalsound} at some point just to take {disfmarker} i to transform the data and then {disfmarker} {vocalsound} and then uh use th use it for the SRI system .&#10;Speaker: Grad B&#10;Content: b You me you mean um ta&#10;Speaker: Professor C">
      <data key="d0">1</data>
    </edge>
    <edge source="The high error rate, such as forty-one, in the SRI system's identification of digits could be due to the training data. The SRI system was trained on &quot;this TV broadcast type stuff&quot; which has a much wider range of channels and noise compared to the clean TI-digits training data. This discrepancy between the training data and the test data (digits in this study) causes the system to perform poorly, resulting in a high error rate." target="vocalsound} if you transform the data using that system .&#10;Speaker: Grad B&#10;Content: r Right , so {disfmarker} so I guess this SRI system is trained on a lot of s Broadcast News or Switchboard data . Is that right ?&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad B&#10;Content: Do you know which one it is ?&#10;Speaker: PhD E&#10;Content: It 's trained on a lot of different things . Um . It 's trained on uh a lot of Switchboard , Call Home ,&#10;Speaker: Grad B&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: um a bunch of different sources , some digits , there 's some digits training in there .&#10;Speaker: Grad B&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Hmm .&#10;Speaker: Grad B&#10;Content: O one thing I 'm wondering about is what this mean subtraction method {vocalsound} um will do if it 's faced with additive noise . Cuz I {disfmarker} I {disfmarker} it 's cuz I don't know what log magnitude spectral subt">
      <data key="d0">1</data>
    </edge>
    <edge source="The high error rate, such as forty-one, in the SRI system's identification of digits could be due to the training data. The SRI system was trained on &quot;this TV broadcast type stuff&quot; which has a much wider range of channels and noise compared to the clean TI-digits training data. This discrepancy between the training data and the test data (digits in this study) causes the system to perform poorly, resulting in a high error rate." target="In the context of this discussion, a reasonable word error rate on digit strings is around 1%, while a subpar performance would be an error rate of 4-5% or higher. This system, which wasn't trained on the task and used as a comparison, has a word error rate of about 1% on digits, which is considered reasonable. However, an error rate of 4-5% or more would be considered poor performance for this task.">
      <data key="d0">1</data>
    </edge>
    <edge source="The high error rate, such as forty-one, in the SRI system's identification of digits could be due to the training data. The SRI system was trained on &quot;this TV broadcast type stuff&quot; which has a much wider range of channels and noise compared to the clean TI-digits training data. This discrepancy between the training data and the test data (digits in this study) causes the system to perform poorly, resulting in a high error rate." target="Based on the information provided in the transcript, the error rate reduction in speech recognition achieved by using a simulated impulse response and training on clean speech only was from approximately 80% to 4%. This conclusion is based on indirect information since the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated.&#10;&#10;The transcript mentions that when using a simulated impulse response, the error rate went from something like eighteen percent to four percent for vocalsound and from forty-one percent to eight percent for mike far digits on channel F. It can be inferred that these reductions in error rates were due to using a simulated impulse response and training on clean speech only.&#10;&#10;However, the transcript does not provide information about the error rate when not using artificial reverb or if the system was trained on noisy speech. The comparison is made between using artificial reverb and clean speech versus an unspecified setup that may or may not have used artificial reverb and could have been trained on noisy speech.&#10;&#10;Therefore, while there was a significant reduction in error rates for speech recognition when using a simulated impulse response and training on clean speech only, the exact difference in error rates between these setups cannot be determined from the information provided in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source=": OK , so it 's then {disfmarker} then it 's {disfmarker} it 's {disfmarker} it 's reasonable to expect it would be helpful if we used it with the SRI system and&#10;Speaker: Professor C&#10;Content: Yeah , I mean , as helpful {disfmarker} I mean , so that 's the question . Yeah , w we 're often asked this when we work with a system that {disfmarker} that isn't {disfmarker} isn't sort of industry {disfmarker} industry standard great ,&#10;Speaker: Grad B&#10;Content: Uh - huh .&#10;Speaker: Professor C&#10;Content: uh and we see some reduction in error using some clever method , then , you know , will it work on a {disfmarker} {vocalsound} on a {disfmarker} on a good system . So uh you know , this other one 's {disfmarker} it was a pretty good system . I think , you know , one {disfmarker} one percent word error rate on digits is {disfmarker} uh digit strings is not {" target="In the context of this discussion, a reasonable word error rate on digit strings is around 1%, while a subpar performance would be an error rate of 4-5% or higher. This system, which wasn't trained on the task and used as a comparison, has a word error rate of about 1% on digits, which is considered reasonable. However, an error rate of 4-5% or more would be considered poor performance for this task.">
      <data key="d0">1</data>
    </edge>
    <edge source="The suggested approach that is reputed to be better than neural nets or Gaussian mixtures in terms of requiring fewer parameters and performing well even with a modest amount of data is not explicitly mentioned in the provided transcript. However, the speakers discuss an idea where they simplify the model and directly feed the log magnitude spectrum into it, without applying any clever techniques. This could potentially be a more parameter-efficient approach, but it's not specified if this method has any particular name or reputation for better performance with less data." target="1. The speakers mention changing the filter bank from a trapezoidal integration to a triangular one because some people have reported better performance using triangular filters in RASTA. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;&#10;2. The fundamental difference in using a cubic root for smoothing is contrasted with the log used in another case. This is an advantage for the PLP (Perceptual Linear Prediction) method, as it provides smoothing at the end through an auto-process.">
      <data key="d0">1</data>
    </edge>
    <edge source="The suggested approach that is reputed to be better than neural nets or Gaussian mixtures in terms of requiring fewer parameters and performing well even with a modest amount of data is not explicitly mentioned in the provided transcript. However, the speakers discuss an idea where they simplify the model and directly feed the log magnitude spectrum into it, without applying any clever techniques. This could potentially be a more parameter-efficient approach, but it's not specified if this method has any particular name or reputation for better performance with less data." target="1. The method for adjusting the bandwidth of the first filters involves copying the configuration of the second filters over to the first filters. This is done in the RASTA (Relative Spectral) processing technique, where some people have reported better performance using triangular filters instead of trapezoidal ones. Typical mel cepstral filter banks use triangular filters, which might be why they work better in this context.&#10;&#10;2. Specifying a different number of filters causes the last ones to be ignored because the system assumes that the same number of filters should be applied across all frequency bins. If a different number is specified, the excess filters will not have any corresponding input and thus will be ignored. This behavior allows for controlling the bandwidth of the filters in the early stages of processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="The suggested approach that is reputed to be better than neural nets or Gaussian mixtures in terms of requiring fewer parameters and performing well even with a modest amount of data is not explicitly mentioned in the provided transcript. However, the speakers discuss an idea where they simplify the model and directly feed the log magnitude spectrum into it, without applying any clever techniques. This could potentially be a more parameter-efficient approach, but it's not specified if this method has any particular name or reputation for better performance with less data." target="1. The concept of condensed nearest neighbor (CNN) refers to a modification of the traditional nearest neighbor classification algorithm. In the standard nearest neighbor method, you compare a query point to all training samples to find the closest one(s). This can be computationally expensive and time-consuming, especially with large datasets.&#10;2. To address this issue, CNN was proposed to reduce the number of comparisons needed. The idea is to select a subset of representative examples from the original dataset that can accurately classify any new input. By comparing the query point only to these condensed representatives instead of the entire dataset, you significantly decrease computation time without compromising classification accuracy.&#10;3. CNN can be related to support vector machines (SVM) and neural nets approaches in terms of dimensionality reduction and model simplicity. SVMs aim to find a hyperplane that maximally separates classes with a margin, focusing on critical support vectors instead of all training samples. Similarly, condensed nearest neighbor selects key examples, which can be seen as a form of data compression or dimensionality reduction.&#10;4. Neural nets and Gaussian mixtures, on the other hand, typically require more parameters and data to achieve good performance compared to SVMs and CNN. However, they can model complex decision boundaries and are better suited for multi-class problems and cases where relationships between features are not easily captured by a linear boundary or distance metric.&#10;5. In summary, condensed nearest neighbor is a more efficient variant of traditional nearest neighbor classification that reduces computation time by selecting representative examples from the original dataset. This approach shares similarities with support vector machines, focusing on key data points, while differing from neural nets and Gaussian mixtures, which often require more parameters and data to perform well.">
      <data key="d0">1</data>
    </edge>
    <edge source="The suggested approach that is reputed to be better than neural nets or Gaussian mixtures in terms of requiring fewer parameters and performing well even with a modest amount of data is not explicitly mentioned in the provided transcript. However, the speakers discuss an idea where they simplify the model and directly feed the log magnitude spectrum into it, without applying any clever techniques. This could potentially be a more parameter-efficient approach, but it's not specified if this method has any particular name or reputation for better performance with less data." target="In the context of this discussion, a reasonable word error rate on digit strings is around 1%, while a subpar performance would be an error rate of 4-5% or higher. This system, which wasn't trained on the task and used as a comparison, has a word error rate of about 1% on digits, which is considered reasonable. However, an error rate of 4-5% or more would be considered poor performance for this task.">
      <data key="d0">1</data>
    </edge>
    <edge source="The suggested approach that is reputed to be better than neural nets or Gaussian mixtures in terms of requiring fewer parameters and performing well even with a modest amount of data is not explicitly mentioned in the provided transcript. However, the speakers discuss an idea where they simplify the model and directly feed the log magnitude spectrum into it, without applying any clever techniques. This could potentially be a more parameter-efficient approach, but it's not specified if this method has any particular name or reputation for better performance with less data." target="1. The concept being discussed involves calculating a distance measure from MFCC features to phonological features, which is then translated into a binary value (0 or 1) for classification purposes. This approach is used for binary classification where each feature is evaluated as either present (1) or absent (0).&#10;2. In the context of speech recognition, this distance measure is calculated between the MFCC features derived from audio signals and the phonological features corresponding to phones. The distances are then translated into binary values representing whether specific phonological features are present in the given phone.&#10;3. This method utilizes distances by converting them into probabilities for phonological feature occurrences, which can be used for recognizing phones based on their audio signals. Mississippi State researchers have successfully applied this technique using support vector machines to estimate probabilities for speech recognition tasks. The accuracy of the phone recognition depends on the precise identification and marking of the phonological events within the audio signal, as mentioned by Professor C.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers mention changing the filter bank from a trapezoidal integration to a triangular one because some people have reported better performance using triangular filters in RASTA. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;&#10;2. The fundamental difference in using a cubic root for smoothing is contrasted with the log used in another case. This is an advantage for the PLP (Perceptual Linear Prediction) method, as it provides smoothing at the end through an auto-process." target="1. The method for adjusting the bandwidth of the first filters involves copying the configuration of the second filters over to the first filters. This is done in the RASTA (Relative Spectral) processing technique, where some people have reported better performance using triangular filters instead of trapezoidal ones. Typical mel cepstral filter banks use triangular filters, which might be why they work better in this context.&#10;&#10;2. Specifying a different number of filters causes the last ones to be ignored because the system assumes that the same number of filters should be applied across all frequency bins. If a different number is specified, the excess filters will not have any corresponding input and thus will be ignored. This behavior allows for controlling the bandwidth of the filters in the early stages of processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers mention changing the filter bank from a trapezoidal integration to a triangular one because some people have reported better performance using triangular filters in RASTA. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;&#10;2. The fundamental difference in using a cubic root for smoothing is contrasted with the log used in another case. This is an advantage for the PLP (Perceptual Linear Prediction) method, as it provides smoothing at the end through an auto-process." target="1. The filters being used in the process of integrating over FFT bins to create a filter bank are referred to as the mel cepstral filter bank, which typically uses triangular filters. These filters ignore the lowest and highest bins by copying the configuration of the second filters over to the first filters, as done in the RASTA (Relative Spectral) processing technique. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;2. The mel or bark scale refers to a psychoacoustic scale of pitches, where the width of each filter is determined by the analysis bandwidth used in the integration over FFT bins. It's mentioned that there may not be an independent way to specify a particular range for filter creation. Instead, the filter bank is created by integrating over FFT bins, and the filters are defined by the mel or bark scale.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers mention changing the filter bank from a trapezoidal integration to a triangular one because some people have reported better performance using triangular filters in RASTA. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;&#10;2. The fundamental difference in using a cubic root for smoothing is contrasted with the log used in another case. This is an advantage for the PLP (Perceptual Linear Prediction) method, as it provides smoothing at the end through an auto-process." target="1. The SRI system reads wavefiles directly and performs cepstral computation on the fly, but it doesn't store the cepstral files anywhere for comparison with PLP features. This is in contrast to the PLP system, which stores the cepstrum on disk and reads those in during feature extraction.&#10;&#10;2. The SRI system requires a front-end parameter file even after the cepstrum is computed because it's used to specify the kind of computation that the mel cepstrum uses. This may be related to the fact that some people have reported better performance using triangular filters in RASTA, instead of trapezoidal ones, which are typically used in mel cepstral filter banks.&#10;&#10;In summary, the main differences between the SRI and PLP systems lie in file storage (with the former reading wavefiles directly and computing cepstra on the fly and the latter storing computed cepstra on disk) and configuration parameters (where the SRI system requires a front-end parameter file even after the cepstrum is computed). The reason for this difference may be related to the type of filter bank used in each system.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers mention changing the filter bank from a trapezoidal integration to a triangular one because some people have reported better performance using triangular filters in RASTA. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;&#10;2. The fundamental difference in using a cubic root for smoothing is contrasted with the log used in another case. This is an advantage for the PLP (Perceptual Linear Prediction) method, as it provides smoothing at the end through an auto-process." target="To allow the Perceptual Linear Prediction (PLP) system to output mel cepstrum, one of the changes that could be made is to alter the filter bank from a trapezoidal integration to a triangular one. This change might lead to better results since typical mel cepstral filter banks use triangular filters. By doing this, the PLP system's output could be compared directly with the mel cepstrum produced by the SRI system.&#10;&#10;Comparing the outputs would be useful for phonetic classification as it can provide insights into the performance of each system and help identify which approach might be more suitable for specific applications. If both systems produce similar results, it could indicate that either method can be used interchangeably. However, if there are significant differences, it might suggest that one method is more appropriate for certain tasks than the other. Ultimately, this comparison can lead to a better understanding of the strengths and weaknesses of each system, informing future research and development in the field of phonetic classification.">
      <data key="d0">1</data>
    </edge>
    <edge source="The support vectors, or the critical examples, are saved during the process of recreating the boundary for the test set in a Support Vector Machine (SVM) because they provide several benefits compared to just saving the boundary itself. The support vectors represent the data points that lie closest to the decision boundary, and they effectively define the boundary. By storing these critical examples, SVMs can:&#10;&#10;1. Handle new classes: If new classes appear in the test set that were not present during training, the current boundary might not be useful. However, having the support vectors allows the algorithm to adapt and create a new boundary that better suits the updated data distribution.&#10;2. Adapt to changes in the data distribution: In real-world scenarios, data distributions can change over time. By saving the support vectors, SVMs can efficiently update the decision boundary when needed, without requiring access to the entire original dataset.&#10;3. Reduce computational complexity: Classifying new examples involves calculating their distance to the decision boundary. By using support vectors instead of the whole boundary, the computational cost is significantly reduced because there are far fewer support vectors than potential points on the boundary.&#10;4. Improve interpretability: Keeping the support vectors can provide more insights into the dataset and the classification problem, making it easier for analysts to understand why certain decisions are made.&#10;&#10;In summary, saving support vectors instead of just the decision boundary allows SVMs to be more flexible, adaptable, efficient, and interpretable when dealing with new or changing data distributions." target=" and remembers those&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: and {disfmarker} {vocalsound} and uses them to recreate the boundary for the test set . So , given these {vocalsound} um these features , or {disfmarker} or these {disfmarker} these examples , {pause} um , {pause} critical examples , {vocalsound} which they call support f support vectors , {vocalsound} then um {vocalsound} given a new example , {vocalsound} if the new example falls {vocalsound} um away from the boundary in one direction then it 's classified as being a part of this particular class&#10;Speaker: PhD E&#10;Content: Oh .&#10;Speaker: Grad A&#10;Content: and otherwise it 's the other class .&#10;Speaker: PhD E&#10;Content: So why save the examples ? Why not just save what the boundary itself is ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Um . Hmm . Let 's see . Uh . Yeah , that 's a good question . I {disfmarker} yeah">
      <data key="d0">1</data>
    </edge>
    <edge source="The support vectors, or the critical examples, are saved during the process of recreating the boundary for the test set in a Support Vector Machine (SVM) because they provide several benefits compared to just saving the boundary itself. The support vectors represent the data points that lie closest to the decision boundary, and they effectively define the boundary. By storing these critical examples, SVMs can:&#10;&#10;1. Handle new classes: If new classes appear in the test set that were not present during training, the current boundary might not be useful. However, having the support vectors allows the algorithm to adapt and create a new boundary that better suits the updated data distribution.&#10;2. Adapt to changes in the data distribution: In real-world scenarios, data distributions can change over time. By saving the support vectors, SVMs can efficiently update the decision boundary when needed, without requiring access to the entire original dataset.&#10;3. Reduce computational complexity: Classifying new examples involves calculating their distance to the decision boundary. By using support vectors instead of the whole boundary, the computational cost is significantly reduced because there are far fewer support vectors than potential points on the boundary.&#10;4. Improve interpretability: Keeping the support vectors can provide more insights into the dataset and the classification problem, making it easier for analysts to understand why certain decisions are made.&#10;&#10;In summary, saving support vectors instead of just the decision boundary allows SVMs to be more flexible, adaptable, efficient, and interpretable when dealing with new or changing data distributions." target=" A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Does there some kind of a distance metric that they use or how do they {disfmarker} for cla what do they do for classification ?&#10;Speaker: Grad A&#10;Content: Um . Right . So , {vocalsound} the {disfmarker} the simple idea behind a support vector machine is {vocalsound} um , {vocalsound} you have {disfmarker} you have this feature space , right ? and then it finds the optimal separating plane , um between these two different um classes ,&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm . Mm - hmm .&#10;Speaker: Grad A&#10;Content: and um {vocalsound} and so {vocalsound} um , what it {disfmarker} i at the end of the day , what it actually does is {vocalsound} it picks {vocalsound} those examples of the features that are closest to the separating boundary , and remembers those&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: and {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="The support vectors, or the critical examples, are saved during the process of recreating the boundary for the test set in a Support Vector Machine (SVM) because they provide several benefits compared to just saving the boundary itself. The support vectors represent the data points that lie closest to the decision boundary, and they effectively define the boundary. By storing these critical examples, SVMs can:&#10;&#10;1. Handle new classes: If new classes appear in the test set that were not present during training, the current boundary might not be useful. However, having the support vectors allows the algorithm to adapt and create a new boundary that better suits the updated data distribution.&#10;2. Adapt to changes in the data distribution: In real-world scenarios, data distributions can change over time. By saving the support vectors, SVMs can efficiently update the decision boundary when needed, without requiring access to the entire original dataset.&#10;3. Reduce computational complexity: Classifying new examples involves calculating their distance to the decision boundary. By using support vectors instead of the whole boundary, the computational cost is significantly reduced because there are far fewer support vectors than potential points on the boundary.&#10;4. Improve interpretability: Keeping the support vectors can provide more insights into the dataset and the classification problem, making it easier for analysts to understand why certain decisions are made.&#10;&#10;In summary, saving support vectors instead of just the decision boundary allows SVMs to be more flexible, adaptable, efficient, and interpretable when dealing with new or changing data distributions." target=" then it can take a while to {disfmarker} to use nearest - neighbor . There 's lots of look ups . So a long time ago people talked about things where you would have uh a condensed nearest - neighbor , where you would {disfmarker} you would {disfmarker} you would pick out uh some representative examples which would uh be sufficient to represent {disfmarker} to {disfmarker} to correctly classify everything that came in .&#10;Speaker: PhD E&#10;Content: Oh . Mm - hmm .&#10;Speaker: Professor C&#10;Content: I {disfmarker} I think s I think support vector stuff sort of goes back to {disfmarker} {vocalsound} to that kind of thing . Um .&#10;Speaker: PhD E&#10;Content: I see . So rather than doing nearest neighbor where you compare to every single one , you just pick a few critical ones , and {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: And th the You know , um neural net approach uh or Gaussian mixtures for that matter">
      <data key="d0">1</data>
    </edge>
    <edge source="The support vectors, or the critical examples, are saved during the process of recreating the boundary for the test set in a Support Vector Machine (SVM) because they provide several benefits compared to just saving the boundary itself. The support vectors represent the data points that lie closest to the decision boundary, and they effectively define the boundary. By storing these critical examples, SVMs can:&#10;&#10;1. Handle new classes: If new classes appear in the test set that were not present during training, the current boundary might not be useful. However, having the support vectors allows the algorithm to adapt and create a new boundary that better suits the updated data distribution.&#10;2. Adapt to changes in the data distribution: In real-world scenarios, data distributions can change over time. By saving the support vectors, SVMs can efficiently update the decision boundary when needed, without requiring access to the entire original dataset.&#10;3. Reduce computational complexity: Classifying new examples involves calculating their distance to the decision boundary. By using support vectors instead of the whole boundary, the computational cost is significantly reduced because there are far fewer support vectors than potential points on the boundary.&#10;4. Improve interpretability: Keeping the support vectors can provide more insights into the dataset and the classification problem, making it easier for analysts to understand why certain decisions are made.&#10;&#10;In summary, saving support vectors instead of just the decision boundary allows SVMs to be more flexible, adaptable, efficient, and interpretable when dealing with new or changing data distributions." target="Support Vector Machines (SVM) are a type of classification algorithm that use a feature space to find the optimal separating plane between two different classes. The goal is to pick those examples of features that are closest to the separating boundary, which are referred to as support vectors. These critical examples effectively define the boundary, providing several benefits compared to just saving the boundary itself:&#10;&#10;1. Handling new classes: By storing support vectors, SVMs can adapt and create a new boundary when new classes appear in the test set that were not present during training.&#10;2. Adapting to changes in data distribution: Support vectors allow SVMs to efficiently update the decision boundary when data distributions change over time without requiring access to the entire original dataset.&#10;3. Reducing computational complexity: Classifying new examples involves calculating their distance to the decision boundary. Using support vectors significantly reduces computation cost since there are far fewer support vectors than potential points on the boundary.&#10;4. Improving interpretability: Keeping the support vectors provides insights into the dataset and classification problem, making it easier for analysts to understand decision-making processes.&#10;&#10;In summary, SVMs use a feature space and pick examples closest to the separating boundary (support vectors) because this approach offers flexibility, adaptability, efficiency, and interpretability when dealing with new or changing data distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source=" A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Does there some kind of a distance metric that they use or how do they {disfmarker} for cla what do they do for classification ?&#10;Speaker: Grad A&#10;Content: Um . Right . So , {vocalsound} the {disfmarker} the simple idea behind a support vector machine is {vocalsound} um , {vocalsound} you have {disfmarker} you have this feature space , right ? and then it finds the optimal separating plane , um between these two different um classes ,&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm . Mm - hmm .&#10;Speaker: Grad A&#10;Content: and um {vocalsound} and so {vocalsound} um , what it {disfmarker} i at the end of the day , what it actually does is {vocalsound} it picks {vocalsound} those examples of the features that are closest to the separating boundary , and remembers those&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: and {disfmark" target="Support Vector Machines (SVM) are a type of classification algorithm that use a feature space to find the optimal separating plane between two different classes. The goal is to pick those examples of features that are closest to the separating boundary, which are referred to as support vectors. These critical examples effectively define the boundary, providing several benefits compared to just saving the boundary itself:&#10;&#10;1. Handling new classes: By storing support vectors, SVMs can adapt and create a new boundary when new classes appear in the test set that were not present during training.&#10;2. Adapting to changes in data distribution: Support vectors allow SVMs to efficiently update the decision boundary when data distributions change over time without requiring access to the entire original dataset.&#10;3. Reducing computational complexity: Classifying new examples involves calculating their distance to the decision boundary. Using support vectors significantly reduces computation cost since there are far fewer support vectors than potential points on the boundary.&#10;4. Improving interpretability: Keeping the support vectors provides insights into the dataset and classification problem, making it easier for analysts to understand decision-making processes.&#10;&#10;In summary, SVMs use a feature space and pick examples closest to the separating boundary (support vectors) because this approach offers flexibility, adaptability, efficiency, and interpretability when dealing with new or changing data distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source=" then it can take a while to {disfmarker} to use nearest - neighbor . There 's lots of look ups . So a long time ago people talked about things where you would have uh a condensed nearest - neighbor , where you would {disfmarker} you would {disfmarker} you would pick out uh some representative examples which would uh be sufficient to represent {disfmarker} to {disfmarker} to correctly classify everything that came in .&#10;Speaker: PhD E&#10;Content: Oh . Mm - hmm .&#10;Speaker: Professor C&#10;Content: I {disfmarker} I think s I think support vector stuff sort of goes back to {disfmarker} {vocalsound} to that kind of thing . Um .&#10;Speaker: PhD E&#10;Content: I see . So rather than doing nearest neighbor where you compare to every single one , you just pick a few critical ones , and {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: And th the You know , um neural net approach uh or Gaussian mixtures for that matter" target="1. The concept of condensed nearest neighbor (CNN) refers to a modification of the traditional nearest neighbor classification algorithm. In the standard nearest neighbor method, you compare a query point to all training samples to find the closest one(s). This can be computationally expensive and time-consuming, especially with large datasets.&#10;2. To address this issue, CNN was proposed to reduce the number of comparisons needed. The idea is to select a subset of representative examples from the original dataset that can accurately classify any new input. By comparing the query point only to these condensed representatives instead of the entire dataset, you significantly decrease computation time without compromising classification accuracy.&#10;3. CNN can be related to support vector machines (SVM) and neural nets approaches in terms of dimensionality reduction and model simplicity. SVMs aim to find a hyperplane that maximally separates classes with a margin, focusing on critical support vectors instead of all training samples. Similarly, condensed nearest neighbor selects key examples, which can be seen as a form of data compression or dimensionality reduction.&#10;4. Neural nets and Gaussian mixtures, on the other hand, typically require more parameters and data to achieve good performance compared to SVMs and CNN. However, they can model complex decision boundaries and are better suited for multi-class problems and cases where relationships between features are not easily captured by a linear boundary or distance metric.&#10;5. In summary, condensed nearest neighbor is a more efficient variant of traditional nearest neighbor classification that reduces computation time by selecting representative examples from the original dataset. This approach shares similarities with support vector machines, focusing on key data points, while differing from neural nets and Gaussian mixtures, which often require more parameters and data to perform well.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The method for adjusting the bandwidth of the first filters involves copying the configuration of the second filters over to the first filters. This is done in the RASTA (Relative Spectral) processing technique, where some people have reported better performance using triangular filters instead of trapezoidal ones. Typical mel cepstral filter banks use triangular filters, which might be why they work better in this context.&#10;&#10;2. Specifying a different number of filters causes the last ones to be ignored because the system assumes that the same number of filters should be applied across all frequency bins. If a different number is specified, the excess filters will not have any corresponding input and thus will be ignored. This behavior allows for controlling the bandwidth of the filters in the early stages of processing." target="s bark scale , and it 's {disfmarker} it {disfmarker} it um {disfmarker} it actually copies the uh um {disfmarker} the second filters over to the first . So the first filters are always {disfmarker} and you can s you can specify a different number of {vocalsound} uh features {disfmarker} different number of filters , I think , as I recall . So you can specify a different number of filters , and whatever {vocalsound} um uh you specify , the last ones are gonna be ignored . So that {disfmarker} that 's a way that you sort of change what the {disfmarker} what the bandwidth is . Y you can't do it without I think changing the number of filters , but {disfmarker}&#10;Speaker: PhD E&#10;Content: I saw something about uh {disfmarker} that looked like it was doing something like that , but I didn't quite understand it . So maybe {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah , so the idea is that the very lowest frequencies and {disfmarker} and typically the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The method for adjusting the bandwidth of the first filters involves copying the configuration of the second filters over to the first filters. This is done in the RASTA (Relative Spectral) processing technique, where some people have reported better performance using triangular filters instead of trapezoidal ones. Typical mel cepstral filter banks use triangular filters, which might be why they work better in this context.&#10;&#10;2. Specifying a different number of filters causes the last ones to be ignored because the system assumes that the same number of filters should be applied across all frequency bins. If a different number is specified, the excess filters will not have any corresponding input and thus will be ignored. This behavior allows for controlling the bandwidth of the filters in the early stages of processing." target="1. The filters being used in the process of integrating over FFT bins to create a filter bank are referred to as the mel cepstral filter bank, which typically uses triangular filters. These filters ignore the lowest and highest bins by copying the configuration of the second filters over to the first filters, as done in the RASTA (Relative Spectral) processing technique. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;2. The mel or bark scale refers to a psychoacoustic scale of pitches, where the width of each filter is determined by the analysis bandwidth used in the integration over FFT bins. It's mentioned that there may not be an independent way to specify a particular range for filter creation. Instead, the filter bank is created by integrating over FFT bins, and the filters are defined by the mel or bark scale.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The method for adjusting the bandwidth of the first filters involves copying the configuration of the second filters over to the first filters. This is done in the RASTA (Relative Spectral) processing technique, where some people have reported better performance using triangular filters instead of trapezoidal ones. Typical mel cepstral filter banks use triangular filters, which might be why they work better in this context.&#10;&#10;2. Specifying a different number of filters causes the last ones to be ignored because the system assumes that the same number of filters should be applied across all frequency bins. If a different number is specified, the excess filters will not have any corresponding input and thus will be ignored. This behavior allows for controlling the bandwidth of the filters in the early stages of processing." target="1. The SRI system reads wavefiles directly and performs cepstral computation on the fly, but it doesn't store the cepstral files anywhere for comparison with PLP features. This is in contrast to the PLP system, which stores the cepstrum on disk and reads those in during feature extraction.&#10;&#10;2. The SRI system requires a front-end parameter file even after the cepstrum is computed because it's used to specify the kind of computation that the mel cepstrum uses. This may be related to the fact that some people have reported better performance using triangular filters in RASTA, instead of trapezoidal ones, which are typically used in mel cepstral filter banks.&#10;&#10;In summary, the main differences between the SRI and PLP systems lie in file storage (with the former reading wavefiles directly and computing cepstra on the fly and the latter storing computed cepstra on disk) and configuration parameters (where the SRI system requires a front-end parameter file even after the cepstrum is computed). The reason for this difference may be related to the type of filter bank used in each system.">
      <data key="d0">1</data>
    </edge>
    <edge source="s bark scale , and it 's {disfmarker} it {disfmarker} it um {disfmarker} it actually copies the uh um {disfmarker} the second filters over to the first . So the first filters are always {disfmarker} and you can s you can specify a different number of {vocalsound} uh features {disfmarker} different number of filters , I think , as I recall . So you can specify a different number of filters , and whatever {vocalsound} um uh you specify , the last ones are gonna be ignored . So that {disfmarker} that 's a way that you sort of change what the {disfmarker} what the bandwidth is . Y you can't do it without I think changing the number of filters , but {disfmarker}&#10;Speaker: PhD E&#10;Content: I saw something about uh {disfmarker} that looked like it was doing something like that , but I didn't quite understand it . So maybe {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah , so the idea is that the very lowest frequencies and {disfmarker} and typically the" target="1. The filters being used in the process of integrating over FFT bins to create a filter bank are referred to as the mel cepstral filter bank, which typically uses triangular filters. These filters ignore the lowest and highest bins by copying the configuration of the second filters over to the first filters, as done in the RASTA (Relative Spectral) processing technique. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;2. The mel or bark scale refers to a psychoacoustic scale of pitches, where the width of each filter is determined by the analysis bandwidth used in the integration over FFT bins. It's mentioned that there may not be an independent way to specify a particular range for filter creation. Instead, the filter bank is created by integrating over FFT bins, and the filters are defined by the mel or bark scale.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, it is not explicitly stated that Professor C confirmed she left the microphone on before leaving for another appointment. However, she did mention &quot;I left it on&quot; in her speech. It's unclear whether she was referring to the microphone or something else, but given the context of the conversation, it's possible that she was indeed confirming that the microphone was left on.&#10;&#10;As for whether this was agreeable to PhD E, there is no explicit response from him regarding this matter in the transcript. Therefore, it cannot be definitively determined if he agreed or disagreed with leaving the microphone on." target=" sum of them , or {disfmarker} ? Oh it {disfmarker} i Oh , so it is a sigmoidal .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: OK . Alright .&#10;Speaker: PhD E&#10;Content: Did the {disfmarker} did they get good results with that ?&#10;Speaker: Professor C&#10;Content: I mean , they 're OK , I {disfmarker} I don't {disfmarker} I don't think they were earth {disfmarker} earth shattering , but I think that {vocalsound} uh this was a couple years ago ,&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: I remember them doing it at some meeting , and {disfmarker} and um I don't think people were very critical because it was interesting just to {disfmarker} to try this and you know , it was the first time they tried it , so {disfmarker} {vocalsound} so the {disfmarker} you know , the numbers were not incredibly good&#10;Speaker: PhD E&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, it is not explicitly stated that Professor C confirmed she left the microphone on before leaving for another appointment. However, she did mention &quot;I left it on&quot; in her speech. It's unclear whether she was referring to the microphone or something else, but given the context of the conversation, it's possible that she was indeed confirming that the microphone was left on.&#10;&#10;As for whether this was agreeable to PhD E, there is no explicit response from him regarding this matter in the transcript. Therefore, it cannot be definitively determined if he agreed or disagreed with leaving the microphone on." target="marker} {vocalsound} so the {disfmarker} you know , the numbers were not incredibly good&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: but there 's you know , it was th reasonable .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: I {disfmarker} I don't remember anymore . I don't even remember what the task was , it {comment} was Broadcast News , or {vocalsound} something . I don't know .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Grad B&#10;Content: Uh s So Barry , if you just have zero and ones , how are you doing the speech recognition ?&#10;Speaker: Grad A&#10;Content: Oh I 'm not do I 'm not planning on doing speech recognition with it . I 'm just doing {vocalsound} detection of phonological features .&#10;Speaker: Grad B&#10;Content: Oh . OK .&#10;Speaker: Grad A&#10;Content: So uh for example , {vocalsound} this {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, it is not explicitly stated that Professor C confirmed she left the microphone on before leaving for another appointment. However, she did mention &quot;I left it on&quot; in her speech. It's unclear whether she was referring to the microphone or something else, but given the context of the conversation, it's possible that she was indeed confirming that the microphone was left on.&#10;&#10;As for whether this was agreeable to PhD E, there is no explicit response from him regarding this matter in the transcript. Therefore, it cannot be definitively determined if he agreed or disagreed with leaving the microphone on." target="Professor C is trying to convey that the number of filters used in an analysis can affect the resultant output. She suggests that using a larger number of filters would result in less information being discarded, as compared to using fewer filters which would throw away more data from the lower and higher frequency bins. This is because when there are fewer filters, more data at the extremes of the frequency range will be lost.&#10;&#10;PhD E mentions the use of analysis bandwidth or checking for potential bugs or inconsistencies in feature computation in the context of the Feacalc code because they want to ensure that all sampling rates are as expected and that there are no errors in the calculation of features. This is an important step to take before making any conclusions about the data analysis. However, PhD E does not explicitly mention using analysis bandwidth in their response, so it is unclear if they are directly addressing Professor C's point about the range of filter creation.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker} {vocalsound} so the {disfmarker} you know , the numbers were not incredibly good&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: but there 's you know , it was th reasonable .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: I {disfmarker} I don't remember anymore . I don't even remember what the task was , it {comment} was Broadcast News , or {vocalsound} something . I don't know .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Grad B&#10;Content: Uh s So Barry , if you just have zero and ones , how are you doing the speech recognition ?&#10;Speaker: Grad A&#10;Content: Oh I 'm not do I 'm not planning on doing speech recognition with it . I 'm just doing {vocalsound} detection of phonological features .&#10;Speaker: Grad B&#10;Content: Oh . OK .&#10;Speaker: Grad A&#10;Content: So uh for example , {vocalsound} this {disf" target="1. Grad A has made progress in securing committee members for their qualifications. So far, they have confirmed Morgan, Hynek, Mike Jordan, and John Ohala.&#10;2. The remaining committee member that Grad A needs to ask is Malek.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The concept of condensed nearest neighbor (CNN) refers to a modification of the traditional nearest neighbor classification algorithm. In the standard nearest neighbor method, you compare a query point to all training samples to find the closest one(s). This can be computationally expensive and time-consuming, especially with large datasets.&#10;2. To address this issue, CNN was proposed to reduce the number of comparisons needed. The idea is to select a subset of representative examples from the original dataset that can accurately classify any new input. By comparing the query point only to these condensed representatives instead of the entire dataset, you significantly decrease computation time without compromising classification accuracy.&#10;3. CNN can be related to support vector machines (SVM) and neural nets approaches in terms of dimensionality reduction and model simplicity. SVMs aim to find a hyperplane that maximally separates classes with a margin, focusing on critical support vectors instead of all training samples. Similarly, condensed nearest neighbor selects key examples, which can be seen as a form of data compression or dimensionality reduction.&#10;4. Neural nets and Gaussian mixtures, on the other hand, typically require more parameters and data to achieve good performance compared to SVMs and CNN. However, they can model complex decision boundaries and are better suited for multi-class problems and cases where relationships between features are not easily captured by a linear boundary or distance metric.&#10;5. In summary, condensed nearest neighbor is a more efficient variant of traditional nearest neighbor classification that reduces computation time by selecting representative examples from the original dataset. This approach shares similarities with support vector machines, focusing on key data points, while differing from neural nets and Gaussian mixtures, which often require more parameters and data to perform well." target="Support Vector Machines (SVM) are a type of classification algorithm that use a feature space to find the optimal separating plane between two different classes. The goal is to pick those examples of features that are closest to the separating boundary, which are referred to as support vectors. These critical examples effectively define the boundary, providing several benefits compared to just saving the boundary itself:&#10;&#10;1. Handling new classes: By storing support vectors, SVMs can adapt and create a new boundary when new classes appear in the test set that were not present during training.&#10;2. Adapting to changes in data distribution: Support vectors allow SVMs to efficiently update the decision boundary when data distributions change over time without requiring access to the entire original dataset.&#10;3. Reducing computational complexity: Classifying new examples involves calculating their distance to the decision boundary. Using support vectors significantly reduces computation cost since there are far fewer support vectors than potential points on the boundary.&#10;4. Improving interpretability: Keeping the support vectors provides insights into the dataset and classification problem, making it easier for analysts to understand decision-making processes.&#10;&#10;In summary, SVMs use a feature space and pick examples closest to the separating boundary (support vectors) because this approach offers flexibility, adaptability, efficiency, and interpretability when dealing with new or changing data distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is trying to convey the idea that the number of filters used in an analysis can affect the resultant output. He suggests that using a larger number of filters would result in less information being thrown away, as compared to using fewer filters which would discard more data from the lower and higher frequency bins.&#10;&#10;Professor C also mentions that there may not be an independent way to specify a particular range for filter creation, but rather, it might be determined by the analysis bandwidth used in the integration over FFT bins.&#10;&#10;PhD E responds by referring to the Feacalc code and checking for potential bugs or inconsistencies in feature computation. They also mention the need to ensure that all sampling rates are as expected.&#10;&#10;However, there is no explicit mention of using analysis bandwidth in PhD E's response." target="er}&#10;Speaker: Professor C&#10;Content: Yeah , so the idea is that the very lowest frequencies and {disfmarker} and typically the veriest {comment} highest frequencies are kind of junk .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Professor C&#10;Content: And so um you just {disfmarker} for continuity you just approximate them by {disfmarker} {vocalsound} by the second to highest and second to lowest . It 's just a simple thing we put in .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And {disfmarker} and so if you h&#10;Speaker: PhD E&#10;Content: But {disfmarker} so the {disfmarker} but that 's a fixed uh thing ?&#10;Speaker: Professor C&#10;Content: Yeah , {comment} I think that 's a fixed thing .&#10;Speaker: PhD E&#10;Content: There 's nothing that lets you {disfmarker}&#10;Speaker: Professor C&#10;Content: But see {disfmarker} see my point ? If you had {d">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is trying to convey the idea that the number of filters used in an analysis can affect the resultant output. He suggests that using a larger number of filters would result in less information being thrown away, as compared to using fewer filters which would discard more data from the lower and higher frequency bins.&#10;&#10;Professor C also mentions that there may not be an independent way to specify a particular range for filter creation, but rather, it might be determined by the analysis bandwidth used in the integration over FFT bins.&#10;&#10;PhD E responds by referring to the Feacalc code and checking for potential bugs or inconsistencies in feature computation. They also mention the need to ensure that all sampling rates are as expected.&#10;&#10;However, there is no explicit mention of using analysis bandwidth in PhD E's response." target="1. The filters being used in the process of integrating over FFT bins to create a filter bank are referred to as the mel cepstral filter bank, which typically uses triangular filters. These filters ignore the lowest and highest bins by copying the configuration of the second filters over to the first filters, as done in the RASTA (Relative Spectral) processing technique. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;2. The mel or bark scale refers to a psychoacoustic scale of pitches, where the width of each filter is determined by the analysis bandwidth used in the integration over FFT bins. It's mentioned that there may not be an independent way to specify a particular range for filter creation. Instead, the filter bank is created by integrating over FFT bins, and the filters are defined by the mel or bark scale.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is trying to convey the idea that the number of filters used in an analysis can affect the resultant output. He suggests that using a larger number of filters would result in less information being thrown away, as compared to using fewer filters which would discard more data from the lower and higher frequency bins.&#10;&#10;Professor C also mentions that there may not be an independent way to specify a particular range for filter creation, but rather, it might be determined by the analysis bandwidth used in the integration over FFT bins.&#10;&#10;PhD E responds by referring to the Feacalc code and checking for potential bugs or inconsistencies in feature computation. They also mention the need to ensure that all sampling rates are as expected.&#10;&#10;However, there is no explicit mention of using analysis bandwidth in PhD E's response." target="Professor C is trying to convey that the number of filters used in an analysis can affect the resultant output. She suggests that using a larger number of filters would result in less information being discarded, as compared to using fewer filters which would throw away more data from the lower and higher frequency bins. This is because when there are fewer filters, more data at the extremes of the frequency range will be lost.&#10;&#10;PhD E mentions the use of analysis bandwidth or checking for potential bugs or inconsistencies in feature computation in the context of the Feacalc code because they want to ensure that all sampling rates are as expected and that there are no errors in the calculation of features. This is an important step to take before making any conclusions about the data analysis. However, PhD E does not explicitly mention using analysis bandwidth in their response, so it is unclear if they are directly addressing Professor C's point about the range of filter creation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The filters being used in the process of integrating over FFT bins to create a filter bank are referred to as the mel cepstral filter bank, which typically uses triangular filters. These filters ignore the lowest and highest bins by copying the configuration of the second filters over to the first filters, as done in the RASTA (Relative Spectral) processing technique. This change might lead to better results since typical mel cepstral filter banks use triangular filters.&#10;2. The mel or bark scale refers to a psychoacoustic scale of pitches, where the width of each filter is determined by the analysis bandwidth used in the integration over FFT bins. It's mentioned that there may not be an independent way to specify a particular range for filter creation. Instead, the filter bank is created by integrating over FFT bins, and the filters are defined by the mel or bark scale." target="To allow the Perceptual Linear Prediction (PLP) system to output mel cepstrum, one of the changes that could be made is to alter the filter bank from a trapezoidal integration to a triangular one. This change might lead to better results since typical mel cepstral filter banks use triangular filters. By doing this, the PLP system's output could be compared directly with the mel cepstrum produced by the SRI system.&#10;&#10;Comparing the outputs would be useful for phonetic classification as it can provide insights into the performance of each system and help identify which approach might be more suitable for specific applications. If both systems produce similar results, it could indicate that either method can be used interchangeably. However, if there are significant differences, it might suggest that one method is more appropriate for certain tasks than the other. Ultimately, this comparison can lead to a better understanding of the strengths and weaknesses of each system, informing future research and development in the field of phonetic classification.">
      <data key="d0">1</data>
    </edge>
    <edge source="In the context of this discussion, a reasonable word error rate on digit strings is around 1%, while a subpar performance would be an error rate of 4-5% or higher. This system, which wasn't trained on the task and used as a comparison, has a word error rate of about 1% on digits, which is considered reasonable. However, an error rate of 4-5% or more would be considered poor performance for this task." target=" think , you know , one {disfmarker} one percent word error rate on digits is {disfmarker} uh digit strings is not {vocalsound} uh you know stellar , but {disfmarker} but given that this is real {vocalsound} digits , as opposed to uh sort of laboratory {disfmarker}&#10;Speaker: Grad B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Well .&#10;Speaker: PhD E&#10;Content: And it wasn't trained on this task either .&#10;Speaker: Professor C&#10;Content: And it wasn't trained on this task . Actually one percent is sort of {disfmarker} you know , sort of in a reasonable range .&#10;Speaker: Grad B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: People would say &quot; yeah , I could {disfmarker} I can imagine getting that &quot; . And uh so the {disfmarker} the four or five percent or something is {disfmarker} is {disfmarker} is quite poor .&#10;Speaker: Grad B&#10;Content: Mm - hmm .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="In the context of this discussion, a reasonable word error rate on digit strings is around 1%, while a subpar performance would be an error rate of 4-5% or higher. This system, which wasn't trained on the task and used as a comparison, has a word error rate of about 1% on digits, which is considered reasonable. However, an error rate of 4-5% or more would be considered poor performance for this task." target=" Mm - hmm .&#10;Speaker: Professor C&#10;Content: I mean you could feed it a bunch of s you could feed two numbers that you wanted to multiply into a net and have a bunch of nonlinearities in the middle and train it to get the product of the output and it would work . But , it 's kind of crazy , cuz we know how to multiply and you {disfmarker} you 'd be you know much lower error usually {vocalsound} if you just multiplied it out . But suppose you don't really know what the right thing is . And that 's what these sort of dumb machine learning methods are good at . So . Um . Anyway . It 's just a thought .&#10;Speaker: PhD E&#10;Content: How long does it take , Carmen , to train up one of these nets ?&#10;Speaker: PhD D&#10;Content: Oh , not too much .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Mmm , one day or less .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: Yeah , it 's probably worth it .&#10;Speaker: Grad A&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="In the context of this discussion, a reasonable word error rate on digit strings is around 1%, while a subpar performance would be an error rate of 4-5% or higher. This system, which wasn't trained on the task and used as a comparison, has a word error rate of about 1% on digits, which is considered reasonable. However, an error rate of 4-5% or more would be considered poor performance for this task." target="1. The SRI system reads wavefiles directly and performs cepstral computation on the fly, but it doesn't store the cepstral files anywhere for comparison with PLP features. This is in contrast to the PLP system, which stores the cepstrum on disk and reads those in during feature extraction.&#10;&#10;2. The SRI system requires a front-end parameter file even after the cepstrum is computed because it's used to specify the kind of computation that the mel cepstrum uses. This may be related to the fact that some people have reported better performance using triangular filters in RASTA, instead of trapezoidal ones, which are typically used in mel cepstral filter banks.&#10;&#10;In summary, the main differences between the SRI and PLP systems lie in file storage (with the former reading wavefiles directly and computing cepstra on the fly and the latter storing computed cepstra on disk) and configuration parameters (where the SRI system requires a front-end parameter file even after the cepstrum is computed). The reason for this difference may be related to the type of filter bank used in each system.">
      <data key="d0">1</data>
    </edge>
    <edge source="In the context of this discussion, a reasonable word error rate on digit strings is around 1%, while a subpar performance would be an error rate of 4-5% or higher. This system, which wasn't trained on the task and used as a comparison, has a word error rate of about 1% on digits, which is considered reasonable. However, an error rate of 4-5% or more would be considered poor performance for this task." target="Based on the information provided in the transcript, the error rate reduction in speech recognition achieved by using a simulated impulse response and training on clean speech only was from approximately 80% to 4%. This conclusion is based on indirect information since the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated.&#10;&#10;The transcript mentions that when using a simulated impulse response, the error rate went from something like eighteen percent to four percent for vocalsound and from forty-one percent to eight percent for mike far digits on channel F. It can be inferred that these reductions in error rates were due to using a simulated impulse response and training on clean speech only.&#10;&#10;However, the transcript does not provide information about the error rate when not using artificial reverb or if the system was trained on noisy speech. The comparison is made between using artificial reverb and clean speech versus an unspecified setup that may or may not have used artificial reverb and could have been trained on noisy speech.&#10;&#10;Therefore, while there was a significant reduction in error rates for speech recognition when using a simulated impulse response and training on clean speech only, the exact difference in error rates between these setups cannot be determined from the information provided in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The SRI system reads wavefiles directly and performs cepstral computation on the fly, but it doesn't store the cepstral files anywhere for comparison with PLP features. This is in contrast to the PLP system, which stores the cepstrum on disk and reads those in during feature extraction.&#10;&#10;2. The SRI system requires a front-end parameter file even after the cepstrum is computed because it's used to specify the kind of computation that the mel cepstrum uses. This may be related to the fact that some people have reported better performance using triangular filters in RASTA, instead of trapezoidal ones, which are typically used in mel cepstral filter banks.&#10;&#10;In summary, the main differences between the SRI and PLP systems lie in file storage (with the former reading wavefiles directly and computing cepstra on the fly and the latter storing computed cepstra on disk) and configuration parameters (where the SRI system requires a front-end parameter file even after the cepstrum is computed). The reason for this difference may be related to the type of filter bank used in each system." target="To allow the Perceptual Linear Prediction (PLP) system to output mel cepstrum, one of the changes that could be made is to alter the filter bank from a trapezoidal integration to a triangular one. This change might lead to better results since typical mel cepstral filter banks use triangular filters. By doing this, the PLP system's output could be compared directly with the mel cepstrum produced by the SRI system.&#10;&#10;Comparing the outputs would be useful for phonetic classification as it can provide insights into the performance of each system and help identify which approach might be more suitable for specific applications. If both systems produce similar results, it could indicate that either method can be used interchangeably. However, if there are significant differences, it might suggest that one method is more appropriate for certain tasks than the other. Ultimately, this comparison can lead to a better understanding of the strengths and weaknesses of each system, informing future research and development in the field of phonetic classification.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Grad A has made progress in securing committee members for their qualifications. So far, they have confirmed Morgan, Hynek, Mike Jordan, and John Ohala.&#10;2. The remaining committee member that Grad A needs to ask is Malek." target=" . I mean , I mostly just ignored it because I was too busy and {vocalsound} didn't get to it . So uh Wanna talk a little bit about what we were talking about this morning ?&#10;Speaker: Grad A&#10;Content: Oh ! um {pause} uh {pause} Yeah .&#10;Speaker: Professor C&#10;Content: Just briefly , or {pause} Or anything else ?&#10;Speaker: Grad A&#10;Content: So . I {disfmarker} I guess some of the progress , I {disfmarker} I 've been getting a {disfmarker} getting my committee members for the quals . And um so far I have Morgan and Hynek , {vocalsound} Mike Jordan , and I asked John Ohala and he agreed . Yeah . Yeah .&#10;Speaker: PhD E&#10;Content: Cool .&#10;Speaker: Grad A&#10;Content: So I 'm {disfmarker} I {disfmarker} I just need to ask um Malek . One more . Um . Tsk . Then uh I talked a little bit about {vocalsound} um continuing with these dynamic ev um acoustic events , and um {vocalsound} {">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Grad A has made progress in securing committee members for their qualifications. So far, they have confirmed Morgan, Hynek, Mike Jordan, and John Ohala.&#10;2. The remaining committee member that Grad A needs to ask is Malek." target="&#10;Speaker: Grad A&#10;Content: Right , yeah .&#10;Speaker: Professor C&#10;Content: There 's {disfmarker} you have {disfmarker} you have the paper , right ? The Mississippi State paper ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: Yeah , if you 're interested y you could look ,&#10;Speaker: Grad B&#10;Content: And {disfmarker} OK . OK .&#10;Speaker: Grad A&#10;Content: Yeah , I can {disfmarker} I can show you {disfmarker} I {disfmarker}&#10;Speaker: Professor C&#10;Content: yeah .&#10;Speaker: Grad A&#10;Content: yeah , our {disfmarker}&#10;Speaker: PhD E&#10;Content: So in your {disfmarker} in {disfmarker} in the thing that you 're doing , uh you have a vector of ones and zeros for each phone ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Uh , is this the class project , or {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: Grad A&#10;Content: Right , yeah .&#10;Speaker: Professor C&#10;Content: There 's {disfmarker} you have {disfmarker} you have the paper , right ? The Mississippi State paper ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: Yeah , if you 're interested y you could look ,&#10;Speaker: Grad B&#10;Content: And {disfmarker} OK . OK .&#10;Speaker: Grad A&#10;Content: Yeah , I can {disfmarker} I can show you {disfmarker} I {disfmarker}&#10;Speaker: Professor C&#10;Content: yeah .&#10;Speaker: Grad A&#10;Content: yeah , our {disfmarker}&#10;Speaker: PhD E&#10;Content: So in your {disfmarker} in {disfmarker} in the thing that you 're doing , uh you have a vector of ones and zeros for each phone ?&#10;Speaker: Grad A&#10;Content: Mm - hmm . Uh , is this the class project , or {disfmarker}" target="In the project, the speakers are using a vector of ones and zeros to represent each phone where each element in the vector corresponds to whether the phone exhibits a particular phonological feature or not. This vector is obtained by learning a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals. The output of the system is a vector of ones and zeros for each phone, where each element in the vector represents the presence or absence of a specific phonological feature.">
      <data key="d0">1</data>
    </edge>
    <edge source="The system learns a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals, to binary vectors representing the presence or absence of specific phonological features in corresponding phones. The output of the system is a vector of ones and zeros for each phone, where each element corresponds to whether the phone exhibits a particular phonological feature or not. This information can be further used to classify phones based on their audio signals." target="1. The concept being discussed involves calculating a distance measure from MFCC features to phonological features, which is then translated into a binary value (0 or 1) for classification purposes. This approach is used for binary classification where each feature is evaluated as either present (1) or absent (0).&#10;2. In the context of speech recognition, this distance measure is calculated between the MFCC features derived from audio signals and the phonological features corresponding to phones. The distances are then translated into binary values representing whether specific phonological features are present in the given phone.&#10;3. This method utilizes distances by converting them into probabilities for phonological feature occurrences, which can be used for recognizing phones based on their audio signals. Mississippi State researchers have successfully applied this technique using support vector machines to estimate probabilities for speech recognition tasks. The accuracy of the phone recognition depends on the precise identification and marking of the phonological events within the audio signal, as mentioned by Professor C.">
      <data key="d0">1</data>
    </edge>
    <edge source="The system learns a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals, to binary vectors representing the presence or absence of specific phonological features in corresponding phones. The output of the system is a vector of ones and zeros for each phone, where each element corresponds to whether the phone exhibits a particular phonological feature or not. This information can be further used to classify phones based on their audio signals." target="In the project, the speakers are using a vector of ones and zeros to represent each phone where each element in the vector corresponds to whether the phone exhibits a particular phonological feature or not. This vector is obtained by learning a mapping from Mel-frequency cepstral coefficients (MFCC) features, which are derived from audio signals. The output of the system is a vector of ones and zeros for each phone, where each element in the vector represents the presence or absence of a specific phonological feature.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The concept being discussed involves calculating a distance measure from MFCC features to phonological features, which is then translated into a binary value (0 or 1) for classification purposes. This approach is used for binary classification where each feature is evaluated as either present (1) or absent (0).&#10;2. In the context of speech recognition, this distance measure is calculated between the MFCC features derived from audio signals and the phonological features corresponding to phones. The distances are then translated into binary values representing whether specific phonological features are present in the given phone.&#10;3. This method utilizes distances by converting them into probabilities for phonological feature occurrences, which can be used for recognizing phones based on their audio signals. Mississippi State researchers have successfully applied this technique using support vector machines to estimate probabilities for speech recognition tasks. The accuracy of the phone recognition depends on the precise identification and marking of the phonological events within the audio signal, as mentioned by Professor C." target="isfmarker} there are pap Well , basically , it 's {disfmarker} it 's um {vocalsound} you {disfmarker} you get a distance measure at the end of the day , and then that distance measure is {disfmarker} is um {disfmarker} {vocalsound} is translated to a zero or one . Um .&#10;Speaker: Professor C&#10;Content: But that 's looking at it for {disfmarker} for classification {disfmarker} for binary classification ,&#10;Speaker: Grad A&#10;Content: That 's for classification , right .&#10;Speaker: Professor C&#10;Content: right ?&#10;Speaker: PhD E&#10;Content: And you get that for each class , you get a zero or a one .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: But you have the distances to work with .&#10;Speaker: Grad A&#10;Content: You have the distances to work with ,&#10;Speaker: Professor C&#10;Content: Cuz actually Mississippi State people did use support vector machines for uh uh speech recognition and they were using it to estimate probabilities .&#10;Speaker: Grad">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The concept being discussed involves calculating a distance measure from MFCC features to phonological features, which is then translated into a binary value (0 or 1) for classification purposes. This approach is used for binary classification where each feature is evaluated as either present (1) or absent (0).&#10;2. In the context of speech recognition, this distance measure is calculated between the MFCC features derived from audio signals and the phonological features corresponding to phones. The distances are then translated into binary values representing whether specific phonological features are present in the given phone.&#10;3. This method utilizes distances by converting them into probabilities for phonological feature occurrences, which can be used for recognizing phones based on their audio signals. Mississippi State researchers have successfully applied this technique using support vector machines to estimate probabilities for speech recognition tasks. The accuracy of the phone recognition depends on the precise identification and marking of the phonological events within the audio signal, as mentioned by Professor C." target="Based on the information provided in the transcript, the error rate reduction in speech recognition achieved by using a simulated impulse response and training on clean speech only was from approximately 80% to 4%. This conclusion is based on indirect information since the exact accuracy rate for the voiced-unvoiced frame error measurement is not explicitly stated.&#10;&#10;The transcript mentions that when using a simulated impulse response, the error rate went from something like eighteen percent to four percent for vocalsound and from forty-one percent to eight percent for mike far digits on channel F. It can be inferred that these reductions in error rates were due to using a simulated impulse response and training on clean speech only.&#10;&#10;However, the transcript does not provide information about the error rate when not using artificial reverb or if the system was trained on noisy speech. The comparison is made between using artificial reverb and clean speech versus an unspecified setup that may or may not have used artificial reverb and could have been trained on noisy speech.&#10;&#10;Therefore, while there was a significant reduction in error rates for speech recognition when using a simulated impulse response and training on clean speech only, the exact difference in error rates between these setups cannot be determined from the information provided in the transcript.">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
