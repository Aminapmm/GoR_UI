<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="The transcriber's role in this context is not to evaluate the speech and nonspeech in specific channels by comparing it to reference material, but rather to segment and label the multichannel audio data. This task was previously done manually, as stated by Grad F when they mentioned &quot;Well we did the hand segmentation the one by hand.&quot; The proposal now is for the transcribers to continue their work in preparing the data, which includes adjusting timebands and formatting into a multi-channel structure.&#10;&#10;PhD G mentions that there are different types of noises like &quot;uh - huh&quot;s, &quot;hmm&quot;s, and grunts, and Postdoc B brings up lip smacks. The team can use the transcribed data to develop and train their system to recognize and categorize these speech and nonspeech elements in specific channels. However, it is not the transcriber's responsibility to provide this evaluation or comparison with reference material, as no such reference materials are available in this context." />
    <node id=" , that 's {disfmarker} that 's great for {disfmarker} for my purpose . And , the thing is I {disfmarker} I , then the evaluation of {disfmarker} of the system is a little bit hard , as I don't have any references .&#10;Speaker: Grad F&#10;Content: Well we did the hand {disfmarker} the one by hand .&#10;Speaker: PhD A&#10;Content: Yeah , that 's the one {disfmarker} one wh where I do the training on so I can't do the evaluation on So the thing is , can the transcribers perhaps do some , some {disfmarker} some meetings in {disfmarker} in terms of speech - nonspeech in {disfmarker} in the specific channels ?&#10;Speaker: Grad F&#10;Content: Uh .&#10;Speaker: Postdoc B&#10;Content: Well , I have {disfmarker}&#10;Speaker: PhD D&#10;Content: Well won't you have that from their transcriptions ?&#10;Speaker: Postdoc B&#10;Content: Well , OK , so , now we need {disfmarker}&#10;" />
    <node id=" of {pause} interrupting {comment} and so forth .&#10;Speaker: Grad F&#10;Content: Uh - huh .&#10;Speaker: PhD G&#10;Content: And some of them are , {vocalsound} yeah , &quot; uh - huh &quot;s , and &quot; hmm &quot;s , and , &quot; hmm ! &quot; &quot; hmm &quot; {comment} &quot; OK &quot; , &quot; uh &quot; {comment} Grunts , uh , that might be interesting .&#10;Speaker: Postdoc B&#10;Content: He 's got lip {disfmarker} {pause} lipsmacks .&#10;Speaker: PhD G&#10;Content: In the meetings .&#10;Speaker: Professor C&#10;Content: We should move on .&#10;Speaker: Postdoc B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , new version of , uh , presegmentation ?&#10;Speaker: PhD A&#10;Content: Uh , oh yeah , um , {vocalsound} I worked a little bit on the {disfmarker} on the presegmentation to {disfmarker} to get another version which does channel - specific , uh , speech - nonspeech detection . And , what I did" />
    <node id="Speaker: PhD G&#10;Content: Right . It 's not the {disfmarker} it 's not the fact that we can't process a twenty second segment , it 's the fact that , there 's twenty seconds in which to place one word in the wrong place&#10;Speaker: Grad E&#10;Content: so it 's not {disfmarker}&#10;Speaker: Postdoc B&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: You know , if {disfmarker} if someone has a very short utterance there , and that 's where , we , might wanna have this individual , you know , ha have your pre pre - process input .&#10;Speaker: PhD A&#10;Content: Yep . Yeah . Sure .&#10;Speaker: Postdoc B&#10;Content: That 's very important .&#10;Speaker: PhD A&#10;Content: I {disfmarker} I {disfmarker} I thought that perhaps the transcribers could start then from the {disfmarker} those mult multi - channel , uh , speech - nonspeech detections , if they would like to .&#10;Speaker:" />
    <node id=" did {disfmarker} I did , um , uh , so , last night I did , uh ,&#10;Speaker: Professor C&#10;Content: yeah .&#10;Speaker: Postdoc B&#10;Content: Oh gosh , well , last night , I did about half an hour in , three hours , which is not , terrific ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: but , um , anyway , it 's an hour and a half per {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah . Well , that 's probably .&#10;Speaker: PhD A&#10;Content: So .&#10;Speaker: Postdoc B&#10;Content: Well , I can't calculate on my , {vocalsound} on my feet .&#10;Speaker: PhD A&#10;Content: Do the transcribers actually start wi with , uh , transcribing new meetings , or {pause} are they ?&#10;Speaker: Postdoc B&#10;Content: Well , um they 're still working {disfmarker} they still have enough to finish that I haven't assigned a new meeting ,&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker:" />
    <node id=" samples from twelve meetings that would only take an hour and I could get the transcribers to do that right {disfmarker} I mean , what I mean is , that would be an hour sampled , and then they 'd transcribe those {disfmarker} that hour , right ? That 's what I should do ?&#10;Speaker: Professor C&#10;Content: Yeah . And .&#10;Speaker: PhD A&#10;Content: That 's {disfmarker} that 's .&#10;Speaker: Postdoc B&#10;Content: I don't mean transcribe&#10;Speaker: Professor C&#10;Content: Right . Ye - But you 're {disfmarker} y&#10;Speaker: Postdoc B&#10;Content: I mean {disfmarker} I mean adjust . So they get it into the multi - channel format and then adjust the timebands so it 's precise .&#10;Speaker: Professor C&#10;Content: So that should be faster than the ten times kind of thing ,&#10;Speaker: Postdoc B&#10;Content: Absolutely . I did {disfmarker} I did , um , uh , so , last night I did , uh ,&#10;Speaker: Professor C&#10;Content:" />
    <node id=" you run a regular dictionary , um , even if you have variants , in there , which most people don't , you don't always get , out , the actual pronunciations ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: so that 's why the human transcriber 's giving you the {disfmarker} that pronunciation ,&#10;Speaker: Postdoc B&#10;Content: Yeah . Oh .&#10;Speaker: Professor C&#10;Content: Actually maybe they 're using phone recognizers .&#10;Speaker: PhD G&#10;Content: and so y they {disfmarker} they {disfmarker} I thought that they were {disfmarker}&#10;Speaker: Professor C&#10;Content: Is that what they 're doing ?&#10;Speaker: Grad F&#10;Content: They are .&#10;Speaker: Professor C&#10;Content: Oh , OK .&#10;Speaker: PhD G&#10;Content: we should catch up on what Steve is ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: uh {disfmarker} I think that would be a good i good idea .&#10;Speaker: Professor C&#10;Content" />
    <node id="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." />
    <node id=" um , that easy to go from the , dictionary , word pronuncia the dictionary phone pronunciation , to the gestural one without this intermediate or a syllable level kind of , representation .&#10;Speaker: Grad F&#10;Content: Well I don't think Morgan 's suggesting that we do that , though .&#10;Speaker: Professor C&#10;Content: Do you mean ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah , I mean , I I I 'm jus at the moment of course we 're just talking about what , to provide as a tool for people to do research who have different ideas about how to do it . So for instance , you might have someone who just has a wor has words with states , and has uh {disfmarker} uh , comes from articulatory gestures to that . And someone else , might actually want some phonetic uh intermediate thing . So I think it would be {disfmarker} be best to have all of it if we could . But {pause} um ,&#10;Speaker: Grad F&#10;Content: But {disfmarker} What I 'm imagining is a score - like notation , where each line is a particular feature" />
    <node id="&#10;Speaker: Postdoc B&#10;Content: It would seem to me that the points of articulation would be m more , g uh , I mean that 's {disfmarker} I think about articulatory features , I think about , points of articulation , which means , uh , rather than vowels .&#10;Speaker: Grad F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Points of articulation ? What do you mean ?&#10;Speaker: Postdoc B&#10;Content: So , is it , uh , bilabial or dental or is it , you know , palatal .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: Which {disfmarker} which are all like where {disfmarker} where your tongue comes to rest .&#10;Speaker: Professor C&#10;Content: Place , place .&#10;Speaker: PhD D&#10;Content: Place of ar place of articulation .&#10;Speaker: Grad F&#10;Content: Uvular .&#10;Speaker: PhD A&#10;Content: Place .&#10;Speaker: Postdoc B&#10;Content: Place . Thank you , what {disfmarker} whate" />
    <node id="&#10;Speaker: Postdoc B&#10;Content: W Well it seems like you could do both .&#10;Speaker: Grad F&#10;Content: maybe meeting data isn't the right corpus .&#10;Speaker: Postdoc B&#10;Content: I mean , I was thinking that it would be interesting , to do it with respect to , parts of Switchboard anyway , in terms of ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: uh {disfmarker} partly to see , if you could , generate first guesses at what the articulatory feature would be , based on the phone representation at that lower level .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: It might be a time gain . But also in terms of comparability of , um ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Well cuz the yeah , and then also , if you did it on Switchboard , you would have , the full continuum of transcriptions .&#10;Speaker: Postdoc B&#10;Content: what you gain Yep .&#10;Speaker: PhD D" />
    <node id=": PhD A&#10;Content: Place .&#10;Speaker: Postdoc B&#10;Content: Place . Thank you , what {disfmarker} whatev whatever I s said , that 's {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: OK .&#10;Speaker: Postdoc B&#10;Content: I really meant place .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: OK , I see .&#10;Speaker: Professor C&#10;Content: Yeah . OK we got our jargon then , OK .&#10;Speaker: Postdoc B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh .&#10;Speaker: PhD G&#10;Content: Well it 's also , there 's , really a difference between , the pronunciation models in the dictionary , and , the pronunciations that people produce . And , so , You get , some of that information from Steve 's work on the {disfmarker} on the labeling&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: and it really , I" />
    <node id="Speaker: PhD G&#10;Content: That 's what I meant is {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: an and in some places it would fill in , So {disfmarker} the kinds of gestural features are not everywhere .&#10;Speaker: Grad F&#10;Content: Well {disfmarker}&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: So there are some things that you don't have access to either from your ear or the spectrogram ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: but you know what phone it was and that 's about all you can {disfmarker} all you can say .&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: And then there are other cases where , nasality , voicing {disfmarker}&#10;Speaker: PhD D&#10;Content: It 's basically just having , multiple levels of {disfmarker} of , information and marking , on the signal .&#10;Speaker: PhD G&#10;Content" />
    <node id="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." />
    <node id=" I mean , we could {pause} easily , get a section , you know , like say a minute or so , from every meeting that we have so f from the newer ones that we 're working on , everyone that we have . And then , should provide this .&#10;Speaker: PhD A&#10;Content: If it 's not the first minute of {disfmarker} of the meeting , that {disfmarker} that 's OK with me , but , in {disfmarker} in the first minute , uh , Often there are some {disfmarker} some strange things going on which {disfmarker} which aren't really , well , for , which {disfmarker} which aren't re re really good . So . What {disfmarker} what I 'd quite like , perhaps , is , to have , some five minutes of {disfmarker} of {disfmarker} of different meetings , so .&#10;Speaker: Postdoc B&#10;Content: Somewhere not in the very beginning , five minutes , OK .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: And , then I wanted to ask you" />
    <node id="isfmarker} that , there is a system ,&#10;Speaker: PhD D&#10;Content: So , current {disfmarker} This week .&#10;Speaker: Postdoc B&#10;Content: Yes . Might not be what you need .&#10;Speaker: Grad F&#10;Content: Yeah , so if we could get a couple meetings done with that level of precision I think that would be a good idea .&#10;Speaker: PhD A&#10;Content: OK . Yeah .&#10;Speaker: Postdoc B&#10;Content: Oh , OK . Uh , how {disfmarker} how m much time {disfmarker} so the meetings vary in length , what are we talking about in terms of the number of minutes you 'd like to have as your {disfmarker} as your training set ?&#10;Speaker: PhD A&#10;Content: It seems to me that it would be good to have , a few minutes from {disfmarker} from different meetings , so . But I 'm not sure about how much .&#10;Speaker: Postdoc B&#10;Content: OK , now you 're saying different meetings because of different speakers or because of different audio quality or both or {disfmarker} ?&#10;" />
    <node id="Speaker: Postdoc B&#10;Content: OK , so then , if that 's five minutes per meeting we 've got like twelve minutes , twelve meetings , roughly , that I 'm {disfmarker} that I 've been working with , then {disfmarker}&#10;Speaker: Professor C&#10;Content: Of {disfmarker} of {disfmarker} of the meetings that you 're working with , how many of them are different , tha&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Professor C&#10;Content: are there any of them that are different than , these two meetings ?&#10;Speaker: Postdoc B&#10;Content: Well {disfmarker} oh wa in terms of the speakers or the conditions or the ?&#10;Speaker: Professor C&#10;Content: Yeah , speakers . Sorry .&#10;Speaker: PhD A&#10;Content: Yeah , that {disfmarker}&#10;Speaker: Postdoc B&#10;Content: Um , we have different combinations of speakers .&#10;Speaker: Professor C&#10;Content: So .&#10;Speaker: Postdoc B&#10;Content: I mean , just from what I 've seen , uh , there are some where ," />
    <node id="pause} future work . Well {disfmarker} well what I want to do is to {disfmarker} to look into cross - correlations for {disfmarker} for removing those , false overlaps .&#10;Speaker: Postdoc B&#10;Content: Wonderful .&#10;Speaker: PhD G&#10;Content: Are the , um , wireless , different than the wired , mikes , at all ? I mean , have you noticed any difference ?&#10;Speaker: PhD A&#10;Content: I 'm {disfmarker} I 'm not sure , um , if {disfmarker} if there are any wired mikes in those meetings , or , uh , I have {disfmarker} have to loo have a look at them but , I 'm {disfmarker} I 'm {disfmarker} I think there 's no difference between ,&#10;Speaker: PhD G&#10;Content: So it 's just the lapel versus everything else ?&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Postdoc B&#10;Content: OK , so then , if that 's five minutes per meeting we 've got like twelve minutes , twelve meetings" />
    <node id="Content: OK , now you 're saying different meetings because of different speakers or because of different audio quality or both or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Both {disfmarker} both . Different {disfmarker} different number of speakers , different speakers , different {pause} conditions .&#10;Speaker: Postdoc B&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Yeah , we don't have that much variety in meetings yet , uh , I mean we have this meeting and the feature meeting and we have a couple others that we have uh , couple examples of . But {disfmarker} but , uh ,&#10;Speaker: PhD A&#10;Content: Yeah , m Yeah .&#10;Speaker: Postdoc B&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: Even probably with the gains {pause} differently will affect it , you mean {disfmarker}&#10;Speaker: PhD A&#10;Content: Uh , not really as {disfmarker}&#10;Speaker: Professor C&#10;Content: Poten - potentially .&#10;Speaker: PhD A&#10;Content: uh , because of the normalization , yeah .&#10;" />
    <node id="The &quot;blending thing&quot; referred to in the spectrograms is the observation that the speech signal doesn't always correspond to a clear sequence of phones as given in dictionary pronunciations. Instead, there are instances where different features (such as voicing or nasality) from different phones overlap and blend together in complex ways. This blending is not easily named or categorized, which makes it difficult to represent using traditional phonetic transcriptions. The speakers suggest that this blending might be better represented using a score-like notation where each line corresponds to a particular feature, allowing for a more accurate representation of the overlapping and complex nature of these speech signals." />
    <node id=" here and he was looking at the spectrograms of the more difficult ones . Uh , he didn't know what to say , about , what is the sequence of phones there . They came up with some compromise . Because that really wasn't what it look like . It didn't look like a sequence of phones&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: it look like this blending thing happening here and here and here .&#10;Speaker: Grad F&#10;Content: Yeah , so you have this feature here , and , overlap , yeah .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: There was no name for that .&#10;Speaker: PhD G&#10;Content: But {disfmarker} Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: But it still is {disfmarker} there 's a {disfmarker} there are two steps . One {disfmarker} you know , one is going from a dictionary pronunciation of something , like , &quot; gonna see" />
    <node id=" Professor C&#10;Content: Right .&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: and it really , I actually think that data should be used more . That maybe , although I think the meeting context is great , that he has transcriptions that give you the actual phone sequence . And you can go from {disfmarker} not from that to the articulatory features , but that would be a better starting point for marking , the gestural features , then , data where you don't have that , because , we {disfmarker} you wanna know , both about the way that they 're producing a certain sound , and what kinds of , you know what kinds of , phonemic , differences you get between these , transcribed , sequences and the dictionary ones .&#10;Speaker: Professor C&#10;Content: Well you might be right that mi might be the way at getting at , what I was talking about , but the particular reason why I was interested in doing that was because I remember , when that happened , and , John Ohala was over here and he was looking at the spectrograms of the more difficult ones . Uh , he didn't know what to say , about , what is the sequence" />
    <node id=": Grad F&#10;Content: But {disfmarker} What I 'm imagining is a score - like notation , where each line is a particular feature .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Right ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: so you would say , you know , it 's voiced through here , and so you have label here , and you have nas nasal here , and , they {disfmarker} they could be overlapping in all sorts of bizarre ways that don't correspond to the timing on phones .&#10;Speaker: Professor C&#10;Content: I mean this is the kind of reason why {disfmarker} I remember when at one of the Switchboard , workshops , that uh when we talked about doing the transcription project , Dave Talkin said , &quot; can't be done &quot; .&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: He was {disfmarker} he was , what {disfmarker} what he meant was that this isn't , you know , a sequence of phones , and when you actually look at Switchboard that" />
    <node id=" the transcription of what was really said , and which syllables were reduced . Uh , if you 're gonna add the features it 's also useful to have some level of representation which is , is a reduced {disfmarker} it 's a pronunciation variant , that currently the dictionaries don't give you&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: because if you add them to the dictionary and you run recognition , you , you add confusion .&#10;Speaker: Professor C&#10;Content: Right . Right .&#10;Speaker: PhD G&#10;Content: So people purposely don't add them . So it 's useful to know which variant was {disfmarker} was produced , at least at the phone level .&#10;Speaker: PhD D&#10;Content: So it would be {disfmarker} it would be great if we had , either these kind of , labelings on , the same portion of Switchboard that Steve marked , or , Steve 's type markings on this data , with these .&#10;Speaker: PhD G&#10;Content: Right . That 's all , I mean . Exactly .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;" />
    <node id="1. The plan for investigating cross-correlations to remove false overlaps in the future involves PhD A looking into this area further. They have already done some preliminary work with three meetings, which seemed to yield good results, despite some issues with lapel microphones. Cross-correlations may be used to address these lapel microphone problems.&#10;2. There is no explicit mention in the transcript of a noticeable difference between wireless and wired microphones in those meetings. When PhD G asked if wireless microphones were different from wired ones, PhD A expressed uncertainty about whether there were even any wired microphones present in the meetings to compare with. Later, they confirmed that the distinction was merely lapel versus everything else, implying no significant difference between wireless and wired mikes." />
    <node id="&#10;Speaker: PhD A&#10;Content: So , uh it c can be do in an unsupervised way .&#10;Speaker: Postdoc B&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: So .&#10;Speaker: Postdoc B&#10;Content: Excellent . Excellent , OK .&#10;Speaker: PhD A&#10;Content: I 'm {disfmarker} I 'm not sure , but , for {disfmarker} for {disfmarker} for those three meetings whi which I {disfmarker} which I did , it seems to be , quite well , but , there are some {disfmarker} some {disfmarker} as I said some problems with the lapel mike , but , perhaps we can do something with {disfmarker} with cross - correlations to , to get rid of the {disfmarker} of those . And . Yeah . That 's {disfmarker} that 's what I {disfmarker} that 's my {pause} future work . Well {disfmarker} well what I want to do is to {disfmarker} to look into cross - correlations" />
    <node id="aker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: uh , but for free recognition I 'm {disfmarker} it 'll probably not be good enough . We 'll probably get lots of errors because of the cross - talk , and , noises and things .&#10;Speaker: PhD A&#10;Content: Yep .&#10;Speaker: Professor C&#10;Content: Good s I think that 's probably our agenda , or starting up there .&#10;Speaker: Postdoc B&#10;Content: Oh I wanted to ask one thing , the microphones {disfmarker} the new microphones ,&#10;Speaker: Professor C&#10;Content: Yeah ? K .&#10;Speaker: Postdoc B&#10;Content: when do we get , uh ?&#10;Speaker: Grad F&#10;Content: Uh , they said it would take about a week .&#10;Speaker: Postdoc B&#10;Content: Oh , exciting . K . K .&#10;Speaker: Professor C&#10;Content: K .&#10;Speaker: PhD D&#10;Content: You ordered them already ?&#10;Speaker: Grad F&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Great .&#10;Speaker: PhD G&#10;Content" />
    <node id="The speakers, including Postdoc B, Grad F, and PhD G, expressed interest in the idea of iterating and developing models to perform a forced alignment and acquire the data. They discussed the importance of refining the process and the potential benefits of such an approach. However, they also acknowledged that the success of this method would depend on various factors, and it might not always yield optimal results. PhD G mentioned that they would let the group know when they have a solution for forced alignment." />
    <node id=": Yeah . Yeah . Anyway , this is , not an urgent thing at all ,&#10;Speaker: Postdoc B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: just it came up .&#10;Speaker: PhD D&#10;Content: It 'd be very interesting though , to have {pause} that data .&#10;Speaker: Postdoc B&#10;Content: I think so , too .&#10;Speaker: Grad F&#10;Content: I wonder , how would you do a forced alignment ?&#10;Speaker: PhD G&#10;Content: Yeah . Might {disfmarker}&#10;Speaker: Postdoc B&#10;Content: Interesting idea .&#10;Speaker: Grad F&#10;Content: To {disfmarker} to {disfmarker} I mean , you 'd wanna iterate , somehow . Yeah . It 's interesting thing to think about .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: PhD G&#10;Content: It might be {disfmarker}&#10;Speaker: Grad F&#10;Content: I mean you 'd {disfmarker} you 'd want models for spreading .&#10;Speaker: PhD G&#10;Content: I was thinking it might be n&#10;Spe" />
    <node id="Speaker: Postdoc B&#10;Content: Alright .&#10;Speaker: Professor C&#10;Content: So , uh {disfmarker}&#10;Speaker: Grad F&#10;Content: Um , so I wanted to discuss digits briefly , but that won't take too long .&#10;Speaker: Professor C&#10;Content: Oh good . Right . OK , agenda items , Uh , we have digits , What else we got ?&#10;Speaker: PhD A&#10;Content: New version of the presegmentation .&#10;Speaker: Professor C&#10;Content: New version of presegmentation .&#10;Speaker: Postdoc B&#10;Content: Um , do we wanna say something about the , an update of the , uh , transcript ?&#10;Speaker: PhD G&#10;Content: Yeah , why don't you summarize the {disfmarker}&#10;Speaker: Professor C&#10;Content: Update on transcripts .&#10;Speaker: PhD G&#10;Content: And I guess that includes some {disfmarker} the filtering for the , the ASI refs , too .&#10;Speaker: Postdoc B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: Filtering for what ?&#10;Speaker: PhD G&#10;Content:" />
    <node id="Content: It {disfmarker} you know it really depends on a lot of things ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: but , I would have maybe a transciber , uh , look at the result of a forced alignment and then adjust those .&#10;Speaker: PhD A&#10;Content: Yeah . To a adjust them , or , yeah . Yeah , yeah .&#10;Speaker: PhD G&#10;Content: That might save some time .&#10;Speaker: PhD A&#10;Content: Yeah , great .&#10;Speaker: PhD G&#10;Content: If they 're horrible it won't help at all , but they might not be horrible .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: So {disfmarker} but I 'll let you know when we , uh , have that .&#10;Speaker: PhD A&#10;Content: OK , great .&#10;Speaker: Postdoc B&#10;Content: How many minutes would you want from {disfmarker} I mean , we could {pause} easily , get a section , you know , like say a minute or so , from every meeting that we have so f" />
    <node id=" .&#10;Speaker: PhD G&#10;Content: So {disfmarker}&#10;Speaker: Professor C&#10;Content: There {disfmarker} the first {disfmarker} the first half .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: And so I was in the process of like editing them but this is wonderful news .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Alright .&#10;Speaker: Postdoc B&#10;Content: We funded the experiment with , uh {disfmarker} also we were thinking maybe applying that that to getting the , Yeah , that 'll be , very useful to getting the overlaps to be more precise all the way through .&#10;Speaker: Professor C&#10;Content: So this , blends nicely into the update on transcripts .&#10;Speaker: Postdoc B&#10;Content: Yes , it does . So , um , {comment} um , Liz , and {disfmarker} and Don , and I met this morning , in the BARCO room , with the lecture hall ,&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD G&#10;Content:" />
    <node id=": PhD G&#10;Content: uh {disfmarker} I think that would be a good i good idea .&#10;Speaker: Professor C&#10;Content: Yeah , so I think that i i we also don't have , I mean , we 've got a good start on it , but we don't have a really good , meeting , recorder or recognizer or transcriber or anything yet , so . So , I mean another way to look at this is to , is to , uh , do some stuff on Switchboard which has all this other , stuff to it .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: And then , um , As we get , further down the road and we can do more things ahead of time , we can , do some of the same things to the meeting data .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: And I 'm {disfmarker} and these people might {disfmarker} they {disfmarker} they are , s most of" />
    <node id="1. The &quot;real situation&quot; of car noise in this study refers to the background noise recorded from people driving around in a car, without any artificial addition of noise to manipulate the signal-to-noise ratio. This is different from other studies where an artificial signal-to-noise ratio might be added to simulate real-world conditions. The advantage of using real car noise is that it provides a more authentic and accurate representation of the environment in which speech recognition systems would actually be used. However, it may also present additional challenges due to its complexity and variability.&#10;2. The models' performance was not as good as the best score because, as acknowledged by Professor C, &quot;the models are pretty crappy.&quot; Despite the best score being a relatively low error rate of 5% per digit, this is still far from perfect accuracy. It is possible that the complexity and variability of the car noise data made it difficult for the models to achieve higher levels of accuracy. Additionally, the use of lapel microphones in the meetings may have introduced additional challenges due to their propensity for picking up unwanted background noise.&#10;&#10;In summary, the &quot;real situation&quot; of car noise in this study provides a more authentic representation of real-world conditions than artificial signal-to-noise ratios, but it also presents additional challenges for speech recognition systems. The models' performance was not as good as the best score due to their inherent limitations and the complexity of the data." />
    <node id=" and this is uh car noise uh , uh things , but {disfmarker} but real {disfmarker} real situation ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: well , &quot; real &quot; , Um , the {disfmarker} uh there 's one microphone that 's close , that they have as {disfmarker} as this sort of thing , close versus distant . Uh but in a car , instead of {disfmarker} instead of having a projector noise it 's {disfmarker} it 's car noise . Uh but it wasn't artificially added to get some {disfmarker} some artificial signal - to - noise ratio . It was just people driving around in a car . So , that 's {disfmarker} that 's an indication , uh that was with , many sites competing , and this was the very best score and so forth , so . More typical numbers like&#10;Speaker: PhD D&#10;Content: Although the models weren't , that good , right ? I mean , the models are pretty crappy ?&#10;Speaker: Professor C&#10;Content: You 're right . I" />
    <node id=" . But , either way .&#10;Speaker: Professor C&#10;Content: Yeah . Yeah just by way of uh , uh , a uh , order of magnitude , uh , um , we 've been working with this Aurora , uh data set . And , uh , the best score , on the , nicest part of the data , that is , where you 've got training and test set that are basically the same kinds of noise and so forth , uh , is about , uh {disfmarker} I think the best score was something like five percent , uh , error , per digit .&#10;Speaker: PhD A&#10;Content: Per digit .&#10;Speaker: Professor C&#10;Content: So , that {disfmarker}&#10;Speaker: Grad F&#10;Content: Per digit .&#10;Speaker: Professor C&#10;Content: You 're right . So if you were doing {pause} ten digit , uh , recognition , {vocalsound} you would really be in trouble . So {disfmarker} So the {disfmarker} The point there , and this is uh car noise uh , uh things , but {disfmarker} but real {disfmarker} real situation ,&#10;Speaker" />
    <node id="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways." />
    <node id="er} You know I hadn't , uh , incorporated , a convention explicitly to handle acronyms , for example , but if someone says , PZM it would be nice to have that be directly interpretable from , the transcript what they said ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: or Pi - uh Tcl {disfmarker} TCL I mean . It 's like y it 's {disfmarker} and so , um , I 've {disfmarker} I 've incorporated also convention , with that but that 's easy to handle at the post editing phase , and I 'll mention it to , transcribers for the next phase but that 's OK . And then , a similar conv uh , convention for numbers . So if they say one - eighty - three versus one eight three . Um , and also I 'll be , um , encoding , as I do my post - editing , the , things that are in curly brackets , which are clarificational material . And eh to incorporate , uh , keyword , at the beginning . So , it 's gonna be either a gloss or it 's gonna be" />
    <node id=" {disfmarker} you 'd want models for spreading .&#10;Speaker: PhD G&#10;Content: I was thinking it might be n&#10;Speaker: PhD D&#10;Content: Of the f acoustic features ?&#10;Speaker: Grad F&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Well it might be neat to do some , phonetic , features on these , nonword words . Are {disfmarker} are these kinds of words that people never {disfmarker} the &quot; huh &quot;s and the &quot; hmm &quot;s and the &quot; huh &quot; {vocalsound} and the uh {disfmarker} These k No , I 'm serious . There are all these kinds of {pause} functional , uh , elements . I don't know what you call {pause} them . But not just fill pauses but all kinds of ways of {pause} interrupting {comment} and so forth .&#10;Speaker: Grad F&#10;Content: Uh - huh .&#10;Speaker: PhD G" />
    <node id="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." />
    <node id=" B&#10;Content: We could , I mean you {disfmarker} you recorded one last week or so . I could get that new one in this week {disfmarker} I get that new one in .&#10;Speaker: Grad F&#10;Content: Yep . u&#10;Speaker: PhD G&#10;Content: We 're gonna be recording them every {pause} Monday ,&#10;Speaker: Professor C&#10;Content: Yeah . Cuz I think he really needs variety ,&#10;Speaker: PhD G&#10;Content: so {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: Great .&#10;Speaker: Professor C&#10;Content: and {disfmarker} and having as much variety for speaker certainly would be a big part of that I think .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: OK , so if I , OK , included {disfmarker} include , OK , then , uh , if I were to include all together samples from twelve meetings that would only take an hour and I could get the transcribers to do that right {disfmarker} I mean , what" />
    <node id="1. Acoustic features: The lowest level of transcription would include acoustic features, which involve physical measurements of the speech signal such as frequency, amplitude, and duration.&#10;&#10;2. Phonetic level: This refers to the representation of speech sounds using symbols from the International Phonetic Alphabet (IPA). It includes detailed information about the pronunciation of each word, including distinctions between similar sounds like &quot;p&quot; and &quot;b&quot; or &quot;s&quot; and &quot;z&quot;.&#10;&#10;3. Relative gain: This term likely refers to the relative loudness or volume of different parts of the speech signal. Including this information in the transcriptions would provide insight into how loudly or softly certain words or sounds are spoken.&#10;&#10;4. Telephone bandwidth limitations: Since some of the discussions involve telephone-quality audio, it is important to consider the impact of limited bandwidth on the quality and detail of the transcriptions. This might result in less precise acoustic measurements or phonetic representations, as well as potential difficulties in accurately distinguishing between different speakers' voices.&#10;&#10;5. Articulatory gestures: Although not explicitly mentioned in the provided excerpt, Professor C suggests including articulatory features in the transcriptions. This would involve representing the movements and positions of the speaker's speech articulators (e.g., tongue, lips, jaw) during speech production.&#10;&#10;6. Comparability on Switchboard: If using the Switchboard dataset, the full continuum of transcriptions should include information about how each word or sound compares to others in terms of acoustic and phonetic properties. This would facilitate analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts." />
    <node id=" you would have , the full continuum of transcriptions .&#10;Speaker: Postdoc B&#10;Content: what you gain Yep .&#10;Speaker: PhD D&#10;Content: You 'd have it , from the lowest level , the ac acoustic features , then you 'd have the , you know , the phonetic level that Steve did ,&#10;Speaker: Postdoc B&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: Yeah that {disfmarker} that 's all I was thinking about .&#10;Speaker: Postdoc B&#10;Content: And you could tell that {disfmarker}&#10;Speaker: PhD D&#10;Content: and , yeah .&#10;Speaker: PhD G&#10;Content: it is telephone band , so , the bandwidth might be {disfmarker}&#10;Speaker: PhD D&#10;Content: It 'd be a complete , set then .&#10;Speaker: Postdoc B&#10;Content: And you get the relative gain up ahead .&#10;Speaker: Professor C&#10;Content: It 's so it 's a little different . So I mean i we 'll see wha how much we can , uh , get the people to do , and how much money" />
    <node id="Based on the transcript, the exact time it would take to transcribe the compiled audio samples from twelve meetings is not explicitly mentioned. However, Postdoc B proposed recording five-minute segments from various meetings, which would result in one hour of audio samples. They also mention that they have already processed half an hour of material.&#10;&#10;The feasibility of this task depends on several factors, such as the number of speakers, audio quality, and the effectiveness of any pre-processing techniques or conventions for encoding clarificational material and keywords. The transcript mentions concerns about accurately processing and transcribing short segments of speech, particularly when a single word can be placed incorrectly, leading to misunderstandings or errors.&#10;&#10;To make this task more feasible, several suggestions were made, including pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for precise representation of audio data. Implementing these improvements could help ensure better transcription accuracy and make the task more manageable." />
    <node id="1. The plan for examining cross-correlations to eliminate false overlaps involves PhD A looking into this area further, based on some preliminary work with three meetings that showed promising results. They aim to investigate the use of cross-correlations to address issues with lapel microphones, which may be causing some of these false overlaps.&#10;2. There was no explicit mention or noticeable difference observed between wireless and wired microphones during the meetings. The distinction brought up by PhD G was between lapel microphones and all other types of microphones, regardless of whether they were wireless or wired." />
    <node id="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature." />
    <node id=" weren't , that good , right ? I mean , the models are pretty crappy ?&#10;Speaker: Professor C&#10;Content: You 're right . I think that we could have done better on the models , but the thing is that we got {disfmarker} this {disfmarker} this is the kind of typical number , for all of the , uh , uh , things in this task , all of the , um , languages . And so I {disfmarker} I think we 'd probably {disfmarker} the models would be better in some than in others . Um , so , uh . Anyway , just an indication once you get into this kind of realm even if you 're looking at connected digits it can be pretty hard .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Postdoc B&#10;Content: Hmm . It 's gonna be fun to see how we , compare at this . Very exciting . s @ @ .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: How did we do on the TI - digits ?&#10;Speaker: Grad F&#10;Content: Well the prosodics are so much different s it 's" />
    <node id=" D&#10;Content: H How&#10;Speaker: Grad F&#10;Content: But in TI - digits , they 're reading things like zip codes and phone numbers and things like that ,&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: do we do on TI - digits ?&#10;Speaker: Grad F&#10;Content: so it 's gonna be different . I don't remember . I mean , very good , right ?&#10;Speaker: Professor C&#10;Content: Yeah , I mean we were in the .&#10;Speaker: Grad F&#10;Content: One and a half percent , two percent , something like that ?&#10;Speaker: Professor C&#10;Content: Uh , I th no I think we got under a percent , but it was {disfmarker} but it 's {disfmarker} but I mean . The very best system that I saw in the literature was a point two five percent or something that somebody had at {disfmarker} at Bell Labs , or . Uh , but . But , uh , sort of pulling out all the stops .&#10;Speaker: Grad F&#10;Content: Oh really ?&#10;Speaker: Postdoc B&#10;Content: s @ @ ." />
    <node id="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs." />
    <node id="aker: Grad F&#10;Content: Aurora I don't know . I don't know what they do in Aurora .&#10;Speaker: PhD G&#10;Content: or , a {disfmarker} a digit at a time , or {disfmarker} ?&#10;Speaker: Professor C&#10;Content: Uh , I 'm not sure how {disfmarker}&#10;Speaker: PhD G&#10;Content: Cuz it 's {disfmarker}&#10;Speaker: Professor C&#10;Content: no , no I mean it 's connected {disfmarker} it 's connected , uh , digits ,&#10;Speaker: PhD G&#10;Content: Connected .&#10;Speaker: Professor C&#10;Content: yeah . But .&#10;Speaker: Grad F&#10;Content: But {disfmarker} Right .&#10;Speaker: PhD G&#10;Content: So there 's also the {disfmarker} not just the prosody but the cross {disfmarker} the cross - word modeling is probably quite different .&#10;Speaker: PhD D&#10;Content: H How&#10;Speaker: Grad F&#10;Content: But in TI - digits , they 're reading things like zip codes and phone numbers" />
    <node id=" the , w uh as you can see from the numbers on the digits we 're almost done . The digits goes up to {pause} about four thousand . Um , and so , uh , we probably will be done with the TI - digits in , um , another couple weeks . um , depending on how many we read each time . So there were a bunch that we skipped . You know , someone fills out the form and then they 're not at the meeting and so it 's blank . Um , but those are almost all filled in as well . And so , once we 're {disfmarker} it 's done it would be very nice to train up a recognizer and actually start working with this data .&#10;Speaker: PhD D&#10;Content: So we 'll have a corpus that 's the size of TI - digits ?&#10;Speaker: Grad F&#10;Content: And so {disfmarker} One particular test set of TI - digits .&#10;Speaker: PhD D&#10;Content: Test set , OK .&#10;Speaker: Grad F&#10;Content: So , I {disfmarker} I extracted , Ther - there was a file sitting around which people have used here as a test set . It had" />
    <node id="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time." />
    <node id=" a little concerned that maybe the kind of phenomena , in w i i The reason for doing it is because the {disfmarker} the argument is that certainly with conversational speech , the stuff that we 've looked at here before , um , just doing the simple mapping , from , um , the phone , to the corresponding features that you could look up in a book , uh , isn't right . It isn't actually right . In fact there 's these overlapping processes where some voicing some up and then some , you know , some nasality is {disfmarker} comes in here , and so forth . And you do this gross thing saying &quot; Well I guess it 's this phone starting there &quot; . So , uh , that 's the reasoning . But , It could be that when we 're reading digits , because it 's {disfmarker} it 's for such a limited set , that maybe {disfmarker} maybe that phenomenon doesn't occur as much . I don't know . Di - an anybody {disfmarker} ? {pause} Do you have any {disfmarker} ? {pause} Anybody have any opinion about that ,&#10;Speaker: Postdoc B&#10;Content" />
    <node id="The speakers, specifically PhD A, are interested in using manually transcribed references for comparison with other automatic systems or found boundaries. They express uncertainty about the reliability of automatically determined boundaries. Both PhD A and Postdoc B agree that manually tweaking the current method could be an option to improve its accuracy. However, there are concerns about the feasibility of accurately processing and transcribing short segments of speech, which might lead to misunderstandings or errors in transcription due to factors like overlapping processes (voicing and nasality) during speech production. Strategies to address these issues include pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for better representation of audio data." />
    <node id=" but , I don't know yet whether these , segments that contain a lot of pauses around the words , will work or not .&#10;Speaker: PhD A&#10;Content: I {disfmarker} I would quite like to have some manually transcribed references for {disfmarker} for the system , as I 'm not sure if {disfmarker} if it 's really good to compare with {disfmarker} with some other automatic , found boundaries .&#10;Speaker: PhD G&#10;Content: Yeah . Right .&#10;Speaker: Postdoc B&#10;Content: Well , no , if we were to start with this and then tweak it h manually , would that {disfmarker} that would be OK ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: Yeah {pause} sure .&#10;Speaker: PhD G&#10;Content: They might be OK .&#10;Speaker: Postdoc B&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: It {disfmarker} you know it really depends on a lot of things ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;" />
    <node id="1. M Four: The speakers briefly used the term &quot;M four&quot; to refer to a meeting about meetings up to the fourth level (a meeting about a meeting about a meeting about meetings). This was suggested as an alternative to repeatedly saying &quot;the meeting about the meeting about...&quot;&#10;&#10;2. Pauses between lines: Grad F reminded the group to include pauses between the lines when transcribing or analyzing the content, likely for better readability or more accurate processing.&#10;&#10;3. Crown P Z: The group discussed using audio samples from the Crown P Z (possibly a collection of recordings), which would provide diverse speech samples for their project. However, they acknowledged that some segments might not be useful due to variations in audio quality and speakers.&#10;&#10;4. Iterating and developing models: Postdoc B, Grad F, and PhD G expressed interest in refining the process of iterating and creating models to perform forced alignment and acquire data. They recognized that this method's success would depend on various factors and might not always yield optimal results. PhD G mentioned they would inform the group when a solution for forced alignment is available.&#10;&#10;5. Cross-correlations and false overlaps: The group discussed recording five-minute segments from various meetings (excluding the first minute) to analyze cross-correlations and remove false overlaps during recordings. This approach aims to account for different speakers and varying audio quality in the samples. Postdoc B agreed to prepare a set of such segments from twelve meetings for further analysis.&#10;&#10;The main objective of the meeting was to explore ways of processing and transcribing speech samples from various sources, with an emphasis on improving accuracy and removing false overlaps during recordings." />
    <node id=" god .&#10;Speaker: Grad F&#10;Content: Cuz then it would be a meeting about the meeting about the meeting about meetings .&#10;Speaker: Postdoc B&#10;Content: &#10;Speaker: Professor C&#10;Content: Yeah ? Just start saying &quot; M four &quot; . Yeah , OK .&#10;Speaker: Grad F&#10;Content: Yeah . M to the fourth .&#10;Speaker: Professor C&#10;Content: Should we do the digits ?&#10;Speaker: Grad F&#10;Content: Yep , go for it .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: S {pause} s&#10;Speaker: Grad F&#10;Content: Pause between the lines , remember ?&#10;Speaker: Grad E&#10;Content: Excuse me .&#10;Speaker: Grad F&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: Huh ." />
    <node id=": That says a lot .&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: For the recor for the record Adam is not a paid employee or a consultant of Crown .&#10;Speaker: Grad F&#10;Content: and {disfmarker} Excuse me ?&#10;Speaker: Postdoc B&#10;Content: Oh .&#10;Speaker: Professor C&#10;Content: I said &quot; For the record Adam is {disfmarker} is not a paid consultant or employee of Crown &quot; .&#10;Speaker: Grad F&#10;Content: Excuse me ?&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Grad F&#10;Content: That 's right .&#10;Speaker: PhD G&#10;Content: However , he may be solicited after these meetings are distributed .&#10;Speaker: Grad F&#10;Content: Well we 're using the Crown P Z&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Don't worry about finishing your dissertation .&#10;Speaker: Grad F&#10;Content: These are Crown aren't they ?&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: Grad F&#10;Content: The P Z" />
    <node id="The speakers in the meeting discussed the possibility of having transcribers also handle recording and formatting audio clips directly in the transcripts. This idea was raised by Grad F, who mentioned that they had personally processed eight meetings manually. Professor C asked for opinions on this matter. Postdoc B expressed support for the idea, stating that it would add an extra dimension to the transcribers' skill set and contribute to the composition of the transcript by incorporating those numbers directly.&#10;&#10;The potential benefits of having transcribers handle recording and formatting audio clips in the transcripts include:&#10;&#10;1. Enhancing transcribers' skill set: By performing these tasks, transcribers can develop additional skills beyond simple transcription.&#10;2. Improved integration of audio data: Directly incorporating audio clips into the transcripts allows for a more seamless combination of textual and audio information, potentially facilitating further analysis.&#10;3. Time-saving: If transcribers are already familiar with the content and context of the meetings, they might be able to efficiently record and format relevant audio clips while working on the transcripts.&#10;4. Better understanding of the material: Engaging in the recording and formatting process can provide transcribers with a deeper understanding of the content and its structure, which may lead to more accurate transcriptions.&#10;5. Increased control over audio data: By handling audio clips directly, transcribers have greater control over how the audio data is presented and integrated into the transcripts, potentially reducing errors or misunderstandings caused by automatic processing." />
    <node id=" two &quot; . And you find that , and , hit the key and it records it in a file in a particular format .&#10;Speaker: Professor C&#10;Content: So is this {disfmarker}&#10;Speaker: Grad F&#10;Content: And so the {disfmarker} the question is , should we have the transcribers do that or should we just do it ? Well , some of us . I 've been do I 've done , eight meetings , something like that , just by hand . Just myself , rather . So it will not take long . Um {disfmarker}&#10;Speaker: Professor C&#10;Content: Uh , what {disfmarker} what do you think ?&#10;Speaker: Postdoc B&#10;Content: My feeling is that we discussed this right before coffee and I think it 's a {disfmarker} it 's a fine idea partly because , um , it 's not un unrelated to their present skill set , but it will add , for them , an extra dimension , it might be an interesting break for them . And also it is contributing to the , uh , c composition of the transcript cuz we can incorporate those numbers directly and it 'll be a" />
    <node id="Based on the transcript, Grad F used normalized features to distinguish between foreground and background speech in multiple channels of a recording. The normalization techniques included:&#10;&#10;1. Normalized energy: Calculated by dividing the energy of each channel by the mean energy over all channels and the minimum energy within each channel.&#10;2. Normalization of loudness, spectral features (such as modified loudness), and other unspecified features. These were also incorporated into the feature vector for improved distinction between foreground and background speech.&#10;&#10;The system was tested on three or four meetings and seemed to work fairly well, although there were some issues with lapel microphones. The performance of these normalization techniques in distinguishing between foreground and background speech appears to be promising, but a more thorough evaluation and potential optimization may be required for better results." />
    <node id="ation to {disfmarker} to get another version which does channel - specific , uh , speech - nonspeech detection . And , what I did is I used some normalized features which , uh , look in into the {disfmarker} which is normalized energy , uh , energy normalized by the mean over the channels and by the , minimum over the , other . within each channel . And to {disfmarker} to , mm , to , yeah , to normalize also loudness and {disfmarker} and modified loudness and things and that those special features actually are in my feature vector .&#10;Speaker: Grad F&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: And , and , therefore to be able to , uh , somewhat distinguish between foreground and background speech in {disfmarker} in the different {disfmarker} in {disfmarker} each channel . And , eh , I tested it on {disfmarker} on three or four meetings and it seems to work , well yeah , fairly well , I {disfmarker} I would say . There are some problems with the lapel mike .&#10;Speaker: Grad F&#10;Content: Of" />
    <node id="1. Experiment Funding Update: Postdoc B mentioned that they had successfully funded their experiment. Although the source of funding is not specified in the transcript, it's clear that the project has received sufficient financial support to proceed." />
    <node id="Entering additional information such as names, times, and dates found at the top of a form is important for transcribers when automatically recording digital data to ensure that the transcripts are well-organized, easily identifiable, and searchable. This metadata can help in managing and locating specific transcripts, improving the overall efficiency of using the transcribed materials. It also contributes to creating a more complete and comprehensive transcript by incorporating relevant contextual information directly." />
    <node id=" for them . And also it is contributing to the , uh , c composition of the transcript cuz we can incorporate those numbers directly and it 'll be a more complete transcript . So I 'm {disfmarker} I think it 's fine , that part .&#10;Speaker: Grad F&#10;Content: There is {disfmarker} there is {disfmarker}&#10;Speaker: Professor C&#10;Content: So you think it 's fine to have the transcribers do it ?&#10;Speaker: Postdoc B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Yeah , OK .&#10;Speaker: Grad F&#10;Content: There 's one other small bit , which is just entering the information which at s which is at the top of this form , onto the computer , to go along with the {disfmarker} where the digits are recorded automatically .&#10;Speaker: PhD D&#10;Content: Good .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: And so it 's just , you know , typing in name , times {disfmarker} time , date , and so on . Um , which again" />
    <node id="It is suggested that the same person enters and extracts the data as a double-check to ensure accuracy in the data entry process. This is because when one person handles both tasks, they can cross-reference and verify the data more effectively, minimizing the chance of errors. Although these two tasks do not necessarily need to be performed simultaneously, having the same person responsible for both tasks helps maintain consistency and increases the likelihood of accurate data." />
    <node id="marker}&#10;Speaker: Grad F&#10;Content: No they don't have {disfmarker} this {disfmarker} you have to enter the data before , you do the second task , but they don't have to happen at the same time .&#10;Speaker: PhD D&#10;Content: OK .&#10;Speaker: Grad F&#10;Content: So it 's {disfmarker} it 's just I have a file whi which has this information on it , and then when you start using my scripts , for extracting the times , it adds the times at the bottom of the file . And so , um , I mean , it 's easy to create the files and leave them blank , and so actually we could do it in either order .&#10;Speaker: PhD D&#10;Content: Oh , OK .&#10;Speaker: Grad F&#10;Content: Um , it 's {disfmarker} it 's sort of nice to have the same person do it just as a double - check , to make sure you 're entering for the right person . But , either way .&#10;Speaker: Professor C&#10;Content: Yeah . Yeah just by way of uh , uh , a uh , order of magnitude" />
    <node id=" so it 's just , you know , typing in name , times {disfmarker} time , date , and so on . Um , which again either they can do , but it is , you know , firing up an editor , or , again , I can do . Or someone else can do .&#10;Speaker: Postdoc B&#10;Content: And , that , you know , I 'm not , that {disfmarker} that one I 'm not so sure if it 's into the {disfmarker} the , things that , I , wanted to use the hours for , because the , the time that they 'd be spending doing that they wouldn't be able to be putting more words on .&#10;Speaker: Professor C&#10;Content: Mmm .&#10;Speaker: Postdoc B&#10;Content: But that 's really your choice , it 's your {disfmarker}&#10;Speaker: PhD D&#10;Content: So are these two separate tasks that can happen ? Or do they have to happen at the same time before {disfmarker}&#10;Speaker: Grad F&#10;Content: No they don't have {disfmarker} this {disfmarker} you have" />
    <node id="1. Limited pronunciation variants: While some dictionaries may include multiple pronunciations for a given word, many do not. This can lead to incomplete or inaccurate transcriptions if the chosen dictionary does not provide all necessary variants.&#10;&#10;2. Telephone bandwidth limitations: Dictionaries based on telephone-quality audio might result in less precise acoustic measurements or phonetic representations due to the limited bandwidth available in such recordings.&#10;&#10;3. Inability to capture human nuances: Dictionaries may not account for regional accents, dialects, or other factors that could influence how a word is pronounced in natural speech. As a result, transcriptions generated from dictionaries might fail to accurately represent these variations.&#10;&#10;4. Lack of articulatory gestures: Regular dictionaries typically do not include information about articulatory gestures, which can be valuable for researchers focusing on speech production and related areas.&#10;&#10;5. Insufficient comparability on Switchboard: If using the Switchboard dataset, regular dictionary transcriptions might lack crucial contextual and comparative information about how each word or sound compares to others in terms of acoustic and phonetic properties. This shortcoming could hinder analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts." />
    <node id=" entry , and then ? Or {pause} would we {disfmarker}&#10;Speaker: Professor C&#10;Content: Well , regular dictionary , I mean , this is a pretty rich dictionary . It 's got , got a fair number of pronunciations in it&#10;Speaker: Postdoc B&#10;Content: But {disfmarker}&#10;Speaker: PhD D&#10;Content: Or you could start from the {disfmarker} if we were gonna , do the same set , of sentences that Steve had , done , we could start with those transcriptions .&#10;Speaker: Postdoc B&#10;Content: Mm - hmm . So I was thinking {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: That 's actually what I was thinking , is tha {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: the problem is when you run , uh , if you run a regular dictionary , um , even if you have variants , in there , which most people don't , you don't always get , out , the" />
    <node id="Based on the transcript, the proposed adjustment to be made on the audio samples from twelve meetings is to convert them into a multi-channel format and adjust the timebands for precise representation. This adjustment should help improve the accuracy of processing and transcribing short segments of speech in these recordings." />
    <node id="1. Editing the first half of the project: The team has been working on editing the first half of the project. Postdoc B mentioned they were in the process of editing but welcomed the news about funding for the experiment as it would be useful for refining the process and making the overlaps more precise throughout.&#10;&#10;2. Funding of the experiment: Experiment funding update indicates that Postdoc B successfully obtained sufficient financial support to proceed with the project, although the source is not specified in the transcript.&#10;&#10;3. Meeting in the BARCO room: Postdoc B, Liz, and Don met in the BARCO room to discuss an issue concerning the interplay between the transcript format and the processing required for the SRI recognizer. The postdoc mentioned checking for spelling errors, ensuring consistency of conventions, and running it through a channelize program to get the data into multi-channel format. They also planned to discuss this issue further in the afternoon.&#10;&#10;The team is making progress on editing and refining their project while simultaneously securing funding and addressing technical challenges. The collaboration mentioned between Liz, Don, and Postdoc B aims to improve the transcript format for better processing." />
    <node id=" , in the BARCO room , with the lecture hall ,&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: Yeah , please . Go ahead . And this afternoon .&#10;Speaker: Postdoc B&#10;Content: and this afternoon , it drifted into the afternoon , {comment} {vocalsound} uh , concerning this issue of , um , the , well there 's basically the issue of the interplay between the transcript format and the processing that , they need to do for , the SRI recognizer . And , um , well , so , I mentioned the process that I 'm going through with the data , so , you know , I get the data back from the transcri Well , s uh , metaphorically , get the data back from the transcriber , and then I , check for simple things like spelling errors and things like that . And , um , I 'm going to be doing a more thorough editing , with respect to consistency of the conventions . But they 're {disfmarker} they 're generally very good . And , then , I run it through , uh , the channelize program to get it into the multi - channel format , OK . And {pause} the" />
    <node id="To accurately extract actual digits from a recording with false starts, misreads, and miscues, the proposed approach mentioned in the transcript involves the use of scripts and X Waves software. The user would select the portion of the audio that contains the desired digits, then use the software to compare the selected speech against a reference standard (in this case, the digit sequences &quot;six nine,&quot; &quot;nine two two,&quot; etc.). If the spoken digits match the reference standard, the user can proceed to the next sequence. This process allows for the precise identification and extraction of the correct digits from the recording, even when false starts, misreads, or miscues are present.&#10;&#10;This approach is based on the assumption that overlapping processes such as voicing and nasality may be less prevalent in digit reading, leading to clearer speech signals. This could potentially result in a higher level of accuracy for the extracted digits. However, it should be noted that further research would be needed to confirm this hypothesis.&#10;&#10;In summary, accurately extracting actual digits from a recording with false starts, misreads, and miscues requires careful selection of relevant audio segments and comparison against reference standards using software tools like scripts and X Waves. This approach can improve the overall accuracy of transcription by up to 1-2%." />
    <node id=": And then we should get really smart over the next year or two , and it {disfmarker} that should get better .&#10;Speaker: Grad F&#10;Content: Right . And inc increase it by one or two percent , yeah .&#10;Speaker: Professor C&#10;Content: Yeah , {vocalsound} Yeah .&#10;Speaker: Grad F&#10;Content: Um , but , in order to do that we need to extract out the actual digits .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: Grad F&#10;Content: Um , so that {disfmarker} the reason it 's not just a transcript is that there 're false starts , and misreads , and miscues and things like that . And so I have a set of scripts and X Waves where you just select the portion , hit R , um , it tells you what the next one should be , and you just look for that . You know , so it {disfmarker} it 'll put on the screen , &quot; The next set is six nine , nine two two &quot; . And you find that , and , hit the key and it records it in a file in a particular format .&#10;Speaker: Professor C&#10;" />
    <node id="The compromised string referred to in the conversation is the best compromise or representation that a group of people could come up with to describe what happened during speech production. It is not necessarily the exact transcription of what was really said, but rather an approximation that takes into account overlapping processes such as voicing and nasality.&#10;&#10;Compared to phone labels, which are simplified representations of speech sounds, this compromised string is likely to be more accurate in capturing the complexities of actual speech production. This is because the compromised string can potentially incorporate features that are not included in phone labels, such as reduced syllables and pronunciation variants.&#10;&#10;However, it is important to note that the compromised string may still introduce some level of confusion or ambiguity, especially if it is not based on a well-defined set of rules or conventions. This is why further research is needed to determine the best way to represent and transcribe speech production in a way that balances accuracy and clarity." />
    <node id="Speaker: PhD G&#10;Content: Right . That 's all , I mean . Exactly .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Exactly .&#10;Speaker: Professor C&#10;Content: Yeah , no I {disfmarker} I don't disagree with that .&#10;Speaker: PhD G&#10;Content: And Steve 's type is fairly {disfmarker} it 's not that slow , uh , uh , I dunno exactly what the , timing was , but .&#10;Speaker: Professor C&#10;Content: Yeah u I don't disagree with it the on the only thing is that , What you actually will end {disfmarker} en end up with is something , i it 's all compromised , right , so , the string that you end up with isn't , actually , what happened . But it 's {disfmarker} it 's the best compromise that a group of people scratching their heads could come up with to describe what happened .&#10;Speaker: PhD D&#10;Content: And it 's more accurate than , phone labels .&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;" />
    <node id=" it 's more accurate than , phone labels .&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: But . And it 's more accurate than the {disfmarker} than the dictionary or , if you 've got a pronunciation uh lexicon that has three or four ,&#10;Speaker: Grad F&#10;Content: The word .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: this might be have been the fifth one that you tr that you pruned or whatever ,&#10;Speaker: PhD D&#10;Content: So it 's like a continuum .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: so sure .&#10;Speaker: PhD D&#10;Content: It 's {disfmarker} you 're going all the way down ,&#10;Speaker: PhD G&#10;Content: Right . Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: yeah .&#10;Speaker: PhD G&#10;Content: That 's what I meant is {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah ." />
    <node id="The difference between pronunciation models in dictionaries and the pronunciations that people produce lies in the variability and complexity of human speech. Dictionary pronunciations are typically represented using standardized symbols and conventions, which may not capture the full range of variation and reduction that occur in natural speech.&#10;&#10;Steve's work on labeling contributes to obtaining information about the actual pronunciations produced by speakers, including reduced syllables and variants that are not typically included in dictionaries. This information can be useful for developing more accurate speech recognition systems and models of speech production. By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts." />
    <node id="1. The question is asking whether the current corpus (a collection of audio samples) might be appropriate to attempt achieving a closer correspondence between speech and transcriptions, and if so, what the participants think about increasing predictability in the discussion. This could mean finding ways to make the speech more predictable and easier to transcribe accurately. The participants will need to consider the quality of the audio samples, the clarity of the speakers' pronunciation, and the effectiveness of any proposed methods for improving transcription accuracy." />
    <node id="} Do you have any {disfmarker} ? {pause} Anybody have any opinion about that ,&#10;Speaker: Postdoc B&#10;Content: and that people might articulate more , and you that might end up with more {disfmarker} a closer correspondence .&#10;Speaker: Professor C&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Grad F&#10;Content: Yeah {disfmarker} that 's {disfmarker} I {disfmarker} I agree .&#10;Speaker: PhD D&#10;Content: Sort of less predictability ,&#10;Speaker: Grad F&#10;Content: That {disfmarker} it 's just {disfmarker}&#10;Speaker: Postdoc B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: and {disfmarker} You hafta {disfmarker}&#10;Speaker: Grad F&#10;Content: It 's a {disfmarker} Well {disfmarker} Would , this corpus really be the right one to even try that on ?&#10;Speaker: PhD G&#10;Content: Well it" />
    <node id="1. Include reduced pronunciations in dictionaries: There is a suggestion to include reduced pronunciations, such as &quot;gonna&quot;, &quot;guh see you tomorrow&quot;, etc., in dictionaries or have people explicitly mark these reductions during transcription. This would provide more comprehensive and accurate representations of actual pronunciations produced by speakers.&#10;&#10;2. Develop intermediate representations: There is also a proposal to create intermediate representations between dictionary word pronunciations and gestural ones, as the direct transition from one to another might not be straightforward or easy. These intermediate representations could help bridge the gap between standardized dictionary pronunciations and the more variable and reduced pronunciations found in natural speech.&#10;&#10;3. Analyze transcriptions and labelings: By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts. This information can be useful for developing more accurate speech recognition systems and models of speech production." />
    <node id="er} there are two steps . One {disfmarker} you know , one is going from a dictionary pronunciation of something , like , &quot; gonna see you tomorrow &quot; ,&#10;Speaker: Grad F&#10;Content: And {disfmarker} Or &quot; gonta &quot; .&#10;Speaker: Professor C&#10;Content: Right . Yeah .&#10;Speaker: PhD G&#10;Content: it could be &quot; going to &quot; or &quot; gonna &quot; or &quot; gonta s &quot; you know .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: And , yeah . &quot; Gonna see you tomorrow &quot; , uh , &quot; guh see you tomorrow &quot; . And , that it would be nice to have these , intermediate , or these {disfmarker} some {disfmarker} these reduced pronunciations that those transcribers had marked or to have people mark those as well .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: Because , it 's not , um , that easy to go from the , dictionary , word pronuncia the dictionary phone pronunciation , to the gestural one without this intermediate or a syllable" />
    <edge source="The transcriber's role in this context is not to evaluate the speech and nonspeech in specific channels by comparing it to reference material, but rather to segment and label the multichannel audio data. This task was previously done manually, as stated by Grad F when they mentioned &quot;Well we did the hand segmentation the one by hand.&quot; The proposal now is for the transcribers to continue their work in preparing the data, which includes adjusting timebands and formatting into a multi-channel structure.&#10;&#10;PhD G mentions that there are different types of noises like &quot;uh - huh&quot;s, &quot;hmm&quot;s, and grunts, and Postdoc B brings up lip smacks. The team can use the transcribed data to develop and train their system to recognize and categorize these speech and nonspeech elements in specific channels. However, it is not the transcriber's responsibility to provide this evaluation or comparison with reference material, as no such reference materials are available in this context." target=" , that 's {disfmarker} that 's great for {disfmarker} for my purpose . And , the thing is I {disfmarker} I , then the evaluation of {disfmarker} of the system is a little bit hard , as I don't have any references .&#10;Speaker: Grad F&#10;Content: Well we did the hand {disfmarker} the one by hand .&#10;Speaker: PhD A&#10;Content: Yeah , that 's the one {disfmarker} one wh where I do the training on so I can't do the evaluation on So the thing is , can the transcribers perhaps do some , some {disfmarker} some meetings in {disfmarker} in terms of speech - nonspeech in {disfmarker} in the specific channels ?&#10;Speaker: Grad F&#10;Content: Uh .&#10;Speaker: Postdoc B&#10;Content: Well , I have {disfmarker}&#10;Speaker: PhD D&#10;Content: Well won't you have that from their transcriptions ?&#10;Speaker: Postdoc B&#10;Content: Well , OK , so , now we need {disfmarker}&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcriber's role in this context is not to evaluate the speech and nonspeech in specific channels by comparing it to reference material, but rather to segment and label the multichannel audio data. This task was previously done manually, as stated by Grad F when they mentioned &quot;Well we did the hand segmentation the one by hand.&quot; The proposal now is for the transcribers to continue their work in preparing the data, which includes adjusting timebands and formatting into a multi-channel structure.&#10;&#10;PhD G mentions that there are different types of noises like &quot;uh - huh&quot;s, &quot;hmm&quot;s, and grunts, and Postdoc B brings up lip smacks. The team can use the transcribed data to develop and train their system to recognize and categorize these speech and nonspeech elements in specific channels. However, it is not the transcriber's responsibility to provide this evaluation or comparison with reference material, as no such reference materials are available in this context." target=" of {pause} interrupting {comment} and so forth .&#10;Speaker: Grad F&#10;Content: Uh - huh .&#10;Speaker: PhD G&#10;Content: And some of them are , {vocalsound} yeah , &quot; uh - huh &quot;s , and &quot; hmm &quot;s , and , &quot; hmm ! &quot; &quot; hmm &quot; {comment} &quot; OK &quot; , &quot; uh &quot; {comment} Grunts , uh , that might be interesting .&#10;Speaker: Postdoc B&#10;Content: He 's got lip {disfmarker} {pause} lipsmacks .&#10;Speaker: PhD G&#10;Content: In the meetings .&#10;Speaker: Professor C&#10;Content: We should move on .&#10;Speaker: Postdoc B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , new version of , uh , presegmentation ?&#10;Speaker: PhD A&#10;Content: Uh , oh yeah , um , {vocalsound} I worked a little bit on the {disfmarker} on the presegmentation to {disfmarker} to get another version which does channel - specific , uh , speech - nonspeech detection . And , what I did">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcriber's role in this context is not to evaluate the speech and nonspeech in specific channels by comparing it to reference material, but rather to segment and label the multichannel audio data. This task was previously done manually, as stated by Grad F when they mentioned &quot;Well we did the hand segmentation the one by hand.&quot; The proposal now is for the transcribers to continue their work in preparing the data, which includes adjusting timebands and formatting into a multi-channel structure.&#10;&#10;PhD G mentions that there are different types of noises like &quot;uh - huh&quot;s, &quot;hmm&quot;s, and grunts, and Postdoc B brings up lip smacks. The team can use the transcribed data to develop and train their system to recognize and categorize these speech and nonspeech elements in specific channels. However, it is not the transcriber's responsibility to provide this evaluation or comparison with reference material, as no such reference materials are available in this context." target="Speaker: PhD G&#10;Content: Right . It 's not the {disfmarker} it 's not the fact that we can't process a twenty second segment , it 's the fact that , there 's twenty seconds in which to place one word in the wrong place&#10;Speaker: Grad E&#10;Content: so it 's not {disfmarker}&#10;Speaker: Postdoc B&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: You know , if {disfmarker} if someone has a very short utterance there , and that 's where , we , might wanna have this individual , you know , ha have your pre pre - process input .&#10;Speaker: PhD A&#10;Content: Yep . Yeah . Sure .&#10;Speaker: Postdoc B&#10;Content: That 's very important .&#10;Speaker: PhD A&#10;Content: I {disfmarker} I {disfmarker} I thought that perhaps the transcribers could start then from the {disfmarker} those mult multi - channel , uh , speech - nonspeech detections , if they would like to .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcriber's role in this context is not to evaluate the speech and nonspeech in specific channels by comparing it to reference material, but rather to segment and label the multichannel audio data. This task was previously done manually, as stated by Grad F when they mentioned &quot;Well we did the hand segmentation the one by hand.&quot; The proposal now is for the transcribers to continue their work in preparing the data, which includes adjusting timebands and formatting into a multi-channel structure.&#10;&#10;PhD G mentions that there are different types of noises like &quot;uh - huh&quot;s, &quot;hmm&quot;s, and grunts, and Postdoc B brings up lip smacks. The team can use the transcribed data to develop and train their system to recognize and categorize these speech and nonspeech elements in specific channels. However, it is not the transcriber's responsibility to provide this evaluation or comparison with reference material, as no such reference materials are available in this context." target=" did {disfmarker} I did , um , uh , so , last night I did , uh ,&#10;Speaker: Professor C&#10;Content: yeah .&#10;Speaker: Postdoc B&#10;Content: Oh gosh , well , last night , I did about half an hour in , three hours , which is not , terrific ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: but , um , anyway , it 's an hour and a half per {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah . Well , that 's probably .&#10;Speaker: PhD A&#10;Content: So .&#10;Speaker: Postdoc B&#10;Content: Well , I can't calculate on my , {vocalsound} on my feet .&#10;Speaker: PhD A&#10;Content: Do the transcribers actually start wi with , uh , transcribing new meetings , or {pause} are they ?&#10;Speaker: Postdoc B&#10;Content: Well , um they 're still working {disfmarker} they still have enough to finish that I haven't assigned a new meeting ,&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcriber's role in this context is not to evaluate the speech and nonspeech in specific channels by comparing it to reference material, but rather to segment and label the multichannel audio data. This task was previously done manually, as stated by Grad F when they mentioned &quot;Well we did the hand segmentation the one by hand.&quot; The proposal now is for the transcribers to continue their work in preparing the data, which includes adjusting timebands and formatting into a multi-channel structure.&#10;&#10;PhD G mentions that there are different types of noises like &quot;uh - huh&quot;s, &quot;hmm&quot;s, and grunts, and Postdoc B brings up lip smacks. The team can use the transcribed data to develop and train their system to recognize and categorize these speech and nonspeech elements in specific channels. However, it is not the transcriber's responsibility to provide this evaluation or comparison with reference material, as no such reference materials are available in this context." target=" samples from twelve meetings that would only take an hour and I could get the transcribers to do that right {disfmarker} I mean , what I mean is , that would be an hour sampled , and then they 'd transcribe those {disfmarker} that hour , right ? That 's what I should do ?&#10;Speaker: Professor C&#10;Content: Yeah . And .&#10;Speaker: PhD A&#10;Content: That 's {disfmarker} that 's .&#10;Speaker: Postdoc B&#10;Content: I don't mean transcribe&#10;Speaker: Professor C&#10;Content: Right . Ye - But you 're {disfmarker} y&#10;Speaker: Postdoc B&#10;Content: I mean {disfmarker} I mean adjust . So they get it into the multi - channel format and then adjust the timebands so it 's precise .&#10;Speaker: Professor C&#10;Content: So that should be faster than the ten times kind of thing ,&#10;Speaker: Postdoc B&#10;Content: Absolutely . I did {disfmarker} I did , um , uh , so , last night I did , uh ,&#10;Speaker: Professor C&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcriber's role in this context is not to evaluate the speech and nonspeech in specific channels by comparing it to reference material, but rather to segment and label the multichannel audio data. This task was previously done manually, as stated by Grad F when they mentioned &quot;Well we did the hand segmentation the one by hand.&quot; The proposal now is for the transcribers to continue their work in preparing the data, which includes adjusting timebands and formatting into a multi-channel structure.&#10;&#10;PhD G mentions that there are different types of noises like &quot;uh - huh&quot;s, &quot;hmm&quot;s, and grunts, and Postdoc B brings up lip smacks. The team can use the transcribed data to develop and train their system to recognize and categorize these speech and nonspeech elements in specific channels. However, it is not the transcriber's responsibility to provide this evaluation or comparison with reference material, as no such reference materials are available in this context." target=" you run a regular dictionary , um , even if you have variants , in there , which most people don't , you don't always get , out , the actual pronunciations ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: so that 's why the human transcriber 's giving you the {disfmarker} that pronunciation ,&#10;Speaker: Postdoc B&#10;Content: Yeah . Oh .&#10;Speaker: Professor C&#10;Content: Actually maybe they 're using phone recognizers .&#10;Speaker: PhD G&#10;Content: and so y they {disfmarker} they {disfmarker} I thought that they were {disfmarker}&#10;Speaker: Professor C&#10;Content: Is that what they 're doing ?&#10;Speaker: Grad F&#10;Content: They are .&#10;Speaker: Professor C&#10;Content: Oh , OK .&#10;Speaker: PhD G&#10;Content: we should catch up on what Steve is ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: uh {disfmarker} I think that would be a good i good idea .&#10;Speaker: Professor C&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source=" of {pause} interrupting {comment} and so forth .&#10;Speaker: Grad F&#10;Content: Uh - huh .&#10;Speaker: PhD G&#10;Content: And some of them are , {vocalsound} yeah , &quot; uh - huh &quot;s , and &quot; hmm &quot;s , and , &quot; hmm ! &quot; &quot; hmm &quot; {comment} &quot; OK &quot; , &quot; uh &quot; {comment} Grunts , uh , that might be interesting .&#10;Speaker: Postdoc B&#10;Content: He 's got lip {disfmarker} {pause} lipsmacks .&#10;Speaker: PhD G&#10;Content: In the meetings .&#10;Speaker: Professor C&#10;Content: We should move on .&#10;Speaker: Postdoc B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , new version of , uh , presegmentation ?&#10;Speaker: PhD A&#10;Content: Uh , oh yeah , um , {vocalsound} I worked a little bit on the {disfmarker} on the presegmentation to {disfmarker} to get another version which does channel - specific , uh , speech - nonspeech detection . And , what I did" target="Based on the transcript, Grad F used normalized features to distinguish between foreground and background speech in multiple channels of a recording. The normalization techniques included:&#10;&#10;1. Normalized energy: Calculated by dividing the energy of each channel by the mean energy over all channels and the minimum energy within each channel.&#10;2. Normalization of loudness, spectral features (such as modified loudness), and other unspecified features. These were also incorporated into the feature vector for improved distinction between foreground and background speech.&#10;&#10;The system was tested on three or four meetings and seemed to work fairly well, although there were some issues with lapel microphones. The performance of these normalization techniques in distinguishing between foreground and background speech appears to be promising, but a more thorough evaluation and potential optimization may be required for better results.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD G&#10;Content: Right . It 's not the {disfmarker} it 's not the fact that we can't process a twenty second segment , it 's the fact that , there 's twenty seconds in which to place one word in the wrong place&#10;Speaker: Grad E&#10;Content: so it 's not {disfmarker}&#10;Speaker: Postdoc B&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: You know , if {disfmarker} if someone has a very short utterance there , and that 's where , we , might wanna have this individual , you know , ha have your pre pre - process input .&#10;Speaker: PhD A&#10;Content: Yep . Yeah . Sure .&#10;Speaker: Postdoc B&#10;Content: That 's very important .&#10;Speaker: PhD A&#10;Content: I {disfmarker} I {disfmarker} I thought that perhaps the transcribers could start then from the {disfmarker} those mult multi - channel , uh , speech - nonspeech detections , if they would like to .&#10;Speaker:" target="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways.">
      <data key="d0">1</data>
    </edge>
    <edge source=" did {disfmarker} I did , um , uh , so , last night I did , uh ,&#10;Speaker: Professor C&#10;Content: yeah .&#10;Speaker: Postdoc B&#10;Content: Oh gosh , well , last night , I did about half an hour in , three hours , which is not , terrific ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: but , um , anyway , it 's an hour and a half per {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah . Well , that 's probably .&#10;Speaker: PhD A&#10;Content: So .&#10;Speaker: Postdoc B&#10;Content: Well , I can't calculate on my , {vocalsound} on my feet .&#10;Speaker: PhD A&#10;Content: Do the transcribers actually start wi with , uh , transcribing new meetings , or {pause} are they ?&#10;Speaker: Postdoc B&#10;Content: Well , um they 're still working {disfmarker} they still have enough to finish that I haven't assigned a new meeting ,&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker:" target="Based on the transcript, the exact time it would take to transcribe the compiled audio samples from twelve meetings is not explicitly mentioned. However, Postdoc B proposed recording five-minute segments from various meetings, which would result in one hour of audio samples. They also mention that they have already processed half an hour of material.&#10;&#10;The feasibility of this task depends on several factors, such as the number of speakers, audio quality, and the effectiveness of any pre-processing techniques or conventions for encoding clarificational material and keywords. The transcript mentions concerns about accurately processing and transcribing short segments of speech, particularly when a single word can be placed incorrectly, leading to misunderstandings or errors.&#10;&#10;To make this task more feasible, several suggestions were made, including pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for precise representation of audio data. Implementing these improvements could help ensure better transcription accuracy and make the task more manageable.">
      <data key="d0">1</data>
    </edge>
    <edge source=" samples from twelve meetings that would only take an hour and I could get the transcribers to do that right {disfmarker} I mean , what I mean is , that would be an hour sampled , and then they 'd transcribe those {disfmarker} that hour , right ? That 's what I should do ?&#10;Speaker: Professor C&#10;Content: Yeah . And .&#10;Speaker: PhD A&#10;Content: That 's {disfmarker} that 's .&#10;Speaker: Postdoc B&#10;Content: I don't mean transcribe&#10;Speaker: Professor C&#10;Content: Right . Ye - But you 're {disfmarker} y&#10;Speaker: Postdoc B&#10;Content: I mean {disfmarker} I mean adjust . So they get it into the multi - channel format and then adjust the timebands so it 's precise .&#10;Speaker: Professor C&#10;Content: So that should be faster than the ten times kind of thing ,&#10;Speaker: Postdoc B&#10;Content: Absolutely . I did {disfmarker} I did , um , uh , so , last night I did , uh ,&#10;Speaker: Professor C&#10;Content:" target="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples.">
      <data key="d0">1</data>
    </edge>
    <edge source=" samples from twelve meetings that would only take an hour and I could get the transcribers to do that right {disfmarker} I mean , what I mean is , that would be an hour sampled , and then they 'd transcribe those {disfmarker} that hour , right ? That 's what I should do ?&#10;Speaker: Professor C&#10;Content: Yeah . And .&#10;Speaker: PhD A&#10;Content: That 's {disfmarker} that 's .&#10;Speaker: Postdoc B&#10;Content: I don't mean transcribe&#10;Speaker: Professor C&#10;Content: Right . Ye - But you 're {disfmarker} y&#10;Speaker: Postdoc B&#10;Content: I mean {disfmarker} I mean adjust . So they get it into the multi - channel format and then adjust the timebands so it 's precise .&#10;Speaker: Professor C&#10;Content: So that should be faster than the ten times kind of thing ,&#10;Speaker: Postdoc B&#10;Content: Absolutely . I did {disfmarker} I did , um , uh , so , last night I did , uh ,&#10;Speaker: Professor C&#10;Content:" target="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential.">
      <data key="d0">1</data>
    </edge>
    <edge source=" samples from twelve meetings that would only take an hour and I could get the transcribers to do that right {disfmarker} I mean , what I mean is , that would be an hour sampled , and then they 'd transcribe those {disfmarker} that hour , right ? That 's what I should do ?&#10;Speaker: Professor C&#10;Content: Yeah . And .&#10;Speaker: PhD A&#10;Content: That 's {disfmarker} that 's .&#10;Speaker: Postdoc B&#10;Content: I don't mean transcribe&#10;Speaker: Professor C&#10;Content: Right . Ye - But you 're {disfmarker} y&#10;Speaker: Postdoc B&#10;Content: I mean {disfmarker} I mean adjust . So they get it into the multi - channel format and then adjust the timebands so it 's precise .&#10;Speaker: Professor C&#10;Content: So that should be faster than the ten times kind of thing ,&#10;Speaker: Postdoc B&#10;Content: Absolutely . I did {disfmarker} I did , um , uh , so , last night I did , uh ,&#10;Speaker: Professor C&#10;Content:" target="Based on the transcript, the exact time it would take to transcribe the compiled audio samples from twelve meetings is not explicitly mentioned. However, Postdoc B proposed recording five-minute segments from various meetings, which would result in one hour of audio samples. They also mention that they have already processed half an hour of material.&#10;&#10;The feasibility of this task depends on several factors, such as the number of speakers, audio quality, and the effectiveness of any pre-processing techniques or conventions for encoding clarificational material and keywords. The transcript mentions concerns about accurately processing and transcribing short segments of speech, particularly when a single word can be placed incorrectly, leading to misunderstandings or errors.&#10;&#10;To make this task more feasible, several suggestions were made, including pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for precise representation of audio data. Implementing these improvements could help ensure better transcription accuracy and make the task more manageable.">
      <data key="d0">1</data>
    </edge>
    <edge source=" samples from twelve meetings that would only take an hour and I could get the transcribers to do that right {disfmarker} I mean , what I mean is , that would be an hour sampled , and then they 'd transcribe those {disfmarker} that hour , right ? That 's what I should do ?&#10;Speaker: Professor C&#10;Content: Yeah . And .&#10;Speaker: PhD A&#10;Content: That 's {disfmarker} that 's .&#10;Speaker: Postdoc B&#10;Content: I don't mean transcribe&#10;Speaker: Professor C&#10;Content: Right . Ye - But you 're {disfmarker} y&#10;Speaker: Postdoc B&#10;Content: I mean {disfmarker} I mean adjust . So they get it into the multi - channel format and then adjust the timebands so it 's precise .&#10;Speaker: Professor C&#10;Content: So that should be faster than the ten times kind of thing ,&#10;Speaker: Postdoc B&#10;Content: Absolutely . I did {disfmarker} I did , um , uh , so , last night I did , uh ,&#10;Speaker: Professor C&#10;Content:" target="Based on the transcript, the proposed adjustment to be made on the audio samples from twelve meetings is to convert them into a multi-channel format and adjust the timebands for precise representation. This adjustment should help improve the accuracy of processing and transcribing short segments of speech in these recordings.">
      <data key="d0">1</data>
    </edge>
    <edge source=" you run a regular dictionary , um , even if you have variants , in there , which most people don't , you don't always get , out , the actual pronunciations ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: so that 's why the human transcriber 's giving you the {disfmarker} that pronunciation ,&#10;Speaker: Postdoc B&#10;Content: Yeah . Oh .&#10;Speaker: Professor C&#10;Content: Actually maybe they 're using phone recognizers .&#10;Speaker: PhD G&#10;Content: and so y they {disfmarker} they {disfmarker} I thought that they were {disfmarker}&#10;Speaker: Professor C&#10;Content: Is that what they 're doing ?&#10;Speaker: Grad F&#10;Content: They are .&#10;Speaker: Professor C&#10;Content: Oh , OK .&#10;Speaker: PhD G&#10;Content: we should catch up on what Steve is ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: uh {disfmarker} I think that would be a good i good idea .&#10;Speaker: Professor C&#10;Content" target="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes.">
      <data key="d0">1</data>
    </edge>
    <edge source=" you run a regular dictionary , um , even if you have variants , in there , which most people don't , you don't always get , out , the actual pronunciations ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: so that 's why the human transcriber 's giving you the {disfmarker} that pronunciation ,&#10;Speaker: Postdoc B&#10;Content: Yeah . Oh .&#10;Speaker: Professor C&#10;Content: Actually maybe they 're using phone recognizers .&#10;Speaker: PhD G&#10;Content: and so y they {disfmarker} they {disfmarker} I thought that they were {disfmarker}&#10;Speaker: Professor C&#10;Content: Is that what they 're doing ?&#10;Speaker: Grad F&#10;Content: They are .&#10;Speaker: Professor C&#10;Content: Oh , OK .&#10;Speaker: PhD G&#10;Content: we should catch up on what Steve is ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: uh {disfmarker} I think that would be a good i good idea .&#10;Speaker: Professor C&#10;Content" target="1. Limited pronunciation variants: While some dictionaries may include multiple pronunciations for a given word, many do not. This can lead to incomplete or inaccurate transcriptions if the chosen dictionary does not provide all necessary variants.&#10;&#10;2. Telephone bandwidth limitations: Dictionaries based on telephone-quality audio might result in less precise acoustic measurements or phonetic representations due to the limited bandwidth available in such recordings.&#10;&#10;3. Inability to capture human nuances: Dictionaries may not account for regional accents, dialects, or other factors that could influence how a word is pronounced in natural speech. As a result, transcriptions generated from dictionaries might fail to accurately represent these variations.&#10;&#10;4. Lack of articulatory gestures: Regular dictionaries typically do not include information about articulatory gestures, which can be valuable for researchers focusing on speech production and related areas.&#10;&#10;5. Insufficient comparability on Switchboard: If using the Switchboard dataset, regular dictionary transcriptions might lack crucial contextual and comparative information about how each word or sound compares to others in terms of acoustic and phonetic properties. This shortcoming could hinder analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source=" you run a regular dictionary , um , even if you have variants , in there , which most people don't , you don't always get , out , the actual pronunciations ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: so that 's why the human transcriber 's giving you the {disfmarker} that pronunciation ,&#10;Speaker: Postdoc B&#10;Content: Yeah . Oh .&#10;Speaker: Professor C&#10;Content: Actually maybe they 're using phone recognizers .&#10;Speaker: PhD G&#10;Content: and so y they {disfmarker} they {disfmarker} I thought that they were {disfmarker}&#10;Speaker: Professor C&#10;Content: Is that what they 're doing ?&#10;Speaker: Grad F&#10;Content: They are .&#10;Speaker: Professor C&#10;Content: Oh , OK .&#10;Speaker: PhD G&#10;Content: we should catch up on what Steve is ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: uh {disfmarker} I think that would be a good i good idea .&#10;Speaker: Professor C&#10;Content" target="The difference between pronunciation models in dictionaries and the pronunciations that people produce lies in the variability and complexity of human speech. Dictionary pronunciations are typically represented using standardized symbols and conventions, which may not capture the full range of variation and reduction that occur in natural speech.&#10;&#10;Steve's work on labeling contributes to obtaining information about the actual pronunciations produced by speakers, including reduced syllables and variants that are not typically included in dictionaries. This information can be useful for developing more accurate speech recognition systems and models of speech production. By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." target=" um , that easy to go from the , dictionary , word pronuncia the dictionary phone pronunciation , to the gestural one without this intermediate or a syllable level kind of , representation .&#10;Speaker: Grad F&#10;Content: Well I don't think Morgan 's suggesting that we do that , though .&#10;Speaker: Professor C&#10;Content: Do you mean ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah , I mean , I I I 'm jus at the moment of course we 're just talking about what , to provide as a tool for people to do research who have different ideas about how to do it . So for instance , you might have someone who just has a wor has words with states , and has uh {disfmarker} uh , comes from articulatory gestures to that . And someone else , might actually want some phonetic uh intermediate thing . So I think it would be {disfmarker} be best to have all of it if we could . But {pause} um ,&#10;Speaker: Grad F&#10;Content: But {disfmarker} What I 'm imagining is a score - like notation , where each line is a particular feature">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." target="&#10;Speaker: Postdoc B&#10;Content: It would seem to me that the points of articulation would be m more , g uh , I mean that 's {disfmarker} I think about articulatory features , I think about , points of articulation , which means , uh , rather than vowels .&#10;Speaker: Grad F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Points of articulation ? What do you mean ?&#10;Speaker: Postdoc B&#10;Content: So , is it , uh , bilabial or dental or is it , you know , palatal .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: Which {disfmarker} which are all like where {disfmarker} where your tongue comes to rest .&#10;Speaker: Professor C&#10;Content: Place , place .&#10;Speaker: PhD D&#10;Content: Place of ar place of articulation .&#10;Speaker: Grad F&#10;Content: Uvular .&#10;Speaker: PhD A&#10;Content: Place .&#10;Speaker: Postdoc B&#10;Content: Place . Thank you , what {disfmarker} whate">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." target="&#10;Speaker: Postdoc B&#10;Content: W Well it seems like you could do both .&#10;Speaker: Grad F&#10;Content: maybe meeting data isn't the right corpus .&#10;Speaker: Postdoc B&#10;Content: I mean , I was thinking that it would be interesting , to do it with respect to , parts of Switchboard anyway , in terms of ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: uh {disfmarker} partly to see , if you could , generate first guesses at what the articulatory feature would be , based on the phone representation at that lower level .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: It might be a time gain . But also in terms of comparability of , um ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Well cuz the yeah , and then also , if you did it on Switchboard , you would have , the full continuum of transcriptions .&#10;Speaker: Postdoc B&#10;Content: what you gain Yep .&#10;Speaker: PhD D">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." target=": PhD A&#10;Content: Place .&#10;Speaker: Postdoc B&#10;Content: Place . Thank you , what {disfmarker} whatev whatever I s said , that 's {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: OK .&#10;Speaker: Postdoc B&#10;Content: I really meant place .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: OK , I see .&#10;Speaker: Professor C&#10;Content: Yeah . OK we got our jargon then , OK .&#10;Speaker: Postdoc B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh .&#10;Speaker: PhD G&#10;Content: Well it 's also , there 's , really a difference between , the pronunciation models in the dictionary , and , the pronunciations that people produce . And , so , You get , some of that information from Steve 's work on the {disfmarker} on the labeling&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: and it really , I">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." target="Speaker: PhD G&#10;Content: That 's what I meant is {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: an and in some places it would fill in , So {disfmarker} the kinds of gestural features are not everywhere .&#10;Speaker: Grad F&#10;Content: Well {disfmarker}&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: So there are some things that you don't have access to either from your ear or the spectrogram ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: but you know what phone it was and that 's about all you can {disfmarker} all you can say .&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: And then there are other cases where , nasality , voicing {disfmarker}&#10;Speaker: PhD D&#10;Content: It 's basically just having , multiple levels of {disfmarker} of , information and marking , on the signal .&#10;Speaker: PhD G&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." target="1. The &quot;real situation&quot; of car noise in this study refers to the background noise recorded from people driving around in a car, without any artificial addition of noise to manipulate the signal-to-noise ratio. This is different from other studies where an artificial signal-to-noise ratio might be added to simulate real-world conditions. The advantage of using real car noise is that it provides a more authentic and accurate representation of the environment in which speech recognition systems would actually be used. However, it may also present additional challenges due to its complexity and variability.&#10;2. The models' performance was not as good as the best score because, as acknowledged by Professor C, &quot;the models are pretty crappy.&quot; Despite the best score being a relatively low error rate of 5% per digit, this is still far from perfect accuracy. It is possible that the complexity and variability of the car noise data made it difficult for the models to achieve higher levels of accuracy. Additionally, the use of lapel microphones in the meetings may have introduced additional challenges due to their propensity for picking up unwanted background noise.&#10;&#10;In summary, the &quot;real situation&quot; of car noise in this study provides a more authentic representation of real-world conditions than artificial signal-to-noise ratios, but it also presents additional challenges for speech recognition systems. The models' performance was not as good as the best score due to their inherent limitations and the complexity of the data.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." target="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." target="1. Acoustic features: The lowest level of transcription would include acoustic features, which involve physical measurements of the speech signal such as frequency, amplitude, and duration.&#10;&#10;2. Phonetic level: This refers to the representation of speech sounds using symbols from the International Phonetic Alphabet (IPA). It includes detailed information about the pronunciation of each word, including distinctions between similar sounds like &quot;p&quot; and &quot;b&quot; or &quot;s&quot; and &quot;z&quot;.&#10;&#10;3. Relative gain: This term likely refers to the relative loudness or volume of different parts of the speech signal. Including this information in the transcriptions would provide insight into how loudly or softly certain words or sounds are spoken.&#10;&#10;4. Telephone bandwidth limitations: Since some of the discussions involve telephone-quality audio, it is important to consider the impact of limited bandwidth on the quality and detail of the transcriptions. This might result in less precise acoustic measurements or phonetic representations, as well as potential difficulties in accurately distinguishing between different speakers' voices.&#10;&#10;5. Articulatory gestures: Although not explicitly mentioned in the provided excerpt, Professor C suggests including articulatory features in the transcriptions. This would involve representing the movements and positions of the speaker's speech articulators (e.g., tongue, lips, jaw) during speech production.&#10;&#10;6. Comparability on Switchboard: If using the Switchboard dataset, the full continuum of transcriptions should include information about how each word or sound compares to others in terms of acoustic and phonetic properties. This would facilitate analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." target="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." target="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." target="1. Limited pronunciation variants: While some dictionaries may include multiple pronunciations for a given word, many do not. This can lead to incomplete or inaccurate transcriptions if the chosen dictionary does not provide all necessary variants.&#10;&#10;2. Telephone bandwidth limitations: Dictionaries based on telephone-quality audio might result in less precise acoustic measurements or phonetic representations due to the limited bandwidth available in such recordings.&#10;&#10;3. Inability to capture human nuances: Dictionaries may not account for regional accents, dialects, or other factors that could influence how a word is pronounced in natural speech. As a result, transcriptions generated from dictionaries might fail to accurately represent these variations.&#10;&#10;4. Lack of articulatory gestures: Regular dictionaries typically do not include information about articulatory gestures, which can be valuable for researchers focusing on speech production and related areas.&#10;&#10;5. Insufficient comparability on Switchboard: If using the Switchboard dataset, regular dictionary transcriptions might lack crucial contextual and comparative information about how each word or sound compares to others in terms of acoustic and phonetic properties. This shortcoming could hinder analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professor C is suggesting that the best option for a research tool would be to provide all types of intermediate representations, including articulatory gestures and phonetic features, when converting dictionary word pronunciation to gestural representation. This would allow researchers with different ideas about how to do it to have the flexibility to use the type of representation that suits their needs. For example, some researchers may prefer to work with words that have states coming from articulatory gestures, while others might want a phonetic intermediate thing. By providing all of these options, the tool would be more versatile and widely applicable for research purposes." target="The difference between pronunciation models in dictionaries and the pronunciations that people produce lies in the variability and complexity of human speech. Dictionary pronunciations are typically represented using standardized symbols and conventions, which may not capture the full range of variation and reduction that occur in natural speech.&#10;&#10;Steve's work on labeling contributes to obtaining information about the actual pronunciations produced by speakers, including reduced syllables and variants that are not typically included in dictionaries. This information can be useful for developing more accurate speech recognition systems and models of speech production. By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: Postdoc B&#10;Content: It would seem to me that the points of articulation would be m more , g uh , I mean that 's {disfmarker} I think about articulatory features , I think about , points of articulation , which means , uh , rather than vowels .&#10;Speaker: Grad F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Points of articulation ? What do you mean ?&#10;Speaker: Postdoc B&#10;Content: So , is it , uh , bilabial or dental or is it , you know , palatal .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: Which {disfmarker} which are all like where {disfmarker} where your tongue comes to rest .&#10;Speaker: Professor C&#10;Content: Place , place .&#10;Speaker: PhD D&#10;Content: Place of ar place of articulation .&#10;Speaker: Grad F&#10;Content: Uvular .&#10;Speaker: PhD A&#10;Content: Place .&#10;Speaker: Postdoc B&#10;Content: Place . Thank you , what {disfmarker} whate" target="1. M Four: The speakers briefly used the term &quot;M four&quot; to refer to a meeting about meetings up to the fourth level (a meeting about a meeting about a meeting about meetings). This was suggested as an alternative to repeatedly saying &quot;the meeting about the meeting about...&quot;&#10;&#10;2. Pauses between lines: Grad F reminded the group to include pauses between the lines when transcribing or analyzing the content, likely for better readability or more accurate processing.&#10;&#10;3. Crown P Z: The group discussed using audio samples from the Crown P Z (possibly a collection of recordings), which would provide diverse speech samples for their project. However, they acknowledged that some segments might not be useful due to variations in audio quality and speakers.&#10;&#10;4. Iterating and developing models: Postdoc B, Grad F, and PhD G expressed interest in refining the process of iterating and creating models to perform forced alignment and acquire data. They recognized that this method's success would depend on various factors and might not always yield optimal results. PhD G mentioned they would inform the group when a solution for forced alignment is available.&#10;&#10;5. Cross-correlations and false overlaps: The group discussed recording five-minute segments from various meetings (excluding the first minute) to analyze cross-correlations and remove false overlaps during recordings. This approach aims to account for different speakers and varying audio quality in the samples. Postdoc B agreed to prepare a set of such segments from twelve meetings for further analysis.&#10;&#10;The main objective of the meeting was to explore ways of processing and transcribing speech samples from various sources, with an emphasis on improving accuracy and removing false overlaps during recordings.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: Postdoc B&#10;Content: W Well it seems like you could do both .&#10;Speaker: Grad F&#10;Content: maybe meeting data isn't the right corpus .&#10;Speaker: Postdoc B&#10;Content: I mean , I was thinking that it would be interesting , to do it with respect to , parts of Switchboard anyway , in terms of ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: uh {disfmarker} partly to see , if you could , generate first guesses at what the articulatory feature would be , based on the phone representation at that lower level .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: It might be a time gain . But also in terms of comparability of , um ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Well cuz the yeah , and then also , if you did it on Switchboard , you would have , the full continuum of transcriptions .&#10;Speaker: Postdoc B&#10;Content: what you gain Yep .&#10;Speaker: PhD D" target="The &quot;blending thing&quot; referred to in the spectrograms is the observation that the speech signal doesn't always correspond to a clear sequence of phones as given in dictionary pronunciations. Instead, there are instances where different features (such as voicing or nasality) from different phones overlap and blend together in complex ways. This blending is not easily named or categorized, which makes it difficult to represent using traditional phonetic transcriptions. The speakers suggest that this blending might be better represented using a score-like notation where each line corresponds to a particular feature, allowing for a more accurate representation of the overlapping and complex nature of these speech signals.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: Postdoc B&#10;Content: W Well it seems like you could do both .&#10;Speaker: Grad F&#10;Content: maybe meeting data isn't the right corpus .&#10;Speaker: Postdoc B&#10;Content: I mean , I was thinking that it would be interesting , to do it with respect to , parts of Switchboard anyway , in terms of ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: uh {disfmarker} partly to see , if you could , generate first guesses at what the articulatory feature would be , based on the phone representation at that lower level .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: It might be a time gain . But also in terms of comparability of , um ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Well cuz the yeah , and then also , if you did it on Switchboard , you would have , the full continuum of transcriptions .&#10;Speaker: Postdoc B&#10;Content: what you gain Yep .&#10;Speaker: PhD D" target="1. Acoustic features: The lowest level of transcription would include acoustic features, which involve physical measurements of the speech signal such as frequency, amplitude, and duration.&#10;&#10;2. Phonetic level: This refers to the representation of speech sounds using symbols from the International Phonetic Alphabet (IPA). It includes detailed information about the pronunciation of each word, including distinctions between similar sounds like &quot;p&quot; and &quot;b&quot; or &quot;s&quot; and &quot;z&quot;.&#10;&#10;3. Relative gain: This term likely refers to the relative loudness or volume of different parts of the speech signal. Including this information in the transcriptions would provide insight into how loudly or softly certain words or sounds are spoken.&#10;&#10;4. Telephone bandwidth limitations: Since some of the discussions involve telephone-quality audio, it is important to consider the impact of limited bandwidth on the quality and detail of the transcriptions. This might result in less precise acoustic measurements or phonetic representations, as well as potential difficulties in accurately distinguishing between different speakers' voices.&#10;&#10;5. Articulatory gestures: Although not explicitly mentioned in the provided excerpt, Professor C suggests including articulatory features in the transcriptions. This would involve representing the movements and positions of the speaker's speech articulators (e.g., tongue, lips, jaw) during speech production.&#10;&#10;6. Comparability on Switchboard: If using the Switchboard dataset, the full continuum of transcriptions should include information about how each word or sound compares to others in terms of acoustic and phonetic properties. This would facilitate analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: Postdoc B&#10;Content: W Well it seems like you could do both .&#10;Speaker: Grad F&#10;Content: maybe meeting data isn't the right corpus .&#10;Speaker: Postdoc B&#10;Content: I mean , I was thinking that it would be interesting , to do it with respect to , parts of Switchboard anyway , in terms of ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: uh {disfmarker} partly to see , if you could , generate first guesses at what the articulatory feature would be , based on the phone representation at that lower level .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: It might be a time gain . But also in terms of comparability of , um ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Well cuz the yeah , and then also , if you did it on Switchboard , you would have , the full continuum of transcriptions .&#10;Speaker: Postdoc B&#10;Content: what you gain Yep .&#10;Speaker: PhD D" target="The compromised string referred to in the conversation is the best compromise or representation that a group of people could come up with to describe what happened during speech production. It is not necessarily the exact transcription of what was really said, but rather an approximation that takes into account overlapping processes such as voicing and nasality.&#10;&#10;Compared to phone labels, which are simplified representations of speech sounds, this compromised string is likely to be more accurate in capturing the complexities of actual speech production. This is because the compromised string can potentially incorporate features that are not included in phone labels, such as reduced syllables and pronunciation variants.&#10;&#10;However, it is important to note that the compromised string may still introduce some level of confusion or ambiguity, especially if it is not based on a well-defined set of rules or conventions. This is why further research is needed to determine the best way to represent and transcribe speech production in a way that balances accuracy and clarity.">
      <data key="d0">1</data>
    </edge>
    <edge source=": PhD A&#10;Content: Place .&#10;Speaker: Postdoc B&#10;Content: Place . Thank you , what {disfmarker} whatev whatever I s said , that 's {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: OK .&#10;Speaker: Postdoc B&#10;Content: I really meant place .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: OK , I see .&#10;Speaker: Professor C&#10;Content: Yeah . OK we got our jargon then , OK .&#10;Speaker: Postdoc B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh .&#10;Speaker: PhD G&#10;Content: Well it 's also , there 's , really a difference between , the pronunciation models in the dictionary , and , the pronunciations that people produce . And , so , You get , some of that information from Steve 's work on the {disfmarker} on the labeling&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: and it really , I" target="The difference between pronunciation models in dictionaries and the pronunciations that people produce lies in the variability and complexity of human speech. Dictionary pronunciations are typically represented using standardized symbols and conventions, which may not capture the full range of variation and reduction that occur in natural speech.&#10;&#10;Steve's work on labeling contributes to obtaining information about the actual pronunciations produced by speakers, including reduced syllables and variants that are not typically included in dictionaries. This information can be useful for developing more accurate speech recognition systems and models of speech production. By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD G&#10;Content: That 's what I meant is {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: an and in some places it would fill in , So {disfmarker} the kinds of gestural features are not everywhere .&#10;Speaker: Grad F&#10;Content: Well {disfmarker}&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: So there are some things that you don't have access to either from your ear or the spectrogram ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: but you know what phone it was and that 's about all you can {disfmarker} all you can say .&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: And then there are other cases where , nasality , voicing {disfmarker}&#10;Speaker: PhD D&#10;Content: It 's basically just having , multiple levels of {disfmarker} of , information and marking , on the signal .&#10;Speaker: PhD G&#10;Content" target="The &quot;blending thing&quot; referred to in the spectrograms is the observation that the speech signal doesn't always correspond to a clear sequence of phones as given in dictionary pronunciations. Instead, there are instances where different features (such as voicing or nasality) from different phones overlap and blend together in complex ways. This blending is not easily named or categorized, which makes it difficult to represent using traditional phonetic transcriptions. The speakers suggest that this blending might be better represented using a score-like notation where each line corresponds to a particular feature, allowing for a more accurate representation of the overlapping and complex nature of these speech signals.">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target=" I mean , we could {pause} easily , get a section , you know , like say a minute or so , from every meeting that we have so f from the newer ones that we 're working on , everyone that we have . And then , should provide this .&#10;Speaker: PhD A&#10;Content: If it 's not the first minute of {disfmarker} of the meeting , that {disfmarker} that 's OK with me , but , in {disfmarker} in the first minute , uh , Often there are some {disfmarker} some strange things going on which {disfmarker} which aren't really , well , for , which {disfmarker} which aren't re re really good . So . What {disfmarker} what I 'd quite like , perhaps , is , to have , some five minutes of {disfmarker} of {disfmarker} of different meetings , so .&#10;Speaker: Postdoc B&#10;Content: Somewhere not in the very beginning , five minutes , OK .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: And , then I wanted to ask you">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="isfmarker} that , there is a system ,&#10;Speaker: PhD D&#10;Content: So , current {disfmarker} This week .&#10;Speaker: Postdoc B&#10;Content: Yes . Might not be what you need .&#10;Speaker: Grad F&#10;Content: Yeah , so if we could get a couple meetings done with that level of precision I think that would be a good idea .&#10;Speaker: PhD A&#10;Content: OK . Yeah .&#10;Speaker: Postdoc B&#10;Content: Oh , OK . Uh , how {disfmarker} how m much time {disfmarker} so the meetings vary in length , what are we talking about in terms of the number of minutes you 'd like to have as your {disfmarker} as your training set ?&#10;Speaker: PhD A&#10;Content: It seems to me that it would be good to have , a few minutes from {disfmarker} from different meetings , so . But I 'm not sure about how much .&#10;Speaker: Postdoc B&#10;Content: OK , now you 're saying different meetings because of different speakers or because of different audio quality or both or {disfmarker} ?&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="Speaker: Postdoc B&#10;Content: OK , so then , if that 's five minutes per meeting we 've got like twelve minutes , twelve meetings , roughly , that I 'm {disfmarker} that I 've been working with , then {disfmarker}&#10;Speaker: Professor C&#10;Content: Of {disfmarker} of {disfmarker} of the meetings that you 're working with , how many of them are different , tha&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Professor C&#10;Content: are there any of them that are different than , these two meetings ?&#10;Speaker: Postdoc B&#10;Content: Well {disfmarker} oh wa in terms of the speakers or the conditions or the ?&#10;Speaker: Professor C&#10;Content: Yeah , speakers . Sorry .&#10;Speaker: PhD A&#10;Content: Yeah , that {disfmarker}&#10;Speaker: Postdoc B&#10;Content: Um , we have different combinations of speakers .&#10;Speaker: Professor C&#10;Content: So .&#10;Speaker: Postdoc B&#10;Content: I mean , just from what I 've seen , uh , there are some where ,">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="pause} future work . Well {disfmarker} well what I want to do is to {disfmarker} to look into cross - correlations for {disfmarker} for removing those , false overlaps .&#10;Speaker: Postdoc B&#10;Content: Wonderful .&#10;Speaker: PhD G&#10;Content: Are the , um , wireless , different than the wired , mikes , at all ? I mean , have you noticed any difference ?&#10;Speaker: PhD A&#10;Content: I 'm {disfmarker} I 'm not sure , um , if {disfmarker} if there are any wired mikes in those meetings , or , uh , I have {disfmarker} have to loo have a look at them but , I 'm {disfmarker} I 'm {disfmarker} I think there 's no difference between ,&#10;Speaker: PhD G&#10;Content: So it 's just the lapel versus everything else ?&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Postdoc B&#10;Content: OK , so then , if that 's five minutes per meeting we 've got like twelve minutes , twelve meetings">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="Content: OK , now you 're saying different meetings because of different speakers or because of different audio quality or both or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Both {disfmarker} both . Different {disfmarker} different number of speakers , different speakers , different {pause} conditions .&#10;Speaker: Postdoc B&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Yeah , we don't have that much variety in meetings yet , uh , I mean we have this meeting and the feature meeting and we have a couple others that we have uh , couple examples of . But {disfmarker} but , uh ,&#10;Speaker: PhD A&#10;Content: Yeah , m Yeah .&#10;Speaker: Postdoc B&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: Even probably with the gains {pause} differently will affect it , you mean {disfmarker}&#10;Speaker: PhD A&#10;Content: Uh , not really as {disfmarker}&#10;Speaker: Professor C&#10;Content: Poten - potentially .&#10;Speaker: PhD A&#10;Content: uh , because of the normalization , yeah .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="1. The plan for investigating cross-correlations to remove false overlaps in the future involves PhD A looking into this area further. They have already done some preliminary work with three meetings, which seemed to yield good results, despite some issues with lapel microphones. Cross-correlations may be used to address these lapel microphone problems.&#10;2. There is no explicit mention in the transcript of a noticeable difference between wireless and wired microphones in those meetings. When PhD G asked if wireless microphones were different from wired ones, PhD A expressed uncertainty about whether there were even any wired microphones present in the meetings to compare with. Later, they confirmed that the distinction was merely lapel versus everything else, implying no significant difference between wireless and wired mikes.">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="1. The &quot;real situation&quot; of car noise in this study refers to the background noise recorded from people driving around in a car, without any artificial addition of noise to manipulate the signal-to-noise ratio. This is different from other studies where an artificial signal-to-noise ratio might be added to simulate real-world conditions. The advantage of using real car noise is that it provides a more authentic and accurate representation of the environment in which speech recognition systems would actually be used. However, it may also present additional challenges due to its complexity and variability.&#10;2. The models' performance was not as good as the best score because, as acknowledged by Professor C, &quot;the models are pretty crappy.&quot; Despite the best score being a relatively low error rate of 5% per digit, this is still far from perfect accuracy. It is possible that the complexity and variability of the car noise data made it difficult for the models to achieve higher levels of accuracy. Additionally, the use of lapel microphones in the meetings may have introduced additional challenges due to their propensity for picking up unwanted background noise.&#10;&#10;In summary, the &quot;real situation&quot; of car noise in this study provides a more authentic representation of real-world conditions than artificial signal-to-noise ratios, but it also presents additional challenges for speech recognition systems. The models' performance was not as good as the best score due to their inherent limitations and the complexity of the data.">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways.">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential.">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="Based on the transcript, the exact time it would take to transcribe the compiled audio samples from twelve meetings is not explicitly mentioned. However, Postdoc B proposed recording five-minute segments from various meetings, which would result in one hour of audio samples. They also mention that they have already processed half an hour of material.&#10;&#10;The feasibility of this task depends on several factors, such as the number of speakers, audio quality, and the effectiveness of any pre-processing techniques or conventions for encoding clarificational material and keywords. The transcript mentions concerns about accurately processing and transcribing short segments of speech, particularly when a single word can be placed incorrectly, leading to misunderstandings or errors.&#10;&#10;To make this task more feasible, several suggestions were made, including pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for precise representation of audio data. Implementing these improvements could help ensure better transcription accuracy and make the task more manageable.">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="1. The plan for examining cross-correlations to eliminate false overlaps involves PhD A looking into this area further, based on some preliminary work with three meetings that showed promising results. They aim to investigate the use of cross-correlations to address issues with lapel microphones, which may be causing some of these false overlaps.&#10;2. There was no explicit mention or noticeable difference observed between wireless and wired microphones during the meetings. The distinction brought up by PhD G was between lapel microphones and all other types of microphones, regardless of whether they were wireless or wired.">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="1. M Four: The speakers briefly used the term &quot;M four&quot; to refer to a meeting about meetings up to the fourth level (a meeting about a meeting about a meeting about meetings). This was suggested as an alternative to repeatedly saying &quot;the meeting about the meeting about...&quot;&#10;&#10;2. Pauses between lines: Grad F reminded the group to include pauses between the lines when transcribing or analyzing the content, likely for better readability or more accurate processing.&#10;&#10;3. Crown P Z: The group discussed using audio samples from the Crown P Z (possibly a collection of recordings), which would provide diverse speech samples for their project. However, they acknowledged that some segments might not be useful due to variations in audio quality and speakers.&#10;&#10;4. Iterating and developing models: Postdoc B, Grad F, and PhD G expressed interest in refining the process of iterating and creating models to perform forced alignment and acquire data. They recognized that this method's success would depend on various factors and might not always yield optimal results. PhD G mentioned they would inform the group when a solution for forced alignment is available.&#10;&#10;5. Cross-correlations and false overlaps: The group discussed recording five-minute segments from various meetings (excluding the first minute) to analyze cross-correlations and remove false overlaps during recordings. This approach aims to account for different speakers and varying audio quality in the samples. Postdoc B agreed to prepare a set of such segments from twelve meetings for further analysis.&#10;&#10;The main objective of the meeting was to explore ways of processing and transcribing speech samples from various sources, with an emphasis on improving accuracy and removing false overlaps during recordings.">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="The speakers in the meeting discussed the possibility of having transcribers also handle recording and formatting audio clips directly in the transcripts. This idea was raised by Grad F, who mentioned that they had personally processed eight meetings manually. Professor C asked for opinions on this matter. Postdoc B expressed support for the idea, stating that it would add an extra dimension to the transcribers' skill set and contribute to the composition of the transcript by incorporating those numbers directly.&#10;&#10;The potential benefits of having transcribers handle recording and formatting audio clips in the transcripts include:&#10;&#10;1. Enhancing transcribers' skill set: By performing these tasks, transcribers can develop additional skills beyond simple transcription.&#10;2. Improved integration of audio data: Directly incorporating audio clips into the transcripts allows for a more seamless combination of textual and audio information, potentially facilitating further analysis.&#10;3. Time-saving: If transcribers are already familiar with the content and context of the meetings, they might be able to efficiently record and format relevant audio clips while working on the transcripts.&#10;4. Better understanding of the material: Engaging in the recording and formatting process can provide transcribers with a deeper understanding of the content and its structure, which may lead to more accurate transcriptions.&#10;5. Increased control over audio data: By handling audio clips directly, transcribers have greater control over how the audio data is presented and integrated into the transcripts, potentially reducing errors or misunderstandings caused by automatic processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="1. Experiment Funding Update: Postdoc B mentioned that they had successfully funded their experiment. Although the source of funding is not specified in the transcript, it's clear that the project has received sufficient financial support to proceed.">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="Based on the transcript, the proposed adjustment to be made on the audio samples from twelve meetings is to convert them into a multi-channel format and adjust the timebands for precise representation. This adjustment should help improve the accuracy of processing and transcribing short segments of speech in these recordings.">
      <data key="d0">1</data>
    </edge>
    <edge source="The proposal is to record five-minute segments from various meetings, excluding the very beginning, in order to avoid any unusual occurrences that may take place during the initial minute. This idea was put forth by PhD A, who suggested that this approach would be beneficial as there are often strange things happening in the first minute of meetings which aren't really useful or good. Postdoc B agreed with this assessment and offered to prepare a set of such segments from twelve meetings for further analysis. The primary objective is to examine cross-correlations to remove false overlaps during the recordings. Both different speakers and varying audio quality were identified as factors contributing to the need for diverse samples." target="1. Editing the first half of the project: The team has been working on editing the first half of the project. Postdoc B mentioned they were in the process of editing but welcomed the news about funding for the experiment as it would be useful for refining the process and making the overlaps more precise throughout.&#10;&#10;2. Funding of the experiment: Experiment funding update indicates that Postdoc B successfully obtained sufficient financial support to proceed with the project, although the source is not specified in the transcript.&#10;&#10;3. Meeting in the BARCO room: Postdoc B, Liz, and Don met in the BARCO room to discuss an issue concerning the interplay between the transcript format and the processing required for the SRI recognizer. The postdoc mentioned checking for spelling errors, ensuring consistency of conventions, and running it through a channelize program to get the data into multi-channel format. They also planned to discuss this issue further in the afternoon.&#10;&#10;The team is making progress on editing and refining their project while simultaneously securing funding and addressing technical challenges. The collaboration mentioned between Liz, Don, and Postdoc B aims to improve the transcript format for better processing.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I mean , we could {pause} easily , get a section , you know , like say a minute or so , from every meeting that we have so f from the newer ones that we 're working on , everyone that we have . And then , should provide this .&#10;Speaker: PhD A&#10;Content: If it 's not the first minute of {disfmarker} of the meeting , that {disfmarker} that 's OK with me , but , in {disfmarker} in the first minute , uh , Often there are some {disfmarker} some strange things going on which {disfmarker} which aren't really , well , for , which {disfmarker} which aren't re re really good . So . What {disfmarker} what I 'd quite like , perhaps , is , to have , some five minutes of {disfmarker} of {disfmarker} of different meetings , so .&#10;Speaker: Postdoc B&#10;Content: Somewhere not in the very beginning , five minutes , OK .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: And , then I wanted to ask you" target="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker} that , there is a system ,&#10;Speaker: PhD D&#10;Content: So , current {disfmarker} This week .&#10;Speaker: Postdoc B&#10;Content: Yes . Might not be what you need .&#10;Speaker: Grad F&#10;Content: Yeah , so if we could get a couple meetings done with that level of precision I think that would be a good idea .&#10;Speaker: PhD A&#10;Content: OK . Yeah .&#10;Speaker: Postdoc B&#10;Content: Oh , OK . Uh , how {disfmarker} how m much time {disfmarker} so the meetings vary in length , what are we talking about in terms of the number of minutes you 'd like to have as your {disfmarker} as your training set ?&#10;Speaker: PhD A&#10;Content: It seems to me that it would be good to have , a few minutes from {disfmarker} from different meetings , so . But I 'm not sure about how much .&#10;Speaker: Postdoc B&#10;Content: OK , now you 're saying different meetings because of different speakers or because of different audio quality or both or {disfmarker} ?&#10;" target="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: Postdoc B&#10;Content: OK , so then , if that 's five minutes per meeting we 've got like twelve minutes , twelve meetings , roughly , that I 'm {disfmarker} that I 've been working with , then {disfmarker}&#10;Speaker: Professor C&#10;Content: Of {disfmarker} of {disfmarker} of the meetings that you 're working with , how many of them are different , tha&#10;Speaker: PhD A&#10;Content: No .&#10;Speaker: Professor C&#10;Content: are there any of them that are different than , these two meetings ?&#10;Speaker: Postdoc B&#10;Content: Well {disfmarker} oh wa in terms of the speakers or the conditions or the ?&#10;Speaker: Professor C&#10;Content: Yeah , speakers . Sorry .&#10;Speaker: PhD A&#10;Content: Yeah , that {disfmarker}&#10;Speaker: Postdoc B&#10;Content: Um , we have different combinations of speakers .&#10;Speaker: Professor C&#10;Content: So .&#10;Speaker: Postdoc B&#10;Content: I mean , just from what I 've seen , uh , there are some where ," target="The speakers, including Postdoc B, Grad F, and PhD G, expressed interest in the idea of iterating and developing models to perform a forced alignment and acquire the data. They discussed the importance of refining the process and the potential benefits of such an approach. However, they also acknowledged that the success of this method would depend on various factors, and it might not always yield optimal results. PhD G mentioned that they would let the group know when they have a solution for forced alignment.">
      <data key="d0">1</data>
    </edge>
    <edge source="pause} future work . Well {disfmarker} well what I want to do is to {disfmarker} to look into cross - correlations for {disfmarker} for removing those , false overlaps .&#10;Speaker: Postdoc B&#10;Content: Wonderful .&#10;Speaker: PhD G&#10;Content: Are the , um , wireless , different than the wired , mikes , at all ? I mean , have you noticed any difference ?&#10;Speaker: PhD A&#10;Content: I 'm {disfmarker} I 'm not sure , um , if {disfmarker} if there are any wired mikes in those meetings , or , uh , I have {disfmarker} have to loo have a look at them but , I 'm {disfmarker} I 'm {disfmarker} I think there 's no difference between ,&#10;Speaker: PhD G&#10;Content: So it 's just the lapel versus everything else ?&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Postdoc B&#10;Content: OK , so then , if that 's five minutes per meeting we 've got like twelve minutes , twelve meetings" target="1. The plan for investigating cross-correlations to remove false overlaps in the future involves PhD A looking into this area further. They have already done some preliminary work with three meetings, which seemed to yield good results, despite some issues with lapel microphones. Cross-correlations may be used to address these lapel microphone problems.&#10;2. There is no explicit mention in the transcript of a noticeable difference between wireless and wired microphones in those meetings. When PhD G asked if wireless microphones were different from wired ones, PhD A expressed uncertainty about whether there were even any wired microphones present in the meetings to compare with. Later, they confirmed that the distinction was merely lapel versus everything else, implying no significant difference between wireless and wired mikes.">
      <data key="d0">1</data>
    </edge>
    <edge source="pause} future work . Well {disfmarker} well what I want to do is to {disfmarker} to look into cross - correlations for {disfmarker} for removing those , false overlaps .&#10;Speaker: Postdoc B&#10;Content: Wonderful .&#10;Speaker: PhD G&#10;Content: Are the , um , wireless , different than the wired , mikes , at all ? I mean , have you noticed any difference ?&#10;Speaker: PhD A&#10;Content: I 'm {disfmarker} I 'm not sure , um , if {disfmarker} if there are any wired mikes in those meetings , or , uh , I have {disfmarker} have to loo have a look at them but , I 'm {disfmarker} I 'm {disfmarker} I think there 's no difference between ,&#10;Speaker: PhD G&#10;Content: So it 's just the lapel versus everything else ?&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Postdoc B&#10;Content: OK , so then , if that 's five minutes per meeting we 've got like twelve minutes , twelve meetings" target="1. The plan for examining cross-correlations to eliminate false overlaps involves PhD A looking into this area further, based on some preliminary work with three meetings that showed promising results. They aim to investigate the use of cross-correlations to address issues with lapel microphones, which may be causing some of these false overlaps.&#10;2. There was no explicit mention or noticeable difference observed between wireless and wired microphones during the meetings. The distinction brought up by PhD G was between lapel microphones and all other types of microphones, regardless of whether they were wireless or wired.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: OK , now you 're saying different meetings because of different speakers or because of different audio quality or both or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Both {disfmarker} both . Different {disfmarker} different number of speakers , different speakers , different {pause} conditions .&#10;Speaker: Postdoc B&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Yeah , we don't have that much variety in meetings yet , uh , I mean we have this meeting and the feature meeting and we have a couple others that we have uh , couple examples of . But {disfmarker} but , uh ,&#10;Speaker: PhD A&#10;Content: Yeah , m Yeah .&#10;Speaker: Postdoc B&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: Even probably with the gains {pause} differently will affect it , you mean {disfmarker}&#10;Speaker: PhD A&#10;Content: Uh , not really as {disfmarker}&#10;Speaker: Professor C&#10;Content: Poten - potentially .&#10;Speaker: PhD A&#10;Content: uh , because of the normalization , yeah .&#10;" target="1. The plan for investigating cross-correlations to remove false overlaps in the future involves PhD A looking into this area further. They have already done some preliminary work with three meetings, which seemed to yield good results, despite some issues with lapel microphones. Cross-correlations may be used to address these lapel microphone problems.&#10;2. There is no explicit mention in the transcript of a noticeable difference between wireless and wired microphones in those meetings. When PhD G asked if wireless microphones were different from wired ones, PhD A expressed uncertainty about whether there were even any wired microphones present in the meetings to compare with. Later, they confirmed that the distinction was merely lapel versus everything else, implying no significant difference between wireless and wired mikes.">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;blending thing&quot; referred to in the spectrograms is the observation that the speech signal doesn't always correspond to a clear sequence of phones as given in dictionary pronunciations. Instead, there are instances where different features (such as voicing or nasality) from different phones overlap and blend together in complex ways. This blending is not easily named or categorized, which makes it difficult to represent using traditional phonetic transcriptions. The speakers suggest that this blending might be better represented using a score-like notation where each line corresponds to a particular feature, allowing for a more accurate representation of the overlapping and complex nature of these speech signals." target=" here and he was looking at the spectrograms of the more difficult ones . Uh , he didn't know what to say , about , what is the sequence of phones there . They came up with some compromise . Because that really wasn't what it look like . It didn't look like a sequence of phones&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: it look like this blending thing happening here and here and here .&#10;Speaker: Grad F&#10;Content: Yeah , so you have this feature here , and , overlap , yeah .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: There was no name for that .&#10;Speaker: PhD G&#10;Content: But {disfmarker} Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: But it still is {disfmarker} there 's a {disfmarker} there are two steps . One {disfmarker} you know , one is going from a dictionary pronunciation of something , like , &quot; gonna see">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;blending thing&quot; referred to in the spectrograms is the observation that the speech signal doesn't always correspond to a clear sequence of phones as given in dictionary pronunciations. Instead, there are instances where different features (such as voicing or nasality) from different phones overlap and blend together in complex ways. This blending is not easily named or categorized, which makes it difficult to represent using traditional phonetic transcriptions. The speakers suggest that this blending might be better represented using a score-like notation where each line corresponds to a particular feature, allowing for a more accurate representation of the overlapping and complex nature of these speech signals." target=" Professor C&#10;Content: Right .&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: and it really , I actually think that data should be used more . That maybe , although I think the meeting context is great , that he has transcriptions that give you the actual phone sequence . And you can go from {disfmarker} not from that to the articulatory features , but that would be a better starting point for marking , the gestural features , then , data where you don't have that , because , we {disfmarker} you wanna know , both about the way that they 're producing a certain sound , and what kinds of , you know what kinds of , phonemic , differences you get between these , transcribed , sequences and the dictionary ones .&#10;Speaker: Professor C&#10;Content: Well you might be right that mi might be the way at getting at , what I was talking about , but the particular reason why I was interested in doing that was because I remember , when that happened , and , John Ohala was over here and he was looking at the spectrograms of the more difficult ones . Uh , he didn't know what to say , about , what is the sequence">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;blending thing&quot; referred to in the spectrograms is the observation that the speech signal doesn't always correspond to a clear sequence of phones as given in dictionary pronunciations. Instead, there are instances where different features (such as voicing or nasality) from different phones overlap and blend together in complex ways. This blending is not easily named or categorized, which makes it difficult to represent using traditional phonetic transcriptions. The speakers suggest that this blending might be better represented using a score-like notation where each line corresponds to a particular feature, allowing for a more accurate representation of the overlapping and complex nature of these speech signals." target=": Grad F&#10;Content: But {disfmarker} What I 'm imagining is a score - like notation , where each line is a particular feature .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Right ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: so you would say , you know , it 's voiced through here , and so you have label here , and you have nas nasal here , and , they {disfmarker} they could be overlapping in all sorts of bizarre ways that don't correspond to the timing on phones .&#10;Speaker: Professor C&#10;Content: I mean this is the kind of reason why {disfmarker} I remember when at one of the Switchboard , workshops , that uh when we talked about doing the transcription project , Dave Talkin said , &quot; can't be done &quot; .&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: He was {disfmarker} he was , what {disfmarker} what he meant was that this isn't , you know , a sequence of phones , and when you actually look at Switchboard that">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;blending thing&quot; referred to in the spectrograms is the observation that the speech signal doesn't always correspond to a clear sequence of phones as given in dictionary pronunciations. Instead, there are instances where different features (such as voicing or nasality) from different phones overlap and blend together in complex ways. This blending is not easily named or categorized, which makes it difficult to represent using traditional phonetic transcriptions. The speakers suggest that this blending might be better represented using a score-like notation where each line corresponds to a particular feature, allowing for a more accurate representation of the overlapping and complex nature of these speech signals." target=" the transcription of what was really said , and which syllables were reduced . Uh , if you 're gonna add the features it 's also useful to have some level of representation which is , is a reduced {disfmarker} it 's a pronunciation variant , that currently the dictionaries don't give you&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: because if you add them to the dictionary and you run recognition , you , you add confusion .&#10;Speaker: Professor C&#10;Content: Right . Right .&#10;Speaker: PhD G&#10;Content: So people purposely don't add them . So it 's useful to know which variant was {disfmarker} was produced , at least at the phone level .&#10;Speaker: PhD D&#10;Content: So it would be {disfmarker} it would be great if we had , either these kind of , labelings on , the same portion of Switchboard that Steve marked , or , Steve 's type markings on this data , with these .&#10;Speaker: PhD G&#10;Content: Right . That 's all , I mean . Exactly .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;blending thing&quot; referred to in the spectrograms is the observation that the speech signal doesn't always correspond to a clear sequence of phones as given in dictionary pronunciations. Instead, there are instances where different features (such as voicing or nasality) from different phones overlap and blend together in complex ways. This blending is not easily named or categorized, which makes it difficult to represent using traditional phonetic transcriptions. The speakers suggest that this blending might be better represented using a score-like notation where each line corresponds to a particular feature, allowing for a more accurate representation of the overlapping and complex nature of these speech signals." target="1. The plan for investigating cross-correlations to remove false overlaps in the future involves PhD A looking into this area further. They have already done some preliminary work with three meetings, which seemed to yield good results, despite some issues with lapel microphones. Cross-correlations may be used to address these lapel microphone problems.&#10;2. There is no explicit mention in the transcript of a noticeable difference between wireless and wired microphones in those meetings. When PhD G asked if wireless microphones were different from wired ones, PhD A expressed uncertainty about whether there were even any wired microphones present in the meetings to compare with. Later, they confirmed that the distinction was merely lapel versus everything else, implying no significant difference between wireless and wired mikes.">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;blending thing&quot; referred to in the spectrograms is the observation that the speech signal doesn't always correspond to a clear sequence of phones as given in dictionary pronunciations. Instead, there are instances where different features (such as voicing or nasality) from different phones overlap and blend together in complex ways. This blending is not easily named or categorized, which makes it difficult to represent using traditional phonetic transcriptions. The speakers suggest that this blending might be better represented using a score-like notation where each line corresponds to a particular feature, allowing for a more accurate representation of the overlapping and complex nature of these speech signals." target="1. The &quot;real situation&quot; of car noise in this study refers to the background noise recorded from people driving around in a car, without any artificial addition of noise to manipulate the signal-to-noise ratio. This is different from other studies where an artificial signal-to-noise ratio might be added to simulate real-world conditions. The advantage of using real car noise is that it provides a more authentic and accurate representation of the environment in which speech recognition systems would actually be used. However, it may also present additional challenges due to its complexity and variability.&#10;2. The models' performance was not as good as the best score because, as acknowledged by Professor C, &quot;the models are pretty crappy.&quot; Despite the best score being a relatively low error rate of 5% per digit, this is still far from perfect accuracy. It is possible that the complexity and variability of the car noise data made it difficult for the models to achieve higher levels of accuracy. Additionally, the use of lapel microphones in the meetings may have introduced additional challenges due to their propensity for picking up unwanted background noise.&#10;&#10;In summary, the &quot;real situation&quot; of car noise in this study provides a more authentic representation of real-world conditions than artificial signal-to-noise ratios, but it also presents additional challenges for speech recognition systems. The models' performance was not as good as the best score due to their inherent limitations and the complexity of the data.">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;blending thing&quot; referred to in the spectrograms is the observation that the speech signal doesn't always correspond to a clear sequence of phones as given in dictionary pronunciations. Instead, there are instances where different features (such as voicing or nasality) from different phones overlap and blend together in complex ways. This blending is not easily named or categorized, which makes it difficult to represent using traditional phonetic transcriptions. The speakers suggest that this blending might be better represented using a score-like notation where each line corresponds to a particular feature, allowing for a more accurate representation of the overlapping and complex nature of these speech signals." target="1. The plan for examining cross-correlations to eliminate false overlaps involves PhD A looking into this area further, based on some preliminary work with three meetings that showed promising results. They aim to investigate the use of cross-correlations to address issues with lapel microphones, which may be causing some of these false overlaps.&#10;2. There was no explicit mention or noticeable difference observed between wireless and wired microphones during the meetings. The distinction brought up by PhD G was between lapel microphones and all other types of microphones, regardless of whether they were wireless or wired.">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;blending thing&quot; referred to in the spectrograms is the observation that the speech signal doesn't always correspond to a clear sequence of phones as given in dictionary pronunciations. Instead, there are instances where different features (such as voicing or nasality) from different phones overlap and blend together in complex ways. This blending is not easily named or categorized, which makes it difficult to represent using traditional phonetic transcriptions. The speakers suggest that this blending might be better represented using a score-like notation where each line corresponds to a particular feature, allowing for a more accurate representation of the overlapping and complex nature of these speech signals." target="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time.">
      <data key="d0">1</data>
    </edge>
    <edge source="The &quot;blending thing&quot; referred to in the spectrograms is the observation that the speech signal doesn't always correspond to a clear sequence of phones as given in dictionary pronunciations. Instead, there are instances where different features (such as voicing or nasality) from different phones overlap and blend together in complex ways. This blending is not easily named or categorized, which makes it difficult to represent using traditional phonetic transcriptions. The speakers suggest that this blending might be better represented using a score-like notation where each line corresponds to a particular feature, allowing for a more accurate representation of the overlapping and complex nature of these speech signals." target="Based on the transcript, Grad F used normalized features to distinguish between foreground and background speech in multiple channels of a recording. The normalization techniques included:&#10;&#10;1. Normalized energy: Calculated by dividing the energy of each channel by the mean energy over all channels and the minimum energy within each channel.&#10;2. Normalization of loudness, spectral features (such as modified loudness), and other unspecified features. These were also incorporated into the feature vector for improved distinction between foreground and background speech.&#10;&#10;The system was tested on three or four meetings and seemed to work fairly well, although there were some issues with lapel microphones. The performance of these normalization techniques in distinguishing between foreground and background speech appears to be promising, but a more thorough evaluation and potential optimization may be required for better results.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Grad F&#10;Content: But {disfmarker} What I 'm imagining is a score - like notation , where each line is a particular feature .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Right ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: so you would say , you know , it 's voiced through here , and so you have label here , and you have nas nasal here , and , they {disfmarker} they could be overlapping in all sorts of bizarre ways that don't correspond to the timing on phones .&#10;Speaker: Professor C&#10;Content: I mean this is the kind of reason why {disfmarker} I remember when at one of the Switchboard , workshops , that uh when we talked about doing the transcription project , Dave Talkin said , &quot; can't be done &quot; .&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: He was {disfmarker} he was , what {disfmarker} what he meant was that this isn't , you know , a sequence of phones , and when you actually look at Switchboard that" target="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways.">
      <data key="d0">1</data>
    </edge>
    <edge source=" the transcription of what was really said , and which syllables were reduced . Uh , if you 're gonna add the features it 's also useful to have some level of representation which is , is a reduced {disfmarker} it 's a pronunciation variant , that currently the dictionaries don't give you&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: because if you add them to the dictionary and you run recognition , you , you add confusion .&#10;Speaker: Professor C&#10;Content: Right . Right .&#10;Speaker: PhD G&#10;Content: So people purposely don't add them . So it 's useful to know which variant was {disfmarker} was produced , at least at the phone level .&#10;Speaker: PhD D&#10;Content: So it would be {disfmarker} it would be great if we had , either these kind of , labelings on , the same portion of Switchboard that Steve marked , or , Steve 's type markings on this data , with these .&#10;Speaker: PhD G&#10;Content: Right . That 's all , I mean . Exactly .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;" target="The compromised string referred to in the conversation is the best compromise or representation that a group of people could come up with to describe what happened during speech production. It is not necessarily the exact transcription of what was really said, but rather an approximation that takes into account overlapping processes such as voicing and nasality.&#10;&#10;Compared to phone labels, which are simplified representations of speech sounds, this compromised string is likely to be more accurate in capturing the complexities of actual speech production. This is because the compromised string can potentially incorporate features that are not included in phone labels, such as reduced syllables and pronunciation variants.&#10;&#10;However, it is important to note that the compromised string may still introduce some level of confusion or ambiguity, especially if it is not based on a well-defined set of rules or conventions. This is why further research is needed to determine the best way to represent and transcribe speech production in a way that balances accuracy and clarity.">
      <data key="d0">1</data>
    </edge>
    <edge source=" the transcription of what was really said , and which syllables were reduced . Uh , if you 're gonna add the features it 's also useful to have some level of representation which is , is a reduced {disfmarker} it 's a pronunciation variant , that currently the dictionaries don't give you&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: because if you add them to the dictionary and you run recognition , you , you add confusion .&#10;Speaker: Professor C&#10;Content: Right . Right .&#10;Speaker: PhD G&#10;Content: So people purposely don't add them . So it 's useful to know which variant was {disfmarker} was produced , at least at the phone level .&#10;Speaker: PhD D&#10;Content: So it would be {disfmarker} it would be great if we had , either these kind of , labelings on , the same portion of Switchboard that Steve marked , or , Steve 's type markings on this data , with these .&#10;Speaker: PhD G&#10;Content: Right . That 's all , I mean . Exactly .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;" target="The difference between pronunciation models in dictionaries and the pronunciations that people produce lies in the variability and complexity of human speech. Dictionary pronunciations are typically represented using standardized symbols and conventions, which may not capture the full range of variation and reduction that occur in natural speech.&#10;&#10;Steve's work on labeling contributes to obtaining information about the actual pronunciations produced by speakers, including reduced syllables and variants that are not typically included in dictionaries. This information can be useful for developing more accurate speech recognition systems and models of speech production. By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for investigating cross-correlations to remove false overlaps in the future involves PhD A looking into this area further. They have already done some preliminary work with three meetings, which seemed to yield good results, despite some issues with lapel microphones. Cross-correlations may be used to address these lapel microphone problems.&#10;2. There is no explicit mention in the transcript of a noticeable difference between wireless and wired microphones in those meetings. When PhD G asked if wireless microphones were different from wired ones, PhD A expressed uncertainty about whether there were even any wired microphones present in the meetings to compare with. Later, they confirmed that the distinction was merely lapel versus everything else, implying no significant difference between wireless and wired mikes." target="&#10;Speaker: PhD A&#10;Content: So , uh it c can be do in an unsupervised way .&#10;Speaker: Postdoc B&#10;Content: Uh - huh .&#10;Speaker: PhD A&#10;Content: So .&#10;Speaker: Postdoc B&#10;Content: Excellent . Excellent , OK .&#10;Speaker: PhD A&#10;Content: I 'm {disfmarker} I 'm not sure , but , for {disfmarker} for {disfmarker} for those three meetings whi which I {disfmarker} which I did , it seems to be , quite well , but , there are some {disfmarker} some {disfmarker} as I said some problems with the lapel mike , but , perhaps we can do something with {disfmarker} with cross - correlations to , to get rid of the {disfmarker} of those . And . Yeah . That 's {disfmarker} that 's what I {disfmarker} that 's my {pause} future work . Well {disfmarker} well what I want to do is to {disfmarker} to look into cross - correlations">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for investigating cross-correlations to remove false overlaps in the future involves PhD A looking into this area further. They have already done some preliminary work with three meetings, which seemed to yield good results, despite some issues with lapel microphones. Cross-correlations may be used to address these lapel microphone problems.&#10;2. There is no explicit mention in the transcript of a noticeable difference between wireless and wired microphones in those meetings. When PhD G asked if wireless microphones were different from wired ones, PhD A expressed uncertainty about whether there were even any wired microphones present in the meetings to compare with. Later, they confirmed that the distinction was merely lapel versus everything else, implying no significant difference between wireless and wired mikes." target="aker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: uh , but for free recognition I 'm {disfmarker} it 'll probably not be good enough . We 'll probably get lots of errors because of the cross - talk , and , noises and things .&#10;Speaker: PhD A&#10;Content: Yep .&#10;Speaker: Professor C&#10;Content: Good s I think that 's probably our agenda , or starting up there .&#10;Speaker: Postdoc B&#10;Content: Oh I wanted to ask one thing , the microphones {disfmarker} the new microphones ,&#10;Speaker: Professor C&#10;Content: Yeah ? K .&#10;Speaker: Postdoc B&#10;Content: when do we get , uh ?&#10;Speaker: Grad F&#10;Content: Uh , they said it would take about a week .&#10;Speaker: Postdoc B&#10;Content: Oh , exciting . K . K .&#10;Speaker: Professor C&#10;Content: K .&#10;Speaker: PhD D&#10;Content: You ordered them already ?&#10;Speaker: Grad F&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Great .&#10;Speaker: PhD G&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for investigating cross-correlations to remove false overlaps in the future involves PhD A looking into this area further. They have already done some preliminary work with three meetings, which seemed to yield good results, despite some issues with lapel microphones. Cross-correlations may be used to address these lapel microphone problems.&#10;2. There is no explicit mention in the transcript of a noticeable difference between wireless and wired microphones in those meetings. When PhD G asked if wireless microphones were different from wired ones, PhD A expressed uncertainty about whether there were even any wired microphones present in the meetings to compare with. Later, they confirmed that the distinction was merely lapel versus everything else, implying no significant difference between wireless and wired mikes." target="1. The &quot;real situation&quot; of car noise in this study refers to the background noise recorded from people driving around in a car, without any artificial addition of noise to manipulate the signal-to-noise ratio. This is different from other studies where an artificial signal-to-noise ratio might be added to simulate real-world conditions. The advantage of using real car noise is that it provides a more authentic and accurate representation of the environment in which speech recognition systems would actually be used. However, it may also present additional challenges due to its complexity and variability.&#10;2. The models' performance was not as good as the best score because, as acknowledged by Professor C, &quot;the models are pretty crappy.&quot; Despite the best score being a relatively low error rate of 5% per digit, this is still far from perfect accuracy. It is possible that the complexity and variability of the car noise data made it difficult for the models to achieve higher levels of accuracy. Additionally, the use of lapel microphones in the meetings may have introduced additional challenges due to their propensity for picking up unwanted background noise.&#10;&#10;In summary, the &quot;real situation&quot; of car noise in this study provides a more authentic representation of real-world conditions than artificial signal-to-noise ratios, but it also presents additional challenges for speech recognition systems. The models' performance was not as good as the best score due to their inherent limitations and the complexity of the data.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for investigating cross-correlations to remove false overlaps in the future involves PhD A looking into this area further. They have already done some preliminary work with three meetings, which seemed to yield good results, despite some issues with lapel microphones. Cross-correlations may be used to address these lapel microphone problems.&#10;2. There is no explicit mention in the transcript of a noticeable difference between wireless and wired microphones in those meetings. When PhD G asked if wireless microphones were different from wired ones, PhD A expressed uncertainty about whether there were even any wired microphones present in the meetings to compare with. Later, they confirmed that the distinction was merely lapel versus everything else, implying no significant difference between wireless and wired mikes." target="1. Acoustic features: The lowest level of transcription would include acoustic features, which involve physical measurements of the speech signal such as frequency, amplitude, and duration.&#10;&#10;2. Phonetic level: This refers to the representation of speech sounds using symbols from the International Phonetic Alphabet (IPA). It includes detailed information about the pronunciation of each word, including distinctions between similar sounds like &quot;p&quot; and &quot;b&quot; or &quot;s&quot; and &quot;z&quot;.&#10;&#10;3. Relative gain: This term likely refers to the relative loudness or volume of different parts of the speech signal. Including this information in the transcriptions would provide insight into how loudly or softly certain words or sounds are spoken.&#10;&#10;4. Telephone bandwidth limitations: Since some of the discussions involve telephone-quality audio, it is important to consider the impact of limited bandwidth on the quality and detail of the transcriptions. This might result in less precise acoustic measurements or phonetic representations, as well as potential difficulties in accurately distinguishing between different speakers' voices.&#10;&#10;5. Articulatory gestures: Although not explicitly mentioned in the provided excerpt, Professor C suggests including articulatory features in the transcriptions. This would involve representing the movements and positions of the speaker's speech articulators (e.g., tongue, lips, jaw) during speech production.&#10;&#10;6. Comparability on Switchboard: If using the Switchboard dataset, the full continuum of transcriptions should include information about how each word or sound compares to others in terms of acoustic and phonetic properties. This would facilitate analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for investigating cross-correlations to remove false overlaps in the future involves PhD A looking into this area further. They have already done some preliminary work with three meetings, which seemed to yield good results, despite some issues with lapel microphones. Cross-correlations may be used to address these lapel microphone problems.&#10;2. There is no explicit mention in the transcript of a noticeable difference between wireless and wired microphones in those meetings. When PhD G asked if wireless microphones were different from wired ones, PhD A expressed uncertainty about whether there were even any wired microphones present in the meetings to compare with. Later, they confirmed that the distinction was merely lapel versus everything else, implying no significant difference between wireless and wired mikes." target="Based on the transcript, the exact time it would take to transcribe the compiled audio samples from twelve meetings is not explicitly mentioned. However, Postdoc B proposed recording five-minute segments from various meetings, which would result in one hour of audio samples. They also mention that they have already processed half an hour of material.&#10;&#10;The feasibility of this task depends on several factors, such as the number of speakers, audio quality, and the effectiveness of any pre-processing techniques or conventions for encoding clarificational material and keywords. The transcript mentions concerns about accurately processing and transcribing short segments of speech, particularly when a single word can be placed incorrectly, leading to misunderstandings or errors.&#10;&#10;To make this task more feasible, several suggestions were made, including pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for precise representation of audio data. Implementing these improvements could help ensure better transcription accuracy and make the task more manageable.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for investigating cross-correlations to remove false overlaps in the future involves PhD A looking into this area further. They have already done some preliminary work with three meetings, which seemed to yield good results, despite some issues with lapel microphones. Cross-correlations may be used to address these lapel microphone problems.&#10;2. There is no explicit mention in the transcript of a noticeable difference between wireless and wired microphones in those meetings. When PhD G asked if wireless microphones were different from wired ones, PhD A expressed uncertainty about whether there were even any wired microphones present in the meetings to compare with. Later, they confirmed that the distinction was merely lapel versus everything else, implying no significant difference between wireless and wired mikes." target="1. The plan for examining cross-correlations to eliminate false overlaps involves PhD A looking into this area further, based on some preliminary work with three meetings that showed promising results. They aim to investigate the use of cross-correlations to address issues with lapel microphones, which may be causing some of these false overlaps.&#10;2. There was no explicit mention or noticeable difference observed between wireless and wired microphones during the meetings. The distinction brought up by PhD G was between lapel microphones and all other types of microphones, regardless of whether they were wireless or wired.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for investigating cross-correlations to remove false overlaps in the future involves PhD A looking into this area further. They have already done some preliminary work with three meetings, which seemed to yield good results, despite some issues with lapel microphones. Cross-correlations may be used to address these lapel microphone problems.&#10;2. There is no explicit mention in the transcript of a noticeable difference between wireless and wired microphones in those meetings. When PhD G asked if wireless microphones were different from wired ones, PhD A expressed uncertainty about whether there were even any wired microphones present in the meetings to compare with. Later, they confirmed that the distinction was merely lapel versus everything else, implying no significant difference between wireless and wired mikes." target="Based on the transcript, the proposed adjustment to be made on the audio samples from twelve meetings is to convert them into a multi-channel format and adjust the timebands for precise representation. This adjustment should help improve the accuracy of processing and transcribing short segments of speech in these recordings.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, including Postdoc B, Grad F, and PhD G, expressed interest in the idea of iterating and developing models to perform a forced alignment and acquire the data. They discussed the importance of refining the process and the potential benefits of such an approach. However, they also acknowledged that the success of this method would depend on various factors, and it might not always yield optimal results. PhD G mentioned that they would let the group know when they have a solution for forced alignment." target=": Yeah . Yeah . Anyway , this is , not an urgent thing at all ,&#10;Speaker: Postdoc B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: just it came up .&#10;Speaker: PhD D&#10;Content: It 'd be very interesting though , to have {pause} that data .&#10;Speaker: Postdoc B&#10;Content: I think so , too .&#10;Speaker: Grad F&#10;Content: I wonder , how would you do a forced alignment ?&#10;Speaker: PhD G&#10;Content: Yeah . Might {disfmarker}&#10;Speaker: Postdoc B&#10;Content: Interesting idea .&#10;Speaker: Grad F&#10;Content: To {disfmarker} to {disfmarker} I mean , you 'd wanna iterate , somehow . Yeah . It 's interesting thing to think about .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: PhD G&#10;Content: It might be {disfmarker}&#10;Speaker: Grad F&#10;Content: I mean you 'd {disfmarker} you 'd want models for spreading .&#10;Speaker: PhD G&#10;Content: I was thinking it might be n&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, including Postdoc B, Grad F, and PhD G, expressed interest in the idea of iterating and developing models to perform a forced alignment and acquire the data. They discussed the importance of refining the process and the potential benefits of such an approach. However, they also acknowledged that the success of this method would depend on various factors, and it might not always yield optimal results. PhD G mentioned that they would let the group know when they have a solution for forced alignment." target="Speaker: Postdoc B&#10;Content: Alright .&#10;Speaker: Professor C&#10;Content: So , uh {disfmarker}&#10;Speaker: Grad F&#10;Content: Um , so I wanted to discuss digits briefly , but that won't take too long .&#10;Speaker: Professor C&#10;Content: Oh good . Right . OK , agenda items , Uh , we have digits , What else we got ?&#10;Speaker: PhD A&#10;Content: New version of the presegmentation .&#10;Speaker: Professor C&#10;Content: New version of presegmentation .&#10;Speaker: Postdoc B&#10;Content: Um , do we wanna say something about the , an update of the , uh , transcript ?&#10;Speaker: PhD G&#10;Content: Yeah , why don't you summarize the {disfmarker}&#10;Speaker: Professor C&#10;Content: Update on transcripts .&#10;Speaker: PhD G&#10;Content: And I guess that includes some {disfmarker} the filtering for the , the ASI refs , too .&#10;Speaker: Postdoc B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: Filtering for what ?&#10;Speaker: PhD G&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, including Postdoc B, Grad F, and PhD G, expressed interest in the idea of iterating and developing models to perform a forced alignment and acquire the data. They discussed the importance of refining the process and the potential benefits of such an approach. However, they also acknowledged that the success of this method would depend on various factors, and it might not always yield optimal results. PhD G mentioned that they would let the group know when they have a solution for forced alignment." target="Content: It {disfmarker} you know it really depends on a lot of things ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: but , I would have maybe a transciber , uh , look at the result of a forced alignment and then adjust those .&#10;Speaker: PhD A&#10;Content: Yeah . To a adjust them , or , yeah . Yeah , yeah .&#10;Speaker: PhD G&#10;Content: That might save some time .&#10;Speaker: PhD A&#10;Content: Yeah , great .&#10;Speaker: PhD G&#10;Content: If they 're horrible it won't help at all , but they might not be horrible .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: So {disfmarker} but I 'll let you know when we , uh , have that .&#10;Speaker: PhD A&#10;Content: OK , great .&#10;Speaker: Postdoc B&#10;Content: How many minutes would you want from {disfmarker} I mean , we could {pause} easily , get a section , you know , like say a minute or so , from every meeting that we have so f">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, including Postdoc B, Grad F, and PhD G, expressed interest in the idea of iterating and developing models to perform a forced alignment and acquire the data. They discussed the importance of refining the process and the potential benefits of such an approach. However, they also acknowledged that the success of this method would depend on various factors, and it might not always yield optimal results. PhD G mentioned that they would let the group know when they have a solution for forced alignment." target=" .&#10;Speaker: PhD G&#10;Content: So {disfmarker}&#10;Speaker: Professor C&#10;Content: There {disfmarker} the first {disfmarker} the first half .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: And so I was in the process of like editing them but this is wonderful news .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Alright .&#10;Speaker: Postdoc B&#10;Content: We funded the experiment with , uh {disfmarker} also we were thinking maybe applying that that to getting the , Yeah , that 'll be , very useful to getting the overlaps to be more precise all the way through .&#10;Speaker: Professor C&#10;Content: So this , blends nicely into the update on transcripts .&#10;Speaker: Postdoc B&#10;Content: Yes , it does . So , um , {comment} um , Liz , and {disfmarker} and Don , and I met this morning , in the BARCO room , with the lecture hall ,&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD G&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, including Postdoc B, Grad F, and PhD G, expressed interest in the idea of iterating and developing models to perform a forced alignment and acquire the data. They discussed the importance of refining the process and the potential benefits of such an approach. However, they also acknowledged that the success of this method would depend on various factors, and it might not always yield optimal results. PhD G mentioned that they would let the group know when they have a solution for forced alignment." target=": PhD G&#10;Content: uh {disfmarker} I think that would be a good i good idea .&#10;Speaker: Professor C&#10;Content: Yeah , so I think that i i we also don't have , I mean , we 've got a good start on it , but we don't have a really good , meeting , recorder or recognizer or transcriber or anything yet , so . So , I mean another way to look at this is to , is to , uh , do some stuff on Switchboard which has all this other , stuff to it .&#10;Speaker: PhD G&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: And then , um , As we get , further down the road and we can do more things ahead of time , we can , do some of the same things to the meeting data .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: And I 'm {disfmarker} and these people might {disfmarker} they {disfmarker} they are , s most of">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, including Postdoc B, Grad F, and PhD G, expressed interest in the idea of iterating and developing models to perform a forced alignment and acquire the data. They discussed the importance of refining the process and the potential benefits of such an approach. However, they also acknowledged that the success of this method would depend on various factors, and it might not always yield optimal results. PhD G mentioned that they would let the group know when they have a solution for forced alignment." target="1. M Four: The speakers briefly used the term &quot;M four&quot; to refer to a meeting about meetings up to the fourth level (a meeting about a meeting about a meeting about meetings). This was suggested as an alternative to repeatedly saying &quot;the meeting about the meeting about...&quot;&#10;&#10;2. Pauses between lines: Grad F reminded the group to include pauses between the lines when transcribing or analyzing the content, likely for better readability or more accurate processing.&#10;&#10;3. Crown P Z: The group discussed using audio samples from the Crown P Z (possibly a collection of recordings), which would provide diverse speech samples for their project. However, they acknowledged that some segments might not be useful due to variations in audio quality and speakers.&#10;&#10;4. Iterating and developing models: Postdoc B, Grad F, and PhD G expressed interest in refining the process of iterating and creating models to perform forced alignment and acquire data. They recognized that this method's success would depend on various factors and might not always yield optimal results. PhD G mentioned they would inform the group when a solution for forced alignment is available.&#10;&#10;5. Cross-correlations and false overlaps: The group discussed recording five-minute segments from various meetings (excluding the first minute) to analyze cross-correlations and remove false overlaps during recordings. This approach aims to account for different speakers and varying audio quality in the samples. Postdoc B agreed to prepare a set of such segments from twelve meetings for further analysis.&#10;&#10;The main objective of the meeting was to explore ways of processing and transcribing speech samples from various sources, with an emphasis on improving accuracy and removing false overlaps during recordings.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, including Postdoc B, Grad F, and PhD G, expressed interest in the idea of iterating and developing models to perform a forced alignment and acquire the data. They discussed the importance of refining the process and the potential benefits of such an approach. However, they also acknowledged that the success of this method would depend on various factors, and it might not always yield optimal results. PhD G mentioned that they would let the group know when they have a solution for forced alignment." target="It is suggested that the same person enters and extracts the data as a double-check to ensure accuracy in the data entry process. This is because when one person handles both tasks, they can cross-reference and verify the data more effectively, minimizing the chance of errors. Although these two tasks do not necessarily need to be performed simultaneously, having the same person responsible for both tasks helps maintain consistency and increases the likelihood of accurate data.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, including Postdoc B, Grad F, and PhD G, expressed interest in the idea of iterating and developing models to perform a forced alignment and acquire the data. They discussed the importance of refining the process and the potential benefits of such an approach. However, they also acknowledged that the success of this method would depend on various factors, and it might not always yield optimal results. PhD G mentioned that they would let the group know when they have a solution for forced alignment." target="1. Editing the first half of the project: The team has been working on editing the first half of the project. Postdoc B mentioned they were in the process of editing but welcomed the news about funding for the experiment as it would be useful for refining the process and making the overlaps more precise throughout.&#10;&#10;2. Funding of the experiment: Experiment funding update indicates that Postdoc B successfully obtained sufficient financial support to proceed with the project, although the source is not specified in the transcript.&#10;&#10;3. Meeting in the BARCO room: Postdoc B, Liz, and Don met in the BARCO room to discuss an issue concerning the interplay between the transcript format and the processing required for the SRI recognizer. The postdoc mentioned checking for spelling errors, ensuring consistency of conventions, and running it through a channelize program to get the data into multi-channel format. They also planned to discuss this issue further in the afternoon.&#10;&#10;The team is making progress on editing and refining their project while simultaneously securing funding and addressing technical challenges. The collaboration mentioned between Liz, Don, and Postdoc B aims to improve the transcript format for better processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: Postdoc B&#10;Content: Alright .&#10;Speaker: Professor C&#10;Content: So , uh {disfmarker}&#10;Speaker: Grad F&#10;Content: Um , so I wanted to discuss digits briefly , but that won't take too long .&#10;Speaker: Professor C&#10;Content: Oh good . Right . OK , agenda items , Uh , we have digits , What else we got ?&#10;Speaker: PhD A&#10;Content: New version of the presegmentation .&#10;Speaker: Professor C&#10;Content: New version of presegmentation .&#10;Speaker: Postdoc B&#10;Content: Um , do we wanna say something about the , an update of the , uh , transcript ?&#10;Speaker: PhD G&#10;Content: Yeah , why don't you summarize the {disfmarker}&#10;Speaker: Professor C&#10;Content: Update on transcripts .&#10;Speaker: PhD G&#10;Content: And I guess that includes some {disfmarker} the filtering for the , the ASI refs , too .&#10;Speaker: Postdoc B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: Filtering for what ?&#10;Speaker: PhD G&#10;Content:" target="1. Experiment Funding Update: Postdoc B mentioned that they had successfully funded their experiment. Although the source of funding is not specified in the transcript, it's clear that the project has received sufficient financial support to proceed.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: Postdoc B&#10;Content: Alright .&#10;Speaker: Professor C&#10;Content: So , uh {disfmarker}&#10;Speaker: Grad F&#10;Content: Um , so I wanted to discuss digits briefly , but that won't take too long .&#10;Speaker: Professor C&#10;Content: Oh good . Right . OK , agenda items , Uh , we have digits , What else we got ?&#10;Speaker: PhD A&#10;Content: New version of the presegmentation .&#10;Speaker: Professor C&#10;Content: New version of presegmentation .&#10;Speaker: Postdoc B&#10;Content: Um , do we wanna say something about the , an update of the , uh , transcript ?&#10;Speaker: PhD G&#10;Content: Yeah , why don't you summarize the {disfmarker}&#10;Speaker: Professor C&#10;Content: Update on transcripts .&#10;Speaker: PhD G&#10;Content: And I guess that includes some {disfmarker} the filtering for the , the ASI refs , too .&#10;Speaker: Postdoc B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: Filtering for what ?&#10;Speaker: PhD G&#10;Content:" target="It is suggested that the same person enters and extracts the data as a double-check to ensure accuracy in the data entry process. This is because when one person handles both tasks, they can cross-reference and verify the data more effectively, minimizing the chance of errors. Although these two tasks do not necessarily need to be performed simultaneously, having the same person responsible for both tasks helps maintain consistency and increases the likelihood of accurate data.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: Postdoc B&#10;Content: Alright .&#10;Speaker: Professor C&#10;Content: So , uh {disfmarker}&#10;Speaker: Grad F&#10;Content: Um , so I wanted to discuss digits briefly , but that won't take too long .&#10;Speaker: Professor C&#10;Content: Oh good . Right . OK , agenda items , Uh , we have digits , What else we got ?&#10;Speaker: PhD A&#10;Content: New version of the presegmentation .&#10;Speaker: Professor C&#10;Content: New version of presegmentation .&#10;Speaker: Postdoc B&#10;Content: Um , do we wanna say something about the , an update of the , uh , transcript ?&#10;Speaker: PhD G&#10;Content: Yeah , why don't you summarize the {disfmarker}&#10;Speaker: Professor C&#10;Content: Update on transcripts .&#10;Speaker: PhD G&#10;Content: And I guess that includes some {disfmarker} the filtering for the , the ASI refs , too .&#10;Speaker: Postdoc B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: Filtering for what ?&#10;Speaker: PhD G&#10;Content:" target="1. Editing the first half of the project: The team has been working on editing the first half of the project. Postdoc B mentioned they were in the process of editing but welcomed the news about funding for the experiment as it would be useful for refining the process and making the overlaps more precise throughout.&#10;&#10;2. Funding of the experiment: Experiment funding update indicates that Postdoc B successfully obtained sufficient financial support to proceed with the project, although the source is not specified in the transcript.&#10;&#10;3. Meeting in the BARCO room: Postdoc B, Liz, and Don met in the BARCO room to discuss an issue concerning the interplay between the transcript format and the processing required for the SRI recognizer. The postdoc mentioned checking for spelling errors, ensuring consistency of conventions, and running it through a channelize program to get the data into multi-channel format. They also planned to discuss this issue further in the afternoon.&#10;&#10;The team is making progress on editing and refining their project while simultaneously securing funding and addressing technical challenges. The collaboration mentioned between Liz, Don, and Postdoc B aims to improve the transcript format for better processing.">
      <data key="d0">1</data>
    </edge>
    <edge source=" .&#10;Speaker: PhD G&#10;Content: So {disfmarker}&#10;Speaker: Professor C&#10;Content: There {disfmarker} the first {disfmarker} the first half .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: And so I was in the process of like editing them but this is wonderful news .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Alright .&#10;Speaker: Postdoc B&#10;Content: We funded the experiment with , uh {disfmarker} also we were thinking maybe applying that that to getting the , Yeah , that 'll be , very useful to getting the overlaps to be more precise all the way through .&#10;Speaker: Professor C&#10;Content: So this , blends nicely into the update on transcripts .&#10;Speaker: Postdoc B&#10;Content: Yes , it does . So , um , {comment} um , Liz , and {disfmarker} and Don , and I met this morning , in the BARCO room , with the lecture hall ,&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD G&#10;Content:" target="1. Experiment Funding Update: Postdoc B mentioned that they had successfully funded their experiment. Although the source of funding is not specified in the transcript, it's clear that the project has received sufficient financial support to proceed.">
      <data key="d0">1</data>
    </edge>
    <edge source=" .&#10;Speaker: PhD G&#10;Content: So {disfmarker}&#10;Speaker: Professor C&#10;Content: There {disfmarker} the first {disfmarker} the first half .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: And so I was in the process of like editing them but this is wonderful news .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Alright .&#10;Speaker: Postdoc B&#10;Content: We funded the experiment with , uh {disfmarker} also we were thinking maybe applying that that to getting the , Yeah , that 'll be , very useful to getting the overlaps to be more precise all the way through .&#10;Speaker: Professor C&#10;Content: So this , blends nicely into the update on transcripts .&#10;Speaker: Postdoc B&#10;Content: Yes , it does . So , um , {comment} um , Liz , and {disfmarker} and Don , and I met this morning , in the BARCO room , with the lecture hall ,&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD G&#10;Content:" target="1. Editing the first half of the project: The team has been working on editing the first half of the project. Postdoc B mentioned they were in the process of editing but welcomed the news about funding for the experiment as it would be useful for refining the process and making the overlaps more precise throughout.&#10;&#10;2. Funding of the experiment: Experiment funding update indicates that Postdoc B successfully obtained sufficient financial support to proceed with the project, although the source is not specified in the transcript.&#10;&#10;3. Meeting in the BARCO room: Postdoc B, Liz, and Don met in the BARCO room to discuss an issue concerning the interplay between the transcript format and the processing required for the SRI recognizer. The postdoc mentioned checking for spelling errors, ensuring consistency of conventions, and running it through a channelize program to get the data into multi-channel format. They also planned to discuss this issue further in the afternoon.&#10;&#10;The team is making progress on editing and refining their project while simultaneously securing funding and addressing technical challenges. The collaboration mentioned between Liz, Don, and Postdoc B aims to improve the transcript format for better processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The &quot;real situation&quot; of car noise in this study refers to the background noise recorded from people driving around in a car, without any artificial addition of noise to manipulate the signal-to-noise ratio. This is different from other studies where an artificial signal-to-noise ratio might be added to simulate real-world conditions. The advantage of using real car noise is that it provides a more authentic and accurate representation of the environment in which speech recognition systems would actually be used. However, it may also present additional challenges due to its complexity and variability.&#10;2. The models' performance was not as good as the best score because, as acknowledged by Professor C, &quot;the models are pretty crappy.&quot; Despite the best score being a relatively low error rate of 5% per digit, this is still far from perfect accuracy. It is possible that the complexity and variability of the car noise data made it difficult for the models to achieve higher levels of accuracy. Additionally, the use of lapel microphones in the meetings may have introduced additional challenges due to their propensity for picking up unwanted background noise.&#10;&#10;In summary, the &quot;real situation&quot; of car noise in this study provides a more authentic representation of real-world conditions than artificial signal-to-noise ratios, but it also presents additional challenges for speech recognition systems. The models' performance was not as good as the best score due to their inherent limitations and the complexity of the data." target=" and this is uh car noise uh , uh things , but {disfmarker} but real {disfmarker} real situation ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: well , &quot; real &quot; , Um , the {disfmarker} uh there 's one microphone that 's close , that they have as {disfmarker} as this sort of thing , close versus distant . Uh but in a car , instead of {disfmarker} instead of having a projector noise it 's {disfmarker} it 's car noise . Uh but it wasn't artificially added to get some {disfmarker} some artificial signal - to - noise ratio . It was just people driving around in a car . So , that 's {disfmarker} that 's an indication , uh that was with , many sites competing , and this was the very best score and so forth , so . More typical numbers like&#10;Speaker: PhD D&#10;Content: Although the models weren't , that good , right ? I mean , the models are pretty crappy ?&#10;Speaker: Professor C&#10;Content: You 're right . I">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The &quot;real situation&quot; of car noise in this study refers to the background noise recorded from people driving around in a car, without any artificial addition of noise to manipulate the signal-to-noise ratio. This is different from other studies where an artificial signal-to-noise ratio might be added to simulate real-world conditions. The advantage of using real car noise is that it provides a more authentic and accurate representation of the environment in which speech recognition systems would actually be used. However, it may also present additional challenges due to its complexity and variability.&#10;2. The models' performance was not as good as the best score because, as acknowledged by Professor C, &quot;the models are pretty crappy.&quot; Despite the best score being a relatively low error rate of 5% per digit, this is still far from perfect accuracy. It is possible that the complexity and variability of the car noise data made it difficult for the models to achieve higher levels of accuracy. Additionally, the use of lapel microphones in the meetings may have introduced additional challenges due to their propensity for picking up unwanted background noise.&#10;&#10;In summary, the &quot;real situation&quot; of car noise in this study provides a more authentic representation of real-world conditions than artificial signal-to-noise ratios, but it also presents additional challenges for speech recognition systems. The models' performance was not as good as the best score due to their inherent limitations and the complexity of the data." target=" . But , either way .&#10;Speaker: Professor C&#10;Content: Yeah . Yeah just by way of uh , uh , a uh , order of magnitude , uh , um , we 've been working with this Aurora , uh data set . And , uh , the best score , on the , nicest part of the data , that is , where you 've got training and test set that are basically the same kinds of noise and so forth , uh , is about , uh {disfmarker} I think the best score was something like five percent , uh , error , per digit .&#10;Speaker: PhD A&#10;Content: Per digit .&#10;Speaker: Professor C&#10;Content: So , that {disfmarker}&#10;Speaker: Grad F&#10;Content: Per digit .&#10;Speaker: Professor C&#10;Content: You 're right . So if you were doing {pause} ten digit , uh , recognition , {vocalsound} you would really be in trouble . So {disfmarker} So the {disfmarker} The point there , and this is uh car noise uh , uh things , but {disfmarker} but real {disfmarker} real situation ,&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The &quot;real situation&quot; of car noise in this study refers to the background noise recorded from people driving around in a car, without any artificial addition of noise to manipulate the signal-to-noise ratio. This is different from other studies where an artificial signal-to-noise ratio might be added to simulate real-world conditions. The advantage of using real car noise is that it provides a more authentic and accurate representation of the environment in which speech recognition systems would actually be used. However, it may also present additional challenges due to its complexity and variability.&#10;2. The models' performance was not as good as the best score because, as acknowledged by Professor C, &quot;the models are pretty crappy.&quot; Despite the best score being a relatively low error rate of 5% per digit, this is still far from perfect accuracy. It is possible that the complexity and variability of the car noise data made it difficult for the models to achieve higher levels of accuracy. Additionally, the use of lapel microphones in the meetings may have introduced additional challenges due to their propensity for picking up unwanted background noise.&#10;&#10;In summary, the &quot;real situation&quot; of car noise in this study provides a more authentic representation of real-world conditions than artificial signal-to-noise ratios, but it also presents additional challenges for speech recognition systems. The models' performance was not as good as the best score due to their inherent limitations and the complexity of the data." target="1. The plan for examining cross-correlations to eliminate false overlaps involves PhD A looking into this area further, based on some preliminary work with three meetings that showed promising results. They aim to investigate the use of cross-correlations to address issues with lapel microphones, which may be causing some of these false overlaps.&#10;2. There was no explicit mention or noticeable difference observed between wireless and wired microphones during the meetings. The distinction brought up by PhD G was between lapel microphones and all other types of microphones, regardless of whether they were wireless or wired.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The &quot;real situation&quot; of car noise in this study refers to the background noise recorded from people driving around in a car, without any artificial addition of noise to manipulate the signal-to-noise ratio. This is different from other studies where an artificial signal-to-noise ratio might be added to simulate real-world conditions. The advantage of using real car noise is that it provides a more authentic and accurate representation of the environment in which speech recognition systems would actually be used. However, it may also present additional challenges due to its complexity and variability.&#10;2. The models' performance was not as good as the best score because, as acknowledged by Professor C, &quot;the models are pretty crappy.&quot; Despite the best score being a relatively low error rate of 5% per digit, this is still far from perfect accuracy. It is possible that the complexity and variability of the car noise data made it difficult for the models to achieve higher levels of accuracy. Additionally, the use of lapel microphones in the meetings may have introduced additional challenges due to their propensity for picking up unwanted background noise.&#10;&#10;In summary, the &quot;real situation&quot; of car noise in this study provides a more authentic representation of real-world conditions than artificial signal-to-noise ratios, but it also presents additional challenges for speech recognition systems. The models' performance was not as good as the best score due to their inherent limitations and the complexity of the data." target="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The &quot;real situation&quot; of car noise in this study refers to the background noise recorded from people driving around in a car, without any artificial addition of noise to manipulate the signal-to-noise ratio. This is different from other studies where an artificial signal-to-noise ratio might be added to simulate real-world conditions. The advantage of using real car noise is that it provides a more authentic and accurate representation of the environment in which speech recognition systems would actually be used. However, it may also present additional challenges due to its complexity and variability.&#10;2. The models' performance was not as good as the best score because, as acknowledged by Professor C, &quot;the models are pretty crappy.&quot; Despite the best score being a relatively low error rate of 5% per digit, this is still far from perfect accuracy. It is possible that the complexity and variability of the car noise data made it difficult for the models to achieve higher levels of accuracy. Additionally, the use of lapel microphones in the meetings may have introduced additional challenges due to their propensity for picking up unwanted background noise.&#10;&#10;In summary, the &quot;real situation&quot; of car noise in this study provides a more authentic representation of real-world conditions than artificial signal-to-noise ratios, but it also presents additional challenges for speech recognition systems. The models' performance was not as good as the best score due to their inherent limitations and the complexity of the data." target="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways." target="er} You know I hadn't , uh , incorporated , a convention explicitly to handle acronyms , for example , but if someone says , PZM it would be nice to have that be directly interpretable from , the transcript what they said ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: Postdoc B&#10;Content: or Pi - uh Tcl {disfmarker} TCL I mean . It 's like y it 's {disfmarker} and so , um , I 've {disfmarker} I 've incorporated also convention , with that but that 's easy to handle at the post editing phase , and I 'll mention it to , transcribers for the next phase but that 's OK . And then , a similar conv uh , convention for numbers . So if they say one - eighty - three versus one eight three . Um , and also I 'll be , um , encoding , as I do my post - editing , the , things that are in curly brackets , which are clarificational material . And eh to incorporate , uh , keyword , at the beginning . So , it 's gonna be either a gloss or it 's gonna be">
      <data key="d0">1</data>
    </edge>
    <edge source="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways." target=" {disfmarker} you 'd want models for spreading .&#10;Speaker: PhD G&#10;Content: I was thinking it might be n&#10;Speaker: PhD D&#10;Content: Of the f acoustic features ?&#10;Speaker: Grad F&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Well it might be neat to do some , phonetic , features on these , nonword words . Are {disfmarker} are these kinds of words that people never {disfmarker} the &quot; huh &quot;s and the &quot; hmm &quot;s and the &quot; huh &quot; {vocalsound} and the uh {disfmarker} These k No , I 'm serious . There are all these kinds of {pause} functional , uh , elements . I don't know what you call {pause} them . But not just fill pauses but all kinds of ways of {pause} interrupting {comment} and so forth .&#10;Speaker: Grad F&#10;Content: Uh - huh .&#10;Speaker: PhD G">
      <data key="d0">1</data>
    </edge>
    <edge source="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways." target="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways." target="1. Acoustic features: The lowest level of transcription would include acoustic features, which involve physical measurements of the speech signal such as frequency, amplitude, and duration.&#10;&#10;2. Phonetic level: This refers to the representation of speech sounds using symbols from the International Phonetic Alphabet (IPA). It includes detailed information about the pronunciation of each word, including distinctions between similar sounds like &quot;p&quot; and &quot;b&quot; or &quot;s&quot; and &quot;z&quot;.&#10;&#10;3. Relative gain: This term likely refers to the relative loudness or volume of different parts of the speech signal. Including this information in the transcriptions would provide insight into how loudly or softly certain words or sounds are spoken.&#10;&#10;4. Telephone bandwidth limitations: Since some of the discussions involve telephone-quality audio, it is important to consider the impact of limited bandwidth on the quality and detail of the transcriptions. This might result in less precise acoustic measurements or phonetic representations, as well as potential difficulties in accurately distinguishing between different speakers' voices.&#10;&#10;5. Articulatory gestures: Although not explicitly mentioned in the provided excerpt, Professor C suggests including articulatory features in the transcriptions. This would involve representing the movements and positions of the speaker's speech articulators (e.g., tongue, lips, jaw) during speech production.&#10;&#10;6. Comparability on Switchboard: If using the Switchboard dataset, the full continuum of transcriptions should include information about how each word or sound compares to others in terms of acoustic and phonetic properties. This would facilitate analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways." target="Based on the transcript, the exact time it would take to transcribe the compiled audio samples from twelve meetings is not explicitly mentioned. However, Postdoc B proposed recording five-minute segments from various meetings, which would result in one hour of audio samples. They also mention that they have already processed half an hour of material.&#10;&#10;The feasibility of this task depends on several factors, such as the number of speakers, audio quality, and the effectiveness of any pre-processing techniques or conventions for encoding clarificational material and keywords. The transcript mentions concerns about accurately processing and transcribing short segments of speech, particularly when a single word can be placed incorrectly, leading to misunderstandings or errors.&#10;&#10;To make this task more feasible, several suggestions were made, including pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for precise representation of audio data. Implementing these improvements could help ensure better transcription accuracy and make the task more manageable.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways." target="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways." target="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways." target="The speakers, specifically PhD A, are interested in using manually transcribed references for comparison with other automatic systems or found boundaries. They express uncertainty about the reliability of automatically determined boundaries. Both PhD A and Postdoc B agree that manually tweaking the current method could be an option to improve its accuracy. However, there are concerns about the feasibility of accurately processing and transcribing short segments of speech, which might lead to misunderstandings or errors in transcription due to factors like overlapping processes (voicing and nasality) during speech production. Strategies to address these issues include pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for better representation of audio data.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways." target="The speakers in the meeting discussed the possibility of having transcribers also handle recording and formatting audio clips directly in the transcripts. This idea was raised by Grad F, who mentioned that they had personally processed eight meetings manually. Professor C asked for opinions on this matter. Postdoc B expressed support for the idea, stating that it would add an extra dimension to the transcribers' skill set and contribute to the composition of the transcript by incorporating those numbers directly.&#10;&#10;The potential benefits of having transcribers handle recording and formatting audio clips in the transcripts include:&#10;&#10;1. Enhancing transcribers' skill set: By performing these tasks, transcribers can develop additional skills beyond simple transcription.&#10;2. Improved integration of audio data: Directly incorporating audio clips into the transcripts allows for a more seamless combination of textual and audio information, potentially facilitating further analysis.&#10;3. Time-saving: If transcribers are already familiar with the content and context of the meetings, they might be able to efficiently record and format relevant audio clips while working on the transcripts.&#10;4. Better understanding of the material: Engaging in the recording and formatting process can provide transcribers with a deeper understanding of the content and its structure, which may lead to more accurate transcriptions.&#10;5. Increased control over audio data: By handling audio clips directly, transcribers have greater control over how the audio data is presented and integrated into the transcripts, potentially reducing errors or misunderstandings caused by automatic processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="The main concern discussed is the difficulty in accurately processing and transcribing short segments of speech, such as twenty seconds, during which a single word can be placed incorrectly. This could potentially lead to misunderstandings or errors in the transcription.&#10;&#10;One suggestion offered to address this concern is to implement pre-processing for individuals who have very short utterances. This would help ensure that their contributions are captured more accurately. Another suggestion is to standardize the handling of acronyms and numbers in the transcripts, making them directly interpretable from the text.&#10;&#10;Additionally, it was proposed to incorporate conventions for encoding clarificational material and keywords at the beginning of each line or section. This would help improve the readability and accuracy of the transcriptions.&#10;&#10;Lastly, there was a suggestion to create a score-like notation where each line represents a particular feature, such as voicing or nasality. This would allow for a more precise representation of the audio data, even if the timing on phones is overlapping in bizarre ways." target="Entering additional information such as names, times, and dates found at the top of a form is important for transcribers when automatically recording digital data to ensure that the transcripts are well-organized, easily identifiable, and searchable. This metadata can help in managing and locating specific transcripts, improving the overall efficiency of using the transcribed materials. It also contributes to creating a more complete and comprehensive transcript by incorporating relevant contextual information directly.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target=" B&#10;Content: We could , I mean you {disfmarker} you recorded one last week or so . I could get that new one in this week {disfmarker} I get that new one in .&#10;Speaker: Grad F&#10;Content: Yep . u&#10;Speaker: PhD G&#10;Content: We 're gonna be recording them every {pause} Monday ,&#10;Speaker: Professor C&#10;Content: Yeah . Cuz I think he really needs variety ,&#10;Speaker: PhD G&#10;Content: so {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: Great .&#10;Speaker: Professor C&#10;Content: and {disfmarker} and having as much variety for speaker certainly would be a big part of that I think .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Postdoc B&#10;Content: OK , so if I , OK , included {disfmarker} include , OK , then , uh , if I were to include all together samples from twelve meetings that would only take an hour and I could get the transcribers to do that right {disfmarker} I mean , what">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target="1. Acoustic features: The lowest level of transcription would include acoustic features, which involve physical measurements of the speech signal such as frequency, amplitude, and duration.&#10;&#10;2. Phonetic level: This refers to the representation of speech sounds using symbols from the International Phonetic Alphabet (IPA). It includes detailed information about the pronunciation of each word, including distinctions between similar sounds like &quot;p&quot; and &quot;b&quot; or &quot;s&quot; and &quot;z&quot;.&#10;&#10;3. Relative gain: This term likely refers to the relative loudness or volume of different parts of the speech signal. Including this information in the transcriptions would provide insight into how loudly or softly certain words or sounds are spoken.&#10;&#10;4. Telephone bandwidth limitations: Since some of the discussions involve telephone-quality audio, it is important to consider the impact of limited bandwidth on the quality and detail of the transcriptions. This might result in less precise acoustic measurements or phonetic representations, as well as potential difficulties in accurately distinguishing between different speakers' voices.&#10;&#10;5. Articulatory gestures: Although not explicitly mentioned in the provided excerpt, Professor C suggests including articulatory features in the transcriptions. This would involve representing the movements and positions of the speaker's speech articulators (e.g., tongue, lips, jaw) during speech production.&#10;&#10;6. Comparability on Switchboard: If using the Switchboard dataset, the full continuum of transcriptions should include information about how each word or sound compares to others in terms of acoustic and phonetic properties. This would facilitate analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target="Based on the transcript, the exact time it would take to transcribe the compiled audio samples from twelve meetings is not explicitly mentioned. However, Postdoc B proposed recording five-minute segments from various meetings, which would result in one hour of audio samples. They also mention that they have already processed half an hour of material.&#10;&#10;The feasibility of this task depends on several factors, such as the number of speakers, audio quality, and the effectiveness of any pre-processing techniques or conventions for encoding clarificational material and keywords. The transcript mentions concerns about accurately processing and transcribing short segments of speech, particularly when a single word can be placed incorrectly, leading to misunderstandings or errors.&#10;&#10;To make this task more feasible, several suggestions were made, including pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for precise representation of audio data. Implementing these improvements could help ensure better transcription accuracy and make the task more manageable.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target="1. The plan for examining cross-correlations to eliminate false overlaps involves PhD A looking into this area further, based on some preliminary work with three meetings that showed promising results. They aim to investigate the use of cross-correlations to address issues with lapel microphones, which may be causing some of these false overlaps.&#10;2. There was no explicit mention or noticeable difference observed between wireless and wired microphones during the meetings. The distinction brought up by PhD G was between lapel microphones and all other types of microphones, regardless of whether they were wireless or wired.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target="The speakers, specifically PhD A, are interested in using manually transcribed references for comparison with other automatic systems or found boundaries. They express uncertainty about the reliability of automatically determined boundaries. Both PhD A and Postdoc B agree that manually tweaking the current method could be an option to improve its accuracy. However, there are concerns about the feasibility of accurately processing and transcribing short segments of speech, which might lead to misunderstandings or errors in transcription due to factors like overlapping processes (voicing and nasality) during speech production. Strategies to address these issues include pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for better representation of audio data.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target="1. M Four: The speakers briefly used the term &quot;M four&quot; to refer to a meeting about meetings up to the fourth level (a meeting about a meeting about a meeting about meetings). This was suggested as an alternative to repeatedly saying &quot;the meeting about the meeting about...&quot;&#10;&#10;2. Pauses between lines: Grad F reminded the group to include pauses between the lines when transcribing or analyzing the content, likely for better readability or more accurate processing.&#10;&#10;3. Crown P Z: The group discussed using audio samples from the Crown P Z (possibly a collection of recordings), which would provide diverse speech samples for their project. However, they acknowledged that some segments might not be useful due to variations in audio quality and speakers.&#10;&#10;4. Iterating and developing models: Postdoc B, Grad F, and PhD G expressed interest in refining the process of iterating and creating models to perform forced alignment and acquire data. They recognized that this method's success would depend on various factors and might not always yield optimal results. PhD G mentioned they would inform the group when a solution for forced alignment is available.&#10;&#10;5. Cross-correlations and false overlaps: The group discussed recording five-minute segments from various meetings (excluding the first minute) to analyze cross-correlations and remove false overlaps during recordings. This approach aims to account for different speakers and varying audio quality in the samples. Postdoc B agreed to prepare a set of such segments from twelve meetings for further analysis.&#10;&#10;The main objective of the meeting was to explore ways of processing and transcribing speech samples from various sources, with an emphasis on improving accuracy and removing false overlaps during recordings.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target="The speakers in the meeting discussed the possibility of having transcribers also handle recording and formatting audio clips directly in the transcripts. This idea was raised by Grad F, who mentioned that they had personally processed eight meetings manually. Professor C asked for opinions on this matter. Postdoc B expressed support for the idea, stating that it would add an extra dimension to the transcribers' skill set and contribute to the composition of the transcript by incorporating those numbers directly.&#10;&#10;The potential benefits of having transcribers handle recording and formatting audio clips in the transcripts include:&#10;&#10;1. Enhancing transcribers' skill set: By performing these tasks, transcribers can develop additional skills beyond simple transcription.&#10;2. Improved integration of audio data: Directly incorporating audio clips into the transcripts allows for a more seamless combination of textual and audio information, potentially facilitating further analysis.&#10;3. Time-saving: If transcribers are already familiar with the content and context of the meetings, they might be able to efficiently record and format relevant audio clips while working on the transcripts.&#10;4. Better understanding of the material: Engaging in the recording and formatting process can provide transcribers with a deeper understanding of the content and its structure, which may lead to more accurate transcriptions.&#10;5. Increased control over audio data: By handling audio clips directly, transcribers have greater control over how the audio data is presented and integrated into the transcripts, potentially reducing errors or misunderstandings caused by automatic processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target="1. Experiment Funding Update: Postdoc B mentioned that they had successfully funded their experiment. Although the source of funding is not specified in the transcript, it's clear that the project has received sufficient financial support to proceed.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target="1. Limited pronunciation variants: While some dictionaries may include multiple pronunciations for a given word, many do not. This can lead to incomplete or inaccurate transcriptions if the chosen dictionary does not provide all necessary variants.&#10;&#10;2. Telephone bandwidth limitations: Dictionaries based on telephone-quality audio might result in less precise acoustic measurements or phonetic representations due to the limited bandwidth available in such recordings.&#10;&#10;3. Inability to capture human nuances: Dictionaries may not account for regional accents, dialects, or other factors that could influence how a word is pronounced in natural speech. As a result, transcriptions generated from dictionaries might fail to accurately represent these variations.&#10;&#10;4. Lack of articulatory gestures: Regular dictionaries typically do not include information about articulatory gestures, which can be valuable for researchers focusing on speech production and related areas.&#10;&#10;5. Insufficient comparability on Switchboard: If using the Switchboard dataset, regular dictionary transcriptions might lack crucial contextual and comparative information about how each word or sound compares to others in terms of acoustic and phonetic properties. This shortcoming could hinder analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target="Based on the transcript, the proposed adjustment to be made on the audio samples from twelve meetings is to convert them into a multi-channel format and adjust the timebands for precise representation. This adjustment should help improve the accuracy of processing and transcribing short segments of speech in these recordings.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Postdoc B suggested compiling one hour of audio samples from twelve meetings for further analysis. The actual time it would take to transcribe these compiled samples is not explicitly mentioned in the conversation. However, it's agreed that this approach would provide a variety of speech samples, which could be beneficial for creating a language learning tool.&#10;&#10;One concern raised during the discussion was the accuracy of processing and transcribing short segments of speech. To address this issue, suggestions like pre-processing for individuals with very short utterances, standardizing acronyms and numbers, and incorporating conventions for encoding clarificational material and keywords were made. These improvements would help ensure better transcription accuracy.&#10;&#10;In conclusion, while the exact time to transcribe the compiled audio samples is not specified in the conversation, it's acknowledged that this method could offer a variety of speech samples useful for language learning. To make this option more feasible, implementing the suggested improvements in processing and transcribing short segments of speech would be essential." target="1. The question is asking whether the current corpus (a collection of audio samples) might be appropriate to attempt achieving a closer correspondence between speech and transcriptions, and if so, what the participants think about increasing predictability in the discussion. This could mean finding ways to make the speech more predictable and easier to transcribe accurately. The participants will need to consider the quality of the audio samples, the clarity of the speakers' pronunciation, and the effectiveness of any proposed methods for improving transcription accuracy.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Acoustic features: The lowest level of transcription would include acoustic features, which involve physical measurements of the speech signal such as frequency, amplitude, and duration.&#10;&#10;2. Phonetic level: This refers to the representation of speech sounds using symbols from the International Phonetic Alphabet (IPA). It includes detailed information about the pronunciation of each word, including distinctions between similar sounds like &quot;p&quot; and &quot;b&quot; or &quot;s&quot; and &quot;z&quot;.&#10;&#10;3. Relative gain: This term likely refers to the relative loudness or volume of different parts of the speech signal. Including this information in the transcriptions would provide insight into how loudly or softly certain words or sounds are spoken.&#10;&#10;4. Telephone bandwidth limitations: Since some of the discussions involve telephone-quality audio, it is important to consider the impact of limited bandwidth on the quality and detail of the transcriptions. This might result in less precise acoustic measurements or phonetic representations, as well as potential difficulties in accurately distinguishing between different speakers' voices.&#10;&#10;5. Articulatory gestures: Although not explicitly mentioned in the provided excerpt, Professor C suggests including articulatory features in the transcriptions. This would involve representing the movements and positions of the speaker's speech articulators (e.g., tongue, lips, jaw) during speech production.&#10;&#10;6. Comparability on Switchboard: If using the Switchboard dataset, the full continuum of transcriptions should include information about how each word or sound compares to others in terms of acoustic and phonetic properties. This would facilitate analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts." target=" you would have , the full continuum of transcriptions .&#10;Speaker: Postdoc B&#10;Content: what you gain Yep .&#10;Speaker: PhD D&#10;Content: You 'd have it , from the lowest level , the ac acoustic features , then you 'd have the , you know , the phonetic level that Steve did ,&#10;Speaker: Postdoc B&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: Yeah that {disfmarker} that 's all I was thinking about .&#10;Speaker: Postdoc B&#10;Content: And you could tell that {disfmarker}&#10;Speaker: PhD D&#10;Content: and , yeah .&#10;Speaker: PhD G&#10;Content: it is telephone band , so , the bandwidth might be {disfmarker}&#10;Speaker: PhD D&#10;Content: It 'd be a complete , set then .&#10;Speaker: Postdoc B&#10;Content: And you get the relative gain up ahead .&#10;Speaker: Professor C&#10;Content: It 's so it 's a little different . So I mean i we 'll see wha how much we can , uh , get the people to do , and how much money">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Acoustic features: The lowest level of transcription would include acoustic features, which involve physical measurements of the speech signal such as frequency, amplitude, and duration.&#10;&#10;2. Phonetic level: This refers to the representation of speech sounds using symbols from the International Phonetic Alphabet (IPA). It includes detailed information about the pronunciation of each word, including distinctions between similar sounds like &quot;p&quot; and &quot;b&quot; or &quot;s&quot; and &quot;z&quot;.&#10;&#10;3. Relative gain: This term likely refers to the relative loudness or volume of different parts of the speech signal. Including this information in the transcriptions would provide insight into how loudly or softly certain words or sounds are spoken.&#10;&#10;4. Telephone bandwidth limitations: Since some of the discussions involve telephone-quality audio, it is important to consider the impact of limited bandwidth on the quality and detail of the transcriptions. This might result in less precise acoustic measurements or phonetic representations, as well as potential difficulties in accurately distinguishing between different speakers' voices.&#10;&#10;5. Articulatory gestures: Although not explicitly mentioned in the provided excerpt, Professor C suggests including articulatory features in the transcriptions. This would involve representing the movements and positions of the speaker's speech articulators (e.g., tongue, lips, jaw) during speech production.&#10;&#10;6. Comparability on Switchboard: If using the Switchboard dataset, the full continuum of transcriptions should include information about how each word or sound compares to others in terms of acoustic and phonetic properties. This would facilitate analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts." target="Based on the transcript, Grad F used normalized features to distinguish between foreground and background speech in multiple channels of a recording. The normalization techniques included:&#10;&#10;1. Normalized energy: Calculated by dividing the energy of each channel by the mean energy over all channels and the minimum energy within each channel.&#10;2. Normalization of loudness, spectral features (such as modified loudness), and other unspecified features. These were also incorporated into the feature vector for improved distinction between foreground and background speech.&#10;&#10;The system was tested on three or four meetings and seemed to work fairly well, although there were some issues with lapel microphones. The performance of these normalization techniques in distinguishing between foreground and background speech appears to be promising, but a more thorough evaluation and potential optimization may be required for better results.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Acoustic features: The lowest level of transcription would include acoustic features, which involve physical measurements of the speech signal such as frequency, amplitude, and duration.&#10;&#10;2. Phonetic level: This refers to the representation of speech sounds using symbols from the International Phonetic Alphabet (IPA). It includes detailed information about the pronunciation of each word, including distinctions between similar sounds like &quot;p&quot; and &quot;b&quot; or &quot;s&quot; and &quot;z&quot;.&#10;&#10;3. Relative gain: This term likely refers to the relative loudness or volume of different parts of the speech signal. Including this information in the transcriptions would provide insight into how loudly or softly certain words or sounds are spoken.&#10;&#10;4. Telephone bandwidth limitations: Since some of the discussions involve telephone-quality audio, it is important to consider the impact of limited bandwidth on the quality and detail of the transcriptions. This might result in less precise acoustic measurements or phonetic representations, as well as potential difficulties in accurately distinguishing between different speakers' voices.&#10;&#10;5. Articulatory gestures: Although not explicitly mentioned in the provided excerpt, Professor C suggests including articulatory features in the transcriptions. This would involve representing the movements and positions of the speaker's speech articulators (e.g., tongue, lips, jaw) during speech production.&#10;&#10;6. Comparability on Switchboard: If using the Switchboard dataset, the full continuum of transcriptions should include information about how each word or sound compares to others in terms of acoustic and phonetic properties. This would facilitate analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts." target="1. Limited pronunciation variants: While some dictionaries may include multiple pronunciations for a given word, many do not. This can lead to incomplete or inaccurate transcriptions if the chosen dictionary does not provide all necessary variants.&#10;&#10;2. Telephone bandwidth limitations: Dictionaries based on telephone-quality audio might result in less precise acoustic measurements or phonetic representations due to the limited bandwidth available in such recordings.&#10;&#10;3. Inability to capture human nuances: Dictionaries may not account for regional accents, dialects, or other factors that could influence how a word is pronounced in natural speech. As a result, transcriptions generated from dictionaries might fail to accurately represent these variations.&#10;&#10;4. Lack of articulatory gestures: Regular dictionaries typically do not include information about articulatory gestures, which can be valuable for researchers focusing on speech production and related areas.&#10;&#10;5. Insufficient comparability on Switchboard: If using the Switchboard dataset, regular dictionary transcriptions might lack crucial contextual and comparative information about how each word or sound compares to others in terms of acoustic and phonetic properties. This shortcoming could hinder analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the exact time it would take to transcribe the compiled audio samples from twelve meetings is not explicitly mentioned. However, Postdoc B proposed recording five-minute segments from various meetings, which would result in one hour of audio samples. They also mention that they have already processed half an hour of material.&#10;&#10;The feasibility of this task depends on several factors, such as the number of speakers, audio quality, and the effectiveness of any pre-processing techniques or conventions for encoding clarificational material and keywords. The transcript mentions concerns about accurately processing and transcribing short segments of speech, particularly when a single word can be placed incorrectly, leading to misunderstandings or errors.&#10;&#10;To make this task more feasible, several suggestions were made, including pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for precise representation of audio data. Implementing these improvements could help ensure better transcription accuracy and make the task more manageable." target="The speakers in the meeting discussed the possibility of having transcribers also handle recording and formatting audio clips directly in the transcripts. This idea was raised by Grad F, who mentioned that they had personally processed eight meetings manually. Professor C asked for opinions on this matter. Postdoc B expressed support for the idea, stating that it would add an extra dimension to the transcribers' skill set and contribute to the composition of the transcript by incorporating those numbers directly.&#10;&#10;The potential benefits of having transcribers handle recording and formatting audio clips in the transcripts include:&#10;&#10;1. Enhancing transcribers' skill set: By performing these tasks, transcribers can develop additional skills beyond simple transcription.&#10;2. Improved integration of audio data: Directly incorporating audio clips into the transcripts allows for a more seamless combination of textual and audio information, potentially facilitating further analysis.&#10;3. Time-saving: If transcribers are already familiar with the content and context of the meetings, they might be able to efficiently record and format relevant audio clips while working on the transcripts.&#10;4. Better understanding of the material: Engaging in the recording and formatting process can provide transcribers with a deeper understanding of the content and its structure, which may lead to more accurate transcriptions.&#10;5. Increased control over audio data: By handling audio clips directly, transcribers have greater control over how the audio data is presented and integrated into the transcripts, potentially reducing errors or misunderstandings caused by automatic processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the exact time it would take to transcribe the compiled audio samples from twelve meetings is not explicitly mentioned. However, Postdoc B proposed recording five-minute segments from various meetings, which would result in one hour of audio samples. They also mention that they have already processed half an hour of material.&#10;&#10;The feasibility of this task depends on several factors, such as the number of speakers, audio quality, and the effectiveness of any pre-processing techniques or conventions for encoding clarificational material and keywords. The transcript mentions concerns about accurately processing and transcribing short segments of speech, particularly when a single word can be placed incorrectly, leading to misunderstandings or errors.&#10;&#10;To make this task more feasible, several suggestions were made, including pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for precise representation of audio data. Implementing these improvements could help ensure better transcription accuracy and make the task more manageable." target="1. Experiment Funding Update: Postdoc B mentioned that they had successfully funded their experiment. Although the source of funding is not specified in the transcript, it's clear that the project has received sufficient financial support to proceed.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the exact time it would take to transcribe the compiled audio samples from twelve meetings is not explicitly mentioned. However, Postdoc B proposed recording five-minute segments from various meetings, which would result in one hour of audio samples. They also mention that they have already processed half an hour of material.&#10;&#10;The feasibility of this task depends on several factors, such as the number of speakers, audio quality, and the effectiveness of any pre-processing techniques or conventions for encoding clarificational material and keywords. The transcript mentions concerns about accurately processing and transcribing short segments of speech, particularly when a single word can be placed incorrectly, leading to misunderstandings or errors.&#10;&#10;To make this task more feasible, several suggestions were made, including pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for precise representation of audio data. Implementing these improvements could help ensure better transcription accuracy and make the task more manageable." target="Based on the transcript, the proposed adjustment to be made on the audio samples from twelve meetings is to convert them into a multi-channel format and adjust the timebands for precise representation. This adjustment should help improve the accuracy of processing and transcribing short segments of speech in these recordings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for examining cross-correlations to eliminate false overlaps involves PhD A looking into this area further, based on some preliminary work with three meetings that showed promising results. They aim to investigate the use of cross-correlations to address issues with lapel microphones, which may be causing some of these false overlaps.&#10;2. There was no explicit mention or noticeable difference observed between wireless and wired microphones during the meetings. The distinction brought up by PhD G was between lapel microphones and all other types of microphones, regardless of whether they were wireless or wired." target="Based on the transcript, the proposed adjustment to be made on the audio samples from twelve meetings is to convert them into a multi-channel format and adjust the timebands for precise representation. This adjustment should help improve the accuracy of processing and transcribing short segments of speech in these recordings.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature." target=" weren't , that good , right ? I mean , the models are pretty crappy ?&#10;Speaker: Professor C&#10;Content: You 're right . I think that we could have done better on the models , but the thing is that we got {disfmarker} this {disfmarker} this is the kind of typical number , for all of the , uh , uh , things in this task , all of the , um , languages . And so I {disfmarker} I think we 'd probably {disfmarker} the models would be better in some than in others . Um , so , uh . Anyway , just an indication once you get into this kind of realm even if you 're looking at connected digits it can be pretty hard .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Postdoc B&#10;Content: Hmm . It 's gonna be fun to see how we , compare at this . Very exciting . s @ @ .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: How did we do on the TI - digits ?&#10;Speaker: Grad F&#10;Content: Well the prosodics are so much different s it 's">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature." target=" D&#10;Content: H How&#10;Speaker: Grad F&#10;Content: But in TI - digits , they 're reading things like zip codes and phone numbers and things like that ,&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: do we do on TI - digits ?&#10;Speaker: Grad F&#10;Content: so it 's gonna be different . I don't remember . I mean , very good , right ?&#10;Speaker: Professor C&#10;Content: Yeah , I mean we were in the .&#10;Speaker: Grad F&#10;Content: One and a half percent , two percent , something like that ?&#10;Speaker: Professor C&#10;Content: Uh , I th no I think we got under a percent , but it was {disfmarker} but it 's {disfmarker} but I mean . The very best system that I saw in the literature was a point two five percent or something that somebody had at {disfmarker} at Bell Labs , or . Uh , but . But , uh , sort of pulling out all the stops .&#10;Speaker: Grad F&#10;Content: Oh really ?&#10;Speaker: Postdoc B&#10;Content: s @ @ .">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature." target="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature." target="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature." target="The speakers, specifically PhD A, are interested in using manually transcribed references for comparison with other automatic systems or found boundaries. They express uncertainty about the reliability of automatically determined boundaries. Both PhD A and Postdoc B agree that manually tweaking the current method could be an option to improve its accuracy. However, there are concerns about the feasibility of accurately processing and transcribing short segments of speech, which might lead to misunderstandings or errors in transcription due to factors like overlapping processes (voicing and nasality) during speech production. Strategies to address these issues include pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for better representation of audio data.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature." target="Based on the transcript, Grad F used normalized features to distinguish between foreground and background speech in multiple channels of a recording. The normalization techniques included:&#10;&#10;1. Normalized energy: Calculated by dividing the energy of each channel by the mean energy over all channels and the minimum energy within each channel.&#10;2. Normalization of loudness, spectral features (such as modified loudness), and other unspecified features. These were also incorporated into the feature vector for improved distinction between foreground and background speech.&#10;&#10;The system was tested on three or four meetings and seemed to work fairly well, although there were some issues with lapel microphones. The performance of these normalization techniques in distinguishing between foreground and background speech appears to be promising, but a more thorough evaluation and potential optimization may be required for better results.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature." target="Entering additional information such as names, times, and dates found at the top of a form is important for transcribers when automatically recording digital data to ensure that the transcripts are well-organized, easily identifiable, and searchable. This metadata can help in managing and locating specific transcripts, improving the overall efficiency of using the transcribed materials. It also contributes to creating a more complete and comprehensive transcript by incorporating relevant contextual information directly.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature." target="1. Limited pronunciation variants: While some dictionaries may include multiple pronunciations for a given word, many do not. This can lead to incomplete or inaccurate transcriptions if the chosen dictionary does not provide all necessary variants.&#10;&#10;2. Telephone bandwidth limitations: Dictionaries based on telephone-quality audio might result in less precise acoustic measurements or phonetic representations due to the limited bandwidth available in such recordings.&#10;&#10;3. Inability to capture human nuances: Dictionaries may not account for regional accents, dialects, or other factors that could influence how a word is pronounced in natural speech. As a result, transcriptions generated from dictionaries might fail to accurately represent these variations.&#10;&#10;4. Lack of articulatory gestures: Regular dictionaries typically do not include information about articulatory gestures, which can be valuable for researchers focusing on speech production and related areas.&#10;&#10;5. Insufficient comparability on Switchboard: If using the Switchboard dataset, regular dictionary transcriptions might lack crucial contextual and comparative information about how each word or sound compares to others in terms of acoustic and phonetic properties. This shortcoming could hinder analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as reported in the literature." target="To accurately extract actual digits from a recording with false starts, misreads, and miscues, the proposed approach mentioned in the transcript involves the use of scripts and X Waves software. The user would select the portion of the audio that contains the desired digits, then use the software to compare the selected speech against a reference standard (in this case, the digit sequences &quot;six nine,&quot; &quot;nine two two,&quot; etc.). If the spoken digits match the reference standard, the user can proceed to the next sequence. This process allows for the precise identification and extraction of the correct digits from the recording, even when false starts, misreads, or miscues are present.&#10;&#10;This approach is based on the assumption that overlapping processes such as voicing and nasality may be less prevalent in digit reading, leading to clearer speech signals. This could potentially result in a higher level of accuracy for the extracted digits. However, it should be noted that further research would be needed to confirm this hypothesis.&#10;&#10;In summary, accurately extracting actual digits from a recording with false starts, misreads, and miscues requires careful selection of relevant audio segments and comparison against reference standards using software tools like scripts and X Waves. This approach can improve the overall accuracy of transcription by up to 1-2%.">
      <data key="d0">1</data>
    </edge>
    <edge source=" D&#10;Content: H How&#10;Speaker: Grad F&#10;Content: But in TI - digits , they 're reading things like zip codes and phone numbers and things like that ,&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: PhD D&#10;Content: do we do on TI - digits ?&#10;Speaker: Grad F&#10;Content: so it 's gonna be different . I don't remember . I mean , very good , right ?&#10;Speaker: Professor C&#10;Content: Yeah , I mean we were in the .&#10;Speaker: Grad F&#10;Content: One and a half percent , two percent , something like that ?&#10;Speaker: Professor C&#10;Content: Uh , I th no I think we got under a percent , but it was {disfmarker} but it 's {disfmarker} but I mean . The very best system that I saw in the literature was a point two five percent or something that somebody had at {disfmarker} at Bell Labs , or . Uh , but . But , uh , sort of pulling out all the stops .&#10;Speaker: Grad F&#10;Content: Oh really ?&#10;Speaker: Postdoc B&#10;Content: s @ @ ." target="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs." target="aker: Grad F&#10;Content: Aurora I don't know . I don't know what they do in Aurora .&#10;Speaker: PhD G&#10;Content: or , a {disfmarker} a digit at a time , or {disfmarker} ?&#10;Speaker: Professor C&#10;Content: Uh , I 'm not sure how {disfmarker}&#10;Speaker: PhD G&#10;Content: Cuz it 's {disfmarker}&#10;Speaker: Professor C&#10;Content: no , no I mean it 's connected {disfmarker} it 's connected , uh , digits ,&#10;Speaker: PhD G&#10;Content: Connected .&#10;Speaker: Professor C&#10;Content: yeah . But .&#10;Speaker: Grad F&#10;Content: But {disfmarker} Right .&#10;Speaker: PhD G&#10;Content: So there 's also the {disfmarker} not just the prosody but the cross {disfmarker} the cross - word modeling is probably quite different .&#10;Speaker: PhD D&#10;Content: H How&#10;Speaker: Grad F&#10;Content: But in TI - digits , they 're reading things like zip codes and phone numbers">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs." target=" the , w uh as you can see from the numbers on the digits we 're almost done . The digits goes up to {pause} about four thousand . Um , and so , uh , we probably will be done with the TI - digits in , um , another couple weeks . um , depending on how many we read each time . So there were a bunch that we skipped . You know , someone fills out the form and then they 're not at the meeting and so it 's blank . Um , but those are almost all filled in as well . And so , once we 're {disfmarker} it 's done it would be very nice to train up a recognizer and actually start working with this data .&#10;Speaker: PhD D&#10;Content: So we 'll have a corpus that 's the size of TI - digits ?&#10;Speaker: Grad F&#10;Content: And so {disfmarker} One particular test set of TI - digits .&#10;Speaker: PhD D&#10;Content: Test set , OK .&#10;Speaker: Grad F&#10;Content: So , I {disfmarker} I extracted , Ther - there was a file sitting around which people have used here as a test set . It had">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs." target="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs." target="The speakers, specifically PhD A, are interested in using manually transcribed references for comparison with other automatic systems or found boundaries. They express uncertainty about the reliability of automatically determined boundaries. Both PhD A and Postdoc B agree that manually tweaking the current method could be an option to improve its accuracy. However, there are concerns about the feasibility of accurately processing and transcribing short segments of speech, which might lead to misunderstandings or errors in transcription due to factors like overlapping processes (voicing and nasality) during speech production. Strategies to address these issues include pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for better representation of audio data.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs." target="Based on the transcript, Grad F used normalized features to distinguish between foreground and background speech in multiple channels of a recording. The normalization techniques included:&#10;&#10;1. Normalized energy: Calculated by dividing the energy of each channel by the mean energy over all channels and the minimum energy within each channel.&#10;2. Normalization of loudness, spectral features (such as modified loudness), and other unspecified features. These were also incorporated into the feature vector for improved distinction between foreground and background speech.&#10;&#10;The system was tested on three or four meetings and seemed to work fairly well, although there were some issues with lapel microphones. The performance of these normalization techniques in distinguishing between foreground and background speech appears to be promising, but a more thorough evaluation and potential optimization may be required for better results.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs." target="Entering additional information such as names, times, and dates found at the top of a form is important for transcribers when automatically recording digital data to ensure that the transcripts are well-organized, easily identifiable, and searchable. This metadata can help in managing and locating specific transcripts, improving the overall efficiency of using the transcribed materials. It also contributes to creating a more complete and comprehensive transcript by incorporating relevant contextual information directly.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs." target="To accurately extract actual digits from a recording with false starts, misreads, and miscues, the proposed approach mentioned in the transcript involves the use of scripts and X Waves software. The user would select the portion of the audio that contains the desired digits, then use the software to compare the selected speech against a reference standard (in this case, the digit sequences &quot;six nine,&quot; &quot;nine two two,&quot; etc.). If the spoken digits match the reference standard, the user can proceed to the next sequence. This process allows for the precise identification and extraction of the correct digits from the recording, even when false starts, misreads, or miscues are present.&#10;&#10;This approach is based on the assumption that overlapping processes such as voicing and nasality may be less prevalent in digit reading, leading to clearer speech signals. This could potentially result in a higher level of accuracy for the extracted digits. However, it should be noted that further research would be needed to confirm this hypothesis.&#10;&#10;In summary, accurately extracting actual digits from a recording with false starts, misreads, and miscues requires careful selection of relevant audio segments and comparison against reference standards using software tools like scripts and X Waves. This approach can improve the overall accuracy of transcription by up to 1-2%.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs." target="The compromised string referred to in the conversation is the best compromise or representation that a group of people could come up with to describe what happened during speech production. It is not necessarily the exact transcription of what was really said, but rather an approximation that takes into account overlapping processes such as voicing and nasality.&#10;&#10;Compared to phone labels, which are simplified representations of speech sounds, this compromised string is likely to be more accurate in capturing the complexities of actual speech production. This is because the compromised string can potentially incorporate features that are not included in phone labels, such as reduced syllables and pronunciation variants.&#10;&#10;However, it is important to note that the compromised string may still introduce some level of confusion or ambiguity, especially if it is not based on a well-defined set of rules or conventions. This is why further research is needed to determine the best way to represent and transcribe speech production in a way that balances accuracy and clarity.">
      <data key="d0">1</data>
    </edge>
    <edge source="0.25% was the lowest error rate achieved by any speech recognition system in reading TI digits, as mentioned in the conversation. This information was reported in the literature and referred to a system developed at Bell Labs." target="1. The question is asking whether the current corpus (a collection of audio samples) might be appropriate to attempt achieving a closer correspondence between speech and transcriptions, and if so, what the participants think about increasing predictability in the discussion. This could mean finding ways to make the speech more predictable and easier to transcribe accurately. The participants will need to consider the quality of the audio samples, the clarity of the speakers' pronunciation, and the effectiveness of any proposed methods for improving transcription accuracy.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time." target=" a little concerned that maybe the kind of phenomena , in w i i The reason for doing it is because the {disfmarker} the argument is that certainly with conversational speech , the stuff that we 've looked at here before , um , just doing the simple mapping , from , um , the phone , to the corresponding features that you could look up in a book , uh , isn't right . It isn't actually right . In fact there 's these overlapping processes where some voicing some up and then some , you know , some nasality is {disfmarker} comes in here , and so forth . And you do this gross thing saying &quot; Well I guess it 's this phone starting there &quot; . So , uh , that 's the reasoning . But , It could be that when we 're reading digits , because it 's {disfmarker} it 's for such a limited set , that maybe {disfmarker} maybe that phenomenon doesn't occur as much . I don't know . Di - an anybody {disfmarker} ? {pause} Do you have any {disfmarker} ? {pause} Anybody have any opinion about that ,&#10;Speaker: Postdoc B&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time." target="The speakers, specifically PhD A, are interested in using manually transcribed references for comparison with other automatic systems or found boundaries. They express uncertainty about the reliability of automatically determined boundaries. Both PhD A and Postdoc B agree that manually tweaking the current method could be an option to improve its accuracy. However, there are concerns about the feasibility of accurately processing and transcribing short segments of speech, which might lead to misunderstandings or errors in transcription due to factors like overlapping processes (voicing and nasality) during speech production. Strategies to address these issues include pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for better representation of audio data.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time." target="1. Experiment Funding Update: Postdoc B mentioned that they had successfully funded their experiment. Although the source of funding is not specified in the transcript, it's clear that the project has received sufficient financial support to proceed.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time." target="It is suggested that the same person enters and extracts the data as a double-check to ensure accuracy in the data entry process. This is because when one person handles both tasks, they can cross-reference and verify the data more effectively, minimizing the chance of errors. Although these two tasks do not necessarily need to be performed simultaneously, having the same person responsible for both tasks helps maintain consistency and increases the likelihood of accurate data.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time." target="To accurately extract actual digits from a recording with false starts, misreads, and miscues, the proposed approach mentioned in the transcript involves the use of scripts and X Waves software. The user would select the portion of the audio that contains the desired digits, then use the software to compare the selected speech against a reference standard (in this case, the digit sequences &quot;six nine,&quot; &quot;nine two two,&quot; etc.). If the spoken digits match the reference standard, the user can proceed to the next sequence. This process allows for the precise identification and extraction of the correct digits from the recording, even when false starts, misreads, or miscues are present.&#10;&#10;This approach is based on the assumption that overlapping processes such as voicing and nasality may be less prevalent in digit reading, leading to clearer speech signals. This could potentially result in a higher level of accuracy for the extracted digits. However, it should be noted that further research would be needed to confirm this hypothesis.&#10;&#10;In summary, accurately extracting actual digits from a recording with false starts, misreads, and miscues requires careful selection of relevant audio segments and comparison against reference standards using software tools like scripts and X Waves. This approach can improve the overall accuracy of transcription by up to 1-2%.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time." target="The compromised string referred to in the conversation is the best compromise or representation that a group of people could come up with to describe what happened during speech production. It is not necessarily the exact transcription of what was really said, but rather an approximation that takes into account overlapping processes such as voicing and nasality.&#10;&#10;Compared to phone labels, which are simplified representations of speech sounds, this compromised string is likely to be more accurate in capturing the complexities of actual speech production. This is because the compromised string can potentially incorporate features that are not included in phone labels, such as reduced syllables and pronunciation variants.&#10;&#10;However, it is important to note that the compromised string may still introduce some level of confusion or ambiguity, especially if it is not based on a well-defined set of rules or conventions. This is why further research is needed to determine the best way to represent and transcribe speech production in a way that balances accuracy and clarity.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time." target="The difference between pronunciation models in dictionaries and the pronunciations that people produce lies in the variability and complexity of human speech. Dictionary pronunciations are typically represented using standardized symbols and conventions, which may not capture the full range of variation and reduction that occur in natural speech.&#10;&#10;Steve's work on labeling contributes to obtaining information about the actual pronunciations produced by speakers, including reduced syllables and variants that are not typically included in dictionaries. This information can be useful for developing more accurate speech recognition systems and models of speech production. By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time." target="1. The question is asking whether the current corpus (a collection of audio samples) might be appropriate to attempt achieving a closer correspondence between speech and transcriptions, and if so, what the participants think about increasing predictability in the discussion. This could mean finding ways to make the speech more predictable and easier to transcribe accurately. The participants will need to consider the quality of the audio samples, the clarity of the speakers' pronunciation, and the effectiveness of any proposed methods for improving transcription accuracy.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible that the phenomenon of overlapping processes, such as voicing and nasality, during speech production might be less prevalent when reading digits. This is because the set of sounds used in reading digits is limited, which could potentially result in clearer and more distinct speech signals. However, this is just a possibility and further research would be needed to confirm it. The speaker (Postdoc B) is asking for opinions on this matter, indicating that there isn't a definitive answer at this time." target="1. Include reduced pronunciations in dictionaries: There is a suggestion to include reduced pronunciations, such as &quot;gonna&quot;, &quot;guh see you tomorrow&quot;, etc., in dictionaries or have people explicitly mark these reductions during transcription. This would provide more comprehensive and accurate representations of actual pronunciations produced by speakers.&#10;&#10;2. Develop intermediate representations: There is also a proposal to create intermediate representations between dictionary word pronunciations and gestural ones, as the direct transition from one to another might not be straightforward or easy. These intermediate representations could help bridge the gap between standardized dictionary pronunciations and the more variable and reduced pronunciations found in natural speech.&#10;&#10;3. Analyze transcriptions and labelings: By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts. This information can be useful for developing more accurate speech recognition systems and models of speech production.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, specifically PhD A, are interested in using manually transcribed references for comparison with other automatic systems or found boundaries. They express uncertainty about the reliability of automatically determined boundaries. Both PhD A and Postdoc B agree that manually tweaking the current method could be an option to improve its accuracy. However, there are concerns about the feasibility of accurately processing and transcribing short segments of speech, which might lead to misunderstandings or errors in transcription due to factors like overlapping processes (voicing and nasality) during speech production. Strategies to address these issues include pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for better representation of audio data." target=" but , I don't know yet whether these , segments that contain a lot of pauses around the words , will work or not .&#10;Speaker: PhD A&#10;Content: I {disfmarker} I would quite like to have some manually transcribed references for {disfmarker} for the system , as I 'm not sure if {disfmarker} if it 's really good to compare with {disfmarker} with some other automatic , found boundaries .&#10;Speaker: PhD G&#10;Content: Yeah . Right .&#10;Speaker: Postdoc B&#10;Content: Well , no , if we were to start with this and then tweak it h manually , would that {disfmarker} that would be OK ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: Yeah {pause} sure .&#10;Speaker: PhD G&#10;Content: They might be OK .&#10;Speaker: Postdoc B&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: It {disfmarker} you know it really depends on a lot of things ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, specifically PhD A, are interested in using manually transcribed references for comparison with other automatic systems or found boundaries. They express uncertainty about the reliability of automatically determined boundaries. Both PhD A and Postdoc B agree that manually tweaking the current method could be an option to improve its accuracy. However, there are concerns about the feasibility of accurately processing and transcribing short segments of speech, which might lead to misunderstandings or errors in transcription due to factors like overlapping processes (voicing and nasality) during speech production. Strategies to address these issues include pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for better representation of audio data." target="The speakers in the meeting discussed the possibility of having transcribers also handle recording and formatting audio clips directly in the transcripts. This idea was raised by Grad F, who mentioned that they had personally processed eight meetings manually. Professor C asked for opinions on this matter. Postdoc B expressed support for the idea, stating that it would add an extra dimension to the transcribers' skill set and contribute to the composition of the transcript by incorporating those numbers directly.&#10;&#10;The potential benefits of having transcribers handle recording and formatting audio clips in the transcripts include:&#10;&#10;1. Enhancing transcribers' skill set: By performing these tasks, transcribers can develop additional skills beyond simple transcription.&#10;2. Improved integration of audio data: Directly incorporating audio clips into the transcripts allows for a more seamless combination of textual and audio information, potentially facilitating further analysis.&#10;3. Time-saving: If transcribers are already familiar with the content and context of the meetings, they might be able to efficiently record and format relevant audio clips while working on the transcripts.&#10;4. Better understanding of the material: Engaging in the recording and formatting process can provide transcribers with a deeper understanding of the content and its structure, which may lead to more accurate transcriptions.&#10;5. Increased control over audio data: By handling audio clips directly, transcribers have greater control over how the audio data is presented and integrated into the transcripts, potentially reducing errors or misunderstandings caused by automatic processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, specifically PhD A, are interested in using manually transcribed references for comparison with other automatic systems or found boundaries. They express uncertainty about the reliability of automatically determined boundaries. Both PhD A and Postdoc B agree that manually tweaking the current method could be an option to improve its accuracy. However, there are concerns about the feasibility of accurately processing and transcribing short segments of speech, which might lead to misunderstandings or errors in transcription due to factors like overlapping processes (voicing and nasality) during speech production. Strategies to address these issues include pre-processing for individuals with very short utterances, standardizing acronyms and numbers, incorporating conventions for encoding clarificational material and keywords, and creating a score-like notation for better representation of audio data." target="Entering additional information such as names, times, and dates found at the top of a form is important for transcribers when automatically recording digital data to ensure that the transcripts are well-organized, easily identifiable, and searchable. This metadata can help in managing and locating specific transcripts, improving the overall efficiency of using the transcribed materials. It also contributes to creating a more complete and comprehensive transcript by incorporating relevant contextual information directly.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. M Four: The speakers briefly used the term &quot;M four&quot; to refer to a meeting about meetings up to the fourth level (a meeting about a meeting about a meeting about meetings). This was suggested as an alternative to repeatedly saying &quot;the meeting about the meeting about...&quot;&#10;&#10;2. Pauses between lines: Grad F reminded the group to include pauses between the lines when transcribing or analyzing the content, likely for better readability or more accurate processing.&#10;&#10;3. Crown P Z: The group discussed using audio samples from the Crown P Z (possibly a collection of recordings), which would provide diverse speech samples for their project. However, they acknowledged that some segments might not be useful due to variations in audio quality and speakers.&#10;&#10;4. Iterating and developing models: Postdoc B, Grad F, and PhD G expressed interest in refining the process of iterating and creating models to perform forced alignment and acquire data. They recognized that this method's success would depend on various factors and might not always yield optimal results. PhD G mentioned they would inform the group when a solution for forced alignment is available.&#10;&#10;5. Cross-correlations and false overlaps: The group discussed recording five-minute segments from various meetings (excluding the first minute) to analyze cross-correlations and remove false overlaps during recordings. This approach aims to account for different speakers and varying audio quality in the samples. Postdoc B agreed to prepare a set of such segments from twelve meetings for further analysis.&#10;&#10;The main objective of the meeting was to explore ways of processing and transcribing speech samples from various sources, with an emphasis on improving accuracy and removing false overlaps during recordings." target=" god .&#10;Speaker: Grad F&#10;Content: Cuz then it would be a meeting about the meeting about the meeting about meetings .&#10;Speaker: Postdoc B&#10;Content: &#10;Speaker: Professor C&#10;Content: Yeah ? Just start saying &quot; M four &quot; . Yeah , OK .&#10;Speaker: Grad F&#10;Content: Yeah . M to the fourth .&#10;Speaker: Professor C&#10;Content: Should we do the digits ?&#10;Speaker: Grad F&#10;Content: Yep , go for it .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: S {pause} s&#10;Speaker: Grad F&#10;Content: Pause between the lines , remember ?&#10;Speaker: Grad E&#10;Content: Excuse me .&#10;Speaker: Grad F&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: Huh .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. M Four: The speakers briefly used the term &quot;M four&quot; to refer to a meeting about meetings up to the fourth level (a meeting about a meeting about a meeting about meetings). This was suggested as an alternative to repeatedly saying &quot;the meeting about the meeting about...&quot;&#10;&#10;2. Pauses between lines: Grad F reminded the group to include pauses between the lines when transcribing or analyzing the content, likely for better readability or more accurate processing.&#10;&#10;3. Crown P Z: The group discussed using audio samples from the Crown P Z (possibly a collection of recordings), which would provide diverse speech samples for their project. However, they acknowledged that some segments might not be useful due to variations in audio quality and speakers.&#10;&#10;4. Iterating and developing models: Postdoc B, Grad F, and PhD G expressed interest in refining the process of iterating and creating models to perform forced alignment and acquire data. They recognized that this method's success would depend on various factors and might not always yield optimal results. PhD G mentioned they would inform the group when a solution for forced alignment is available.&#10;&#10;5. Cross-correlations and false overlaps: The group discussed recording five-minute segments from various meetings (excluding the first minute) to analyze cross-correlations and remove false overlaps during recordings. This approach aims to account for different speakers and varying audio quality in the samples. Postdoc B agreed to prepare a set of such segments from twelve meetings for further analysis.&#10;&#10;The main objective of the meeting was to explore ways of processing and transcribing speech samples from various sources, with an emphasis on improving accuracy and removing false overlaps during recordings." target=": That says a lot .&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: For the recor for the record Adam is not a paid employee or a consultant of Crown .&#10;Speaker: Grad F&#10;Content: and {disfmarker} Excuse me ?&#10;Speaker: Postdoc B&#10;Content: Oh .&#10;Speaker: Professor C&#10;Content: I said &quot; For the record Adam is {disfmarker} is not a paid consultant or employee of Crown &quot; .&#10;Speaker: Grad F&#10;Content: Excuse me ?&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Grad F&#10;Content: That 's right .&#10;Speaker: PhD G&#10;Content: However , he may be solicited after these meetings are distributed .&#10;Speaker: Grad F&#10;Content: Well we 're using the Crown P Z&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Don't worry about finishing your dissertation .&#10;Speaker: Grad F&#10;Content: These are Crown aren't they ?&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: Grad F&#10;Content: The P Z">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers in the meeting discussed the possibility of having transcribers also handle recording and formatting audio clips directly in the transcripts. This idea was raised by Grad F, who mentioned that they had personally processed eight meetings manually. Professor C asked for opinions on this matter. Postdoc B expressed support for the idea, stating that it would add an extra dimension to the transcribers' skill set and contribute to the composition of the transcript by incorporating those numbers directly.&#10;&#10;The potential benefits of having transcribers handle recording and formatting audio clips in the transcripts include:&#10;&#10;1. Enhancing transcribers' skill set: By performing these tasks, transcribers can develop additional skills beyond simple transcription.&#10;2. Improved integration of audio data: Directly incorporating audio clips into the transcripts allows for a more seamless combination of textual and audio information, potentially facilitating further analysis.&#10;3. Time-saving: If transcribers are already familiar with the content and context of the meetings, they might be able to efficiently record and format relevant audio clips while working on the transcripts.&#10;4. Better understanding of the material: Engaging in the recording and formatting process can provide transcribers with a deeper understanding of the content and its structure, which may lead to more accurate transcriptions.&#10;5. Increased control over audio data: By handling audio clips directly, transcribers have greater control over how the audio data is presented and integrated into the transcripts, potentially reducing errors or misunderstandings caused by automatic processing." target=" two &quot; . And you find that , and , hit the key and it records it in a file in a particular format .&#10;Speaker: Professor C&#10;Content: So is this {disfmarker}&#10;Speaker: Grad F&#10;Content: And so the {disfmarker} the question is , should we have the transcribers do that or should we just do it ? Well , some of us . I 've been do I 've done , eight meetings , something like that , just by hand . Just myself , rather . So it will not take long . Um {disfmarker}&#10;Speaker: Professor C&#10;Content: Uh , what {disfmarker} what do you think ?&#10;Speaker: Postdoc B&#10;Content: My feeling is that we discussed this right before coffee and I think it 's a {disfmarker} it 's a fine idea partly because , um , it 's not un unrelated to their present skill set , but it will add , for them , an extra dimension , it might be an interesting break for them . And also it is contributing to the , uh , c composition of the transcript cuz we can incorporate those numbers directly and it 'll be a">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers in the meeting discussed the possibility of having transcribers also handle recording and formatting audio clips directly in the transcripts. This idea was raised by Grad F, who mentioned that they had personally processed eight meetings manually. Professor C asked for opinions on this matter. Postdoc B expressed support for the idea, stating that it would add an extra dimension to the transcribers' skill set and contribute to the composition of the transcript by incorporating those numbers directly.&#10;&#10;The potential benefits of having transcribers handle recording and formatting audio clips in the transcripts include:&#10;&#10;1. Enhancing transcribers' skill set: By performing these tasks, transcribers can develop additional skills beyond simple transcription.&#10;2. Improved integration of audio data: Directly incorporating audio clips into the transcripts allows for a more seamless combination of textual and audio information, potentially facilitating further analysis.&#10;3. Time-saving: If transcribers are already familiar with the content and context of the meetings, they might be able to efficiently record and format relevant audio clips while working on the transcripts.&#10;4. Better understanding of the material: Engaging in the recording and formatting process can provide transcribers with a deeper understanding of the content and its structure, which may lead to more accurate transcriptions.&#10;5. Increased control over audio data: By handling audio clips directly, transcribers have greater control over how the audio data is presented and integrated into the transcripts, potentially reducing errors or misunderstandings caused by automatic processing." target="Entering additional information such as names, times, and dates found at the top of a form is important for transcribers when automatically recording digital data to ensure that the transcripts are well-organized, easily identifiable, and searchable. This metadata can help in managing and locating specific transcripts, improving the overall efficiency of using the transcribed materials. It also contributes to creating a more complete and comprehensive transcript by incorporating relevant contextual information directly.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Grad F used normalized features to distinguish between foreground and background speech in multiple channels of a recording. The normalization techniques included:&#10;&#10;1. Normalized energy: Calculated by dividing the energy of each channel by the mean energy over all channels and the minimum energy within each channel.&#10;2. Normalization of loudness, spectral features (such as modified loudness), and other unspecified features. These were also incorporated into the feature vector for improved distinction between foreground and background speech.&#10;&#10;The system was tested on three or four meetings and seemed to work fairly well, although there were some issues with lapel microphones. The performance of these normalization techniques in distinguishing between foreground and background speech appears to be promising, but a more thorough evaluation and potential optimization may be required for better results." target="ation to {disfmarker} to get another version which does channel - specific , uh , speech - nonspeech detection . And , what I did is I used some normalized features which , uh , look in into the {disfmarker} which is normalized energy , uh , energy normalized by the mean over the channels and by the , minimum over the , other . within each channel . And to {disfmarker} to , mm , to , yeah , to normalize also loudness and {disfmarker} and modified loudness and things and that those special features actually are in my feature vector .&#10;Speaker: Grad F&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: And , and , therefore to be able to , uh , somewhat distinguish between foreground and background speech in {disfmarker} in the different {disfmarker} in {disfmarker} each channel . And , eh , I tested it on {disfmarker} on three or four meetings and it seems to work , well yeah , fairly well , I {disfmarker} I would say . There are some problems with the lapel mike .&#10;Speaker: Grad F&#10;Content: Of">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Experiment Funding Update: Postdoc B mentioned that they had successfully funded their experiment. Although the source of funding is not specified in the transcript, it's clear that the project has received sufficient financial support to proceed." target="1. Editing the first half of the project: The team has been working on editing the first half of the project. Postdoc B mentioned they were in the process of editing but welcomed the news about funding for the experiment as it would be useful for refining the process and making the overlaps more precise throughout.&#10;&#10;2. Funding of the experiment: Experiment funding update indicates that Postdoc B successfully obtained sufficient financial support to proceed with the project, although the source is not specified in the transcript.&#10;&#10;3. Meeting in the BARCO room: Postdoc B, Liz, and Don met in the BARCO room to discuss an issue concerning the interplay between the transcript format and the processing required for the SRI recognizer. The postdoc mentioned checking for spelling errors, ensuring consistency of conventions, and running it through a channelize program to get the data into multi-channel format. They also planned to discuss this issue further in the afternoon.&#10;&#10;The team is making progress on editing and refining their project while simultaneously securing funding and addressing technical challenges. The collaboration mentioned between Liz, Don, and Postdoc B aims to improve the transcript format for better processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Experiment Funding Update: Postdoc B mentioned that they had successfully funded their experiment. Although the source of funding is not specified in the transcript, it's clear that the project has received sufficient financial support to proceed." target="1. The question is asking whether the current corpus (a collection of audio samples) might be appropriate to attempt achieving a closer correspondence between speech and transcriptions, and if so, what the participants think about increasing predictability in the discussion. This could mean finding ways to make the speech more predictable and easier to transcribe accurately. The participants will need to consider the quality of the audio samples, the clarity of the speakers' pronunciation, and the effectiveness of any proposed methods for improving transcription accuracy.">
      <data key="d0">1</data>
    </edge>
    <edge source="Entering additional information such as names, times, and dates found at the top of a form is important for transcribers when automatically recording digital data to ensure that the transcripts are well-organized, easily identifiable, and searchable. This metadata can help in managing and locating specific transcripts, improving the overall efficiency of using the transcribed materials. It also contributes to creating a more complete and comprehensive transcript by incorporating relevant contextual information directly." target=" for them . And also it is contributing to the , uh , c composition of the transcript cuz we can incorporate those numbers directly and it 'll be a more complete transcript . So I 'm {disfmarker} I think it 's fine , that part .&#10;Speaker: Grad F&#10;Content: There is {disfmarker} there is {disfmarker}&#10;Speaker: Professor C&#10;Content: So you think it 's fine to have the transcribers do it ?&#10;Speaker: Postdoc B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Yeah , OK .&#10;Speaker: Grad F&#10;Content: There 's one other small bit , which is just entering the information which at s which is at the top of this form , onto the computer , to go along with the {disfmarker} where the digits are recorded automatically .&#10;Speaker: PhD D&#10;Content: Good .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: And so it 's just , you know , typing in name , times {disfmarker} time , date , and so on . Um , which again">
      <data key="d0">1</data>
    </edge>
    <edge source="Entering additional information such as names, times, and dates found at the top of a form is important for transcribers when automatically recording digital data to ensure that the transcripts are well-organized, easily identifiable, and searchable. This metadata can help in managing and locating specific transcripts, improving the overall efficiency of using the transcribed materials. It also contributes to creating a more complete and comprehensive transcript by incorporating relevant contextual information directly." target="It is suggested that the same person enters and extracts the data as a double-check to ensure accuracy in the data entry process. This is because when one person handles both tasks, they can cross-reference and verify the data more effectively, minimizing the chance of errors. Although these two tasks do not necessarily need to be performed simultaneously, having the same person responsible for both tasks helps maintain consistency and increases the likelihood of accurate data.">
      <data key="d0">1</data>
    </edge>
    <edge source="Entering additional information such as names, times, and dates found at the top of a form is important for transcribers when automatically recording digital data to ensure that the transcripts are well-organized, easily identifiable, and searchable. This metadata can help in managing and locating specific transcripts, improving the overall efficiency of using the transcribed materials. It also contributes to creating a more complete and comprehensive transcript by incorporating relevant contextual information directly." target="To accurately extract actual digits from a recording with false starts, misreads, and miscues, the proposed approach mentioned in the transcript involves the use of scripts and X Waves software. The user would select the portion of the audio that contains the desired digits, then use the software to compare the selected speech against a reference standard (in this case, the digit sequences &quot;six nine,&quot; &quot;nine two two,&quot; etc.). If the spoken digits match the reference standard, the user can proceed to the next sequence. This process allows for the precise identification and extraction of the correct digits from the recording, even when false starts, misreads, or miscues are present.&#10;&#10;This approach is based on the assumption that overlapping processes such as voicing and nasality may be less prevalent in digit reading, leading to clearer speech signals. This could potentially result in a higher level of accuracy for the extracted digits. However, it should be noted that further research would be needed to confirm this hypothesis.&#10;&#10;In summary, accurately extracting actual digits from a recording with false starts, misreads, and miscues requires careful selection of relevant audio segments and comparison against reference standards using software tools like scripts and X Waves. This approach can improve the overall accuracy of transcription by up to 1-2%.">
      <data key="d0">1</data>
    </edge>
    <edge source="It is suggested that the same person enters and extracts the data as a double-check to ensure accuracy in the data entry process. This is because when one person handles both tasks, they can cross-reference and verify the data more effectively, minimizing the chance of errors. Although these two tasks do not necessarily need to be performed simultaneously, having the same person responsible for both tasks helps maintain consistency and increases the likelihood of accurate data." target="marker}&#10;Speaker: Grad F&#10;Content: No they don't have {disfmarker} this {disfmarker} you have to enter the data before , you do the second task , but they don't have to happen at the same time .&#10;Speaker: PhD D&#10;Content: OK .&#10;Speaker: Grad F&#10;Content: So it 's {disfmarker} it 's just I have a file whi which has this information on it , and then when you start using my scripts , for extracting the times , it adds the times at the bottom of the file . And so , um , I mean , it 's easy to create the files and leave them blank , and so actually we could do it in either order .&#10;Speaker: PhD D&#10;Content: Oh , OK .&#10;Speaker: Grad F&#10;Content: Um , it 's {disfmarker} it 's sort of nice to have the same person do it just as a double - check , to make sure you 're entering for the right person . But , either way .&#10;Speaker: Professor C&#10;Content: Yeah . Yeah just by way of uh , uh , a uh , order of magnitude">
      <data key="d0">1</data>
    </edge>
    <edge source="It is suggested that the same person enters and extracts the data as a double-check to ensure accuracy in the data entry process. This is because when one person handles both tasks, they can cross-reference and verify the data more effectively, minimizing the chance of errors. Although these two tasks do not necessarily need to be performed simultaneously, having the same person responsible for both tasks helps maintain consistency and increases the likelihood of accurate data." target=" so it 's just , you know , typing in name , times {disfmarker} time , date , and so on . Um , which again either they can do , but it is , you know , firing up an editor , or , again , I can do . Or someone else can do .&#10;Speaker: Postdoc B&#10;Content: And , that , you know , I 'm not , that {disfmarker} that one I 'm not so sure if it 's into the {disfmarker} the , things that , I , wanted to use the hours for , because the , the time that they 'd be spending doing that they wouldn't be able to be putting more words on .&#10;Speaker: Professor C&#10;Content: Mmm .&#10;Speaker: Postdoc B&#10;Content: But that 's really your choice , it 's your {disfmarker}&#10;Speaker: PhD D&#10;Content: So are these two separate tasks that can happen ? Or do they have to happen at the same time before {disfmarker}&#10;Speaker: Grad F&#10;Content: No they don't have {disfmarker} this {disfmarker} you have">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Limited pronunciation variants: While some dictionaries may include multiple pronunciations for a given word, many do not. This can lead to incomplete or inaccurate transcriptions if the chosen dictionary does not provide all necessary variants.&#10;&#10;2. Telephone bandwidth limitations: Dictionaries based on telephone-quality audio might result in less precise acoustic measurements or phonetic representations due to the limited bandwidth available in such recordings.&#10;&#10;3. Inability to capture human nuances: Dictionaries may not account for regional accents, dialects, or other factors that could influence how a word is pronounced in natural speech. As a result, transcriptions generated from dictionaries might fail to accurately represent these variations.&#10;&#10;4. Lack of articulatory gestures: Regular dictionaries typically do not include information about articulatory gestures, which can be valuable for researchers focusing on speech production and related areas.&#10;&#10;5. Insufficient comparability on Switchboard: If using the Switchboard dataset, regular dictionary transcriptions might lack crucial contextual and comparative information about how each word or sound compares to others in terms of acoustic and phonetic properties. This shortcoming could hinder analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts." target=" entry , and then ? Or {pause} would we {disfmarker}&#10;Speaker: Professor C&#10;Content: Well , regular dictionary , I mean , this is a pretty rich dictionary . It 's got , got a fair number of pronunciations in it&#10;Speaker: Postdoc B&#10;Content: But {disfmarker}&#10;Speaker: PhD D&#10;Content: Or you could start from the {disfmarker} if we were gonna , do the same set , of sentences that Steve had , done , we could start with those transcriptions .&#10;Speaker: Postdoc B&#10;Content: Mm - hmm . So I was thinking {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: That 's actually what I was thinking , is tha {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: the problem is when you run , uh , if you run a regular dictionary , um , even if you have variants , in there , which most people don't , you don't always get , out , the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Limited pronunciation variants: While some dictionaries may include multiple pronunciations for a given word, many do not. This can lead to incomplete or inaccurate transcriptions if the chosen dictionary does not provide all necessary variants.&#10;&#10;2. Telephone bandwidth limitations: Dictionaries based on telephone-quality audio might result in less precise acoustic measurements or phonetic representations due to the limited bandwidth available in such recordings.&#10;&#10;3. Inability to capture human nuances: Dictionaries may not account for regional accents, dialects, or other factors that could influence how a word is pronounced in natural speech. As a result, transcriptions generated from dictionaries might fail to accurately represent these variations.&#10;&#10;4. Lack of articulatory gestures: Regular dictionaries typically do not include information about articulatory gestures, which can be valuable for researchers focusing on speech production and related areas.&#10;&#10;5. Insufficient comparability on Switchboard: If using the Switchboard dataset, regular dictionary transcriptions might lack crucial contextual and comparative information about how each word or sound compares to others in terms of acoustic and phonetic properties. This shortcoming could hinder analyses aimed at understanding patterns and variations across different speakers, dialects, or contexts." target="1. Include reduced pronunciations in dictionaries: There is a suggestion to include reduced pronunciations, such as &quot;gonna&quot;, &quot;guh see you tomorrow&quot;, etc., in dictionaries or have people explicitly mark these reductions during transcription. This would provide more comprehensive and accurate representations of actual pronunciations produced by speakers.&#10;&#10;2. Develop intermediate representations: There is also a proposal to create intermediate representations between dictionary word pronunciations and gestural ones, as the direct transition from one to another might not be straightforward or easy. These intermediate representations could help bridge the gap between standardized dictionary pronunciations and the more variable and reduced pronunciations found in natural speech.&#10;&#10;3. Analyze transcriptions and labelings: By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts. This information can be useful for developing more accurate speech recognition systems and models of speech production.">
      <data key="d0">1</data>
    </edge>
    <edge source=" entry , and then ? Or {pause} would we {disfmarker}&#10;Speaker: Professor C&#10;Content: Well , regular dictionary , I mean , this is a pretty rich dictionary . It 's got , got a fair number of pronunciations in it&#10;Speaker: Postdoc B&#10;Content: But {disfmarker}&#10;Speaker: PhD D&#10;Content: Or you could start from the {disfmarker} if we were gonna , do the same set , of sentences that Steve had , done , we could start with those transcriptions .&#10;Speaker: Postdoc B&#10;Content: Mm - hmm . So I was thinking {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: That 's actually what I was thinking , is tha {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: the problem is when you run , uh , if you run a regular dictionary , um , even if you have variants , in there , which most people don't , you don't always get , out , the" target="The difference between pronunciation models in dictionaries and the pronunciations that people produce lies in the variability and complexity of human speech. Dictionary pronunciations are typically represented using standardized symbols and conventions, which may not capture the full range of variation and reduction that occur in natural speech.&#10;&#10;Steve's work on labeling contributes to obtaining information about the actual pronunciations produced by speakers, including reduced syllables and variants that are not typically included in dictionaries. This information can be useful for developing more accurate speech recognition systems and models of speech production. By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the proposed adjustment to be made on the audio samples from twelve meetings is to convert them into a multi-channel format and adjust the timebands for precise representation. This adjustment should help improve the accuracy of processing and transcribing short segments of speech in these recordings." target="To accurately extract actual digits from a recording with false starts, misreads, and miscues, the proposed approach mentioned in the transcript involves the use of scripts and X Waves software. The user would select the portion of the audio that contains the desired digits, then use the software to compare the selected speech against a reference standard (in this case, the digit sequences &quot;six nine,&quot; &quot;nine two two,&quot; etc.). If the spoken digits match the reference standard, the user can proceed to the next sequence. This process allows for the precise identification and extraction of the correct digits from the recording, even when false starts, misreads, or miscues are present.&#10;&#10;This approach is based on the assumption that overlapping processes such as voicing and nasality may be less prevalent in digit reading, leading to clearer speech signals. This could potentially result in a higher level of accuracy for the extracted digits. However, it should be noted that further research would be needed to confirm this hypothesis.&#10;&#10;In summary, accurately extracting actual digits from a recording with false starts, misreads, and miscues requires careful selection of relevant audio segments and comparison against reference standards using software tools like scripts and X Waves. This approach can improve the overall accuracy of transcription by up to 1-2%.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the proposed adjustment to be made on the audio samples from twelve meetings is to convert them into a multi-channel format and adjust the timebands for precise representation. This adjustment should help improve the accuracy of processing and transcribing short segments of speech in these recordings." target="1. The question is asking whether the current corpus (a collection of audio samples) might be appropriate to attempt achieving a closer correspondence between speech and transcriptions, and if so, what the participants think about increasing predictability in the discussion. This could mean finding ways to make the speech more predictable and easier to transcribe accurately. The participants will need to consider the quality of the audio samples, the clarity of the speakers' pronunciation, and the effectiveness of any proposed methods for improving transcription accuracy.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, the proposed adjustment to be made on the audio samples from twelve meetings is to convert them into a multi-channel format and adjust the timebands for precise representation. This adjustment should help improve the accuracy of processing and transcribing short segments of speech in these recordings." target="1. Include reduced pronunciations in dictionaries: There is a suggestion to include reduced pronunciations, such as &quot;gonna&quot;, &quot;guh see you tomorrow&quot;, etc., in dictionaries or have people explicitly mark these reductions during transcription. This would provide more comprehensive and accurate representations of actual pronunciations produced by speakers.&#10;&#10;2. Develop intermediate representations: There is also a proposal to create intermediate representations between dictionary word pronunciations and gestural ones, as the direct transition from one to another might not be straightforward or easy. These intermediate representations could help bridge the gap between standardized dictionary pronunciations and the more variable and reduced pronunciations found in natural speech.&#10;&#10;3. Analyze transcriptions and labelings: By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts. This information can be useful for developing more accurate speech recognition systems and models of speech production.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Editing the first half of the project: The team has been working on editing the first half of the project. Postdoc B mentioned they were in the process of editing but welcomed the news about funding for the experiment as it would be useful for refining the process and making the overlaps more precise throughout.&#10;&#10;2. Funding of the experiment: Experiment funding update indicates that Postdoc B successfully obtained sufficient financial support to proceed with the project, although the source is not specified in the transcript.&#10;&#10;3. Meeting in the BARCO room: Postdoc B, Liz, and Don met in the BARCO room to discuss an issue concerning the interplay between the transcript format and the processing required for the SRI recognizer. The postdoc mentioned checking for spelling errors, ensuring consistency of conventions, and running it through a channelize program to get the data into multi-channel format. They also planned to discuss this issue further in the afternoon.&#10;&#10;The team is making progress on editing and refining their project while simultaneously securing funding and addressing technical challenges. The collaboration mentioned between Liz, Don, and Postdoc B aims to improve the transcript format for better processing." target=" , in the BARCO room , with the lecture hall ,&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD G&#10;Content: Yeah , please . Go ahead . And this afternoon .&#10;Speaker: Postdoc B&#10;Content: and this afternoon , it drifted into the afternoon , {comment} {vocalsound} uh , concerning this issue of , um , the , well there 's basically the issue of the interplay between the transcript format and the processing that , they need to do for , the SRI recognizer . And , um , well , so , I mentioned the process that I 'm going through with the data , so , you know , I get the data back from the transcri Well , s uh , metaphorically , get the data back from the transcriber , and then I , check for simple things like spelling errors and things like that . And , um , I 'm going to be doing a more thorough editing , with respect to consistency of the conventions . But they 're {disfmarker} they 're generally very good . And , then , I run it through , uh , the channelize program to get it into the multi - channel format , OK . And {pause} the">
      <data key="d0">1</data>
    </edge>
    <edge source="To accurately extract actual digits from a recording with false starts, misreads, and miscues, the proposed approach mentioned in the transcript involves the use of scripts and X Waves software. The user would select the portion of the audio that contains the desired digits, then use the software to compare the selected speech against a reference standard (in this case, the digit sequences &quot;six nine,&quot; &quot;nine two two,&quot; etc.). If the spoken digits match the reference standard, the user can proceed to the next sequence. This process allows for the precise identification and extraction of the correct digits from the recording, even when false starts, misreads, or miscues are present.&#10;&#10;This approach is based on the assumption that overlapping processes such as voicing and nasality may be less prevalent in digit reading, leading to clearer speech signals. This could potentially result in a higher level of accuracy for the extracted digits. However, it should be noted that further research would be needed to confirm this hypothesis.&#10;&#10;In summary, accurately extracting actual digits from a recording with false starts, misreads, and miscues requires careful selection of relevant audio segments and comparison against reference standards using software tools like scripts and X Waves. This approach can improve the overall accuracy of transcription by up to 1-2%." target=": And then we should get really smart over the next year or two , and it {disfmarker} that should get better .&#10;Speaker: Grad F&#10;Content: Right . And inc increase it by one or two percent , yeah .&#10;Speaker: Professor C&#10;Content: Yeah , {vocalsound} Yeah .&#10;Speaker: Grad F&#10;Content: Um , but , in order to do that we need to extract out the actual digits .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: Grad F&#10;Content: Um , so that {disfmarker} the reason it 's not just a transcript is that there 're false starts , and misreads , and miscues and things like that . And so I have a set of scripts and X Waves where you just select the portion , hit R , um , it tells you what the next one should be , and you just look for that . You know , so it {disfmarker} it 'll put on the screen , &quot; The next set is six nine , nine two two &quot; . And you find that , and , hit the key and it records it in a file in a particular format .&#10;Speaker: Professor C&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The compromised string referred to in the conversation is the best compromise or representation that a group of people could come up with to describe what happened during speech production. It is not necessarily the exact transcription of what was really said, but rather an approximation that takes into account overlapping processes such as voicing and nasality.&#10;&#10;Compared to phone labels, which are simplified representations of speech sounds, this compromised string is likely to be more accurate in capturing the complexities of actual speech production. This is because the compromised string can potentially incorporate features that are not included in phone labels, such as reduced syllables and pronunciation variants.&#10;&#10;However, it is important to note that the compromised string may still introduce some level of confusion or ambiguity, especially if it is not based on a well-defined set of rules or conventions. This is why further research is needed to determine the best way to represent and transcribe speech production in a way that balances accuracy and clarity." target="Speaker: PhD G&#10;Content: Right . That 's all , I mean . Exactly .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD G&#10;Content: Exactly .&#10;Speaker: Professor C&#10;Content: Yeah , no I {disfmarker} I don't disagree with that .&#10;Speaker: PhD G&#10;Content: And Steve 's type is fairly {disfmarker} it 's not that slow , uh , uh , I dunno exactly what the , timing was , but .&#10;Speaker: Professor C&#10;Content: Yeah u I don't disagree with it the on the only thing is that , What you actually will end {disfmarker} en end up with is something , i it 's all compromised , right , so , the string that you end up with isn't , actually , what happened . But it 's {disfmarker} it 's the best compromise that a group of people scratching their heads could come up with to describe what happened .&#10;Speaker: PhD D&#10;Content: And it 's more accurate than , phone labels .&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The compromised string referred to in the conversation is the best compromise or representation that a group of people could come up with to describe what happened during speech production. It is not necessarily the exact transcription of what was really said, but rather an approximation that takes into account overlapping processes such as voicing and nasality.&#10;&#10;Compared to phone labels, which are simplified representations of speech sounds, this compromised string is likely to be more accurate in capturing the complexities of actual speech production. This is because the compromised string can potentially incorporate features that are not included in phone labels, such as reduced syllables and pronunciation variants.&#10;&#10;However, it is important to note that the compromised string may still introduce some level of confusion or ambiguity, especially if it is not based on a well-defined set of rules or conventions. This is why further research is needed to determine the best way to represent and transcribe speech production in a way that balances accuracy and clarity." target=" it 's more accurate than , phone labels .&#10;Speaker: PhD G&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: But . And it 's more accurate than the {disfmarker} than the dictionary or , if you 've got a pronunciation uh lexicon that has three or four ,&#10;Speaker: Grad F&#10;Content: The word .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: this might be have been the fifth one that you tr that you pruned or whatever ,&#10;Speaker: PhD D&#10;Content: So it 's like a continuum .&#10;Speaker: PhD G&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: so sure .&#10;Speaker: PhD D&#10;Content: It 's {disfmarker} you 're going all the way down ,&#10;Speaker: PhD G&#10;Content: Right . Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: yeah .&#10;Speaker: PhD G&#10;Content: That 's what I meant is {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .">
      <data key="d0">1</data>
    </edge>
    <edge source="The difference between pronunciation models in dictionaries and the pronunciations that people produce lies in the variability and complexity of human speech. Dictionary pronunciations are typically represented using standardized symbols and conventions, which may not capture the full range of variation and reduction that occur in natural speech.&#10;&#10;Steve's work on labeling contributes to obtaining information about the actual pronunciations produced by speakers, including reduced syllables and variants that are not typically included in dictionaries. This information can be useful for developing more accurate speech recognition systems and models of speech production. By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts." target="1. Include reduced pronunciations in dictionaries: There is a suggestion to include reduced pronunciations, such as &quot;gonna&quot;, &quot;guh see you tomorrow&quot;, etc., in dictionaries or have people explicitly mark these reductions during transcription. This would provide more comprehensive and accurate representations of actual pronunciations produced by speakers.&#10;&#10;2. Develop intermediate representations: There is also a proposal to create intermediate representations between dictionary word pronunciations and gestural ones, as the direct transition from one to another might not be straightforward or easy. These intermediate representations could help bridge the gap between standardized dictionary pronunciations and the more variable and reduced pronunciations found in natural speech.&#10;&#10;3. Analyze transcriptions and labelings: By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts. This information can be useful for developing more accurate speech recognition systems and models of speech production.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The question is asking whether the current corpus (a collection of audio samples) might be appropriate to attempt achieving a closer correspondence between speech and transcriptions, and if so, what the participants think about increasing predictability in the discussion. This could mean finding ways to make the speech more predictable and easier to transcribe accurately. The participants will need to consider the quality of the audio samples, the clarity of the speakers' pronunciation, and the effectiveness of any proposed methods for improving transcription accuracy." target="} Do you have any {disfmarker} ? {pause} Anybody have any opinion about that ,&#10;Speaker: Postdoc B&#10;Content: and that people might articulate more , and you that might end up with more {disfmarker} a closer correspondence .&#10;Speaker: Professor C&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Grad F&#10;Content: Yeah {disfmarker} that 's {disfmarker} I {disfmarker} I agree .&#10;Speaker: PhD D&#10;Content: Sort of less predictability ,&#10;Speaker: Grad F&#10;Content: That {disfmarker} it 's just {disfmarker}&#10;Speaker: Postdoc B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: and {disfmarker} You hafta {disfmarker}&#10;Speaker: Grad F&#10;Content: It 's a {disfmarker} Well {disfmarker} Would , this corpus really be the right one to even try that on ?&#10;Speaker: PhD G&#10;Content: Well it">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The question is asking whether the current corpus (a collection of audio samples) might be appropriate to attempt achieving a closer correspondence between speech and transcriptions, and if so, what the participants think about increasing predictability in the discussion. This could mean finding ways to make the speech more predictable and easier to transcribe accurately. The participants will need to consider the quality of the audio samples, the clarity of the speakers' pronunciation, and the effectiveness of any proposed methods for improving transcription accuracy." target="1. Include reduced pronunciations in dictionaries: There is a suggestion to include reduced pronunciations, such as &quot;gonna&quot;, &quot;guh see you tomorrow&quot;, etc., in dictionaries or have people explicitly mark these reductions during transcription. This would provide more comprehensive and accurate representations of actual pronunciations produced by speakers.&#10;&#10;2. Develop intermediate representations: There is also a proposal to create intermediate representations between dictionary word pronunciations and gestural ones, as the direct transition from one to another might not be straightforward or easy. These intermediate representations could help bridge the gap between standardized dictionary pronunciations and the more variable and reduced pronunciations found in natural speech.&#10;&#10;3. Analyze transcriptions and labelings: By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts. This information can be useful for developing more accurate speech recognition systems and models of speech production.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Include reduced pronunciations in dictionaries: There is a suggestion to include reduced pronunciations, such as &quot;gonna&quot;, &quot;guh see you tomorrow&quot;, etc., in dictionaries or have people explicitly mark these reductions during transcription. This would provide more comprehensive and accurate representations of actual pronunciations produced by speakers.&#10;&#10;2. Develop intermediate representations: There is also a proposal to create intermediate representations between dictionary word pronunciations and gestural ones, as the direct transition from one to another might not be straightforward or easy. These intermediate representations could help bridge the gap between standardized dictionary pronunciations and the more variable and reduced pronunciations found in natural speech.&#10;&#10;3. Analyze transcriptions and labelings: By analyzing the transcriptions and labelings provided by human transcribers or phone recognizers, researchers can gain insights into the specific pronunciation variants and gestures used by speakers in different contexts. This information can be useful for developing more accurate speech recognition systems and models of speech production." target="er} there are two steps . One {disfmarker} you know , one is going from a dictionary pronunciation of something , like , &quot; gonna see you tomorrow &quot; ,&#10;Speaker: Grad F&#10;Content: And {disfmarker} Or &quot; gonta &quot; .&#10;Speaker: Professor C&#10;Content: Right . Yeah .&#10;Speaker: PhD G&#10;Content: it could be &quot; going to &quot; or &quot; gonna &quot; or &quot; gonta s &quot; you know .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD G&#10;Content: And , yeah . &quot; Gonna see you tomorrow &quot; , uh , &quot; guh see you tomorrow &quot; . And , that it would be nice to have these , intermediate , or these {disfmarker} some {disfmarker} these reduced pronunciations that those transcribers had marked or to have people mark those as well .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD G&#10;Content: Because , it 's not , um , that easy to go from the , dictionary , word pronuncia the dictionary phone pronunciation , to the gestural one without this intermediate or a syllable">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
