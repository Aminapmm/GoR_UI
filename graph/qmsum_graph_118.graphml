<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="PHP B mentioned that Hirsch had experimented with analysis windows ranging from 500 milliseconds to 1 second in length when working with non-stationary noises, such as those modulated with amplitude. He also noted that Hirsch demonstrated a delay in the estimation of noise in his experiments, and that it is important to center the analysis window for accurate results. In the context of a five hertz jack hammer noise, this would suggest that using an analysis window between 500 milliseconds and 1 second, and centering it on the noise, could provide useful information about the noise's characteristics, even if it is non-stationary." />
    <node id=" jack hammer going at five hertz .&#10;Speaker: PhD B&#10;Content: Yeah , that 's {disfmarker}&#10;Speaker: Professor C&#10;Content: I mean , good {disfmarker} good luck . So ,&#10;Speaker: PhD B&#10;Content: So in this case , yeah , sure , you cannot {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: But I think y um , Hirsch does experiment with windows of like between five hundred milliseconds and one second . And well , five hundred wa was not so bad . I mean and he worked on non - stationary noises , like noise modulated with well , wi with amplitude modulations and things like that ,&#10;Speaker: PhD A&#10;Content: Were his , uh , windows centered around the {disfmarker}&#10;Speaker: PhD B&#10;Content: and {disfmarker} But {disfmarker} Um , yeah . Well , I think {disfmarker} Yeah . Well , in {disfmarker} in the paper he showed that actually the estimation of the noise is {disfmarker} is delayed . Well , it" />
    <node id=" in {disfmarker} in the paper he showed that actually the estimation of the noise is {disfmarker} is delayed . Well , it 's {disfmarker} there is {disfmarker} you {disfmarker} you have to center the window , yeah .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: No , I understand it 's better to do but I just think that {disfmarker} that , uh , for real noises wh what {disfmarker} what 's most likely to happen is that there 'll be some things that are relatively stationary&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: where you can use one or another spectral subtraction thing&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and other things where it 's not so stationary and {disfmarker} I mean , you can always pick something that {disfmarker} that falls between your methods ,&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker:" />
    <node id=" to do it on - line , uh , with some forgetting factor or something .&#10;Speaker: PhD A&#10;Content: So do you {disfmarker} is there some long window that extends into the past over which you calculate the average ?&#10;Speaker: Professor C&#10;Content: Well , there 's a lot of different ways of computing the noise spectrum . So one of the things that , uh , Hans - Guenter Hirsch did , uh {disfmarker} and pas and other people {disfmarker} actually , he 's {disfmarker} he wasn't the only one I guess , was to , uh , take some period of {disfmarker} of {disfmarker} of speech and in each band , uh , develop a histogram . So , to get a decent histogram of these energies takes at least a few seconds really . But , uh {disfmarker} I mean you can do it with a smaller amount but it 's pretty rough . And , um , in fact I think the NIST standard method of determining signal - to - noise ratio is based on this .&#10;Speaker: PhD A&#10;Content: A couple seconds ?&#10;Speaker: Professor" />
    <node id=": Didn't they {disfmarker}&#10;Speaker: Professor C&#10;Content: Which means you 're gonna make errors between similar sounds that are son sonorant or obstruent .&#10;Speaker: PhD A&#10;Content: Didn't they also do some kind of an oracle experiment where they said &quot; if we {pause} could detect the sonorants perfectly {pause} and then show how it would improve speech recognition ? I thought I remember hearing about an experiment like that .&#10;Speaker: Professor C&#10;Content: The - these same people ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: I don't remember that .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: That would {disfmarker} that 's {disfmarker} you 're right , that 's exactly the question to follow up this discussion , is suppose you did that , uh , got that right . Um , Yeah .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: What could be the other low level detectors , I mean , for {disfmarker}" />
    <node id=" A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So {disfmarker} so basically now you have this mixture of two Gaussians , you {disfmarker} you n know what they are , and , uh {disfmarker} I mean , sorry , you estimate what they are , and , uh , so this gives you what the signal is and what the noise e energy is in that band in the spectrum . And then you look over the whole thing and now you have a noise spectrum . So , uh , Hans - Guenter Hirsch and others have used that kind of method . And the other thing to do is {disfmarker} which is sort of more trivial and obvious {comment} {disfmarker} is to , uh , uh , determine through magical means that {disfmarker} that , uh , there 's no speech in some period , and then see what the spectrum is .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Uh , but , you know , it 's {disfmarker} that {disfmarker} that {disfmarker} that 's" />
    <node id=" method of determining signal - to - noise ratio is based on this .&#10;Speaker: PhD A&#10;Content: A couple seconds ?&#10;Speaker: Professor C&#10;Content: So {disfmarker} No , no , it 's based on this kind of method ,&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: this histogram method . So you have a histogram . Now , if you have signal and you have noise , you basically have these two bumps in the histogram , which you could approximate as two Gaussians .&#10;Speaker: PhD A&#10;Content: But wh don't they overlap sometimes ?&#10;Speaker: Professor C&#10;Content: Oh , yeah .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: So you have a mixture of two Gaussians .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Right ? And you can use EM to figure out what it is . You know .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So {disfmarker} so basically now you have this mixture of two Ga" />
    <node id="1. Using only the beginning characteristics of a long speech for analysis can be risky because it may not accurately represent the entire speech due to changes that may occur throughout the speech. This could lead to inaccurate conclusions or decisions based on incomplete information.&#10;2. On the other hand, using local characteristics in time for analysis can provide a more accurate representation of the speech as it takes into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results." />
    <node id=" they thinking of changing it to ?&#10;Speaker: Professor C&#10;Content: But {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , well the people who had very low latency want it to be low {disfmarker} uh , very {disfmarker} {vocalsound} very very narrow , uh , latency bound . And the people who have longer latency don't . So .&#10;Speaker: PhD A&#10;Content: Huh .&#10;Speaker: PhD B&#10;Content: So , yeah .&#10;Speaker: Professor C&#10;Content: Unfortunately we 're the main ones with long latency , but&#10;Speaker: PhD A&#10;Content: Ah !&#10;Speaker: Professor C&#10;Content: But , uh ,&#10;Speaker: PhD B&#10;Content: Yeah , and basically the best proposal had something like thirty or forty milliseconds of latency .&#10;Speaker: Professor C&#10;Content: you know , it 's {disfmarker} Yeah .&#10;Speaker: PhD B&#10;Content: So . Well .&#10;Speaker: Professor C&#10;Content: Yeah , so they were basically {disfmarker} I mean , they were" />
    <node id=" , you know , a neat {disfmarker} neat thing and {disfmarker} and , uh {disfmarker} So .&#10;Speaker: Grad E&#10;Content: S so , um , Ohala 's going to help do these , uh {pause} transcriptions of the meeting data ?&#10;Speaker: PhD A&#10;Content: Uh , well I don't know . We d we sort of didn't get that far . Um , we just talked about some possible features that could be marked by humans and , um ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: because of having maybe some extra transcriber time we thought we could go through and mark some portion of the data for that . And , uh {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: I mean , that 's not an immediate problem , that we don't immediately have a lot of extra transcriber time .&#10;Speaker: PhD A&#10;Content: Yeah , right .&#10;Speaker: Professor C&#10;Content: But {disfmarker} but , uh" />
    <node id=" part of the road&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: or whatever . But it 's {disfmarker} it 's {disfmarker} i i i using the local characteristics in time , is probably going to work pretty well .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: But you could get hurt a lot if you just took some something from the beginning of all the speech , of , you know , an hour of speech and then later {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , so they may be {disfmarker} you know , may be overly , uh , complicated for {disfmarker} for this test but {disfmarker} but {disfmarker} but , uh , I don't know . But what you 're saying , you know , makes sense , though . I mean , if possible you shouldn't {disfmarker} you should {disfmarker} you should make it , uh , the center of the {d" />
    <node id=" um , except he used a longer time window ,&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad F&#10;Content: like a second maybe . And the reason for that is RASTA 's time window is too short to , um include the whole , um , reverberation {disfmarker} um , I don't know what you call it the reverberation response . I if you see wh if you see what I mean . The reverberation filter from my mouth to that mike is like {disfmarker} it 's t got it 's too long in the {disfmarker} in the time domain for the um {disfmarker} for the RASTA filtering to take care of it . And , um , then there are a couple of other speech enhancement approaches which haven't been tried for speech recognition yet but have just been tried for enhancement , which , um , have the assumption that um , you can do LPC um analysis of th of the signal you get at the far microphone and the , um , all pole filter that you get out of that should be good . It 's just the , um , excitation signal {comment} that is going to" />
    <node id=" kind of thing could add to the latency . I mean , depending on where the window was that you used to calculate {pause} the signal - to - noise ratio .&#10;Speaker: PhD B&#10;Content: Yeah , sure . But {disfmarker} Mmm .&#10;Speaker: Professor C&#10;Content: Not necessarily . Cuz if you don't look into the future , right ?&#10;Speaker: PhD A&#10;Content: OK , well that {disfmarker} I guess that was my question ,&#10;Speaker: Professor C&#10;Content: if you just {disfmarker} yeah {disfmarker}&#10;Speaker: PhD A&#10;Content: yeah .&#10;Speaker: Professor C&#10;Content: I mean , if you just {disfmarker} if you {disfmarker} you , uh {disfmarker} a at the beginning you have some {disfmarker}&#10;Speaker: PhD A&#10;Content: Guess .&#10;Speaker: Professor C&#10;Content: esti some guess and {disfmarker} and , uh , uh {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah , but it {disfmarker}" />
    <node id=" I 'm going to look at this frequency , &quot; or {disfmarker} What {disfmarker} what {disfmarker} what are they looking at ?&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD A&#10;Content: What are their inputs ?&#10;Speaker: Grad E&#10;Content: Uh Right , so the {disfmarker} OK , so at each for each sub - band {comment} there are basically , uh , several measures of SNR and {disfmarker} and correlation .&#10;Speaker: PhD A&#10;Content: Ah , OK , OK .&#10;Speaker: Grad E&#10;Content: Um , um and he said there 's like twenty of these per {disfmarker} per sub - band . Um , and for {disfmarker} for every s every sub - band , e you {disfmarker} you just pick ahead of time , um , &quot; I 'm going to have like five {pause} i independent logistic tests . &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um" />
    <node id="1. Better Discrimination: The main advantage of changing and using Linear Discriminant Analysis (LDA) discriminatively, as opposed to using Karhunen-LoÃ¨ve Transform (KLT), is that LDA provides better discrimination between classes. This means that LDA can create a more accurate separation between different categories or groups in the data.&#10;&#10;2. Linear but Discriminant: While KLT focuses on preserving the maximum amount of data variance, LDA tries to find linear transformations that maximize the ratio of between-class variance to within-class variance. This results in a linear but discriminant representation of the data.&#10;&#10;3. Comparison with Neural Nets: Using LDA can help researchers understand if having neural nets really provides any significant improvement over more traditional techniques like KLT or LDA. It allows for comparing and evaluating the performance of different methods to determine their relative merits.&#10;&#10;4. Local Characteristics Analysis: By using local characteristics in time for analysis, LDA can provide a more accurate representation of the data, taking into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results.&#10;&#10;5. Better Understanding: Using LDA in this context can help researchers gain a better understanding of how much they benefit from simply putting frames together versus using discriminative methods and nonlinearity in their analysis. It helps them evaluate the contribution of different components in improving the overall performance." />
    <node id=" KLT for LDA .&#10;Speaker: PhD A&#10;Content: Change the what ?&#10;Speaker: PhD D&#10;Content: The contextual KLT .&#10;Speaker: PhD A&#10;Content: I 'm missing that last word . Context&#10;Speaker: Professor C&#10;Content: K {disfmarker} KLT .&#10;Speaker: PhD A&#10;Content: KLT .&#10;Speaker: PhD D&#10;Content: KLT {disfmarker}&#10;Speaker: Grad E&#10;Content: Oh . KLT .&#10;Speaker: PhD A&#10;Content: Oh , KLT .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: KLT , I 'm sorry . Uh , to change and use LDA discriminative .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: But {disfmarker} I don't know .&#10;Speaker: Professor C&#10;Content: Uh ,&#10;Speaker: PhD A&#10;Content: What is the advantage of that ?" />
    <node id=" So .&#10;Speaker: PhD A&#10;Content: but I don't think {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah , but we 've {disfmarker} but we played a little bit with {disfmarker} with asymmetric , guys .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: You can do it . So . So , that 's what {disfmarker} that 's what you 're busy with , s messing around with this ,&#10;Speaker: PhD B&#10;Content: Uh , yeah .&#10;Speaker: Professor C&#10;Content: yeah . And , uh ,&#10;Speaker: PhD D&#10;Content: Also we were thinking to {disfmarker} to , uh , apply the eh , spectral subtraction from Ericsson&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: and to {disfmarker} to change the contextual KLT for LDA .&#10;Speaker: PhD A&#10;Content: Change the what ?&#10;Speaker: PhD D&#10;Content: The contextual KLT" />
    <node id=" we want to do , perhaps it 's to replace {disfmarker} to {disfmarker} to have something that 's discriminant but linear , also . And to see if it {disfmarker} if it improves ov over {disfmarker} over the non - discriminant linear transformation .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: And if the neural net is better than this or , well . So .&#10;Speaker: Professor C&#10;Content: Yeah , well , that 's what I meant , is to see whether {disfmarker} whether it {disfmarker} having the neural net really buys you anything .&#10;Speaker: PhD B&#10;Content: Ye Mmm .&#10;Speaker: Professor C&#10;Content: Uh , I mean , it doe did look like it buys you something over just the KLT .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: But maybe it 's just the discrimination and {disfmarker} and maybe {disfmarker} yeah , maybe the nonlinear discrimination isn't necessary .&#10;Speaker: PhD D&#10;Content:" />
    <node id=" I don't know .&#10;Speaker: Professor C&#10;Content: Uh ,&#10;Speaker: PhD A&#10;Content: What is the advantage of that ?&#10;Speaker: PhD D&#10;Content: Uh {disfmarker}&#10;Speaker: PhD B&#10;Content: Well , it 's that by the for the moment we have , uh , something that 's discriminant and nonlinear . And the other is linear but it 's not discriminant at all . Well , it 's it 's a linear transformation , that {disfmarker} Uh {disfmarker}&#10;Speaker: Professor C&#10;Content: So at least just to understand maybe what the difference was between how much you were getting from just putting the frames together and how much you 're getting from the discriminative , what the nonlinearity does for you or doesn't do for you . Just to understand it a little better I guess .&#10;Speaker: PhD B&#10;Content: Mmm . Well {disfmarker} uh {disfmarker} yeah . Actually what we want to do , perhaps it 's to replace {disfmarker} to {disfmarker} to have something that 's discriminant" />
    <node id="1. Smooth spectral envelopes are commonly used in speech recognition because they have been found to be generally effective. However, the text suggests that they may not be the best option when the goal is to determine sonorant or non-sonorant sounds, as they focus more on the overall envelope rather than the periodicity of the signal.&#10;2. The use of smooth envelopes in vocoders for speech recognition might lead to errors between similar sounds that are sonorant or obstruent, impacting the system's ability to differentiate them accurately.&#10;3. An alternative approach could be building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy, as voicing and sonorance are closely related concepts.&#10;4. There is a mention of an oracle experiment where perfect detection of sonorants could improve speech recognition performance, but there's no further detail provided in the text. This idea highlights the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." />
    <node id=" for that . I mean , this use of a very smooth , uh , spectral envelope is something that , you know , has evolved as being generally a good thing for speech recognition but if you knew that what you were gonna do is detect sonorants or not {disfmarker} So sonorants and non - sonorants is {disfmarker} is {disfmarker} is almost like voiced - unvoiced , except I guess that the voiced stops are {disfmarker} are also called &quot; obstruents &quot; . Uh , so it 's {disfmarker} it 's {disfmarker} uh , but with the exception of the stops I guess it 's pretty much the same as voiced - unvoiced ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: right ? So {disfmarker} so {disfmarker} Um . So , um , if you knew you were doing that , if you were doing something say for a {disfmarker} a , uh {disfmarker} a {disfmarker} a Vocoder , you wouldn't use the same kind of" />
    <node id="isfmarker} a , uh {disfmarker} a {disfmarker} a Vocoder , you wouldn't use the same kind of features . You would use something that was sensitive to the periodicity and {disfmarker} and not just the envelope . Uh , and so in that sense it was an unfair test . Um , so I think that the questioner was right . It {disfmarker} it was in that sense an unfair test . Nonetheless , it was one that was interesting because , uh , this is what we are actually using for speech recognition , these smooth envelopes . And this says that perhaps even , you know , trying to use them in the best way that we can , that {disfmarker} that {disfmarker} that we ordinarily do , with , you know , Gaussian mixtures and H M Ms {comment} and so forth , you {disfmarker} you don't , uh , actually do that well on determining whether something is sonorant or not .&#10;Speaker: PhD A&#10;Content: Didn't they {disfmarker}&#10;Speaker: Professor C&#10;Content: Which means you 're gonna make errors between similar sounds that are" />
    <node id="Content: Hmm .&#10;Speaker: PhD B&#10;Content: What could be the other low level detectors , I mean , for {disfmarker} {comment} Other kind of features , or {disfmarker} ? in addition to detecting sonorants or {disfmarker} ? Th - that 's what you want to {disfmarker} to {disfmarker} to go for also&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD B&#10;Content: or {disfmarker} ?&#10;Speaker: Grad E&#10;Content: What t Oh , build other {disfmarker} other detectors on different {pause} phonetic features ?&#10;Speaker: PhD B&#10;Content: Other low level detectors ? Yeah .&#10;Speaker: Grad E&#10;Content: Um , uh Let 's see , um , Yeah , I d I don't know . e Um , um , I mean , w easiest thing would be to go {disfmarker} go do some voicing stuff but that 's very similar to sonorance .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad E" />
    <node id=" and non - sonorant .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So he hasn't applied it to recognition or if he did he didn't talk about it . It 's {disfmarker} it 's just {disfmarker} And one of the concerns in the audience , actually , was that {disfmarker} that , um , the , uh , uh {disfmarker} he {disfmarker} he did a comparison to , uh , you know , our old foil , the {disfmarker} the nasty old standard recognizer with {vocalsound} mel {disfmarker} mel filter bank at the front , and H M Ms , and {disfmarker} and so forth . And , um , it didn't do nearly as well , especially in {disfmarker} in noise . But the {disfmarker} one of the good questions in the audience was , well , yeah , but that wasn't trained for that . I mean , this use of a very smooth , uh , spectral envelope is something that , you know , has evolved as being generally a good thing" />
    <node id="marker} on phonetic targets , and then combining them some somehow down the line , um , they 're {disfmarker} they 're taking sub - band features and , um , training up a detector that detects for , um , these phonetic features for example , um , he presents um , uh , a detector to detect sonorance . And so what {disfmarker} what it basically is {disfmarker} is , um {disfmarker} it 's {disfmarker} there 's {disfmarker} at the lowest level , there {disfmarker} it 's {disfmarker} it 's an OR ga I mean , it 's an AND gate . So , uh , on each sub - band you have several independent tests , to test whether um , there 's the existence of sonorance in a sub - band . And then , um , it c it 's combined by a soft AND gate . And at the {disfmarker} at the higher level , for every {disfmarker} if , um {disfmarker} The higher level there 's a soft OR gate . Uh , so if {" />
    <node id="1. The speakers describe a method that involves initializing parameters for low-level detectors and using Expectation Maximization (EM) to establish training targets for these detectors.&#10;2. Once this is done, they train the whole system using maximum likelihood. This method of detecting sonorance has been found to be robust compared to traditional full-band Gaussian mixtures estimations of sonorance.&#10;3. They also discuss the possibility of building other detectors based on different phonetic features like voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy due to the close relationship between voicing and sonorance.&#10;4. The speakers briefly mention an &quot;oracle experiment&quot; where perfect detection of sonorants could improve speech recognition performance, highlighting the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." />
    <node id=" &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um , in some {disfmarker} some way and use EM to come up with your training targets for a {disfmarker} for the {disfmarker} the low - level detectors .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And then , once you get that done , you {disfmarker} you {disfmarker} you train the whole {disfmarker} whole thing on maximum likelihood . Um , and h he shows that using this {disfmarker} this method to detect sonorance is it 's very robust compared to , um {disfmarker} {vocalsound} to typical , uh , full - band Gaussian mixtures um estimations of {disfmarker} of sonorance .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Grad E&#10;Content: And , uh so {disfmarker} so that 's just {disf" />
    <node id=" every {disfmarker} if , um {disfmarker} The higher level there 's a soft OR gate . Uh , so if {disfmarker} if this detector detects um , the presence of {disfmarker} of sonorance in any of the sub - bands , then the detect uh , the OR gate at the top says , &quot; OK , well this frame has evidence of sonorance . &quot;&#10;Speaker: PhD A&#10;Content: What are {disfmarker} what are some of the low level detectors that they use ?&#10;Speaker: Grad E&#10;Content: And these are all {disfmarker} Oh , OK . Well , the low level detectors are logistic regressions . Um , and the , uh {disfmarker}&#10;Speaker: Professor C&#10;Content: So that , by the way , basically is a {disfmarker} is one of the units in our {disfmarker} in our {disfmarker} our neural network .&#10;Speaker: Grad E&#10;Content: the one o&#10;Speaker: Professor C&#10;Content: So that 's all it is . It 's a sig it 's a s" />
    <node id="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." />
    <node id="Speaker: PhD A&#10;Content: OK , we 're on .&#10;Speaker: Professor C&#10;Content: OK , what are we talking about today ?&#10;Speaker: PhD B&#10;Content: I don't know . Do you have news from the conference talk ? Uh , that was programmed for yesterday {disfmarker} I guess .&#10;Speaker: Professor C&#10;Content: Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Yesterday&#10;Speaker: Professor C&#10;Content: Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Yesterday morning on video conference .&#10;Speaker: Professor C&#10;Content: Uh ,&#10;Speaker: PhD B&#10;Content: Well&#10;Speaker: Professor C&#10;Content: oh , I 'm sorry .&#10;Speaker: Grad E&#10;Content: Oh . Conference call .&#10;Speaker: Professor C&#10;Content: I know {disfmarker} now I know what you 're talking about . No , nobody 's told me anything .&#10;Speaker: PhD B&#10;Content: Alright .&#10;Speaker: PhD A&#10;Content: Oh , this was the , uh , talk where they were supposed to try to decide {disf" />
    <node id="marker} I was about to read them too . It 's a , uh , blank sheet of paper .&#10;Speaker: PhD A&#10;Content: So I guess we 're {disfmarker} we 're done .&#10;Speaker: Professor C&#10;Content: Yeah , yeah , I 'll do my credit card number later . OK ." />
    <node id=" Well , anyway , I mean , I think {disfmarker} we could cut {disfmarker} we know what else , we could cut down on the neural net time by {disfmarker} by , uh , playing around a little bit , going more into the past , or something like that . We t we talked about that .&#10;Speaker: PhD A&#10;Content: So is the latency from the neural net caused by how far ahead you 're looking ?&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And there 's also {disfmarker} well , there 's the neural net and there 's also this , uh , uh , multi - frame , uh , uh , KLT .&#10;Speaker: PhD A&#10;Content: Wasn't there {disfmarker} Was it in the , uh , recurrent neural nets where they weren't looking ahead at all ?&#10;Speaker: Professor C&#10;Content: They weren't looking ahead much . They p they looked ahead a little bit .&#10;Speaker: PhD A&#10;Content: A little bit ." />
    <node id=": PhD B&#10;Content: Um .&#10;Speaker: Professor C&#10;Content: uh , one thing that would be no {disfmarker} good to find out about from this conference call is that what they were talking about , what they 're proposing doing , was having a third party , um , run a good VAD , and {disfmarker} and determine boundaries .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: And then given those boundaries , then have everybody do the recognition .&#10;Speaker: PhD D&#10;Content: Begin to work .&#10;Speaker: Professor C&#10;Content: The reason for that was that , um , uh {disfmarker} if some one p one group put in the VAD and another didn't , uh , or one had a better VAD than the other since that {disfmarker} they 're not viewing that as being part of the {disfmarker} the task , and that any {disfmarker} any manufacturer would put a bunch of effort into having some s kind of good speech - silence detection . It still wouldn't be perfect but I mean , e the argument was &quot; let 's not have" />
    <node id=": Alright .&#10;Speaker: PhD A&#10;Content: Oh , this was the , uh , talk where they were supposed to try to decide {disfmarker}&#10;Speaker: PhD B&#10;Content: To {disfmarker} to decide what to do ,&#10;Speaker: PhD A&#10;Content: Ah , right .&#10;Speaker: PhD B&#10;Content: yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah . No , that would have been a good thing to find out before this meeting , that 's . No , I have no {disfmarker} I have no idea . Um , Uh , so I mean , let 's {disfmarker} let 's assume for right now that we 're just kind of plugging on ahead ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: because even if they tell us that , uh , the rules are different , uh , we 're still interested in doing what we 're doing . So what are you doing ?&#10;Speaker: PhD B&#10;Content: Mm - hmm . Uh , well , we 've {disf" />
    <node id=" Right .&#10;Speaker: Professor C&#10;Content: and then the {disfmarker} the microscope would start moving or something .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: And there 's physical inertia there , so probably the {disfmarker} the motion itself was all {disfmarker}&#10;Speaker: PhD A&#10;Content: And it felt to , uh , the users that it was instantaneous . I mean , as fast as talking to a person . It {disfmarker} th I don't think anybody ever complained about the delay .&#10;Speaker: Professor C&#10;Content: Yeah , so you would think as long as it 's under half a second or something .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , I 'm not an expert on that&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: but .&#10;Speaker: PhD A&#10;Content: I don't remember the exact numbers but it was something like that .&#10;Spe" />
    <node id="The participants in the video conference were discussing the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries for a conference talk. The intention was that everyone would then do recognition based on those boundaries. They had not communicated this proposal to the group before the meeting, which caused some confusion among the participants.&#10;&#10;As for what they are currently doing, it is not explicitly stated in the conversation, but they mention that they have a meeting scheduled for the afternoon to discuss and understand the task better. It seems they are trying to gather more information about the conference talk and its requirements before making any decisions." />
    <node id=" saying . As long as you get {disfmarker} The sequence ,&#10;Speaker: Professor C&#10;Content: We 're g if we 're doing {disfmarker} if we 're talking about transcription as opposed to something else {disfmarker}&#10;Speaker: PhD A&#10;Content: right ? Yeah . Yeah , yeah , yeah . Yeah . Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So where it could help is maybe at a higher level . Yeah .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: Grad E&#10;Content: Like a understanding application .&#10;Speaker: PhD A&#10;Content: Understanding , yeah . Exactly .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: But that 's this afternoon 's meeting . Yeah . We don't understand anything in this meeting . Yeah , so that 's {disfmarker} yeah , that 's , you know , a neat {disfmarker} neat thing and {disfmarker} and , uh {disfmarker} So ." />
    <node id="The speakers are subtracting in the power domain. This was mentioned by Professor C and confirmed by PhD B when they were discussing the implementation of spectral subtraction. When Professor C asked &quot;And you're subtracting in the...power domain, or magnitude domain. Probably power domain, right?&quot; PhD B responded with &quot;I guess it's power domain, yeah.&quot;&#10;&#10;The reason they chose the power domain over the magnitude domain is not explicitly stated in the transcript. However, it can be inferred that they may have chosen the power domain because it is more closely aligned with the theory of spectral subtraction, as Professor C mentioned when he said &quot;if you look at the theory, it should be in the power domain.&quot; Despite this, Professor C also mentions that he has seen implementations where people do it in the magnitude domain and shrug their shoulders, implying that it may still work effectively for some use cases." />
    <node id=": You do {disfmarker} doing the {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And you 're {disfmarker} you 're subtracting in the {disfmarker} in the {disfmarker} in the {disfmarker} I guess it 's power {disfmarker} power domain , uh , or {disfmarker} or magnitude domain . Probably power domain , right ?&#10;Speaker: PhD B&#10;Content: I guess it 's power domain , yeah .&#10;Speaker: Professor C&#10;Content: why&#10;Speaker: PhD B&#10;Content: I don't remember exactly .&#10;Speaker: Professor C&#10;Content: Yeah ,&#10;Speaker: PhD D&#10;Content: I don't remember .&#10;Speaker: PhD B&#10;Content: But {disfmarker} yeah , so it 's before everything else ,&#10;Speaker: Professor C&#10;Content: yep .&#10;Speaker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Professor C&#10;Content: I mean , if you look at the theory" />
    <node id=" um {disfmarker} you don't just subtract the {disfmarker} the estimate of the noise spectrum . You subtract th that times {disfmarker}&#10;Speaker: PhD B&#10;Content: A little bit more and {disfmarker} Yeah .&#10;Speaker: Professor C&#10;Content: Or {disfmarker} or less , or {disfmarker}&#10;Speaker: PhD A&#10;Content: Really ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Huh !&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And generated this {disfmarker} this ,&#10;Speaker: Professor C&#10;Content: Uh .&#10;Speaker: PhD B&#10;Content: um , so you have the estimation of the power spectra of the noise , and you multiply this by a factor which is depend dependent on the SNR . So . Well .&#10;Speaker: PhD D&#10;Content: Hmm , maybe .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: PhD B&#10;Content: When the speech lev when the signal level is more important , compared to this noise level ," />
    <node id="Content: That 's what spectral subtraction will help with , practically speaking .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: If it varies a lot , to get a If {disfmarker} if {disfmarker} to get a good estimate you need a few seconds of speech , even if it 's centered , right ?&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: if you need a few seconds to get a decent estimate but it 's changed a lot in a few seconds , then it , you know , i it 's kind of a problem .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: I mean , imagine e five hertz is the middle of the {disfmarker} of the speech modulation spectrum ,&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: right ? So imagine a jack hammer going at five hertz .&#10;Speaker: PhD B&#10;Content: Yeah , that 's {disfmarker}&#10;Speaker:" />
    <node id="aker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Professor C&#10;Content: I mean , if you look at the theory , it 's {disfmarker} it should be in the power domain but {disfmarker} but , uh , I 've seen implementations where people do it in the magnitude domain&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and {disfmarker}&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: I have asked people why and they shrug their shoulders and say , &quot; oh , it works . &quot; So .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , and there 's this {disfmarker} I guess there 's this mysterious {disfmarker} I mean people who do this a lot I guess have developed little tricks of the trade . I mean , there 's {disfmarker} there 's this , um {disfmarker} you don't just subtract the {disfmarker} the estimate of the noise spectrum . You subtract th that times {" />
    <node id="isfmarker} and maybe {disfmarker} yeah , maybe the nonlinear discrimination isn't necessary .&#10;Speaker: PhD D&#10;Content: S maybe .&#10;Speaker: PhD B&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor C&#10;Content: Could be .&#10;Speaker: PhD D&#10;Content: Maybe .&#10;Speaker: Professor C&#10;Content: Good {disfmarker} good to know . But the other part you were saying was the spectral subtraction , so you just kind of , uh {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: At what stage do you do that ? Do you {disfmarker} you 're doing that , um {disfmarker} ?&#10;Speaker: PhD B&#10;Content: So it would be on the um {disfmarker} on {disfmarker} on the mel frequency bands ,&#10;Speaker: PhD D&#10;Content: We was think&#10;Speaker: PhD B&#10;Content: so . Yeah , be before everything .&#10;Speaker: Professor C&#10;Content: OK ,&#10;Speaker: PhD D&#10;Content: Yeah" />
    <node id="1. Human interaction and text-based conversation systems differ in conversational dynamics due to factors such as turn-taking, overlapping utterances, and reaction time. In human interaction, individuals often step on each other's utterances, creating a more fluid and simultaneous exchange of information. However, text-based systems typically involve distinct turns with no overlap or immediate response, which can feel abrupt or unnatural.&#10;&#10;2. To improve the flow and reduce abruptness in a text-based conversation system, implementing techniques that mimic human conversational dynamics could be beneficial. For instance, incorporating delays between responses to simulate thinking time or introducing a level of overlapping utterances by allowing multiple responses at once can help create a more natural feel.&#10;&#10;3. The proposal discussed during the video conference aims to address these issues by having a third party run Voice Activity Detection (VAD) and establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness. However, this idea needs further discussion and understanding before implementation." />
    <node id=" to get back to you ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: I mean , {vocalsound} it would be f I mean , it might even be too abrupt . You might have to put in a s a s {vocalsound} a delay .&#10;Speaker: PhD A&#10;Content: Yeah . I mean , it may feel different than talking to a person&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: because when we talk to each other we tend to step on each other 's utterances . So like if I 'm asking you a question , you may start answering before I 'm even done .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So it {disfmarker} it would probably feel different&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: but I don't think it would feel slow .&#10;Speaker: Professor C&#10;Content: Right . Well , anyway , I mean , I think {disfmarker} we could cut {disfmarker} we know what else , we could cut" />
    <node id="Based on the provided transcript, there was no concrete outcome or decision made during the conference talk related to Voice Activity Detection (VAD) and determining boundaries. The participants discussed the possibility of having a third party run a VAD and then doing recognition based on those boundaries, but they did not reach a conclusion. They also mentioned that they had not been informed about this proposal beforehand, which caused some confusion during the meeting. It appears that further discussions and decisions will take place in subsequent meetings or communications." />
    <node id="Based on the transcript, it appears that both PhD B and Sunil are involved in designing new IIR filters to improve the bugs and latency issues. They used LDA filters and FIR filters as a starting point for their design. This information is conveyed in several parts of the transcript, including when PhD B says &quot;we took the LDA filters and we designed new filters, using recursive filters actually&quot; and &quot;so we took the filters the FIR filters and we designed IIR filters that have the same frequency response&quot;. Additionally, when Professor C asks &quot;when you say 'we', is that something Sunil is doing or is that?&quot; PhD B responds by saying &quot;I'm sorry?&quot; which suggests that they are including Sunil in their use of the word &quot;we&quot;. This is later confirmed when Professor C asks &quot;You had a discussion with Sunil about this though?&quot; and PhD B responds by saying &quot;Uh - huh.&quot;" />
    <node id=" . So what are you doing ?&#10;Speaker: PhD B&#10;Content: Mm - hmm . Uh , well , we 've {disfmarker} a little bit worked on trying to see , uh , what were the bugs and the problem with the latencies .&#10;Speaker: PhD D&#10;Content: To improve {disfmarker}&#10;Speaker: PhD B&#10;Content: So , We took {disfmarker} first we took the LDA filters and , {vocalsound} uh , we designed new filters , using uh recursive filters actually .&#10;Speaker: Professor C&#10;Content: So when you say &quot; we &quot; , is that something Sunil is doing or is that {disfmarker} ?&#10;Speaker: PhD B&#10;Content: I 'm sorry ?&#10;Speaker: Professor C&#10;Content: Who is doing that ?&#10;Speaker: PhD B&#10;Content: Uh , us . Yeah .&#10;Speaker: Professor C&#10;Content: Oh , oh . Oh , OK .&#10;Speaker: PhD B&#10;Content: So we took the filters {disfmarker} the FIR filters {vocalsound} and we {comment} designed , uh , IIR filters that" />
    <node id=" So we took the filters {disfmarker} the FIR filters {vocalsound} and we {comment} designed , uh , IIR filters that have the same frequency response .&#10;Speaker: PhD D&#10;Content: But {disfmarker}&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Well , similar , but that have shorter delays .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So they had two filters , one for the low frequency bands and another for the high frequency bands . And so we redesigned two filters . And the low frequency band has sixty - four milliseconds of delay , and the high frequency band filter has something like eleven milliseconds compared to the two hundred milliseconds of the IIR filters . But it 's not yet test . So we have the filters but we still have to implement a routine that does recursive filtering&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Professor C&#10;Content: You {disfmarker} you had a discussion with Sunil about this though ?&#10;Spe" />
    <node id="&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: well , a low - pass filter at {disfmarker} at twenty - five hertz . Uh , because wh when {disfmarker} when we look at the LDA filters , well , they are basically low - pass but they leave a lot of what 's above twenty - five hertz .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Um , and so , yeah , this will be another filter which would add ten milliseconds again .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Um , yeah , and then there 's a third thing , is that , um , basically the way on - line normalization was done uh , is just using this recursion on {disfmarker} on the um , um , on the feature stream ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: and {disfmarker} but this is a filter , so it has also a delay . Uh , and when we look at this filter actually it has a delay of eighty - five milliseconds . So if" />
    <node id=" - six milliseconds ,&#10;Speaker: PhD D&#10;Content: The low f f&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: which , uh {disfmarker} What was the total we ended up with through the whole system ?&#10;Speaker: PhD B&#10;Content: Three hundred and thirty .&#10;Speaker: Professor C&#10;Content: So that would be within {disfmarker} ?&#10;Speaker: PhD B&#10;Content: Yeah , but there are other points actually , uh , which will perhaps add some more delay . Is that some other {disfmarker} other stuff in the process were perhaps not very {disfmarker} um perf well , not very correct , like the downsampling which w was simply dropping frames .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Um , so we will try also to add a nice downsampling having a filter that {disfmarker} that {disfmarker}&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: well , a low - pass filter at {disfmarker} at twenty" />
    <node id="1. PhD B and Sunil designed IIR filters that have the same frequency response as the FIR filters but with shorter delays.&#10;2. The low frequency band filter has a delay of 64 milliseconds, and the high frequency band filter has a delay of approximately 11 milliseconds.&#10;3. This is compared to the original IIR filters, which have a delay of 200 milliseconds in both low and high frequency bands. However, these delays for the new filters are not yet tested." />
    <node id="Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So .&#10;Speaker: PhD B&#10;Content: Alright .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Um , Yeah . Well , there is w one , um , remark about these filters , that they don't have a linear phase . So ,&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: Well , I don't know , perhaps it {disfmarker} perhaps it doesn't hurt because the phase is almost linear but . Um , and so , yeah , for the delay I gave you here , it 's {disfmarker} it 's , uh , computed on the five hertz modulation frequency , which is the {disfmarker} mmm , well , the most important for speech so . Uh , this is the first thing .&#10;Speaker: Professor C&#10;Content: So that would be , uh , a reduction of a hundred and thirty - six milliseconds ,&#10;Speaker: PhD D&#10;Content: The low f f&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker:" />
    <node id=" , all pole filter that you get out of that should be good . It 's just the , um , excitation signal {comment} that is going to be distorted by the reverberation and so you can try and reconstruct a better excitation signal and , um , feed that through the i um , all pole filter and get enhanced speech with reverberation reduced .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: There 's also this , uh , um , uh , echo cancellation stuff that we 've sort of been chasing , so , uh we have , uh {disfmarker} and when we 're saying these digits now we do have a close microphone signal and then there 's the distant microphone signal . And you could as a kind of baseline say , &quot; OK , given that we have both of these , uh , we should be able to do , uh , a cancellation . &quot; So that , uh , um , we {disfmarker} we , uh , essentially identify the system in between {disfmarker} the linear time invariant system between the microphones and {disfmarker} and {disfmarker} and" />
    <node id="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis." />
    <node id="Content: the one o&#10;Speaker: Professor C&#10;Content: So that 's all it is . It 's a sig it 's a sigmoid ,&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: uh , with weighted sum at the input ,&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: which you train by gradient {pause} descent .&#10;Speaker: Grad E&#10;Content: Right . Yeah , so he uses , um , an EM algorithm to {disfmarker} to um train up these um parameters for the logistic regression .&#10;Speaker: Professor C&#10;Content: Well , actually , yeah ,&#10;Speaker: Grad E&#10;Content: The {disfmarker}&#10;Speaker: Professor C&#10;Content: so I was using EM to get the targets . So {disfmarker} so you have this {disfmarker} this {disfmarker} this AND gate {disfmarker} what we were calling an AND gate , but it 's a product {disfmarker} product rule thing at the output . And then he uses , uh , i u and" />
    <node id="1. The tricky part about calculating signal-to-noise (SNR) ratio, as discussed by the speakers, is that sometimes the signal and noise can overlap in the histogram, making it difficult to distinguish between the two. To address this issue, they suggest an alternative method for determining means only in the EM algorithm, which appears to be more reliable for estimating SNR.&#10;2. The alternative method for determining means only in the EM algorithm involves using an iterative approach similar to the EM algorithm (referred to as an &quot;EM-like thing&quot;). This method focuses on estimating the means of the signal and noise distributions without concerning itself with the variances. Once the mean values are determined, they can be used as the signal-to-noise ratio in that specific band.&#10;3. The speakers mention that this alternative method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. This is because the EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary." />
    <node id=" , but , you know , it 's {disfmarker} that {disfmarker} that {disfmarker} that 's tricky to do . It has mistakes . Uh , and if you 've got enough time , uh , this other method appears to be somewhat more reliable . Uh , a variant on that for just determining signal - to - noise ratio is to just , uh {disfmarker} you can do a w a uh {disfmarker} an iterative thing , EM - like thing , to determine means only . I guess it is EM still , but just {disfmarker} just determine the means only . Don't worry about the variances .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And then you just use those mean values as being the {disfmarker} the , uh uh signal - to - noise ratio in that band .&#10;Speaker: PhD A&#10;Content: But what is the {disfmarker} it seems like this kind of thing could add to the latency . I mean , depending on where the window was that you used to calculate {pause} the signal - to - noise" />
    <node id=" spectrum a running estimate ? Or {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Well , that 's {disfmarker} I mean , that 's what differs from different {disfmarker} different tasks and different s uh , spectral subtraction methods .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: Professor C&#10;Content: I mean , if {disfmarker} if you have , uh , fair assurance that , uh , the noise is {disfmarker} is quite stationary , then the smartest thing to do is use as much data as possible to estimate the noise , get a much better estimate , and subtract it off .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: But if it 's varying at all , which is gonna be the case for almost any real situation , you have to do it on - line , uh , with some forgetting factor or something .&#10;Speaker: PhD A&#10;Content: So do you {disfmark" />
    <node id="1. Stationarity of the noise: If the noise is fairly stationary, then it is better to use as much data as possible to get a more accurate estimate of the noise and subtract it off. However, if the noise is non-stationary (which is the case for most real situations), then an online estimation with some forgetting factor needs to be used.&#10;2. Availability of good noise estimation: A good estimation of the noise is necessary for spectral subtraction methods to work effectively. However, getting a good estimate of the noise can be challenging, especially if the noise is non-stationary.&#10;3. Type of noise: The decision between using a running estimate or a stationary noise estimate may also depend on the type of noise present in the signal. For example, sinusoidally modulated amplitude modulated noise may pose a challenge in practice.&#10;4. Use of low-level detectors: The EM algorithm is used to determine the training targets for the low-level detectors used in the analysis, which suggests that these detectors play a role in the decision between using a running estimate or a stationary noise estimate.&#10;5. Signal level importance: The choice between using a running estimate or a stationary noise estimate may also depend on the relative importance of the signal level compared to the noise level. When the signal level is more important, a different approach may be necessary." />
    <node id=" for non - stationary noise ,&#10;Speaker: PhD B&#10;Content: Well , if y if you have a good estimation of the noise ,&#10;Speaker: Professor C&#10;Content: you know ?&#10;Speaker: PhD B&#10;Content: yeah , because well it it has to work .&#10;Speaker: Professor C&#10;Content: But it 's hard to {disfmarker}&#10;Speaker: PhD B&#10;Content: i&#10;Speaker: Professor C&#10;Content: but that 's hard to do .&#10;Speaker: PhD B&#10;Content: Yeah , that 's hard to do . Yeah .&#10;Speaker: Professor C&#10;Content: Yeah . So {disfmarker} so I think that {disfmarker} that what {disfmarker} what is {disfmarker} wh what 's more common is that you 're going to be helped with r slowly varying or stationary noise .&#10;Speaker: PhD B&#10;Content: But {disfmarker} Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's what spectral subtraction will help with , practically speaking .&#10;Speaker: PhD B&#10;Content: Mm - hmm . M" />
    <node id=" can always pick something that {disfmarker} that falls between your methods ,&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: uh , uh , but I don't know if , you know , if sinusoidally , uh , modul amplitude modulated noise is {disfmarker} is sort of a big problem in {disfmarker} in in {disfmarker} practice .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: I think that {vocalsound} it 's uh {disfmarker}&#10;Speaker: PhD A&#10;Content: We could probably get a really good estimate of the noise if we just went to the noise files , and built the averages from them .&#10;Speaker: Professor C&#10;Content: Yeah . Well .&#10;Speaker: PhD B&#10;Content: What {disfmarker} What do you mean ?&#10;Speaker: Professor C&#10;Content: Just cheat {disfmarker} You 're saying , cheat .&#10;Speaker: PhD B&#10;Content: But if the {disfmarker} if the noise is stationary perhaps you don't even need" />
    <node id="Based on the transcript, PhD A requires the names, as well as other details, from Professor C in order to create the &quot;key&quot; files. The reason for needing this information is not explicitly stated in the transcript, but it can be inferred that it may be necessary for properly labeling or identifying the filters or data being used.&#10;&#10;PhD A also mentions that they need the forms filled out, even if there are no digits involved. This suggests that the forms may serve a purpose beyond simply collecting numerical data, and may instead be used to gather important information about the design and implementation of the filters.&#10;&#10;Professor C initially expresses some confusion about why the forms are necessary, but PhD A explains that it's important for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used. They also mention that stress (presumably in the context of speech recognition) may not always help distinguish between words, but that getting the sequence right is still important.&#10;&#10;Overall, while the transcript does not provide a lot of detail about why PhD A needs the information they are requesting from Professor C, it is clear that this information is necessary for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used." />
    <node id=" ?&#10;Speaker: Grad E&#10;Content: Oh , no {disfmarker} ?&#10;Speaker: Professor C&#10;Content: Or do {disfmarker} did any {disfmarker} do we need the names for the other stuff ,&#10;Speaker: PhD A&#10;Content: Uh , yeah , I do need your names and {disfmarker} and the time , and all that ,&#10;Speaker: Professor C&#10;Content: or {disfmarker} ? Oh , OK .&#10;Speaker: PhD A&#10;Content: cuz we put that into the &quot; key &quot; files .&#10;Speaker: Professor C&#10;Content: Oh , OK .&#10;Speaker: PhD A&#10;Content: Um . But w&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: That 's why we have the forms , uh , even if there are no digits .&#10;Speaker: Professor C&#10;Content: OK , yeah , I didn't notice this . I 'm sitting here and I was {disfmarker} I was about to read them too . It 's a , uh , blank sheet of paper .&#10;Speaker: PhD A&#10;Content:" />
    <node id=" . Anyway , O K . Uh , I think that 's {disfmarker} that 's all we have this week .&#10;Speaker: Grad E&#10;Content: Oh .&#10;Speaker: Professor C&#10;Content:  And , uh , I think it 's digit time .&#10;Speaker: PhD A&#10;Content: Actually the , um {disfmarker} For some reason the digit forms are blank .&#10;Speaker: Professor C&#10;Content: Yeah ?&#10;Speaker: PhD A&#10;Content: Uh , I think th that may be due to the fact that {comment} Adam ran out of digits , {comment} uh , and didn't have time to regenerate any .&#10;Speaker: Professor C&#10;Content: Oh ! Oh ! I guess it 's {disfmarker} Well there 's no real reason to write our names on here then ,&#10;Speaker: PhD A&#10;Content: Yeah , if you want to put your credit card numbers and , uh {disfmarker}&#10;Speaker: Professor C&#10;Content: is there ?&#10;Speaker: Grad E&#10;Content: Oh , no {disfmarker} ?&#10;Speaker: Professor C&#10;Content: Or do {" />
    <node id=" E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: right ? It 's a {disfmarker} it 's a good thing to mark and will probably help us ultimate with recognition&#10;Speaker: PhD A&#10;Content: Yeah , there 's a few cases where it can like permit {comment} and permit .&#10;Speaker: Professor C&#10;Content: but {disfmarker}&#10;Speaker: PhD A&#10;Content: But {disfmarker} that 's not very common in English . In other languages it 's more uh , important .&#10;Speaker: Professor C&#10;Content: Well , yeah , but i either case you 'd write PERMIT , right ? So you 'd get the word right .&#10;Speaker: PhD A&#10;Content: No , I 'm saying , i i e I thought you were saying that stress doesn't help you distinguish between words .&#10;Speaker: Professor C&#10;Content: Um ,&#10;Speaker: PhD A&#10;Content: Oh , I see what you 're saying . As long as you get {disfmarker} The sequence ,&#10;Speaker: Professor C&#10;Content: We 're g if we '" />
    <node id="Based on the transcript provided, Grad E's plan for their qualifying proposal involves using Voice Activity Detection (VAD) to determine boundaries and have everyone do recognition based on those boundaries. This topic was not discussed in previous meetings because it was proposed during the video conference yesterday morning and had not been communicated to the group beforehand." />
    <node id="Speaker: Professor C&#10;Content: Does France Telecom do this {disfmarker}&#10;Speaker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: Does France Telecom do th do the same thing ? More or less ?&#10;Speaker: PhD B&#10;Content: I d I {disfmarker} Y you know , perhaps ?&#10;Speaker: PhD D&#10;Content: No . I do I have not here the proposal .&#10;Speaker: Professor C&#10;Content: OK . Um , OK , if we 're {disfmarker} we 're done {disfmarker} done with that , uh , let 's see . Uh , maybe we can talk about a couple other things briefly , just , uh , things that {disfmarker} that we 've been chatting about but haven't made it into these meetings yet . So you 're coming up with your quals proposal , and , uh {disfmarker} Wanna just give a two three minute summary of what you 're planning on doing ?&#10;Speaker: Grad E&#10;Content: Oh , um , two , three" />
    <node id="1. Frication: This is a type of consonantal sound produced by forcing air through a narrow channel in the vocal tract, causing turbulence. Examples include the 's' sound in &quot;snake&quot; or the 'f' sound in &quot;fish.&quot;&#10;2. Abrupt closure: This refers to the way some consonants are formed with a rapid closing and then releasing of the articulators (tongue, lips, etc.), creating a sharp attack and release, such as the 'p' sound in &quot;spot&quot; or the 't' sound in &quot;tot.&quot;&#10;3. R-coloring: This is a phonetic process where a vowel near an /r/ sound takes on some of the characteristics of the /r/ sound, often resulting in a more rounded or backed quality. For instance, in many American English accents, the word &quot;bird&quot; might have an 'r-colored' vowel, making it sound like &quot;[bÉd].&quot;&#10;4. Nasality: This occurs when part of the airflow from speech is directed through the nasal cavity instead of only through the oral cavity. This creates distinct resonances in the sound produced, and results in nasal consonants such as 'm' or 'n.' Additionally, vowels can become nasalized if they appear before or after a nasal consonant.&#10;5. Voicing: This refers to the vibration of the vocal cords during speech production, which distinguishes voiced sounds (e.g., /v/ in &quot;vase&quot; or /z/ in &quot;zip&quot;) from their voiceless counterparts (e.g., /f/ in &quot;fife&quot; or /s/ in &quot;zip&quot;)." />
    <node id=" but that 's very similar to sonorance .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: Um ,&#10;Speaker: PhD A&#10;Content: When we {disfmarker} when we talked with John Ohala the other day we made a list of some of the things that w&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: like frication ,&#10;Speaker: Grad E&#10;Content: Oh ! OK .&#10;Speaker: PhD A&#10;Content: abrupt closure ,&#10;Speaker: Grad E&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD A&#10;Content: R - coloring , nasality , voicing {disfmarker} Uh .&#10;Speaker: Professor C&#10;Content: Yeah , so there 's a half dozen like that that are {disfmarker}&#10;Speaker: Grad E&#10;Content: Yeah , nasality .&#10;Speaker: Professor C&#10;Content: Now this was coming at it from a different angle but maybe it 's a good way to start . Uh , these are things which , uh , John felt that a" />
    <node id="aker: PhD B&#10;Content: And the de - reverberation algorithm , do you have {disfmarker} can you give some more details on this or {disfmarker} ? Does it use one microphone ?&#10;Speaker: Grad F&#10;Content: o o&#10;Speaker: PhD B&#10;Content: Several microphones ? Does it {disfmarker} ?&#10;Speaker: Grad F&#10;Content: OK , well , um , there was something that was done by , um , a guy named Carlos , I forget his last name , {comment} who worked with Hynek , who , um ,&#10;Speaker: Professor C&#10;Content: Avendano .&#10;Speaker: Grad F&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Who , um ,&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad F&#10;Content: um , it was like RASTA in the sense that of it was , um , de - convolution by filtering um , except he used a longer time window ,&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad F&#10;" />
    <node id="1. The proposed solution to address the limitations of LDA filters involves adding an additional low-pass filter at 25 Hz. This filter will act as a supplementary measure to the existing LDA filters, which are essentially low-pass but allow high frequency components above 25 Hz to pass through.&#10;2. A ten-millisecond filter will be incorporated into the system in addition to the low-pass filter. However, the specifics of this filter and its purpose have not been explicitly mentioned in the transcript.&#10;3. The solution also aims to tackle the delay caused by online normalization using recursion on the feature stream. It has been noted that this method of normalization introduces a delay of 85 milliseconds, which needs to be accounted for and minimized in order to improve the system's performance.&#10;4. The transcript suggests that these changes are aimed at improving the overall efficiency and accuracy of the speech recognition or processing system by addressing the shortcomings of LDA filters, delay compensation, and online normalization. However, it is important to note that these proposed solutions have not been fully implemented or tested yet, as mentioned in the conversation between PhD B and Professor C." />
    <node id="Human ability to perceive the difference between various short time intervals in conversation is quite limited. As mentioned in the transcript, Dr. A stated that people might not be able to tell the difference between a quarter of a second and 100 milliseconds or even half a second. They further expressed that responses often feel so quick, which indicates that humans can perceive small time differences but may not accurately differentiate them.&#10;&#10;This perception affects human communication in several ways. It impacts turn-taking and the fluidity of conversations. In text-based systems, delays between responses can make interactions feel abrupt or unnatural; however, in human interaction, individuals tend to step on each other's utterances creating a more simultaneous exchange of information. The limitations in perceiving short time intervals contribute to these conversational dynamics and create challenges for designing natural and efficient text-based conversation systems." />
    <node id=" Professor C&#10;Content: but .&#10;Speaker: PhD A&#10;Content: I don't remember the exact numbers but it was something like that .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: I don't think you can really tell . A person {disfmarker} I don't think a person can tell the difference between , uh , you know , a quarter of a second and a hundred milliseconds , and {disfmarker} I 'm not even sure if we can tell the difference between a quarter of a second and half a second .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: I mean it just {disfmarker} it feels so quick .&#10;Speaker: Professor C&#10;Content: Yeah . I mean , basically if you {disfmarker} yeah , if you said , uh , um , &quot; what 's the , uh , uh {disfmarker} what 's the shortest route to the opera ? &quot; and it took half a second to get back to you ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: I mean , {vocals" />
    <node id=" of latency .&#10;Speaker: Professor C&#10;Content: So . No . What {disfmarker} what does {disfmarker} wa was your experience when you were doing this stuff with , uh , the {disfmarker} the {disfmarker} the surgical , uh , uh , microscopes and so forth . Um , how long was it from when somebody , uh , finished an utterance to when , uh , something started happening ?&#10;Speaker: PhD A&#10;Content: Um , we had a silence detector , so we would look for the end of an utterance based on the silence detector .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: And I {disfmarker} I can't remember now off the top of my head how many frames of silence we had to detect before we would declare it to be the end of an utterance .&#10;Speaker: Professor C&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD A&#10;Content: Um , but it was , uh , I would say it was probably around the order of two hundred and fifty milliseconds .&#10;Speaker: Professor" />
    <node id="The transcript does not provide direct information about why &quot;Adam ran out of digits.&quot; However, it is mentioned that the digit forms are blank, which may be a result of running out of digits. There is also no explicit mention of why this makes it unnecessary to write names on the forms.&#10;&#10;Based on the context, it appears that the group is discussing some sort of data collection or input process involving numerical values and forms requiring names. The discussion between PhD A and Professor C suggests that the forms serve a purpose beyond just collecting numerical data, possibly for associating specific details with each set of numbers.&#10;&#10;Since the digit forms are blank due to running out of digits, it could imply that there is a limitation on how many sets of digits can be generated within a certain time frame or under certain conditions (such as Adam's situation). This limitation might have made writing names on the forms unnecessary because there were no digits available for association.&#10;&#10;Nonetheless, since the transcript does not provide direct information about why Adam ran out of digits, any answer to this question can only be speculative and based on inferences drawn from the context." />
    <node id=" normalization . So it 's&#10;Speaker: Professor C&#10;Content: Uh ,&#10;Speaker: PhD B&#10;Content: plus {disfmarker} plus eighty for the neural net and PCA .&#10;Speaker: Professor C&#10;Content: yeah , but then there 's {disfmarker} Oh .&#10;Speaker: PhD B&#10;Content: So it would be around two hundred and forty {disfmarker} so , well ,&#10;Speaker: Professor C&#10;Content: Just {disfmarker} just barely in there .&#10;Speaker: PhD B&#10;Content: plus {disfmarker} plus the frames , but it 's OK .&#10;Speaker: PhD A&#10;Content: What 's the allowable ?&#10;Speaker: Professor C&#10;Content: Two - fifty , unless they changed the rules .&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: Which there is {disfmarker} there 's some discussion of .&#10;Speaker: PhD A&#10;Content: What were they thinking of changing it to ?&#10;Speaker: Professor C&#10;Content: But {disfmarker}&#10;Speaker: PhD B&#10;Content:" />
    <node id="1. Some people who implement spectral subtraction theory may prefer doing it in the magnitude domain because, while the theory suggests that it should be done in the power domain, practical implementations in the magnitude domain can still work effectively for some use cases. This is mentioned by PhD B and Professor C when they discuss different implementations.&#10;2. The &quot;trick&quot; used to subtract the estimate of the noise spectrum in the magnitude domain (instead of the power domain) is not explicitly stated in the transcript. However, it can be inferred that these practitioners might be using some sort of running estimate or average of the noise spectrum and applying a scaling factor based on the signal-to-noise ratio (SNR). This is suggested when PhD B mentions &quot;you don't just subtract the estimate of the noise spectrum. You subtract that times a factor, which is dependent on the SNR.&quot; Additionally, Professor C mentions that the estimation of the power spectra of the noise should be multiplied by a factor depending on the SNR." />
    <node id="1. Smooth spectral envelopes may not be the best option for determining sonorant or non-sonorant sounds in speech recognition, as they focus more on the overall envelope rather than the periodicity of the signal. An alternative approach could involve building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information.&#10;2. To improve speech quality and reduce reverberation in a recording, an all-pole filter can be used in conjunction with a reconstructed excitation signal that has been processed to minimize distortion caused by reverberation. This technique involves identifying the linear time-invariant system between two microphones and using echo cancellation techniques to correct for it.&#10;3. Incorporating delays between responses in text-based conversation systems can help mimic human conversational dynamics, reducing abruptness and creating a more natural feel. Additionally, implementing techniques such as Voice Activity Detection (VAD) can establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness.&#10;4. Other possible approaches to improving speech quality and reducing reverberation include adding a low-pass filter at 25 Hz to supplement existing LDA filters, incorporating a ten-millisecond filter into the system, and minimizing delay caused by online normalization using recursion on the feature stream. These techniques aim to address shortcomings in LDA filters, delay compensation, and online normalization." />
    <node id=" a c two or three minutes about what we 've been talking about today and other days ?&#10;Speaker: Grad F&#10;Content: Ri Yeah , OK , so , um , we 're interested in , um , methods for far mike speech recognition , um , {pause} mainly , uh , methods that deal with the reverberation {pause} in the far mike signal . So , um , one approach would be , um , say MSG and PLP , like was used in Aurora one and , um , there are other approaches which actually attempt to {pause} remove the reverberation , instead of being robust to it like MSG . And so we 're interested in , um , comparing the performance of {pause} um , a robust approach like MSG with these , um , speech enhancement or de - reverber de - reverberation approaches .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad F&#10;Content: And , um , {vocalsound} it looks like we 're gonna use the Meeting Recorder digits data for that .&#10;Speaker: PhD B&#10;Content: And the de - reverberation algorithm , do you have {disfmarker} can you give some more details on this" />
    <node id=" someone working on this on i in Mons&#10;Speaker: Professor C&#10;Content: Yeah , OK .&#10;Speaker: PhD B&#10;Content: So perhaps , yeah , we should try t to {disfmarker} He 's working on this , on trying to {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: on re reverberation , um {disfmarker}&#10;Speaker: Professor C&#10;Content: The first paper on this is gonna have great references , I can tell already .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: It 's always good to have references , especially when reviewers read it or {disfmarker} or one of the authors and , {vocalsound} feel they 'll &quot; You 're OK , you 've r You cited me . &quot;&#10;Speaker: PhD B&#10;Content: So , yeah . Well , he did echo cancellation and he did some fancier things like , uh , {vocalsound} {vocalsound} uh , training different network on different reverberation conditions and then trying to find the best one , but ." />
    <node id="1. The primary goal of the conversation appears to be a comparison between different data transformation techniques, specifically focusing on the use of a neural net transformation, Linear Discriminant Analysis (LDA), and Karhunen-LoÃ¨ve Transform (KLT). The speakers aim to understand if the additional complexity of a neural net offers any significant benefits in terms of accuracy or performance compared to these other methods.&#10;2. This comparison is essential for evaluating the relative merits of each technique, especially considering that LDA provides better discrimination between classes than KLT and has a linear but discriminant representation of the data. By examining how well the neural net performs in this context, researchers can determine if there is a meaningful improvement over more traditional methods like KLT or LDA.&#10;3. In addition to comparing techniques, the speakers also discuss implementing spectral subtraction theory in different domains (magnitude versus power) and using various methods for determining signal-to-noise ratios (SNR). These discussions demonstrate the importance of understanding the nuances between different implementation strategies and their effects on performance.&#10;&#10;In summary, the purpose of comparing a neural net transformation to KLT and LDA is to determine if there are any significant benefits in terms of accuracy or performance from using the more complex neural net method over these other techniques. This comparison helps researchers understand the best methods for data transformation based on their specific application needs." />
    <node id="1. The speakers discuss a method of determining the signal-to-noise (SNR) ratio based on a histogram with two Gaussian approximations for signal and noise. When the distributions overlap, they suggest using the Expectation Maximization (EM) algorithm to separate them. This iterative method is more reliable for estimating SNR when the signal and noise can overlap in the histogram. The EM algorithm focuses on estimating the means of the signal and noise distributions without considering the variances. Once the mean values are determined, they can be used as the SNR in that specific band.&#10;&#10;This method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. The EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary.&#10;&#10;The use of this method may depend on factors such as:&#10;- Stationarity of the noise&#10;- Availability of good noise estimation&#10;- Type of noise present in the signal&#10;- Use of low-level detectors&#10;- Signal level importance&#10;&#10;Additionally, the speakers use the gradient descent algorithm to train the parameters for the logistic regression and utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm, likely to estimate or determine the training targets for the low-level detectors used in their analysis." />
    <node id="In statistics, a stationary process is one where the joint probability distribution of the random variables remains unchanged over time. In other words, the statistical properties of the process such as mean, variance, and autocorrelation, do not change over time.&#10;&#10;In signal processing, the term &quot;stationary&quot; is often used to refer to signals or processes that change slowly compared to the processing techniques being used. This means that the statistical properties of the signal remain approximately constant over a short period of time, so that techniques such as spectral analysis can be applied effectively. For example, when analyzing a noise signal in a car, the nature of the noise is expected to change relatively slowly over time, compared to the processing techniques being used.&#10;&#10;However, it's important to note that many real-world signals are non-stationary and their statistical properties change over time. In such cases, more sophisticated signal processing techniques may be needed to accurately analyze and interpret the signals. For example, using a running estimate or an online estimation with some forgetting factor may be necessary to estimate the noise in non-stationary signals." />
    <node id=": Professor C&#10;Content: you know , stationary {disfmarker}&#10;Speaker: PhD A&#10;Content: Very slow adaptation .&#10;Speaker: PhD B&#10;Content: and is the c&#10;Speaker: PhD A&#10;Content: th&#10;Speaker: Professor C&#10;Content: Right , the word &quot; stationary &quot; is {disfmarker} has a very precise statistical meaning . But , you know , in {disfmarker} in signal - processing really what we 're talking about I think is things that change slowly , uh , compared with our {disfmarker} our processing techniques .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So if you 're driving along in a car I {disfmarker} I would think that most of the time the nature of the noise is going to change relatively slowly . It 's not gonna stay absolute the same . If you {disfmarker} if you check it out , uh , five minutes later you may be in a different part of the road&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: or whatever . But" />
    <node id="The issue with obtaining a good estimate of the speech modulation spectrum in this scenario is that if the speech varies significantly in just a few seconds, even with the use of spectral subtraction, getting a decent estimate requires a few seconds of speech. However, if the speech has changed a lot within those few seconds, then using that estimate to subtract the noise could be problematic and may not accurately represent the actual noise present in the signal.&#10;&#10;This is because spectral subtraction relies on having an accurate estimate of the noise spectrum, which can be challenging to obtain if the noise is non-stationary or varying rapidly over time. In such cases, using a running estimate or average of the noise spectrum with a scaling factor based on the signal-to-noise ratio (SNR) may help improve the accuracy of the estimation. However, as mentioned by Professor C in the transcript, this approach may not always work well if there are non-stationary components present in the noise.&#10;&#10;Overall, the challenge lies in developing spectral subtraction methods that can effectively handle non-stationary noise and accurately estimate the speech modulation spectrum in real-world situations where the noise characteristics may vary significantly over time." />
    <node id="1. The focus of the individual's work is to improve far-microphone speech recognition, specifically addressing reverberation in the signal. They have experimented with methods such as MSG and PLP, which are robust against reverberation, but they are also interested in comparing these approaches with speech enhancement or de-reverberation techniques that attempt to remove reverberation from the signal directly.&#10;2. Echo cancellation is a technique being explored by the individual. By identifying the linear time-invariant system between two microphones and using echo cancellation techniques, it is possible to correct for the distortion caused by reverberation. The individual aims to use this method in conjunction with an all-pole filter and a processed excitation signal to reduce reverberation.&#10;3. The individual has also been working on training neural networks under different reverberation conditions, trying to find the best network configuration for each scenario. They have experimented with training a network using various reverberation settings and then selecting the most effective configuration based on performance.&#10;4. Lastly, there is discussion of exploring &quot;multiple mike things&quot; where all microphones are at a distance from the speaker, particularly in conditions where there may be an obstruction between them, creating a helpful shadow effect." />
    <node id=" , {vocalsound} {vocalsound} uh , training different network on different reverberation conditions and then trying to find the best one , but . Well .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: The oth the other thing , uh , that Dave was talking about earlier was , uh , uh , multiple mike things , uh , where they 're all distant . So , um , I mean , there 's {disfmarker} there 's all this work on arrays , but the other thing is , uh , {pause} what can we do that 's cleverer that can take some advantage of only two mikes , uh , particularly if there 's an obstruction between them , as we {disfmarker} as we have over there .&#10;Speaker: PhD B&#10;Content: If there is {disfmarker} ?&#10;Speaker: Professor C&#10;Content: An obstruction between them .&#10;Speaker: PhD B&#10;Content: Ah , yeah .&#10;Speaker: Professor C&#10;Content: It creates a shadow which is {disfmarker} is helpful . It 's" />
    <node id="1. Based on the transcript, it is indicated that both Ericsson and France Telecom have implemented spectral subtraction theory in their work. However, the specific details of their implementations are not provided in the given transcript. Therefore, it cannot be definitively stated whether they used a threshold on previous noise estimates to determine what updates the estimate, with only signals below the threshold being used for updating.&#10;2. PhD B mentions a method where an estimate of the noise level is obtained and a threshold is set above it, such as six or ten DB. Any signal under this threshold is then used to update the estimate. PhD D confirms this method, but does not specify whether it was used by Ericsson or France Telecom.&#10;3. Therefore, while it is possible that both Ericsson and France Telecom have used this method, there is no explicit confirmation in the given transcript." />
    <node id=" , uh {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah , they have some kind of threshold on {disfmarker} on the previous estimate , and {disfmarker} So . Yeah . I think . Yeah , I think Ericsson used this kind of threshold . Yeah , so , they h they have an estimate of the noise level and they put a threshold like six or ten DB above , and what 's under this threshold is used to update the estimate . Is {disfmarker} is that right&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I think so .&#10;Speaker: PhD B&#10;Content: So it 's {disfmarker} it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: I have not here the proposal .&#10;Speaker: PhD B&#10;Content: Yeah . It 's like saying what 's under the threshold is silence ,&#10;Speaker: Professor C&#10;Content: Does France Telecom do this {disfmarker}&#10;Speaker: PhD B&#10;Content: and {disf" />
    <node id="Based on the transcript provided, the speakers conducted separate Viterbi analyses on different sub-band labels. However, they did not explicitly discuss the outcome or impact of this action on the overall training process. Therefore, it is not possible to accurately answer this question without additional information. The transcript only indicates that the separate Viterbi analysis was done but does not provide any details about the results or effects of this process." />
    <node id="disfmarker} a person to make that determination . So , um , um , we don't really know how those should be labeled . It could sh be that you should , um , not be paying that much attention to , uh , certain bands for certain sounds , uh , in order to get the best result .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So , um , what we have been doing there , just sort of mixing it all together , is certainly much {disfmarker} much cruder than that . We trained these things up on the {disfmarker} on the , uh the final label . Now we have I guess done experiments {disfmarker} you 've probably done stuff where you have , um , done separate , uh , Viterbis on the different {disfmarker}&#10;Speaker: Grad E&#10;Content: Yeah . Forced alignment on the sub - band labels ?&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: You 've done that . Did {disfmarker} did that help at all ?" />
    <node id=" issue of where the targets come from . So in our case when we 've been training it multi - band things , the way we get the targets for the individual bands is , uh , that we get the phonetic label {disfmarker} for the sound there&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: and we say , &quot; OK , we train every {disfmarker} &quot; What this is saying is , OK , that 's maybe what our ultimate goal is {disfmarker} or not ultimate but penultimate {vocalsound} goal is getting these {disfmarker} these small sound units . But {disfmarker} but , um , along the way how much should we , uh {disfmarker} uh , what should we be training these intermediate things for ? I mean , because , uh , we don't know uh , that this is a particularly good feature . I mean , there 's no way , uh {disfmarker} someone in the audience yesterday was asking , &quot; well couldn't you have people go through and mark the individual bands and say where the {disfmarker} where it was son" />
    <edge source="PHP B mentioned that Hirsch had experimented with analysis windows ranging from 500 milliseconds to 1 second in length when working with non-stationary noises, such as those modulated with amplitude. He also noted that Hirsch demonstrated a delay in the estimation of noise in his experiments, and that it is important to center the analysis window for accurate results. In the context of a five hertz jack hammer noise, this would suggest that using an analysis window between 500 milliseconds and 1 second, and centering it on the noise, could provide useful information about the noise's characteristics, even if it is non-stationary." target=" jack hammer going at five hertz .&#10;Speaker: PhD B&#10;Content: Yeah , that 's {disfmarker}&#10;Speaker: Professor C&#10;Content: I mean , good {disfmarker} good luck . So ,&#10;Speaker: PhD B&#10;Content: So in this case , yeah , sure , you cannot {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: But I think y um , Hirsch does experiment with windows of like between five hundred milliseconds and one second . And well , five hundred wa was not so bad . I mean and he worked on non - stationary noises , like noise modulated with well , wi with amplitude modulations and things like that ,&#10;Speaker: PhD A&#10;Content: Were his , uh , windows centered around the {disfmarker}&#10;Speaker: PhD B&#10;Content: and {disfmarker} But {disfmarker} Um , yeah . Well , I think {disfmarker} Yeah . Well , in {disfmarker} in the paper he showed that actually the estimation of the noise is {disfmarker} is delayed . Well , it">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP B mentioned that Hirsch had experimented with analysis windows ranging from 500 milliseconds to 1 second in length when working with non-stationary noises, such as those modulated with amplitude. He also noted that Hirsch demonstrated a delay in the estimation of noise in his experiments, and that it is important to center the analysis window for accurate results. In the context of a five hertz jack hammer noise, this would suggest that using an analysis window between 500 milliseconds and 1 second, and centering it on the noise, could provide useful information about the noise's characteristics, even if it is non-stationary." target=" in {disfmarker} in the paper he showed that actually the estimation of the noise is {disfmarker} is delayed . Well , it 's {disfmarker} there is {disfmarker} you {disfmarker} you have to center the window , yeah .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: No , I understand it 's better to do but I just think that {disfmarker} that , uh , for real noises wh what {disfmarker} what 's most likely to happen is that there 'll be some things that are relatively stationary&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: where you can use one or another spectral subtraction thing&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and other things where it 's not so stationary and {disfmarker} I mean , you can always pick something that {disfmarker} that falls between your methods ,&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP B mentioned that Hirsch had experimented with analysis windows ranging from 500 milliseconds to 1 second in length when working with non-stationary noises, such as those modulated with amplitude. He also noted that Hirsch demonstrated a delay in the estimation of noise in his experiments, and that it is important to center the analysis window for accurate results. In the context of a five hertz jack hammer noise, this would suggest that using an analysis window between 500 milliseconds and 1 second, and centering it on the noise, could provide useful information about the noise's characteristics, even if it is non-stationary." target=" to do it on - line , uh , with some forgetting factor or something .&#10;Speaker: PhD A&#10;Content: So do you {disfmarker} is there some long window that extends into the past over which you calculate the average ?&#10;Speaker: Professor C&#10;Content: Well , there 's a lot of different ways of computing the noise spectrum . So one of the things that , uh , Hans - Guenter Hirsch did , uh {disfmarker} and pas and other people {disfmarker} actually , he 's {disfmarker} he wasn't the only one I guess , was to , uh , take some period of {disfmarker} of {disfmarker} of speech and in each band , uh , develop a histogram . So , to get a decent histogram of these energies takes at least a few seconds really . But , uh {disfmarker} I mean you can do it with a smaller amount but it 's pretty rough . And , um , in fact I think the NIST standard method of determining signal - to - noise ratio is based on this .&#10;Speaker: PhD A&#10;Content: A couple seconds ?&#10;Speaker: Professor">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP B mentioned that Hirsch had experimented with analysis windows ranging from 500 milliseconds to 1 second in length when working with non-stationary noises, such as those modulated with amplitude. He also noted that Hirsch demonstrated a delay in the estimation of noise in his experiments, and that it is important to center the analysis window for accurate results. In the context of a five hertz jack hammer noise, this would suggest that using an analysis window between 500 milliseconds and 1 second, and centering it on the noise, could provide useful information about the noise's characteristics, even if it is non-stationary." target=": Didn't they {disfmarker}&#10;Speaker: Professor C&#10;Content: Which means you 're gonna make errors between similar sounds that are son sonorant or obstruent .&#10;Speaker: PhD A&#10;Content: Didn't they also do some kind of an oracle experiment where they said &quot; if we {pause} could detect the sonorants perfectly {pause} and then show how it would improve speech recognition ? I thought I remember hearing about an experiment like that .&#10;Speaker: Professor C&#10;Content: The - these same people ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: I don't remember that .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: That would {disfmarker} that 's {disfmarker} you 're right , that 's exactly the question to follow up this discussion , is suppose you did that , uh , got that right . Um , Yeah .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: What could be the other low level detectors , I mean , for {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP B mentioned that Hirsch had experimented with analysis windows ranging from 500 milliseconds to 1 second in length when working with non-stationary noises, such as those modulated with amplitude. He also noted that Hirsch demonstrated a delay in the estimation of noise in his experiments, and that it is important to center the analysis window for accurate results. In the context of a five hertz jack hammer noise, this would suggest that using an analysis window between 500 milliseconds and 1 second, and centering it on the noise, could provide useful information about the noise's characteristics, even if it is non-stationary." target=" A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So {disfmarker} so basically now you have this mixture of two Gaussians , you {disfmarker} you n know what they are , and , uh {disfmarker} I mean , sorry , you estimate what they are , and , uh , so this gives you what the signal is and what the noise e energy is in that band in the spectrum . And then you look over the whole thing and now you have a noise spectrum . So , uh , Hans - Guenter Hirsch and others have used that kind of method . And the other thing to do is {disfmarker} which is sort of more trivial and obvious {comment} {disfmarker} is to , uh , uh , determine through magical means that {disfmarker} that , uh , there 's no speech in some period , and then see what the spectrum is .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Uh , but , you know , it 's {disfmarker} that {disfmarker} that {disfmarker} that 's">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP B mentioned that Hirsch had experimented with analysis windows ranging from 500 milliseconds to 1 second in length when working with non-stationary noises, such as those modulated with amplitude. He also noted that Hirsch demonstrated a delay in the estimation of noise in his experiments, and that it is important to center the analysis window for accurate results. In the context of a five hertz jack hammer noise, this would suggest that using an analysis window between 500 milliseconds and 1 second, and centering it on the noise, could provide useful information about the noise's characteristics, even if it is non-stationary." target=" method of determining signal - to - noise ratio is based on this .&#10;Speaker: PhD A&#10;Content: A couple seconds ?&#10;Speaker: Professor C&#10;Content: So {disfmarker} No , no , it 's based on this kind of method ,&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: this histogram method . So you have a histogram . Now , if you have signal and you have noise , you basically have these two bumps in the histogram , which you could approximate as two Gaussians .&#10;Speaker: PhD A&#10;Content: But wh don't they overlap sometimes ?&#10;Speaker: Professor C&#10;Content: Oh , yeah .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: So you have a mixture of two Gaussians .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Right ? And you can use EM to figure out what it is . You know .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So {disfmarker} so basically now you have this mixture of two Ga">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP B mentioned that Hirsch had experimented with analysis windows ranging from 500 milliseconds to 1 second in length when working with non-stationary noises, such as those modulated with amplitude. He also noted that Hirsch demonstrated a delay in the estimation of noise in his experiments, and that it is important to center the analysis window for accurate results. In the context of a five hertz jack hammer noise, this would suggest that using an analysis window between 500 milliseconds and 1 second, and centering it on the noise, could provide useful information about the noise's characteristics, even if it is non-stationary." target="Human ability to perceive the difference between various short time intervals in conversation is quite limited. As mentioned in the transcript, Dr. A stated that people might not be able to tell the difference between a quarter of a second and 100 milliseconds or even half a second. They further expressed that responses often feel so quick, which indicates that humans can perceive small time differences but may not accurately differentiate them.&#10;&#10;This perception affects human communication in several ways. It impacts turn-taking and the fluidity of conversations. In text-based systems, delays between responses can make interactions feel abrupt or unnatural; however, in human interaction, individuals tend to step on each other's utterances creating a more simultaneous exchange of information. The limitations in perceiving short time intervals contribute to these conversational dynamics and create challenges for designing natural and efficient text-based conversation systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP B mentioned that Hirsch had experimented with analysis windows ranging from 500 milliseconds to 1 second in length when working with non-stationary noises, such as those modulated with amplitude. He also noted that Hirsch demonstrated a delay in the estimation of noise in his experiments, and that it is important to center the analysis window for accurate results. In the context of a five hertz jack hammer noise, this would suggest that using an analysis window between 500 milliseconds and 1 second, and centering it on the noise, could provide useful information about the noise's characteristics, even if it is non-stationary." target="In statistics, a stationary process is one where the joint probability distribution of the random variables remains unchanged over time. In other words, the statistical properties of the process such as mean, variance, and autocorrelation, do not change over time.&#10;&#10;In signal processing, the term &quot;stationary&quot; is often used to refer to signals or processes that change slowly compared to the processing techniques being used. This means that the statistical properties of the signal remain approximately constant over a short period of time, so that techniques such as spectral analysis can be applied effectively. For example, when analyzing a noise signal in a car, the nature of the noise is expected to change relatively slowly over time, compared to the processing techniques being used.&#10;&#10;However, it's important to note that many real-world signals are non-stationary and their statistical properties change over time. In such cases, more sophisticated signal processing techniques may be needed to accurately analyze and interpret the signals. For example, using a running estimate or an online estimation with some forgetting factor may be necessary to estimate the noise in non-stationary signals.">
      <data key="d0">1</data>
    </edge>
    <edge source="PHP B mentioned that Hirsch had experimented with analysis windows ranging from 500 milliseconds to 1 second in length when working with non-stationary noises, such as those modulated with amplitude. He also noted that Hirsch demonstrated a delay in the estimation of noise in his experiments, and that it is important to center the analysis window for accurate results. In the context of a five hertz jack hammer noise, this would suggest that using an analysis window between 500 milliseconds and 1 second, and centering it on the noise, could provide useful information about the noise's characteristics, even if it is non-stationary." target="1. Based on the transcript, it is indicated that both Ericsson and France Telecom have implemented spectral subtraction theory in their work. However, the specific details of their implementations are not provided in the given transcript. Therefore, it cannot be definitively stated whether they used a threshold on previous noise estimates to determine what updates the estimate, with only signals below the threshold being used for updating.&#10;2. PhD B mentions a method where an estimate of the noise level is obtained and a threshold is set above it, such as six or ten DB. Any signal under this threshold is then used to update the estimate. PhD D confirms this method, but does not specify whether it was used by Ericsson or France Telecom.&#10;3. Therefore, while it is possible that both Ericsson and France Telecom have used this method, there is no explicit confirmation in the given transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source=" in {disfmarker} in the paper he showed that actually the estimation of the noise is {disfmarker} is delayed . Well , it 's {disfmarker} there is {disfmarker} you {disfmarker} you have to center the window , yeah .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: No , I understand it 's better to do but I just think that {disfmarker} that , uh , for real noises wh what {disfmarker} what 's most likely to happen is that there 'll be some things that are relatively stationary&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: where you can use one or another spectral subtraction thing&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and other things where it 's not so stationary and {disfmarker} I mean , you can always pick something that {disfmarker} that falls between your methods ,&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker:" target="1. Stationarity of the noise: If the noise is fairly stationary, then it is better to use as much data as possible to get a more accurate estimate of the noise and subtract it off. However, if the noise is non-stationary (which is the case for most real situations), then an online estimation with some forgetting factor needs to be used.&#10;2. Availability of good noise estimation: A good estimation of the noise is necessary for spectral subtraction methods to work effectively. However, getting a good estimate of the noise can be challenging, especially if the noise is non-stationary.&#10;3. Type of noise: The decision between using a running estimate or a stationary noise estimate may also depend on the type of noise present in the signal. For example, sinusoidally modulated amplitude modulated noise may pose a challenge in practice.&#10;4. Use of low-level detectors: The EM algorithm is used to determine the training targets for the low-level detectors used in the analysis, which suggests that these detectors play a role in the decision between using a running estimate or a stationary noise estimate.&#10;5. Signal level importance: The choice between using a running estimate or a stationary noise estimate may also depend on the relative importance of the signal level compared to the noise level. When the signal level is more important, a different approach may be necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source=" in {disfmarker} in the paper he showed that actually the estimation of the noise is {disfmarker} is delayed . Well , it 's {disfmarker} there is {disfmarker} you {disfmarker} you have to center the window , yeah .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: No , I understand it 's better to do but I just think that {disfmarker} that , uh , for real noises wh what {disfmarker} what 's most likely to happen is that there 'll be some things that are relatively stationary&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: where you can use one or another spectral subtraction thing&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and other things where it 's not so stationary and {disfmarker} I mean , you can always pick something that {disfmarker} that falls between your methods ,&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker:" target="1. Some people who implement spectral subtraction theory may prefer doing it in the magnitude domain because, while the theory suggests that it should be done in the power domain, practical implementations in the magnitude domain can still work effectively for some use cases. This is mentioned by PhD B and Professor C when they discuss different implementations.&#10;2. The &quot;trick&quot; used to subtract the estimate of the noise spectrum in the magnitude domain (instead of the power domain) is not explicitly stated in the transcript. However, it can be inferred that these practitioners might be using some sort of running estimate or average of the noise spectrum and applying a scaling factor based on the signal-to-noise ratio (SNR). This is suggested when PhD B mentions &quot;you don't just subtract the estimate of the noise spectrum. You subtract that times a factor, which is dependent on the SNR.&quot; Additionally, Professor C mentions that the estimation of the power spectra of the noise should be multiplied by a factor depending on the SNR.">
      <data key="d0">1</data>
    </edge>
    <edge source=" in {disfmarker} in the paper he showed that actually the estimation of the noise is {disfmarker} is delayed . Well , it 's {disfmarker} there is {disfmarker} you {disfmarker} you have to center the window , yeah .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: No , I understand it 's better to do but I just think that {disfmarker} that , uh , for real noises wh what {disfmarker} what 's most likely to happen is that there 'll be some things that are relatively stationary&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: where you can use one or another spectral subtraction thing&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and other things where it 's not so stationary and {disfmarker} I mean , you can always pick something that {disfmarker} that falls between your methods ,&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker:" target="In statistics, a stationary process is one where the joint probability distribution of the random variables remains unchanged over time. In other words, the statistical properties of the process such as mean, variance, and autocorrelation, do not change over time.&#10;&#10;In signal processing, the term &quot;stationary&quot; is often used to refer to signals or processes that change slowly compared to the processing techniques being used. This means that the statistical properties of the signal remain approximately constant over a short period of time, so that techniques such as spectral analysis can be applied effectively. For example, when analyzing a noise signal in a car, the nature of the noise is expected to change relatively slowly over time, compared to the processing techniques being used.&#10;&#10;However, it's important to note that many real-world signals are non-stationary and their statistical properties change over time. In such cases, more sophisticated signal processing techniques may be needed to accurately analyze and interpret the signals. For example, using a running estimate or an online estimation with some forgetting factor may be necessary to estimate the noise in non-stationary signals.">
      <data key="d0">1</data>
    </edge>
    <edge source=" in {disfmarker} in the paper he showed that actually the estimation of the noise is {disfmarker} is delayed . Well , it 's {disfmarker} there is {disfmarker} you {disfmarker} you have to center the window , yeah .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: No , I understand it 's better to do but I just think that {disfmarker} that , uh , for real noises wh what {disfmarker} what 's most likely to happen is that there 'll be some things that are relatively stationary&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: where you can use one or another spectral subtraction thing&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and other things where it 's not so stationary and {disfmarker} I mean , you can always pick something that {disfmarker} that falls between your methods ,&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker:" target="The issue with obtaining a good estimate of the speech modulation spectrum in this scenario is that if the speech varies significantly in just a few seconds, even with the use of spectral subtraction, getting a decent estimate requires a few seconds of speech. However, if the speech has changed a lot within those few seconds, then using that estimate to subtract the noise could be problematic and may not accurately represent the actual noise present in the signal.&#10;&#10;This is because spectral subtraction relies on having an accurate estimate of the noise spectrum, which can be challenging to obtain if the noise is non-stationary or varying rapidly over time. In such cases, using a running estimate or average of the noise spectrum with a scaling factor based on the signal-to-noise ratio (SNR) may help improve the accuracy of the estimation. However, as mentioned by Professor C in the transcript, this approach may not always work well if there are non-stationary components present in the noise.&#10;&#10;Overall, the challenge lies in developing spectral subtraction methods that can effectively handle non-stationary noise and accurately estimate the speech modulation spectrum in real-world situations where the noise characteristics may vary significantly over time.">
      <data key="d0">1</data>
    </edge>
    <edge source=" to do it on - line , uh , with some forgetting factor or something .&#10;Speaker: PhD A&#10;Content: So do you {disfmarker} is there some long window that extends into the past over which you calculate the average ?&#10;Speaker: Professor C&#10;Content: Well , there 's a lot of different ways of computing the noise spectrum . So one of the things that , uh , Hans - Guenter Hirsch did , uh {disfmarker} and pas and other people {disfmarker} actually , he 's {disfmarker} he wasn't the only one I guess , was to , uh , take some period of {disfmarker} of {disfmarker} of speech and in each band , uh , develop a histogram . So , to get a decent histogram of these energies takes at least a few seconds really . But , uh {disfmarker} I mean you can do it with a smaller amount but it 's pretty rough . And , um , in fact I think the NIST standard method of determining signal - to - noise ratio is based on this .&#10;Speaker: PhD A&#10;Content: A couple seconds ?&#10;Speaker: Professor" target="1. The speakers discuss a method of determining the signal-to-noise (SNR) ratio based on a histogram with two Gaussian approximations for signal and noise. When the distributions overlap, they suggest using the Expectation Maximization (EM) algorithm to separate them. This iterative method is more reliable for estimating SNR when the signal and noise can overlap in the histogram. The EM algorithm focuses on estimating the means of the signal and noise distributions without considering the variances. Once the mean values are determined, they can be used as the SNR in that specific band.&#10;&#10;This method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. The EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary.&#10;&#10;The use of this method may depend on factors such as:&#10;- Stationarity of the noise&#10;- Availability of good noise estimation&#10;- Type of noise present in the signal&#10;- Use of low-level detectors&#10;- Signal level importance&#10;&#10;Additionally, the speakers use the gradient descent algorithm to train the parameters for the logistic regression and utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm, likely to estimate or determine the training targets for the low-level detectors used in their analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Didn't they {disfmarker}&#10;Speaker: Professor C&#10;Content: Which means you 're gonna make errors between similar sounds that are son sonorant or obstruent .&#10;Speaker: PhD A&#10;Content: Didn't they also do some kind of an oracle experiment where they said &quot; if we {pause} could detect the sonorants perfectly {pause} and then show how it would improve speech recognition ? I thought I remember hearing about an experiment like that .&#10;Speaker: Professor C&#10;Content: The - these same people ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: I don't remember that .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: That would {disfmarker} that 's {disfmarker} you 're right , that 's exactly the question to follow up this discussion , is suppose you did that , uh , got that right . Um , Yeah .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: What could be the other low level detectors , I mean , for {disfmarker}" target="1. Smooth spectral envelopes are commonly used in speech recognition because they have been found to be generally effective. However, the text suggests that they may not be the best option when the goal is to determine sonorant or non-sonorant sounds, as they focus more on the overall envelope rather than the periodicity of the signal.&#10;2. The use of smooth envelopes in vocoders for speech recognition might lead to errors between similar sounds that are sonorant or obstruent, impacting the system's ability to differentiate them accurately.&#10;3. An alternative approach could be building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy, as voicing and sonorance are closely related concepts.&#10;4. There is a mention of an oracle experiment where perfect detection of sonorants could improve speech recognition performance, but there's no further detail provided in the text. This idea highlights the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Didn't they {disfmarker}&#10;Speaker: Professor C&#10;Content: Which means you 're gonna make errors between similar sounds that are son sonorant or obstruent .&#10;Speaker: PhD A&#10;Content: Didn't they also do some kind of an oracle experiment where they said &quot; if we {pause} could detect the sonorants perfectly {pause} and then show how it would improve speech recognition ? I thought I remember hearing about an experiment like that .&#10;Speaker: Professor C&#10;Content: The - these same people ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: I don't remember that .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: That would {disfmarker} that 's {disfmarker} you 're right , that 's exactly the question to follow up this discussion , is suppose you did that , uh , got that right . Um , Yeah .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: What could be the other low level detectors , I mean , for {disfmarker}" target="1. The speakers describe a method that involves initializing parameters for low-level detectors and using Expectation Maximization (EM) to establish training targets for these detectors.&#10;2. Once this is done, they train the whole system using maximum likelihood. This method of detecting sonorance has been found to be robust compared to traditional full-band Gaussian mixtures estimations of sonorance.&#10;3. They also discuss the possibility of building other detectors based on different phonetic features like voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy due to the close relationship between voicing and sonorance.&#10;4. The speakers briefly mention an &quot;oracle experiment&quot; where perfect detection of sonorants could improve speech recognition performance, highlighting the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source=" method of determining signal - to - noise ratio is based on this .&#10;Speaker: PhD A&#10;Content: A couple seconds ?&#10;Speaker: Professor C&#10;Content: So {disfmarker} No , no , it 's based on this kind of method ,&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: this histogram method . So you have a histogram . Now , if you have signal and you have noise , you basically have these two bumps in the histogram , which you could approximate as two Gaussians .&#10;Speaker: PhD A&#10;Content: But wh don't they overlap sometimes ?&#10;Speaker: Professor C&#10;Content: Oh , yeah .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: So you have a mixture of two Gaussians .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Right ? And you can use EM to figure out what it is . You know .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So {disfmarker} so basically now you have this mixture of two Ga" target="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source=" method of determining signal - to - noise ratio is based on this .&#10;Speaker: PhD A&#10;Content: A couple seconds ?&#10;Speaker: Professor C&#10;Content: So {disfmarker} No , no , it 's based on this kind of method ,&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: this histogram method . So you have a histogram . Now , if you have signal and you have noise , you basically have these two bumps in the histogram , which you could approximate as two Gaussians .&#10;Speaker: PhD A&#10;Content: But wh don't they overlap sometimes ?&#10;Speaker: Professor C&#10;Content: Oh , yeah .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: So you have a mixture of two Gaussians .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Right ? And you can use EM to figure out what it is . You know .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So {disfmarker} so basically now you have this mixture of two Ga" target="1. The tricky part about calculating signal-to-noise (SNR) ratio, as discussed by the speakers, is that sometimes the signal and noise can overlap in the histogram, making it difficult to distinguish between the two. To address this issue, they suggest an alternative method for determining means only in the EM algorithm, which appears to be more reliable for estimating SNR.&#10;2. The alternative method for determining means only in the EM algorithm involves using an iterative approach similar to the EM algorithm (referred to as an &quot;EM-like thing&quot;). This method focuses on estimating the means of the signal and noise distributions without concerning itself with the variances. Once the mean values are determined, they can be used as the signal-to-noise ratio in that specific band.&#10;3. The speakers mention that this alternative method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. This is because the EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary.">
      <data key="d0">1</data>
    </edge>
    <edge source=" method of determining signal - to - noise ratio is based on this .&#10;Speaker: PhD A&#10;Content: A couple seconds ?&#10;Speaker: Professor C&#10;Content: So {disfmarker} No , no , it 's based on this kind of method ,&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: this histogram method . So you have a histogram . Now , if you have signal and you have noise , you basically have these two bumps in the histogram , which you could approximate as two Gaussians .&#10;Speaker: PhD A&#10;Content: But wh don't they overlap sometimes ?&#10;Speaker: Professor C&#10;Content: Oh , yeah .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: So you have a mixture of two Gaussians .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Right ? And you can use EM to figure out what it is . You know .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So {disfmarker} so basically now you have this mixture of two Ga" target="1. The speakers discuss a method of determining the signal-to-noise (SNR) ratio based on a histogram with two Gaussian approximations for signal and noise. When the distributions overlap, they suggest using the Expectation Maximization (EM) algorithm to separate them. This iterative method is more reliable for estimating SNR when the signal and noise can overlap in the histogram. The EM algorithm focuses on estimating the means of the signal and noise distributions without considering the variances. Once the mean values are determined, they can be used as the SNR in that specific band.&#10;&#10;This method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. The EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary.&#10;&#10;The use of this method may depend on factors such as:&#10;- Stationarity of the noise&#10;- Availability of good noise estimation&#10;- Type of noise present in the signal&#10;- Use of low-level detectors&#10;- Signal level importance&#10;&#10;Additionally, the speakers use the gradient descent algorithm to train the parameters for the logistic regression and utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm, likely to estimate or determine the training targets for the low-level detectors used in their analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using only the beginning characteristics of a long speech for analysis can be risky because it may not accurately represent the entire speech due to changes that may occur throughout the speech. This could lead to inaccurate conclusions or decisions based on incomplete information.&#10;2. On the other hand, using local characteristics in time for analysis can provide a more accurate representation of the speech as it takes into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results." target=" they thinking of changing it to ?&#10;Speaker: Professor C&#10;Content: But {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , well the people who had very low latency want it to be low {disfmarker} uh , very {disfmarker} {vocalsound} very very narrow , uh , latency bound . And the people who have longer latency don't . So .&#10;Speaker: PhD A&#10;Content: Huh .&#10;Speaker: PhD B&#10;Content: So , yeah .&#10;Speaker: Professor C&#10;Content: Unfortunately we 're the main ones with long latency , but&#10;Speaker: PhD A&#10;Content: Ah !&#10;Speaker: Professor C&#10;Content: But , uh ,&#10;Speaker: PhD B&#10;Content: Yeah , and basically the best proposal had something like thirty or forty milliseconds of latency .&#10;Speaker: Professor C&#10;Content: you know , it 's {disfmarker} Yeah .&#10;Speaker: PhD B&#10;Content: So . Well .&#10;Speaker: Professor C&#10;Content: Yeah , so they were basically {disfmarker} I mean , they were">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using only the beginning characteristics of a long speech for analysis can be risky because it may not accurately represent the entire speech due to changes that may occur throughout the speech. This could lead to inaccurate conclusions or decisions based on incomplete information.&#10;2. On the other hand, using local characteristics in time for analysis can provide a more accurate representation of the speech as it takes into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results." target=" , you know , a neat {disfmarker} neat thing and {disfmarker} and , uh {disfmarker} So .&#10;Speaker: Grad E&#10;Content: S so , um , Ohala 's going to help do these , uh {pause} transcriptions of the meeting data ?&#10;Speaker: PhD A&#10;Content: Uh , well I don't know . We d we sort of didn't get that far . Um , we just talked about some possible features that could be marked by humans and , um ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: because of having maybe some extra transcriber time we thought we could go through and mark some portion of the data for that . And , uh {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: I mean , that 's not an immediate problem , that we don't immediately have a lot of extra transcriber time .&#10;Speaker: PhD A&#10;Content: Yeah , right .&#10;Speaker: Professor C&#10;Content: But {disfmarker} but , uh">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using only the beginning characteristics of a long speech for analysis can be risky because it may not accurately represent the entire speech due to changes that may occur throughout the speech. This could lead to inaccurate conclusions or decisions based on incomplete information.&#10;2. On the other hand, using local characteristics in time for analysis can provide a more accurate representation of the speech as it takes into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results." target=" part of the road&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: or whatever . But it 's {disfmarker} it 's {disfmarker} i i i using the local characteristics in time , is probably going to work pretty well .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: But you could get hurt a lot if you just took some something from the beginning of all the speech , of , you know , an hour of speech and then later {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , so they may be {disfmarker} you know , may be overly , uh , complicated for {disfmarker} for this test but {disfmarker} but {disfmarker} but , uh , I don't know . But what you 're saying , you know , makes sense , though . I mean , if possible you shouldn't {disfmarker} you should {disfmarker} you should make it , uh , the center of the {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using only the beginning characteristics of a long speech for analysis can be risky because it may not accurately represent the entire speech due to changes that may occur throughout the speech. This could lead to inaccurate conclusions or decisions based on incomplete information.&#10;2. On the other hand, using local characteristics in time for analysis can provide a more accurate representation of the speech as it takes into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results." target=" um , except he used a longer time window ,&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad F&#10;Content: like a second maybe . And the reason for that is RASTA 's time window is too short to , um include the whole , um , reverberation {disfmarker} um , I don't know what you call it the reverberation response . I if you see wh if you see what I mean . The reverberation filter from my mouth to that mike is like {disfmarker} it 's t got it 's too long in the {disfmarker} in the time domain for the um {disfmarker} for the RASTA filtering to take care of it . And , um , then there are a couple of other speech enhancement approaches which haven't been tried for speech recognition yet but have just been tried for enhancement , which , um , have the assumption that um , you can do LPC um analysis of th of the signal you get at the far microphone and the , um , all pole filter that you get out of that should be good . It 's just the , um , excitation signal {comment} that is going to">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using only the beginning characteristics of a long speech for analysis can be risky because it may not accurately represent the entire speech due to changes that may occur throughout the speech. This could lead to inaccurate conclusions or decisions based on incomplete information.&#10;2. On the other hand, using local characteristics in time for analysis can provide a more accurate representation of the speech as it takes into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results." target=" kind of thing could add to the latency . I mean , depending on where the window was that you used to calculate {pause} the signal - to - noise ratio .&#10;Speaker: PhD B&#10;Content: Yeah , sure . But {disfmarker} Mmm .&#10;Speaker: Professor C&#10;Content: Not necessarily . Cuz if you don't look into the future , right ?&#10;Speaker: PhD A&#10;Content: OK , well that {disfmarker} I guess that was my question ,&#10;Speaker: Professor C&#10;Content: if you just {disfmarker} yeah {disfmarker}&#10;Speaker: PhD A&#10;Content: yeah .&#10;Speaker: Professor C&#10;Content: I mean , if you just {disfmarker} if you {disfmarker} you , uh {disfmarker} a at the beginning you have some {disfmarker}&#10;Speaker: PhD A&#10;Content: Guess .&#10;Speaker: Professor C&#10;Content: esti some guess and {disfmarker} and , uh , uh {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah , but it {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using only the beginning characteristics of a long speech for analysis can be risky because it may not accurately represent the entire speech due to changes that may occur throughout the speech. This could lead to inaccurate conclusions or decisions based on incomplete information.&#10;2. On the other hand, using local characteristics in time for analysis can provide a more accurate representation of the speech as it takes into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results." target=" I 'm going to look at this frequency , &quot; or {disfmarker} What {disfmarker} what {disfmarker} what are they looking at ?&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD A&#10;Content: What are their inputs ?&#10;Speaker: Grad E&#10;Content: Uh Right , so the {disfmarker} OK , so at each for each sub - band {comment} there are basically , uh , several measures of SNR and {disfmarker} and correlation .&#10;Speaker: PhD A&#10;Content: Ah , OK , OK .&#10;Speaker: Grad E&#10;Content: Um , um and he said there 's like twenty of these per {disfmarker} per sub - band . Um , and for {disfmarker} for every s every sub - band , e you {disfmarker} you just pick ahead of time , um , &quot; I 'm going to have like five {pause} i independent logistic tests . &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using only the beginning characteristics of a long speech for analysis can be risky because it may not accurately represent the entire speech due to changes that may occur throughout the speech. This could lead to inaccurate conclusions or decisions based on incomplete information.&#10;2. On the other hand, using local characteristics in time for analysis can provide a more accurate representation of the speech as it takes into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results." target="1. Better Discrimination: The main advantage of changing and using Linear Discriminant Analysis (LDA) discriminatively, as opposed to using Karhunen-LoÃ¨ve Transform (KLT), is that LDA provides better discrimination between classes. This means that LDA can create a more accurate separation between different categories or groups in the data.&#10;&#10;2. Linear but Discriminant: While KLT focuses on preserving the maximum amount of data variance, LDA tries to find linear transformations that maximize the ratio of between-class variance to within-class variance. This results in a linear but discriminant representation of the data.&#10;&#10;3. Comparison with Neural Nets: Using LDA can help researchers understand if having neural nets really provides any significant improvement over more traditional techniques like KLT or LDA. It allows for comparing and evaluating the performance of different methods to determine their relative merits.&#10;&#10;4. Local Characteristics Analysis: By using local characteristics in time for analysis, LDA can provide a more accurate representation of the data, taking into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results.&#10;&#10;5. Better Understanding: Using LDA in this context can help researchers gain a better understanding of how much they benefit from simply putting frames together versus using discriminative methods and nonlinearity in their analysis. It helps them evaluate the contribution of different components in improving the overall performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using only the beginning characteristics of a long speech for analysis can be risky because it may not accurately represent the entire speech due to changes that may occur throughout the speech. This could lead to inaccurate conclusions or decisions based on incomplete information.&#10;2. On the other hand, using local characteristics in time for analysis can provide a more accurate representation of the speech as it takes into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results." target="1. Human interaction and text-based conversation systems differ in conversational dynamics due to factors such as turn-taking, overlapping utterances, and reaction time. In human interaction, individuals often step on each other's utterances, creating a more fluid and simultaneous exchange of information. However, text-based systems typically involve distinct turns with no overlap or immediate response, which can feel abrupt or unnatural.&#10;&#10;2. To improve the flow and reduce abruptness in a text-based conversation system, implementing techniques that mimic human conversational dynamics could be beneficial. For instance, incorporating delays between responses to simulate thinking time or introducing a level of overlapping utterances by allowing multiple responses at once can help create a more natural feel.&#10;&#10;3. The proposal discussed during the video conference aims to address these issues by having a third party run Voice Activity Detection (VAD) and establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness. However, this idea needs further discussion and understanding before implementation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Using only the beginning characteristics of a long speech for analysis can be risky because it may not accurately represent the entire speech due to changes that may occur throughout the speech. This could lead to inaccurate conclusions or decisions based on incomplete information.&#10;2. On the other hand, using local characteristics in time for analysis can provide a more accurate representation of the speech as it takes into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results." target="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source=" they thinking of changing it to ?&#10;Speaker: Professor C&#10;Content: But {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , well the people who had very low latency want it to be low {disfmarker} uh , very {disfmarker} {vocalsound} very very narrow , uh , latency bound . And the people who have longer latency don't . So .&#10;Speaker: PhD A&#10;Content: Huh .&#10;Speaker: PhD B&#10;Content: So , yeah .&#10;Speaker: Professor C&#10;Content: Unfortunately we 're the main ones with long latency , but&#10;Speaker: PhD A&#10;Content: Ah !&#10;Speaker: Professor C&#10;Content: But , uh ,&#10;Speaker: PhD B&#10;Content: Yeah , and basically the best proposal had something like thirty or forty milliseconds of latency .&#10;Speaker: Professor C&#10;Content: you know , it 's {disfmarker} Yeah .&#10;Speaker: PhD B&#10;Content: So . Well .&#10;Speaker: Professor C&#10;Content: Yeah , so they were basically {disfmarker} I mean , they were" target="Based on the transcript, it appears that both PhD B and Sunil are involved in designing new IIR filters to improve the bugs and latency issues. They used LDA filters and FIR filters as a starting point for their design. This information is conveyed in several parts of the transcript, including when PhD B says &quot;we took the LDA filters and we designed new filters, using recursive filters actually&quot; and &quot;so we took the filters the FIR filters and we designed IIR filters that have the same frequency response&quot;. Additionally, when Professor C asks &quot;when you say 'we', is that something Sunil is doing or is that?&quot; PhD B responds by saying &quot;I'm sorry?&quot; which suggests that they are including Sunil in their use of the word &quot;we&quot;. This is later confirmed when Professor C asks &quot;You had a discussion with Sunil about this though?&quot; and PhD B responds by saying &quot;Uh - huh.&quot;">
      <data key="d0">1</data>
    </edge>
    <edge source=" , you know , a neat {disfmarker} neat thing and {disfmarker} and , uh {disfmarker} So .&#10;Speaker: Grad E&#10;Content: S so , um , Ohala 's going to help do these , uh {pause} transcriptions of the meeting data ?&#10;Speaker: PhD A&#10;Content: Uh , well I don't know . We d we sort of didn't get that far . Um , we just talked about some possible features that could be marked by humans and , um ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: because of having maybe some extra transcriber time we thought we could go through and mark some portion of the data for that . And , uh {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: I mean , that 's not an immediate problem , that we don't immediately have a lot of extra transcriber time .&#10;Speaker: PhD A&#10;Content: Yeah , right .&#10;Speaker: Professor C&#10;Content: But {disfmarker} but , uh" target="Based on the transcript provided, Grad E's plan for their qualifying proposal involves using Voice Activity Detection (VAD) to determine boundaries and have everyone do recognition based on those boundaries. This topic was not discussed in previous meetings because it was proposed during the video conference yesterday morning and had not been communicated to the group beforehand.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , you know , a neat {disfmarker} neat thing and {disfmarker} and , uh {disfmarker} So .&#10;Speaker: Grad E&#10;Content: S so , um , Ohala 's going to help do these , uh {pause} transcriptions of the meeting data ?&#10;Speaker: PhD A&#10;Content: Uh , well I don't know . We d we sort of didn't get that far . Um , we just talked about some possible features that could be marked by humans and , um ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: because of having maybe some extra transcriber time we thought we could go through and mark some portion of the data for that . And , uh {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah ,&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: I mean , that 's not an immediate problem , that we don't immediately have a lot of extra transcriber time .&#10;Speaker: PhD A&#10;Content: Yeah , right .&#10;Speaker: Professor C&#10;Content: But {disfmarker} but , uh" target="1. Frication: This is a type of consonantal sound produced by forcing air through a narrow channel in the vocal tract, causing turbulence. Examples include the 's' sound in &quot;snake&quot; or the 'f' sound in &quot;fish.&quot;&#10;2. Abrupt closure: This refers to the way some consonants are formed with a rapid closing and then releasing of the articulators (tongue, lips, etc.), creating a sharp attack and release, such as the 'p' sound in &quot;spot&quot; or the 't' sound in &quot;tot.&quot;&#10;3. R-coloring: This is a phonetic process where a vowel near an /r/ sound takes on some of the characteristics of the /r/ sound, often resulting in a more rounded or backed quality. For instance, in many American English accents, the word &quot;bird&quot; might have an 'r-colored' vowel, making it sound like &quot;[bÉd].&quot;&#10;4. Nasality: This occurs when part of the airflow from speech is directed through the nasal cavity instead of only through the oral cavity. This creates distinct resonances in the sound produced, and results in nasal consonants such as 'm' or 'n.' Additionally, vowels can become nasalized if they appear before or after a nasal consonant.&#10;5. Voicing: This refers to the vibration of the vocal cords during speech production, which distinguishes voiced sounds (e.g., /v/ in &quot;vase&quot; or /z/ in &quot;zip&quot;) from their voiceless counterparts (e.g., /f/ in &quot;fife&quot; or /s/ in &quot;zip&quot;).">
      <data key="d0">1</data>
    </edge>
    <edge source=" kind of thing could add to the latency . I mean , depending on where the window was that you used to calculate {pause} the signal - to - noise ratio .&#10;Speaker: PhD B&#10;Content: Yeah , sure . But {disfmarker} Mmm .&#10;Speaker: Professor C&#10;Content: Not necessarily . Cuz if you don't look into the future , right ?&#10;Speaker: PhD A&#10;Content: OK , well that {disfmarker} I guess that was my question ,&#10;Speaker: Professor C&#10;Content: if you just {disfmarker} yeah {disfmarker}&#10;Speaker: PhD A&#10;Content: yeah .&#10;Speaker: Professor C&#10;Content: I mean , if you just {disfmarker} if you {disfmarker} you , uh {disfmarker} a at the beginning you have some {disfmarker}&#10;Speaker: PhD A&#10;Content: Guess .&#10;Speaker: Professor C&#10;Content: esti some guess and {disfmarker} and , uh , uh {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah , but it {disfmarker}" target="1. The tricky part about calculating signal-to-noise (SNR) ratio, as discussed by the speakers, is that sometimes the signal and noise can overlap in the histogram, making it difficult to distinguish between the two. To address this issue, they suggest an alternative method for determining means only in the EM algorithm, which appears to be more reliable for estimating SNR.&#10;2. The alternative method for determining means only in the EM algorithm involves using an iterative approach similar to the EM algorithm (referred to as an &quot;EM-like thing&quot;). This method focuses on estimating the means of the signal and noise distributions without concerning itself with the variances. Once the mean values are determined, they can be used as the signal-to-noise ratio in that specific band.&#10;3. The speakers mention that this alternative method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. This is because the EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I 'm going to look at this frequency , &quot; or {disfmarker} What {disfmarker} what {disfmarker} what are they looking at ?&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD A&#10;Content: What are their inputs ?&#10;Speaker: Grad E&#10;Content: Uh Right , so the {disfmarker} OK , so at each for each sub - band {comment} there are basically , uh , several measures of SNR and {disfmarker} and correlation .&#10;Speaker: PhD A&#10;Content: Ah , OK , OK .&#10;Speaker: Grad E&#10;Content: Um , um and he said there 's like twenty of these per {disfmarker} per sub - band . Um , and for {disfmarker} for every s every sub - band , e you {disfmarker} you just pick ahead of time , um , &quot; I 'm going to have like five {pause} i independent logistic tests . &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um" target="1. Better Discrimination: The main advantage of changing and using Linear Discriminant Analysis (LDA) discriminatively, as opposed to using Karhunen-LoÃ¨ve Transform (KLT), is that LDA provides better discrimination between classes. This means that LDA can create a more accurate separation between different categories or groups in the data.&#10;&#10;2. Linear but Discriminant: While KLT focuses on preserving the maximum amount of data variance, LDA tries to find linear transformations that maximize the ratio of between-class variance to within-class variance. This results in a linear but discriminant representation of the data.&#10;&#10;3. Comparison with Neural Nets: Using LDA can help researchers understand if having neural nets really provides any significant improvement over more traditional techniques like KLT or LDA. It allows for comparing and evaluating the performance of different methods to determine their relative merits.&#10;&#10;4. Local Characteristics Analysis: By using local characteristics in time for analysis, LDA can provide a more accurate representation of the data, taking into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results.&#10;&#10;5. Better Understanding: Using LDA in this context can help researchers gain a better understanding of how much they benefit from simply putting frames together versus using discriminative methods and nonlinearity in their analysis. It helps them evaluate the contribution of different components in improving the overall performance.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I 'm going to look at this frequency , &quot; or {disfmarker} What {disfmarker} what {disfmarker} what are they looking at ?&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD A&#10;Content: What are their inputs ?&#10;Speaker: Grad E&#10;Content: Uh Right , so the {disfmarker} OK , so at each for each sub - band {comment} there are basically , uh , several measures of SNR and {disfmarker} and correlation .&#10;Speaker: PhD A&#10;Content: Ah , OK , OK .&#10;Speaker: Grad E&#10;Content: Um , um and he said there 's like twenty of these per {disfmarker} per sub - band . Um , and for {disfmarker} for every s every sub - band , e you {disfmarker} you just pick ahead of time , um , &quot; I 'm going to have like five {pause} i independent logistic tests . &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um" target="The participants in the video conference were discussing the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries for a conference talk. The intention was that everyone would then do recognition based on those boundaries. They had not communicated this proposal to the group before the meeting, which caused some confusion among the participants.&#10;&#10;As for what they are currently doing, it is not explicitly stated in the conversation, but they mention that they have a meeting scheduled for the afternoon to discuss and understand the task better. It seems they are trying to gather more information about the conference talk and its requirements before making any decisions.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I 'm going to look at this frequency , &quot; or {disfmarker} What {disfmarker} what {disfmarker} what are they looking at ?&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD A&#10;Content: What are their inputs ?&#10;Speaker: Grad E&#10;Content: Uh Right , so the {disfmarker} OK , so at each for each sub - band {comment} there are basically , uh , several measures of SNR and {disfmarker} and correlation .&#10;Speaker: PhD A&#10;Content: Ah , OK , OK .&#10;Speaker: Grad E&#10;Content: Um , um and he said there 's like twenty of these per {disfmarker} per sub - band . Um , and for {disfmarker} for every s every sub - band , e you {disfmarker} you just pick ahead of time , um , &quot; I 'm going to have like five {pause} i independent logistic tests . &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um" target="The speakers are subtracting in the power domain. This was mentioned by Professor C and confirmed by PhD B when they were discussing the implementation of spectral subtraction. When Professor C asked &quot;And you're subtracting in the...power domain, or magnitude domain. Probably power domain, right?&quot; PhD B responded with &quot;I guess it's power domain, yeah.&quot;&#10;&#10;The reason they chose the power domain over the magnitude domain is not explicitly stated in the transcript. However, it can be inferred that they may have chosen the power domain because it is more closely aligned with the theory of spectral subtraction, as Professor C mentioned when he said &quot;if you look at the theory, it should be in the power domain.&quot; Despite this, Professor C also mentions that he has seen implementations where people do it in the magnitude domain and shrug their shoulders, implying that it may still work effectively for some use cases.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I 'm going to look at this frequency , &quot; or {disfmarker} What {disfmarker} what {disfmarker} what are they looking at ?&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD A&#10;Content: What are their inputs ?&#10;Speaker: Grad E&#10;Content: Uh Right , so the {disfmarker} OK , so at each for each sub - band {comment} there are basically , uh , several measures of SNR and {disfmarker} and correlation .&#10;Speaker: PhD A&#10;Content: Ah , OK , OK .&#10;Speaker: Grad E&#10;Content: Um , um and he said there 's like twenty of these per {disfmarker} per sub - band . Um , and for {disfmarker} for every s every sub - band , e you {disfmarker} you just pick ahead of time , um , &quot; I 'm going to have like five {pause} i independent logistic tests . &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um" target="1. PhD B and Sunil designed IIR filters that have the same frequency response as the FIR filters but with shorter delays.&#10;2. The low frequency band filter has a delay of 64 milliseconds, and the high frequency band filter has a delay of approximately 11 milliseconds.&#10;3. This is compared to the original IIR filters, which have a delay of 200 milliseconds in both low and high frequency bands. However, these delays for the new filters are not yet tested.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I 'm going to look at this frequency , &quot; or {disfmarker} What {disfmarker} what {disfmarker} what are they looking at ?&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD A&#10;Content: What are their inputs ?&#10;Speaker: Grad E&#10;Content: Uh Right , so the {disfmarker} OK , so at each for each sub - band {comment} there are basically , uh , several measures of SNR and {disfmarker} and correlation .&#10;Speaker: PhD A&#10;Content: Ah , OK , OK .&#10;Speaker: Grad E&#10;Content: Um , um and he said there 's like twenty of these per {disfmarker} per sub - band . Um , and for {disfmarker} for every s every sub - band , e you {disfmarker} you just pick ahead of time , um , &quot; I 'm going to have like five {pause} i independent logistic tests . &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um" target="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I 'm going to look at this frequency , &quot; or {disfmarker} What {disfmarker} what {disfmarker} what are they looking at ?&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD A&#10;Content: What are their inputs ?&#10;Speaker: Grad E&#10;Content: Uh Right , so the {disfmarker} OK , so at each for each sub - band {comment} there are basically , uh , several measures of SNR and {disfmarker} and correlation .&#10;Speaker: PhD A&#10;Content: Ah , OK , OK .&#10;Speaker: Grad E&#10;Content: Um , um and he said there 's like twenty of these per {disfmarker} per sub - band . Um , and for {disfmarker} for every s every sub - band , e you {disfmarker} you just pick ahead of time , um , &quot; I 'm going to have like five {pause} i independent logistic tests . &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um" target="Based on the transcript provided, Grad E's plan for their qualifying proposal involves using Voice Activity Detection (VAD) to determine boundaries and have everyone do recognition based on those boundaries. This topic was not discussed in previous meetings because it was proposed during the video conference yesterday morning and had not been communicated to the group beforehand.">
      <data key="d0">1</data>
    </edge>
    <edge source=" I 'm going to look at this frequency , &quot; or {disfmarker} What {disfmarker} what {disfmarker} what are they looking at ?&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD A&#10;Content: What are their inputs ?&#10;Speaker: Grad E&#10;Content: Uh Right , so the {disfmarker} OK , so at each for each sub - band {comment} there are basically , uh , several measures of SNR and {disfmarker} and correlation .&#10;Speaker: PhD A&#10;Content: Ah , OK , OK .&#10;Speaker: Grad E&#10;Content: Um , um and he said there 's like twenty of these per {disfmarker} per sub - band . Um , and for {disfmarker} for every s every sub - band , e you {disfmarker} you just pick ahead of time , um , &quot; I 'm going to have like five {pause} i independent logistic tests . &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um" target="1. Frication: This is a type of consonantal sound produced by forcing air through a narrow channel in the vocal tract, causing turbulence. Examples include the 's' sound in &quot;snake&quot; or the 'f' sound in &quot;fish.&quot;&#10;2. Abrupt closure: This refers to the way some consonants are formed with a rapid closing and then releasing of the articulators (tongue, lips, etc.), creating a sharp attack and release, such as the 'p' sound in &quot;spot&quot; or the 't' sound in &quot;tot.&quot;&#10;3. R-coloring: This is a phonetic process where a vowel near an /r/ sound takes on some of the characteristics of the /r/ sound, often resulting in a more rounded or backed quality. For instance, in many American English accents, the word &quot;bird&quot; might have an 'r-colored' vowel, making it sound like &quot;[bÉd].&quot;&#10;4. Nasality: This occurs when part of the airflow from speech is directed through the nasal cavity instead of only through the oral cavity. This creates distinct resonances in the sound produced, and results in nasal consonants such as 'm' or 'n.' Additionally, vowels can become nasalized if they appear before or after a nasal consonant.&#10;5. Voicing: This refers to the vibration of the vocal cords during speech production, which distinguishes voiced sounds (e.g., /v/ in &quot;vase&quot; or /z/ in &quot;zip&quot;) from their voiceless counterparts (e.g., /f/ in &quot;fife&quot; or /s/ in &quot;zip&quot;).">
      <data key="d0">1</data>
    </edge>
    <edge source=" I 'm going to look at this frequency , &quot; or {disfmarker} What {disfmarker} what {disfmarker} what are they looking at ?&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD A&#10;Content: What are their inputs ?&#10;Speaker: Grad E&#10;Content: Uh Right , so the {disfmarker} OK , so at each for each sub - band {comment} there are basically , uh , several measures of SNR and {disfmarker} and correlation .&#10;Speaker: PhD A&#10;Content: Ah , OK , OK .&#10;Speaker: Grad E&#10;Content: Um , um and he said there 's like twenty of these per {disfmarker} per sub - band . Um , and for {disfmarker} for every s every sub - band , e you {disfmarker} you just pick ahead of time , um , &quot; I 'm going to have like five {pause} i independent logistic tests . &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um" target="Based on the transcript provided, the speakers conducted separate Viterbi analyses on different sub-band labels. However, they did not explicitly discuss the outcome or impact of this action on the overall training process. Therefore, it is not possible to accurately answer this question without additional information. The transcript only indicates that the separate Viterbi analysis was done but does not provide any details about the results or effects of this process.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better Discrimination: The main advantage of changing and using Linear Discriminant Analysis (LDA) discriminatively, as opposed to using Karhunen-LoÃ¨ve Transform (KLT), is that LDA provides better discrimination between classes. This means that LDA can create a more accurate separation between different categories or groups in the data.&#10;&#10;2. Linear but Discriminant: While KLT focuses on preserving the maximum amount of data variance, LDA tries to find linear transformations that maximize the ratio of between-class variance to within-class variance. This results in a linear but discriminant representation of the data.&#10;&#10;3. Comparison with Neural Nets: Using LDA can help researchers understand if having neural nets really provides any significant improvement over more traditional techniques like KLT or LDA. It allows for comparing and evaluating the performance of different methods to determine their relative merits.&#10;&#10;4. Local Characteristics Analysis: By using local characteristics in time for analysis, LDA can provide a more accurate representation of the data, taking into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results.&#10;&#10;5. Better Understanding: Using LDA in this context can help researchers gain a better understanding of how much they benefit from simply putting frames together versus using discriminative methods and nonlinearity in their analysis. It helps them evaluate the contribution of different components in improving the overall performance." target=" KLT for LDA .&#10;Speaker: PhD A&#10;Content: Change the what ?&#10;Speaker: PhD D&#10;Content: The contextual KLT .&#10;Speaker: PhD A&#10;Content: I 'm missing that last word . Context&#10;Speaker: Professor C&#10;Content: K {disfmarker} KLT .&#10;Speaker: PhD A&#10;Content: KLT .&#10;Speaker: PhD D&#10;Content: KLT {disfmarker}&#10;Speaker: Grad E&#10;Content: Oh . KLT .&#10;Speaker: PhD A&#10;Content: Oh , KLT .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: KLT , I 'm sorry . Uh , to change and use LDA discriminative .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: But {disfmarker} I don't know .&#10;Speaker: Professor C&#10;Content: Uh ,&#10;Speaker: PhD A&#10;Content: What is the advantage of that ?">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better Discrimination: The main advantage of changing and using Linear Discriminant Analysis (LDA) discriminatively, as opposed to using Karhunen-LoÃ¨ve Transform (KLT), is that LDA provides better discrimination between classes. This means that LDA can create a more accurate separation between different categories or groups in the data.&#10;&#10;2. Linear but Discriminant: While KLT focuses on preserving the maximum amount of data variance, LDA tries to find linear transformations that maximize the ratio of between-class variance to within-class variance. This results in a linear but discriminant representation of the data.&#10;&#10;3. Comparison with Neural Nets: Using LDA can help researchers understand if having neural nets really provides any significant improvement over more traditional techniques like KLT or LDA. It allows for comparing and evaluating the performance of different methods to determine their relative merits.&#10;&#10;4. Local Characteristics Analysis: By using local characteristics in time for analysis, LDA can provide a more accurate representation of the data, taking into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results.&#10;&#10;5. Better Understanding: Using LDA in this context can help researchers gain a better understanding of how much they benefit from simply putting frames together versus using discriminative methods and nonlinearity in their analysis. It helps them evaluate the contribution of different components in improving the overall performance." target=" So .&#10;Speaker: PhD A&#10;Content: but I don't think {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah , but we 've {disfmarker} but we played a little bit with {disfmarker} with asymmetric , guys .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: You can do it . So . So , that 's what {disfmarker} that 's what you 're busy with , s messing around with this ,&#10;Speaker: PhD B&#10;Content: Uh , yeah .&#10;Speaker: Professor C&#10;Content: yeah . And , uh ,&#10;Speaker: PhD D&#10;Content: Also we were thinking to {disfmarker} to , uh , apply the eh , spectral subtraction from Ericsson&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: and to {disfmarker} to change the contextual KLT for LDA .&#10;Speaker: PhD A&#10;Content: Change the what ?&#10;Speaker: PhD D&#10;Content: The contextual KLT">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better Discrimination: The main advantage of changing and using Linear Discriminant Analysis (LDA) discriminatively, as opposed to using Karhunen-LoÃ¨ve Transform (KLT), is that LDA provides better discrimination between classes. This means that LDA can create a more accurate separation between different categories or groups in the data.&#10;&#10;2. Linear but Discriminant: While KLT focuses on preserving the maximum amount of data variance, LDA tries to find linear transformations that maximize the ratio of between-class variance to within-class variance. This results in a linear but discriminant representation of the data.&#10;&#10;3. Comparison with Neural Nets: Using LDA can help researchers understand if having neural nets really provides any significant improvement over more traditional techniques like KLT or LDA. It allows for comparing and evaluating the performance of different methods to determine their relative merits.&#10;&#10;4. Local Characteristics Analysis: By using local characteristics in time for analysis, LDA can provide a more accurate representation of the data, taking into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results.&#10;&#10;5. Better Understanding: Using LDA in this context can help researchers gain a better understanding of how much they benefit from simply putting frames together versus using discriminative methods and nonlinearity in their analysis. It helps them evaluate the contribution of different components in improving the overall performance." target=" we want to do , perhaps it 's to replace {disfmarker} to {disfmarker} to have something that 's discriminant but linear , also . And to see if it {disfmarker} if it improves ov over {disfmarker} over the non - discriminant linear transformation .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: And if the neural net is better than this or , well . So .&#10;Speaker: Professor C&#10;Content: Yeah , well , that 's what I meant , is to see whether {disfmarker} whether it {disfmarker} having the neural net really buys you anything .&#10;Speaker: PhD B&#10;Content: Ye Mmm .&#10;Speaker: Professor C&#10;Content: Uh , I mean , it doe did look like it buys you something over just the KLT .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: But maybe it 's just the discrimination and {disfmarker} and maybe {disfmarker} yeah , maybe the nonlinear discrimination isn't necessary .&#10;Speaker: PhD D&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better Discrimination: The main advantage of changing and using Linear Discriminant Analysis (LDA) discriminatively, as opposed to using Karhunen-LoÃ¨ve Transform (KLT), is that LDA provides better discrimination between classes. This means that LDA can create a more accurate separation between different categories or groups in the data.&#10;&#10;2. Linear but Discriminant: While KLT focuses on preserving the maximum amount of data variance, LDA tries to find linear transformations that maximize the ratio of between-class variance to within-class variance. This results in a linear but discriminant representation of the data.&#10;&#10;3. Comparison with Neural Nets: Using LDA can help researchers understand if having neural nets really provides any significant improvement over more traditional techniques like KLT or LDA. It allows for comparing and evaluating the performance of different methods to determine their relative merits.&#10;&#10;4. Local Characteristics Analysis: By using local characteristics in time for analysis, LDA can provide a more accurate representation of the data, taking into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results.&#10;&#10;5. Better Understanding: Using LDA in this context can help researchers gain a better understanding of how much they benefit from simply putting frames together versus using discriminative methods and nonlinearity in their analysis. It helps them evaluate the contribution of different components in improving the overall performance." target=" I don't know .&#10;Speaker: Professor C&#10;Content: Uh ,&#10;Speaker: PhD A&#10;Content: What is the advantage of that ?&#10;Speaker: PhD D&#10;Content: Uh {disfmarker}&#10;Speaker: PhD B&#10;Content: Well , it 's that by the for the moment we have , uh , something that 's discriminant and nonlinear . And the other is linear but it 's not discriminant at all . Well , it 's it 's a linear transformation , that {disfmarker} Uh {disfmarker}&#10;Speaker: Professor C&#10;Content: So at least just to understand maybe what the difference was between how much you were getting from just putting the frames together and how much you 're getting from the discriminative , what the nonlinearity does for you or doesn't do for you . Just to understand it a little better I guess .&#10;Speaker: PhD B&#10;Content: Mmm . Well {disfmarker} uh {disfmarker} yeah . Actually what we want to do , perhaps it 's to replace {disfmarker} to {disfmarker} to have something that 's discriminant">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Better Discrimination: The main advantage of changing and using Linear Discriminant Analysis (LDA) discriminatively, as opposed to using Karhunen-LoÃ¨ve Transform (KLT), is that LDA provides better discrimination between classes. This means that LDA can create a more accurate separation between different categories or groups in the data.&#10;&#10;2. Linear but Discriminant: While KLT focuses on preserving the maximum amount of data variance, LDA tries to find linear transformations that maximize the ratio of between-class variance to within-class variance. This results in a linear but discriminant representation of the data.&#10;&#10;3. Comparison with Neural Nets: Using LDA can help researchers understand if having neural nets really provides any significant improvement over more traditional techniques like KLT or LDA. It allows for comparing and evaluating the performance of different methods to determine their relative merits.&#10;&#10;4. Local Characteristics Analysis: By using local characteristics in time for analysis, LDA can provide a more accurate representation of the data, taking into account variations and fluctuations throughout the entire duration. This method is less likely to be misled by initial or temporary conditions, leading to more reliable results.&#10;&#10;5. Better Understanding: Using LDA in this context can help researchers gain a better understanding of how much they benefit from simply putting frames together versus using discriminative methods and nonlinearity in their analysis. It helps them evaluate the contribution of different components in improving the overall performance." target="1. The primary goal of the conversation appears to be a comparison between different data transformation techniques, specifically focusing on the use of a neural net transformation, Linear Discriminant Analysis (LDA), and Karhunen-LoÃ¨ve Transform (KLT). The speakers aim to understand if the additional complexity of a neural net offers any significant benefits in terms of accuracy or performance compared to these other methods.&#10;2. This comparison is essential for evaluating the relative merits of each technique, especially considering that LDA provides better discrimination between classes than KLT and has a linear but discriminant representation of the data. By examining how well the neural net performs in this context, researchers can determine if there is a meaningful improvement over more traditional methods like KLT or LDA.&#10;3. In addition to comparing techniques, the speakers also discuss implementing spectral subtraction theory in different domains (magnitude versus power) and using various methods for determining signal-to-noise ratios (SNR). These discussions demonstrate the importance of understanding the nuances between different implementation strategies and their effects on performance.&#10;&#10;In summary, the purpose of comparing a neural net transformation to KLT and LDA is to determine if there are any significant benefits in terms of accuracy or performance from using the more complex neural net method over these other techniques. This comparison helps researchers understand the best methods for data transformation based on their specific application needs.">
      <data key="d0">1</data>
    </edge>
    <edge source=" we want to do , perhaps it 's to replace {disfmarker} to {disfmarker} to have something that 's discriminant but linear , also . And to see if it {disfmarker} if it improves ov over {disfmarker} over the non - discriminant linear transformation .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD B&#10;Content: And if the neural net is better than this or , well . So .&#10;Speaker: Professor C&#10;Content: Yeah , well , that 's what I meant , is to see whether {disfmarker} whether it {disfmarker} having the neural net really buys you anything .&#10;Speaker: PhD B&#10;Content: Ye Mmm .&#10;Speaker: Professor C&#10;Content: Uh , I mean , it doe did look like it buys you something over just the KLT .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: But maybe it 's just the discrimination and {disfmarker} and maybe {disfmarker} yeah , maybe the nonlinear discrimination isn't necessary .&#10;Speaker: PhD D&#10;Content:" target="1. The primary goal of the conversation appears to be a comparison between different data transformation techniques, specifically focusing on the use of a neural net transformation, Linear Discriminant Analysis (LDA), and Karhunen-LoÃ¨ve Transform (KLT). The speakers aim to understand if the additional complexity of a neural net offers any significant benefits in terms of accuracy or performance compared to these other methods.&#10;2. This comparison is essential for evaluating the relative merits of each technique, especially considering that LDA provides better discrimination between classes than KLT and has a linear but discriminant representation of the data. By examining how well the neural net performs in this context, researchers can determine if there is a meaningful improvement over more traditional methods like KLT or LDA.&#10;3. In addition to comparing techniques, the speakers also discuss implementing spectral subtraction theory in different domains (magnitude versus power) and using various methods for determining signal-to-noise ratios (SNR). These discussions demonstrate the importance of understanding the nuances between different implementation strategies and their effects on performance.&#10;&#10;In summary, the purpose of comparing a neural net transformation to KLT and LDA is to determine if there are any significant benefits in terms of accuracy or performance from using the more complex neural net method over these other techniques. This comparison helps researchers understand the best methods for data transformation based on their specific application needs.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Smooth spectral envelopes are commonly used in speech recognition because they have been found to be generally effective. However, the text suggests that they may not be the best option when the goal is to determine sonorant or non-sonorant sounds, as they focus more on the overall envelope rather than the periodicity of the signal.&#10;2. The use of smooth envelopes in vocoders for speech recognition might lead to errors between similar sounds that are sonorant or obstruent, impacting the system's ability to differentiate them accurately.&#10;3. An alternative approach could be building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy, as voicing and sonorance are closely related concepts.&#10;4. There is a mention of an oracle experiment where perfect detection of sonorants could improve speech recognition performance, but there's no further detail provided in the text. This idea highlights the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." target=" for that . I mean , this use of a very smooth , uh , spectral envelope is something that , you know , has evolved as being generally a good thing for speech recognition but if you knew that what you were gonna do is detect sonorants or not {disfmarker} So sonorants and non - sonorants is {disfmarker} is {disfmarker} is almost like voiced - unvoiced , except I guess that the voiced stops are {disfmarker} are also called &quot; obstruents &quot; . Uh , so it 's {disfmarker} it 's {disfmarker} uh , but with the exception of the stops I guess it 's pretty much the same as voiced - unvoiced ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: right ? So {disfmarker} so {disfmarker} Um . So , um , if you knew you were doing that , if you were doing something say for a {disfmarker} a , uh {disfmarker} a {disfmarker} a Vocoder , you wouldn't use the same kind of">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Smooth spectral envelopes are commonly used in speech recognition because they have been found to be generally effective. However, the text suggests that they may not be the best option when the goal is to determine sonorant or non-sonorant sounds, as they focus more on the overall envelope rather than the periodicity of the signal.&#10;2. The use of smooth envelopes in vocoders for speech recognition might lead to errors between similar sounds that are sonorant or obstruent, impacting the system's ability to differentiate them accurately.&#10;3. An alternative approach could be building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy, as voicing and sonorance are closely related concepts.&#10;4. There is a mention of an oracle experiment where perfect detection of sonorants could improve speech recognition performance, but there's no further detail provided in the text. This idea highlights the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." target="isfmarker} a , uh {disfmarker} a {disfmarker} a Vocoder , you wouldn't use the same kind of features . You would use something that was sensitive to the periodicity and {disfmarker} and not just the envelope . Uh , and so in that sense it was an unfair test . Um , so I think that the questioner was right . It {disfmarker} it was in that sense an unfair test . Nonetheless , it was one that was interesting because , uh , this is what we are actually using for speech recognition , these smooth envelopes . And this says that perhaps even , you know , trying to use them in the best way that we can , that {disfmarker} that {disfmarker} that we ordinarily do , with , you know , Gaussian mixtures and H M Ms {comment} and so forth , you {disfmarker} you don't , uh , actually do that well on determining whether something is sonorant or not .&#10;Speaker: PhD A&#10;Content: Didn't they {disfmarker}&#10;Speaker: Professor C&#10;Content: Which means you 're gonna make errors between similar sounds that are">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Smooth spectral envelopes are commonly used in speech recognition because they have been found to be generally effective. However, the text suggests that they may not be the best option when the goal is to determine sonorant or non-sonorant sounds, as they focus more on the overall envelope rather than the periodicity of the signal.&#10;2. The use of smooth envelopes in vocoders for speech recognition might lead to errors between similar sounds that are sonorant or obstruent, impacting the system's ability to differentiate them accurately.&#10;3. An alternative approach could be building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy, as voicing and sonorance are closely related concepts.&#10;4. There is a mention of an oracle experiment where perfect detection of sonorants could improve speech recognition performance, but there's no further detail provided in the text. This idea highlights the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." target="Content: Hmm .&#10;Speaker: PhD B&#10;Content: What could be the other low level detectors , I mean , for {disfmarker} {comment} Other kind of features , or {disfmarker} ? in addition to detecting sonorants or {disfmarker} ? Th - that 's what you want to {disfmarker} to {disfmarker} to go for also&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD B&#10;Content: or {disfmarker} ?&#10;Speaker: Grad E&#10;Content: What t Oh , build other {disfmarker} other detectors on different {pause} phonetic features ?&#10;Speaker: PhD B&#10;Content: Other low level detectors ? Yeah .&#10;Speaker: Grad E&#10;Content: Um , uh Let 's see , um , Yeah , I d I don't know . e Um , um , I mean , w easiest thing would be to go {disfmarker} go do some voicing stuff but that 's very similar to sonorance .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad E">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Smooth spectral envelopes are commonly used in speech recognition because they have been found to be generally effective. However, the text suggests that they may not be the best option when the goal is to determine sonorant or non-sonorant sounds, as they focus more on the overall envelope rather than the periodicity of the signal.&#10;2. The use of smooth envelopes in vocoders for speech recognition might lead to errors between similar sounds that are sonorant or obstruent, impacting the system's ability to differentiate them accurately.&#10;3. An alternative approach could be building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy, as voicing and sonorance are closely related concepts.&#10;4. There is a mention of an oracle experiment where perfect detection of sonorants could improve speech recognition performance, but there's no further detail provided in the text. This idea highlights the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." target=" and non - sonorant .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So he hasn't applied it to recognition or if he did he didn't talk about it . It 's {disfmarker} it 's just {disfmarker} And one of the concerns in the audience , actually , was that {disfmarker} that , um , the , uh , uh {disfmarker} he {disfmarker} he did a comparison to , uh , you know , our old foil , the {disfmarker} the nasty old standard recognizer with {vocalsound} mel {disfmarker} mel filter bank at the front , and H M Ms , and {disfmarker} and so forth . And , um , it didn't do nearly as well , especially in {disfmarker} in noise . But the {disfmarker} one of the good questions in the audience was , well , yeah , but that wasn't trained for that . I mean , this use of a very smooth , uh , spectral envelope is something that , you know , has evolved as being generally a good thing">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Smooth spectral envelopes are commonly used in speech recognition because they have been found to be generally effective. However, the text suggests that they may not be the best option when the goal is to determine sonorant or non-sonorant sounds, as they focus more on the overall envelope rather than the periodicity of the signal.&#10;2. The use of smooth envelopes in vocoders for speech recognition might lead to errors between similar sounds that are sonorant or obstruent, impacting the system's ability to differentiate them accurately.&#10;3. An alternative approach could be building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy, as voicing and sonorance are closely related concepts.&#10;4. There is a mention of an oracle experiment where perfect detection of sonorants could improve speech recognition performance, but there's no further detail provided in the text. This idea highlights the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." target="marker} on phonetic targets , and then combining them some somehow down the line , um , they 're {disfmarker} they 're taking sub - band features and , um , training up a detector that detects for , um , these phonetic features for example , um , he presents um , uh , a detector to detect sonorance . And so what {disfmarker} what it basically is {disfmarker} is , um {disfmarker} it 's {disfmarker} there 's {disfmarker} at the lowest level , there {disfmarker} it 's {disfmarker} it 's an OR ga I mean , it 's an AND gate . So , uh , on each sub - band you have several independent tests , to test whether um , there 's the existence of sonorance in a sub - band . And then , um , it c it 's combined by a soft AND gate . And at the {disfmarker} at the higher level , for every {disfmarker} if , um {disfmarker} The higher level there 's a soft OR gate . Uh , so if {">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Smooth spectral envelopes are commonly used in speech recognition because they have been found to be generally effective. However, the text suggests that they may not be the best option when the goal is to determine sonorant or non-sonorant sounds, as they focus more on the overall envelope rather than the periodicity of the signal.&#10;2. The use of smooth envelopes in vocoders for speech recognition might lead to errors between similar sounds that are sonorant or obstruent, impacting the system's ability to differentiate them accurately.&#10;3. An alternative approach could be building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy, as voicing and sonorance are closely related concepts.&#10;4. There is a mention of an oracle experiment where perfect detection of sonorants could improve speech recognition performance, but there's no further detail provided in the text. This idea highlights the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." target="1. The speakers describe a method that involves initializing parameters for low-level detectors and using Expectation Maximization (EM) to establish training targets for these detectors.&#10;2. Once this is done, they train the whole system using maximum likelihood. This method of detecting sonorance has been found to be robust compared to traditional full-band Gaussian mixtures estimations of sonorance.&#10;3. They also discuss the possibility of building other detectors based on different phonetic features like voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy due to the close relationship between voicing and sonorance.&#10;4. The speakers briefly mention an &quot;oracle experiment&quot; where perfect detection of sonorants could improve speech recognition performance, highlighting the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Smooth spectral envelopes are commonly used in speech recognition because they have been found to be generally effective. However, the text suggests that they may not be the best option when the goal is to determine sonorant or non-sonorant sounds, as they focus more on the overall envelope rather than the periodicity of the signal.&#10;2. The use of smooth envelopes in vocoders for speech recognition might lead to errors between similar sounds that are sonorant or obstruent, impacting the system's ability to differentiate them accurately.&#10;3. An alternative approach could be building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy, as voicing and sonorance are closely related concepts.&#10;4. There is a mention of an oracle experiment where perfect detection of sonorants could improve speech recognition performance, but there's no further detail provided in the text. This idea highlights the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." target="1. Human interaction and text-based conversation systems differ in conversational dynamics due to factors such as turn-taking, overlapping utterances, and reaction time. In human interaction, individuals often step on each other's utterances, creating a more fluid and simultaneous exchange of information. However, text-based systems typically involve distinct turns with no overlap or immediate response, which can feel abrupt or unnatural.&#10;&#10;2. To improve the flow and reduce abruptness in a text-based conversation system, implementing techniques that mimic human conversational dynamics could be beneficial. For instance, incorporating delays between responses to simulate thinking time or introducing a level of overlapping utterances by allowing multiple responses at once can help create a more natural feel.&#10;&#10;3. The proposal discussed during the video conference aims to address these issues by having a third party run Voice Activity Detection (VAD) and establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness. However, this idea needs further discussion and understanding before implementation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Smooth spectral envelopes are commonly used in speech recognition because they have been found to be generally effective. However, the text suggests that they may not be the best option when the goal is to determine sonorant or non-sonorant sounds, as they focus more on the overall envelope rather than the periodicity of the signal.&#10;2. The use of smooth envelopes in vocoders for speech recognition might lead to errors between similar sounds that are sonorant or obstruent, impacting the system's ability to differentiate them accurately.&#10;3. An alternative approach could be building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy, as voicing and sonorance are closely related concepts.&#10;4. There is a mention of an oracle experiment where perfect detection of sonorants could improve speech recognition performance, but there's no further detail provided in the text. This idea highlights the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." target="1. The proposed solution to address the limitations of LDA filters involves adding an additional low-pass filter at 25 Hz. This filter will act as a supplementary measure to the existing LDA filters, which are essentially low-pass but allow high frequency components above 25 Hz to pass through.&#10;2. A ten-millisecond filter will be incorporated into the system in addition to the low-pass filter. However, the specifics of this filter and its purpose have not been explicitly mentioned in the transcript.&#10;3. The solution also aims to tackle the delay caused by online normalization using recursion on the feature stream. It has been noted that this method of normalization introduces a delay of 85 milliseconds, which needs to be accounted for and minimized in order to improve the system's performance.&#10;4. The transcript suggests that these changes are aimed at improving the overall efficiency and accuracy of the speech recognition or processing system by addressing the shortcomings of LDA filters, delay compensation, and online normalization. However, it is important to note that these proposed solutions have not been fully implemented or tested yet, as mentioned in the conversation between PhD B and Professor C.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Smooth spectral envelopes are commonly used in speech recognition because they have been found to be generally effective. However, the text suggests that they may not be the best option when the goal is to determine sonorant or non-sonorant sounds, as they focus more on the overall envelope rather than the periodicity of the signal.&#10;2. The use of smooth envelopes in vocoders for speech recognition might lead to errors between similar sounds that are sonorant or obstruent, impacting the system's ability to differentiate them accurately.&#10;3. An alternative approach could be building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy, as voicing and sonorance are closely related concepts.&#10;4. There is a mention of an oracle experiment where perfect detection of sonorants could improve speech recognition performance, but there's no further detail provided in the text. This idea highlights the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." target="1. Smooth spectral envelopes may not be the best option for determining sonorant or non-sonorant sounds in speech recognition, as they focus more on the overall envelope rather than the periodicity of the signal. An alternative approach could involve building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information.&#10;2. To improve speech quality and reduce reverberation in a recording, an all-pole filter can be used in conjunction with a reconstructed excitation signal that has been processed to minimize distortion caused by reverberation. This technique involves identifying the linear time-invariant system between two microphones and using echo cancellation techniques to correct for it.&#10;3. Incorporating delays between responses in text-based conversation systems can help mimic human conversational dynamics, reducing abruptness and creating a more natural feel. Additionally, implementing techniques such as Voice Activity Detection (VAD) can establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness.&#10;4. Other possible approaches to improving speech quality and reducing reverberation include adding a low-pass filter at 25 Hz to supplement existing LDA filters, incorporating a ten-millisecond filter into the system, and minimizing delay caused by online normalization using recursion on the feature stream. These techniques aim to address shortcomings in LDA filters, delay compensation, and online normalization.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Hmm .&#10;Speaker: PhD B&#10;Content: What could be the other low level detectors , I mean , for {disfmarker} {comment} Other kind of features , or {disfmarker} ? in addition to detecting sonorants or {disfmarker} ? Th - that 's what you want to {disfmarker} to {disfmarker} to go for also&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD B&#10;Content: or {disfmarker} ?&#10;Speaker: Grad E&#10;Content: What t Oh , build other {disfmarker} other detectors on different {pause} phonetic features ?&#10;Speaker: PhD B&#10;Content: Other low level detectors ? Yeah .&#10;Speaker: Grad E&#10;Content: Um , uh Let 's see , um , Yeah , I d I don't know . e Um , um , I mean , w easiest thing would be to go {disfmarker} go do some voicing stuff but that 's very similar to sonorance .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad E" target="1. The speakers describe a method that involves initializing parameters for low-level detectors and using Expectation Maximization (EM) to establish training targets for these detectors.&#10;2. Once this is done, they train the whole system using maximum likelihood. This method of detecting sonorance has been found to be robust compared to traditional full-band Gaussian mixtures estimations of sonorance.&#10;3. They also discuss the possibility of building other detectors based on different phonetic features like voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy due to the close relationship between voicing and sonorance.&#10;4. The speakers briefly mention an &quot;oracle experiment&quot; where perfect detection of sonorants could improve speech recognition performance, highlighting the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Hmm .&#10;Speaker: PhD B&#10;Content: What could be the other low level detectors , I mean , for {disfmarker} {comment} Other kind of features , or {disfmarker} ? in addition to detecting sonorants or {disfmarker} ? Th - that 's what you want to {disfmarker} to {disfmarker} to go for also&#10;Speaker: Grad E&#10;Content: Um {disfmarker}&#10;Speaker: PhD B&#10;Content: or {disfmarker} ?&#10;Speaker: Grad E&#10;Content: What t Oh , build other {disfmarker} other detectors on different {pause} phonetic features ?&#10;Speaker: PhD B&#10;Content: Other low level detectors ? Yeah .&#10;Speaker: Grad E&#10;Content: Um , uh Let 's see , um , Yeah , I d I don't know . e Um , um , I mean , w easiest thing would be to go {disfmarker} go do some voicing stuff but that 's very similar to sonorance .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad E" target="1. Frication: This is a type of consonantal sound produced by forcing air through a narrow channel in the vocal tract, causing turbulence. Examples include the 's' sound in &quot;snake&quot; or the 'f' sound in &quot;fish.&quot;&#10;2. Abrupt closure: This refers to the way some consonants are formed with a rapid closing and then releasing of the articulators (tongue, lips, etc.), creating a sharp attack and release, such as the 'p' sound in &quot;spot&quot; or the 't' sound in &quot;tot.&quot;&#10;3. R-coloring: This is a phonetic process where a vowel near an /r/ sound takes on some of the characteristics of the /r/ sound, often resulting in a more rounded or backed quality. For instance, in many American English accents, the word &quot;bird&quot; might have an 'r-colored' vowel, making it sound like &quot;[bÉd].&quot;&#10;4. Nasality: This occurs when part of the airflow from speech is directed through the nasal cavity instead of only through the oral cavity. This creates distinct resonances in the sound produced, and results in nasal consonants such as 'm' or 'n.' Additionally, vowels can become nasalized if they appear before or after a nasal consonant.&#10;5. Voicing: This refers to the vibration of the vocal cords during speech production, which distinguishes voiced sounds (e.g., /v/ in &quot;vase&quot; or /z/ in &quot;zip&quot;) from their voiceless counterparts (e.g., /f/ in &quot;fife&quot; or /s/ in &quot;zip&quot;).">
      <data key="d0">1</data>
    </edge>
    <edge source="marker} on phonetic targets , and then combining them some somehow down the line , um , they 're {disfmarker} they 're taking sub - band features and , um , training up a detector that detects for , um , these phonetic features for example , um , he presents um , uh , a detector to detect sonorance . And so what {disfmarker} what it basically is {disfmarker} is , um {disfmarker} it 's {disfmarker} there 's {disfmarker} at the lowest level , there {disfmarker} it 's {disfmarker} it 's an OR ga I mean , it 's an AND gate . So , uh , on each sub - band you have several independent tests , to test whether um , there 's the existence of sonorance in a sub - band . And then , um , it c it 's combined by a soft AND gate . And at the {disfmarker} at the higher level , for every {disfmarker} if , um {disfmarker} The higher level there 's a soft OR gate . Uh , so if {" target="1. The speakers describe a method that involves initializing parameters for low-level detectors and using Expectation Maximization (EM) to establish training targets for these detectors.&#10;2. Once this is done, they train the whole system using maximum likelihood. This method of detecting sonorance has been found to be robust compared to traditional full-band Gaussian mixtures estimations of sonorance.&#10;3. They also discuss the possibility of building other detectors based on different phonetic features like voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy due to the close relationship between voicing and sonorance.&#10;4. The speakers briefly mention an &quot;oracle experiment&quot; where perfect detection of sonorants could improve speech recognition performance, highlighting the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker} on phonetic targets , and then combining them some somehow down the line , um , they 're {disfmarker} they 're taking sub - band features and , um , training up a detector that detects for , um , these phonetic features for example , um , he presents um , uh , a detector to detect sonorance . And so what {disfmarker} what it basically is {disfmarker} is , um {disfmarker} it 's {disfmarker} there 's {disfmarker} at the lowest level , there {disfmarker} it 's {disfmarker} it 's an OR ga I mean , it 's an AND gate . So , uh , on each sub - band you have several independent tests , to test whether um , there 's the existence of sonorance in a sub - band . And then , um , it c it 's combined by a soft AND gate . And at the {disfmarker} at the higher level , for every {disfmarker} if , um {disfmarker} The higher level there 's a soft OR gate . Uh , so if {" target="Based on the transcript provided, the speakers conducted separate Viterbi analyses on different sub-band labels. However, they did not explicitly discuss the outcome or impact of this action on the overall training process. Therefore, it is not possible to accurately answer this question without additional information. The transcript only indicates that the separate Viterbi analysis was done but does not provide any details about the results or effects of this process.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers describe a method that involves initializing parameters for low-level detectors and using Expectation Maximization (EM) to establish training targets for these detectors.&#10;2. Once this is done, they train the whole system using maximum likelihood. This method of detecting sonorance has been found to be robust compared to traditional full-band Gaussian mixtures estimations of sonorance.&#10;3. They also discuss the possibility of building other detectors based on different phonetic features like voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy due to the close relationship between voicing and sonorance.&#10;4. The speakers briefly mention an &quot;oracle experiment&quot; where perfect detection of sonorants could improve speech recognition performance, highlighting the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." target=" &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um , in some {disfmarker} some way and use EM to come up with your training targets for a {disfmarker} for the {disfmarker} the low - level detectors .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And then , once you get that done , you {disfmarker} you {disfmarker} you train the whole {disfmarker} whole thing on maximum likelihood . Um , and h he shows that using this {disfmarker} this method to detect sonorance is it 's very robust compared to , um {disfmarker} {vocalsound} to typical , uh , full - band Gaussian mixtures um estimations of {disfmarker} of sonorance .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Grad E&#10;Content: And , uh so {disfmarker} so that 's just {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers describe a method that involves initializing parameters for low-level detectors and using Expectation Maximization (EM) to establish training targets for these detectors.&#10;2. Once this is done, they train the whole system using maximum likelihood. This method of detecting sonorance has been found to be robust compared to traditional full-band Gaussian mixtures estimations of sonorance.&#10;3. They also discuss the possibility of building other detectors based on different phonetic features like voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy due to the close relationship between voicing and sonorance.&#10;4. The speakers briefly mention an &quot;oracle experiment&quot; where perfect detection of sonorants could improve speech recognition performance, highlighting the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." target=" every {disfmarker} if , um {disfmarker} The higher level there 's a soft OR gate . Uh , so if {disfmarker} if this detector detects um , the presence of {disfmarker} of sonorance in any of the sub - bands , then the detect uh , the OR gate at the top says , &quot; OK , well this frame has evidence of sonorance . &quot;&#10;Speaker: PhD A&#10;Content: What are {disfmarker} what are some of the low level detectors that they use ?&#10;Speaker: Grad E&#10;Content: And these are all {disfmarker} Oh , OK . Well , the low level detectors are logistic regressions . Um , and the , uh {disfmarker}&#10;Speaker: Professor C&#10;Content: So that , by the way , basically is a {disfmarker} is one of the units in our {disfmarker} in our {disfmarker} our neural network .&#10;Speaker: Grad E&#10;Content: the one o&#10;Speaker: Professor C&#10;Content: So that 's all it is . It 's a sig it 's a s">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers describe a method that involves initializing parameters for low-level detectors and using Expectation Maximization (EM) to establish training targets for these detectors.&#10;2. Once this is done, they train the whole system using maximum likelihood. This method of detecting sonorance has been found to be robust compared to traditional full-band Gaussian mixtures estimations of sonorance.&#10;3. They also discuss the possibility of building other detectors based on different phonetic features like voicing, which is related to sonorance but distinct enough to provide additional information. However, this might not be easy due to the close relationship between voicing and sonorance.&#10;4. The speakers briefly mention an &quot;oracle experiment&quot; where perfect detection of sonorants could improve speech recognition performance, highlighting the potential benefits of accurately detecting sonorant sounds for improving speech recognition systems." target="1. Human interaction and text-based conversation systems differ in conversational dynamics due to factors such as turn-taking, overlapping utterances, and reaction time. In human interaction, individuals often step on each other's utterances, creating a more fluid and simultaneous exchange of information. However, text-based systems typically involve distinct turns with no overlap or immediate response, which can feel abrupt or unnatural.&#10;&#10;2. To improve the flow and reduce abruptness in a text-based conversation system, implementing techniques that mimic human conversational dynamics could be beneficial. For instance, incorporating delays between responses to simulate thinking time or introducing a level of overlapping utterances by allowing multiple responses at once can help create a more natural feel.&#10;&#10;3. The proposal discussed during the video conference aims to address these issues by having a third party run Voice Activity Detection (VAD) and establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness. However, this idea needs further discussion and understanding before implementation.">
      <data key="d0">1</data>
    </edge>
    <edge source=" &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um , in some {disfmarker} some way and use EM to come up with your training targets for a {disfmarker} for the {disfmarker} the low - level detectors .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And then , once you get that done , you {disfmarker} you {disfmarker} you train the whole {disfmarker} whole thing on maximum likelihood . Um , and h he shows that using this {disfmarker} this method to detect sonorance is it 's very robust compared to , um {disfmarker} {vocalsound} to typical , uh , full - band Gaussian mixtures um estimations of {disfmarker} of sonorance .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Grad E&#10;Content: And , uh so {disfmarker} so that 's just {disf" target="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source=" &quot;&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And you initialize these parameters , um , in some {disfmarker} some way and use EM to come up with your training targets for a {disfmarker} for the {disfmarker} the low - level detectors .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: And then , once you get that done , you {disfmarker} you {disfmarker} you train the whole {disfmarker} whole thing on maximum likelihood . Um , and h he shows that using this {disfmarker} this method to detect sonorance is it 's very robust compared to , um {disfmarker} {vocalsound} to typical , uh , full - band Gaussian mixtures um estimations of {disfmarker} of sonorance .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Grad E&#10;Content: And , uh so {disfmarker} so that 's just {disf" target="1. The tricky part about calculating signal-to-noise (SNR) ratio, as discussed by the speakers, is that sometimes the signal and noise can overlap in the histogram, making it difficult to distinguish between the two. To address this issue, they suggest an alternative method for determining means only in the EM algorithm, which appears to be more reliable for estimating SNR.&#10;2. The alternative method for determining means only in the EM algorithm involves using an iterative approach similar to the EM algorithm (referred to as an &quot;EM-like thing&quot;). This method focuses on estimating the means of the signal and noise distributions without concerning itself with the variances. Once the mean values are determined, they can be used as the signal-to-noise ratio in that specific band.&#10;3. The speakers mention that this alternative method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. This is because the EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." target="Speaker: PhD A&#10;Content: OK , we 're on .&#10;Speaker: Professor C&#10;Content: OK , what are we talking about today ?&#10;Speaker: PhD B&#10;Content: I don't know . Do you have news from the conference talk ? Uh , that was programmed for yesterday {disfmarker} I guess .&#10;Speaker: Professor C&#10;Content: Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Yesterday&#10;Speaker: Professor C&#10;Content: Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Yesterday morning on video conference .&#10;Speaker: Professor C&#10;Content: Uh ,&#10;Speaker: PhD B&#10;Content: Well&#10;Speaker: Professor C&#10;Content: oh , I 'm sorry .&#10;Speaker: Grad E&#10;Content: Oh . Conference call .&#10;Speaker: Professor C&#10;Content: I know {disfmarker} now I know what you 're talking about . No , nobody 's told me anything .&#10;Speaker: PhD B&#10;Content: Alright .&#10;Speaker: PhD A&#10;Content: Oh , this was the , uh , talk where they were supposed to try to decide {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." target="marker} I was about to read them too . It 's a , uh , blank sheet of paper .&#10;Speaker: PhD A&#10;Content: So I guess we 're {disfmarker} we 're done .&#10;Speaker: Professor C&#10;Content: Yeah , yeah , I 'll do my credit card number later . OK .">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." target=" Well , anyway , I mean , I think {disfmarker} we could cut {disfmarker} we know what else , we could cut down on the neural net time by {disfmarker} by , uh , playing around a little bit , going more into the past , or something like that . We t we talked about that .&#10;Speaker: PhD A&#10;Content: So is the latency from the neural net caused by how far ahead you 're looking ?&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And there 's also {disfmarker} well , there 's the neural net and there 's also this , uh , uh , multi - frame , uh , uh , KLT .&#10;Speaker: PhD A&#10;Content: Wasn't there {disfmarker} Was it in the , uh , recurrent neural nets where they weren't looking ahead at all ?&#10;Speaker: Professor C&#10;Content: They weren't looking ahead much . They p they looked ahead a little bit .&#10;Speaker: PhD A&#10;Content: A little bit .">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." target=": PhD B&#10;Content: Um .&#10;Speaker: Professor C&#10;Content: uh , one thing that would be no {disfmarker} good to find out about from this conference call is that what they were talking about , what they 're proposing doing , was having a third party , um , run a good VAD , and {disfmarker} and determine boundaries .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: And then given those boundaries , then have everybody do the recognition .&#10;Speaker: PhD D&#10;Content: Begin to work .&#10;Speaker: Professor C&#10;Content: The reason for that was that , um , uh {disfmarker} if some one p one group put in the VAD and another didn't , uh , or one had a better VAD than the other since that {disfmarker} they 're not viewing that as being part of the {disfmarker} the task , and that any {disfmarker} any manufacturer would put a bunch of effort into having some s kind of good speech - silence detection . It still wouldn't be perfect but I mean , e the argument was &quot; let 's not have">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." target=": Alright .&#10;Speaker: PhD A&#10;Content: Oh , this was the , uh , talk where they were supposed to try to decide {disfmarker}&#10;Speaker: PhD B&#10;Content: To {disfmarker} to decide what to do ,&#10;Speaker: PhD A&#10;Content: Ah , right .&#10;Speaker: PhD B&#10;Content: yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah . No , that would have been a good thing to find out before this meeting , that 's . No , I have no {disfmarker} I have no idea . Um , Uh , so I mean , let 's {disfmarker} let 's assume for right now that we 're just kind of plugging on ahead ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: because even if they tell us that , uh , the rules are different , uh , we 're still interested in doing what we 're doing . So what are you doing ?&#10;Speaker: PhD B&#10;Content: Mm - hmm . Uh , well , we 've {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." target=" Right .&#10;Speaker: Professor C&#10;Content: and then the {disfmarker} the microscope would start moving or something .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: And there 's physical inertia there , so probably the {disfmarker} the motion itself was all {disfmarker}&#10;Speaker: PhD A&#10;Content: And it felt to , uh , the users that it was instantaneous . I mean , as fast as talking to a person . It {disfmarker} th I don't think anybody ever complained about the delay .&#10;Speaker: Professor C&#10;Content: Yeah , so you would think as long as it 's under half a second or something .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , I 'm not an expert on that&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: but .&#10;Speaker: PhD A&#10;Content: I don't remember the exact numbers but it was something like that .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." target="The participants in the video conference were discussing the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries for a conference talk. The intention was that everyone would then do recognition based on those boundaries. They had not communicated this proposal to the group before the meeting, which caused some confusion among the participants.&#10;&#10;As for what they are currently doing, it is not explicitly stated in the conversation, but they mention that they have a meeting scheduled for the afternoon to discuss and understand the task better. It seems they are trying to gather more information about the conference talk and its requirements before making any decisions.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." target="1. Human interaction and text-based conversation systems differ in conversational dynamics due to factors such as turn-taking, overlapping utterances, and reaction time. In human interaction, individuals often step on each other's utterances, creating a more fluid and simultaneous exchange of information. However, text-based systems typically involve distinct turns with no overlap or immediate response, which can feel abrupt or unnatural.&#10;&#10;2. To improve the flow and reduce abruptness in a text-based conversation system, implementing techniques that mimic human conversational dynamics could be beneficial. For instance, incorporating delays between responses to simulate thinking time or introducing a level of overlapping utterances by allowing multiple responses at once can help create a more natural feel.&#10;&#10;3. The proposal discussed during the video conference aims to address these issues by having a third party run Voice Activity Detection (VAD) and establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness. However, this idea needs further discussion and understanding before implementation.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." target="Based on the provided transcript, there was no concrete outcome or decision made during the conference talk related to Voice Activity Detection (VAD) and determining boundaries. The participants discussed the possibility of having a third party run a VAD and then doing recognition based on those boundaries, but they did not reach a conclusion. They also mentioned that they had not been informed about this proposal beforehand, which caused some confusion during the meeting. It appears that further discussions and decisions will take place in subsequent meetings or communications.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." target="Based on the transcript, it appears that both PhD B and Sunil are involved in designing new IIR filters to improve the bugs and latency issues. They used LDA filters and FIR filters as a starting point for their design. This information is conveyed in several parts of the transcript, including when PhD B says &quot;we took the LDA filters and we designed new filters, using recursive filters actually&quot; and &quot;so we took the filters the FIR filters and we designed IIR filters that have the same frequency response&quot;. Additionally, when Professor C asks &quot;when you say 'we', is that something Sunil is doing or is that?&quot; PhD B responds by saying &quot;I'm sorry?&quot; which suggests that they are including Sunil in their use of the word &quot;we&quot;. This is later confirmed when Professor C asks &quot;You had a discussion with Sunil about this though?&quot; and PhD B responds by saying &quot;Uh - huh.&quot;">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." target="Based on the transcript provided, Grad E's plan for their qualifying proposal involves using Voice Activity Detection (VAD) to determine boundaries and have everyone do recognition based on those boundaries. This topic was not discussed in previous meetings because it was proposed during the video conference yesterday morning and had not been communicated to the group beforehand.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that the video conference yesterday morning was related to a conference talk that was previously scheduled. During the video conference, the participants discussed the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries, with the intention of having everyone do recognition based on those boundaries. This proposal was not communicated to the group before this meeting, and there is no detailed information about other aspects of the conference talk." target="1. The proposed solution to address the limitations of LDA filters involves adding an additional low-pass filter at 25 Hz. This filter will act as a supplementary measure to the existing LDA filters, which are essentially low-pass but allow high frequency components above 25 Hz to pass through.&#10;2. A ten-millisecond filter will be incorporated into the system in addition to the low-pass filter. However, the specifics of this filter and its purpose have not been explicitly mentioned in the transcript.&#10;3. The solution also aims to tackle the delay caused by online normalization using recursion on the feature stream. It has been noted that this method of normalization introduces a delay of 85 milliseconds, which needs to be accounted for and minimized in order to improve the system's performance.&#10;4. The transcript suggests that these changes are aimed at improving the overall efficiency and accuracy of the speech recognition or processing system by addressing the shortcomings of LDA filters, delay compensation, and online normalization. However, it is important to note that these proposed solutions have not been fully implemented or tested yet, as mentioned in the conversation between PhD B and Professor C.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD A&#10;Content: OK , we 're on .&#10;Speaker: Professor C&#10;Content: OK , what are we talking about today ?&#10;Speaker: PhD B&#10;Content: I don't know . Do you have news from the conference talk ? Uh , that was programmed for yesterday {disfmarker} I guess .&#10;Speaker: Professor C&#10;Content: Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Yesterday&#10;Speaker: Professor C&#10;Content: Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Yesterday morning on video conference .&#10;Speaker: Professor C&#10;Content: Uh ,&#10;Speaker: PhD B&#10;Content: Well&#10;Speaker: Professor C&#10;Content: oh , I 'm sorry .&#10;Speaker: Grad E&#10;Content: Oh . Conference call .&#10;Speaker: Professor C&#10;Content: I know {disfmarker} now I know what you 're talking about . No , nobody 's told me anything .&#10;Speaker: PhD B&#10;Content: Alright .&#10;Speaker: PhD A&#10;Content: Oh , this was the , uh , talk where they were supposed to try to decide {disf" target="The participants in the video conference were discussing the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries for a conference talk. The intention was that everyone would then do recognition based on those boundaries. They had not communicated this proposal to the group before the meeting, which caused some confusion among the participants.&#10;&#10;As for what they are currently doing, it is not explicitly stated in the conversation, but they mention that they have a meeting scheduled for the afternoon to discuss and understand the task better. It seems they are trying to gather more information about the conference talk and its requirements before making any decisions.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD A&#10;Content: OK , we 're on .&#10;Speaker: Professor C&#10;Content: OK , what are we talking about today ?&#10;Speaker: PhD B&#10;Content: I don't know . Do you have news from the conference talk ? Uh , that was programmed for yesterday {disfmarker} I guess .&#10;Speaker: Professor C&#10;Content: Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Yesterday&#10;Speaker: Professor C&#10;Content: Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Yesterday morning on video conference .&#10;Speaker: Professor C&#10;Content: Uh ,&#10;Speaker: PhD B&#10;Content: Well&#10;Speaker: Professor C&#10;Content: oh , I 'm sorry .&#10;Speaker: Grad E&#10;Content: Oh . Conference call .&#10;Speaker: Professor C&#10;Content: I know {disfmarker} now I know what you 're talking about . No , nobody 's told me anything .&#10;Speaker: PhD B&#10;Content: Alright .&#10;Speaker: PhD A&#10;Content: Oh , this was the , uh , talk where they were supposed to try to decide {disf" target="Based on the provided transcript, there was no concrete outcome or decision made during the conference talk related to Voice Activity Detection (VAD) and determining boundaries. The participants discussed the possibility of having a third party run a VAD and then doing recognition based on those boundaries, but they did not reach a conclusion. They also mentioned that they had not been informed about this proposal beforehand, which caused some confusion during the meeting. It appears that further discussions and decisions will take place in subsequent meetings or communications.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD A&#10;Content: OK , we 're on .&#10;Speaker: Professor C&#10;Content: OK , what are we talking about today ?&#10;Speaker: PhD B&#10;Content: I don't know . Do you have news from the conference talk ? Uh , that was programmed for yesterday {disfmarker} I guess .&#10;Speaker: Professor C&#10;Content: Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Yesterday&#10;Speaker: Professor C&#10;Content: Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Yesterday morning on video conference .&#10;Speaker: Professor C&#10;Content: Uh ,&#10;Speaker: PhD B&#10;Content: Well&#10;Speaker: Professor C&#10;Content: oh , I 'm sorry .&#10;Speaker: Grad E&#10;Content: Oh . Conference call .&#10;Speaker: Professor C&#10;Content: I know {disfmarker} now I know what you 're talking about . No , nobody 's told me anything .&#10;Speaker: PhD B&#10;Content: Alright .&#10;Speaker: PhD A&#10;Content: Oh , this was the , uh , talk where they were supposed to try to decide {disf" target="Based on the transcript provided, Grad E's plan for their qualifying proposal involves using Voice Activity Detection (VAD) to determine boundaries and have everyone do recognition based on those boundaries. This topic was not discussed in previous meetings because it was proposed during the video conference yesterday morning and had not been communicated to the group beforehand.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker} I was about to read them too . It 's a , uh , blank sheet of paper .&#10;Speaker: PhD A&#10;Content: So I guess we 're {disfmarker} we 're done .&#10;Speaker: Professor C&#10;Content: Yeah , yeah , I 'll do my credit card number later . OK ." target="Based on the provided transcript, there was no concrete outcome or decision made during the conference talk related to Voice Activity Detection (VAD) and determining boundaries. The participants discussed the possibility of having a third party run a VAD and then doing recognition based on those boundaries, but they did not reach a conclusion. They also mentioned that they had not been informed about this proposal beforehand, which caused some confusion during the meeting. It appears that further discussions and decisions will take place in subsequent meetings or communications.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker} I was about to read them too . It 's a , uh , blank sheet of paper .&#10;Speaker: PhD A&#10;Content: So I guess we 're {disfmarker} we 're done .&#10;Speaker: Professor C&#10;Content: Yeah , yeah , I 'll do my credit card number later . OK ." target="Based on the transcript, PhD A requires the names, as well as other details, from Professor C in order to create the &quot;key&quot; files. The reason for needing this information is not explicitly stated in the transcript, but it can be inferred that it may be necessary for properly labeling or identifying the filters or data being used.&#10;&#10;PhD A also mentions that they need the forms filled out, even if there are no digits involved. This suggests that the forms may serve a purpose beyond simply collecting numerical data, and may instead be used to gather important information about the design and implementation of the filters.&#10;&#10;Professor C initially expresses some confusion about why the forms are necessary, but PhD A explains that it's important for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used. They also mention that stress (presumably in the context of speech recognition) may not always help distinguish between words, but that getting the sequence right is still important.&#10;&#10;Overall, while the transcript does not provide a lot of detail about why PhD A needs the information they are requesting from Professor C, it is clear that this information is necessary for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used.">
      <data key="d0">1</data>
    </edge>
    <edge source="marker} I was about to read them too . It 's a , uh , blank sheet of paper .&#10;Speaker: PhD A&#10;Content: So I guess we 're {disfmarker} we 're done .&#10;Speaker: Professor C&#10;Content: Yeah , yeah , I 'll do my credit card number later . OK ." target="The transcript does not provide direct information about why &quot;Adam ran out of digits.&quot; However, it is mentioned that the digit forms are blank, which may be a result of running out of digits. There is also no explicit mention of why this makes it unnecessary to write names on the forms.&#10;&#10;Based on the context, it appears that the group is discussing some sort of data collection or input process involving numerical values and forms requiring names. The discussion between PhD A and Professor C suggests that the forms serve a purpose beyond just collecting numerical data, possibly for associating specific details with each set of numbers.&#10;&#10;Since the digit forms are blank due to running out of digits, it could imply that there is a limitation on how many sets of digits can be generated within a certain time frame or under certain conditions (such as Adam's situation). This limitation might have made writing names on the forms unnecessary because there were no digits available for association.&#10;&#10;Nonetheless, since the transcript does not provide direct information about why Adam ran out of digits, any answer to this question can only be speculative and based on inferences drawn from the context.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Well , anyway , I mean , I think {disfmarker} we could cut {disfmarker} we know what else , we could cut down on the neural net time by {disfmarker} by , uh , playing around a little bit , going more into the past , or something like that . We t we talked about that .&#10;Speaker: PhD A&#10;Content: So is the latency from the neural net caused by how far ahead you 're looking ?&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And there 's also {disfmarker} well , there 's the neural net and there 's also this , uh , uh , multi - frame , uh , uh , KLT .&#10;Speaker: PhD A&#10;Content: Wasn't there {disfmarker} Was it in the , uh , recurrent neural nets where they weren't looking ahead at all ?&#10;Speaker: Professor C&#10;Content: They weren't looking ahead much . They p they looked ahead a little bit .&#10;Speaker: PhD A&#10;Content: A little bit ." target="1. The primary goal of the conversation appears to be a comparison between different data transformation techniques, specifically focusing on the use of a neural net transformation, Linear Discriminant Analysis (LDA), and Karhunen-LoÃ¨ve Transform (KLT). The speakers aim to understand if the additional complexity of a neural net offers any significant benefits in terms of accuracy or performance compared to these other methods.&#10;2. This comparison is essential for evaluating the relative merits of each technique, especially considering that LDA provides better discrimination between classes than KLT and has a linear but discriminant representation of the data. By examining how well the neural net performs in this context, researchers can determine if there is a meaningful improvement over more traditional methods like KLT or LDA.&#10;3. In addition to comparing techniques, the speakers also discuss implementing spectral subtraction theory in different domains (magnitude versus power) and using various methods for determining signal-to-noise ratios (SNR). These discussions demonstrate the importance of understanding the nuances between different implementation strategies and their effects on performance.&#10;&#10;In summary, the purpose of comparing a neural net transformation to KLT and LDA is to determine if there are any significant benefits in terms of accuracy or performance from using the more complex neural net method over these other techniques. This comparison helps researchers understand the best methods for data transformation based on their specific application needs.">
      <data key="d0">1</data>
    </edge>
    <edge source=": PhD B&#10;Content: Um .&#10;Speaker: Professor C&#10;Content: uh , one thing that would be no {disfmarker} good to find out about from this conference call is that what they were talking about , what they 're proposing doing , was having a third party , um , run a good VAD , and {disfmarker} and determine boundaries .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: And then given those boundaries , then have everybody do the recognition .&#10;Speaker: PhD D&#10;Content: Begin to work .&#10;Speaker: Professor C&#10;Content: The reason for that was that , um , uh {disfmarker} if some one p one group put in the VAD and another didn't , uh , or one had a better VAD than the other since that {disfmarker} they 're not viewing that as being part of the {disfmarker} the task , and that any {disfmarker} any manufacturer would put a bunch of effort into having some s kind of good speech - silence detection . It still wouldn't be perfect but I mean , e the argument was &quot; let 's not have" target="The participants in the video conference were discussing the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries for a conference talk. The intention was that everyone would then do recognition based on those boundaries. They had not communicated this proposal to the group before the meeting, which caused some confusion among the participants.&#10;&#10;As for what they are currently doing, it is not explicitly stated in the conversation, but they mention that they have a meeting scheduled for the afternoon to discuss and understand the task better. It seems they are trying to gather more information about the conference talk and its requirements before making any decisions.">
      <data key="d0">1</data>
    </edge>
    <edge source=": PhD B&#10;Content: Um .&#10;Speaker: Professor C&#10;Content: uh , one thing that would be no {disfmarker} good to find out about from this conference call is that what they were talking about , what they 're proposing doing , was having a third party , um , run a good VAD , and {disfmarker} and determine boundaries .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: And then given those boundaries , then have everybody do the recognition .&#10;Speaker: PhD D&#10;Content: Begin to work .&#10;Speaker: Professor C&#10;Content: The reason for that was that , um , uh {disfmarker} if some one p one group put in the VAD and another didn't , uh , or one had a better VAD than the other since that {disfmarker} they 're not viewing that as being part of the {disfmarker} the task , and that any {disfmarker} any manufacturer would put a bunch of effort into having some s kind of good speech - silence detection . It still wouldn't be perfect but I mean , e the argument was &quot; let 's not have" target="Based on the provided transcript, there was no concrete outcome or decision made during the conference talk related to Voice Activity Detection (VAD) and determining boundaries. The participants discussed the possibility of having a third party run a VAD and then doing recognition based on those boundaries, but they did not reach a conclusion. They also mentioned that they had not been informed about this proposal beforehand, which caused some confusion during the meeting. It appears that further discussions and decisions will take place in subsequent meetings or communications.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Alright .&#10;Speaker: PhD A&#10;Content: Oh , this was the , uh , talk where they were supposed to try to decide {disfmarker}&#10;Speaker: PhD B&#10;Content: To {disfmarker} to decide what to do ,&#10;Speaker: PhD A&#10;Content: Ah , right .&#10;Speaker: PhD B&#10;Content: yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah . No , that would have been a good thing to find out before this meeting , that 's . No , I have no {disfmarker} I have no idea . Um , Uh , so I mean , let 's {disfmarker} let 's assume for right now that we 're just kind of plugging on ahead ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: because even if they tell us that , uh , the rules are different , uh , we 're still interested in doing what we 're doing . So what are you doing ?&#10;Speaker: PhD B&#10;Content: Mm - hmm . Uh , well , we 've {disf" target="The participants in the video conference were discussing the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries for a conference talk. The intention was that everyone would then do recognition based on those boundaries. They had not communicated this proposal to the group before the meeting, which caused some confusion among the participants.&#10;&#10;As for what they are currently doing, it is not explicitly stated in the conversation, but they mention that they have a meeting scheduled for the afternoon to discuss and understand the task better. It seems they are trying to gather more information about the conference talk and its requirements before making any decisions.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Alright .&#10;Speaker: PhD A&#10;Content: Oh , this was the , uh , talk where they were supposed to try to decide {disfmarker}&#10;Speaker: PhD B&#10;Content: To {disfmarker} to decide what to do ,&#10;Speaker: PhD A&#10;Content: Ah , right .&#10;Speaker: PhD B&#10;Content: yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah . No , that would have been a good thing to find out before this meeting , that 's . No , I have no {disfmarker} I have no idea . Um , Uh , so I mean , let 's {disfmarker} let 's assume for right now that we 're just kind of plugging on ahead ,&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: because even if they tell us that , uh , the rules are different , uh , we 're still interested in doing what we 're doing . So what are you doing ?&#10;Speaker: PhD B&#10;Content: Mm - hmm . Uh , well , we 've {disf" target="Based on the provided transcript, there was no concrete outcome or decision made during the conference talk related to Voice Activity Detection (VAD) and determining boundaries. The participants discussed the possibility of having a third party run a VAD and then doing recognition based on those boundaries, but they did not reach a conclusion. They also mentioned that they had not been informed about this proposal beforehand, which caused some confusion during the meeting. It appears that further discussions and decisions will take place in subsequent meetings or communications.">
      <data key="d0">1</data>
    </edge>
    <edge source="The participants in the video conference were discussing the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries for a conference talk. The intention was that everyone would then do recognition based on those boundaries. They had not communicated this proposal to the group before the meeting, which caused some confusion among the participants.&#10;&#10;As for what they are currently doing, it is not explicitly stated in the conversation, but they mention that they have a meeting scheduled for the afternoon to discuss and understand the task better. It seems they are trying to gather more information about the conference talk and its requirements before making any decisions." target=" saying . As long as you get {disfmarker} The sequence ,&#10;Speaker: Professor C&#10;Content: We 're g if we 're doing {disfmarker} if we 're talking about transcription as opposed to something else {disfmarker}&#10;Speaker: PhD A&#10;Content: right ? Yeah . Yeah , yeah , yeah . Yeah . Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So where it could help is maybe at a higher level . Yeah .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: Grad E&#10;Content: Like a understanding application .&#10;Speaker: PhD A&#10;Content: Understanding , yeah . Exactly .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: But that 's this afternoon 's meeting . Yeah . We don't understand anything in this meeting . Yeah , so that 's {disfmarker} yeah , that 's , you know , a neat {disfmarker} neat thing and {disfmarker} and , uh {disfmarker} So .">
      <data key="d0">1</data>
    </edge>
    <edge source="The participants in the video conference were discussing the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries for a conference talk. The intention was that everyone would then do recognition based on those boundaries. They had not communicated this proposal to the group before the meeting, which caused some confusion among the participants.&#10;&#10;As for what they are currently doing, it is not explicitly stated in the conversation, but they mention that they have a meeting scheduled for the afternoon to discuss and understand the task better. It seems they are trying to gather more information about the conference talk and its requirements before making any decisions." target="1. Human interaction and text-based conversation systems differ in conversational dynamics due to factors such as turn-taking, overlapping utterances, and reaction time. In human interaction, individuals often step on each other's utterances, creating a more fluid and simultaneous exchange of information. However, text-based systems typically involve distinct turns with no overlap or immediate response, which can feel abrupt or unnatural.&#10;&#10;2. To improve the flow and reduce abruptness in a text-based conversation system, implementing techniques that mimic human conversational dynamics could be beneficial. For instance, incorporating delays between responses to simulate thinking time or introducing a level of overlapping utterances by allowing multiple responses at once can help create a more natural feel.&#10;&#10;3. The proposal discussed during the video conference aims to address these issues by having a third party run Voice Activity Detection (VAD) and establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness. However, this idea needs further discussion and understanding before implementation.">
      <data key="d0">1</data>
    </edge>
    <edge source="The participants in the video conference were discussing the possibility of having a third party run a Voice Activity Detection (VAD) and determine boundaries for a conference talk. The intention was that everyone would then do recognition based on those boundaries. They had not communicated this proposal to the group before the meeting, which caused some confusion among the participants.&#10;&#10;As for what they are currently doing, it is not explicitly stated in the conversation, but they mention that they have a meeting scheduled for the afternoon to discuss and understand the task better. It seems they are trying to gather more information about the conference talk and its requirements before making any decisions." target="Based on the provided transcript, there was no concrete outcome or decision made during the conference talk related to Voice Activity Detection (VAD) and determining boundaries. The participants discussed the possibility of having a third party run a VAD and then doing recognition based on those boundaries, but they did not reach a conclusion. They also mentioned that they had not been informed about this proposal beforehand, which caused some confusion during the meeting. It appears that further discussions and decisions will take place in subsequent meetings or communications.">
      <data key="d0">1</data>
    </edge>
    <edge source=" saying . As long as you get {disfmarker} The sequence ,&#10;Speaker: Professor C&#10;Content: We 're g if we 're doing {disfmarker} if we 're talking about transcription as opposed to something else {disfmarker}&#10;Speaker: PhD A&#10;Content: right ? Yeah . Yeah , yeah , yeah . Yeah . Right .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So where it could help is maybe at a higher level . Yeah .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: Grad E&#10;Content: Like a understanding application .&#10;Speaker: PhD A&#10;Content: Understanding , yeah . Exactly .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: But that 's this afternoon 's meeting . Yeah . We don't understand anything in this meeting . Yeah , so that 's {disfmarker} yeah , that 's , you know , a neat {disfmarker} neat thing and {disfmarker} and , uh {disfmarker} So ." target="Based on the transcript provided, Grad E's plan for their qualifying proposal involves using Voice Activity Detection (VAD) to determine boundaries and have everyone do recognition based on those boundaries. This topic was not discussed in previous meetings because it was proposed during the video conference yesterday morning and had not been communicated to the group beforehand.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are subtracting in the power domain. This was mentioned by Professor C and confirmed by PhD B when they were discussing the implementation of spectral subtraction. When Professor C asked &quot;And you're subtracting in the...power domain, or magnitude domain. Probably power domain, right?&quot; PhD B responded with &quot;I guess it's power domain, yeah.&quot;&#10;&#10;The reason they chose the power domain over the magnitude domain is not explicitly stated in the transcript. However, it can be inferred that they may have chosen the power domain because it is more closely aligned with the theory of spectral subtraction, as Professor C mentioned when he said &quot;if you look at the theory, it should be in the power domain.&quot; Despite this, Professor C also mentions that he has seen implementations where people do it in the magnitude domain and shrug their shoulders, implying that it may still work effectively for some use cases." target=": You do {disfmarker} doing the {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And you 're {disfmarker} you 're subtracting in the {disfmarker} in the {disfmarker} in the {disfmarker} I guess it 's power {disfmarker} power domain , uh , or {disfmarker} or magnitude domain . Probably power domain , right ?&#10;Speaker: PhD B&#10;Content: I guess it 's power domain , yeah .&#10;Speaker: Professor C&#10;Content: why&#10;Speaker: PhD B&#10;Content: I don't remember exactly .&#10;Speaker: Professor C&#10;Content: Yeah ,&#10;Speaker: PhD D&#10;Content: I don't remember .&#10;Speaker: PhD B&#10;Content: But {disfmarker} yeah , so it 's before everything else ,&#10;Speaker: Professor C&#10;Content: yep .&#10;Speaker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Professor C&#10;Content: I mean , if you look at the theory">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are subtracting in the power domain. This was mentioned by Professor C and confirmed by PhD B when they were discussing the implementation of spectral subtraction. When Professor C asked &quot;And you're subtracting in the...power domain, or magnitude domain. Probably power domain, right?&quot; PhD B responded with &quot;I guess it's power domain, yeah.&quot;&#10;&#10;The reason they chose the power domain over the magnitude domain is not explicitly stated in the transcript. However, it can be inferred that they may have chosen the power domain because it is more closely aligned with the theory of spectral subtraction, as Professor C mentioned when he said &quot;if you look at the theory, it should be in the power domain.&quot; Despite this, Professor C also mentions that he has seen implementations where people do it in the magnitude domain and shrug their shoulders, implying that it may still work effectively for some use cases." target=" um {disfmarker} you don't just subtract the {disfmarker} the estimate of the noise spectrum . You subtract th that times {disfmarker}&#10;Speaker: PhD B&#10;Content: A little bit more and {disfmarker} Yeah .&#10;Speaker: Professor C&#10;Content: Or {disfmarker} or less , or {disfmarker}&#10;Speaker: PhD A&#10;Content: Really ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Huh !&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And generated this {disfmarker} this ,&#10;Speaker: Professor C&#10;Content: Uh .&#10;Speaker: PhD B&#10;Content: um , so you have the estimation of the power spectra of the noise , and you multiply this by a factor which is depend dependent on the SNR . So . Well .&#10;Speaker: PhD D&#10;Content: Hmm , maybe .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: PhD B&#10;Content: When the speech lev when the signal level is more important , compared to this noise level ,">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are subtracting in the power domain. This was mentioned by Professor C and confirmed by PhD B when they were discussing the implementation of spectral subtraction. When Professor C asked &quot;And you're subtracting in the...power domain, or magnitude domain. Probably power domain, right?&quot; PhD B responded with &quot;I guess it's power domain, yeah.&quot;&#10;&#10;The reason they chose the power domain over the magnitude domain is not explicitly stated in the transcript. However, it can be inferred that they may have chosen the power domain because it is more closely aligned with the theory of spectral subtraction, as Professor C mentioned when he said &quot;if you look at the theory, it should be in the power domain.&quot; Despite this, Professor C also mentions that he has seen implementations where people do it in the magnitude domain and shrug their shoulders, implying that it may still work effectively for some use cases." target="Content: That 's what spectral subtraction will help with , practically speaking .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: If it varies a lot , to get a If {disfmarker} if {disfmarker} to get a good estimate you need a few seconds of speech , even if it 's centered , right ?&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: if you need a few seconds to get a decent estimate but it 's changed a lot in a few seconds , then it , you know , i it 's kind of a problem .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: I mean , imagine e five hertz is the middle of the {disfmarker} of the speech modulation spectrum ,&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: right ? So imagine a jack hammer going at five hertz .&#10;Speaker: PhD B&#10;Content: Yeah , that 's {disfmarker}&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are subtracting in the power domain. This was mentioned by Professor C and confirmed by PhD B when they were discussing the implementation of spectral subtraction. When Professor C asked &quot;And you're subtracting in the...power domain, or magnitude domain. Probably power domain, right?&quot; PhD B responded with &quot;I guess it's power domain, yeah.&quot;&#10;&#10;The reason they chose the power domain over the magnitude domain is not explicitly stated in the transcript. However, it can be inferred that they may have chosen the power domain because it is more closely aligned with the theory of spectral subtraction, as Professor C mentioned when he said &quot;if you look at the theory, it should be in the power domain.&quot; Despite this, Professor C also mentions that he has seen implementations where people do it in the magnitude domain and shrug their shoulders, implying that it may still work effectively for some use cases." target="aker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Professor C&#10;Content: I mean , if you look at the theory , it 's {disfmarker} it should be in the power domain but {disfmarker} but , uh , I 've seen implementations where people do it in the magnitude domain&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and {disfmarker}&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: I have asked people why and they shrug their shoulders and say , &quot; oh , it works . &quot; So .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , and there 's this {disfmarker} I guess there 's this mysterious {disfmarker} I mean people who do this a lot I guess have developed little tricks of the trade . I mean , there 's {disfmarker} there 's this , um {disfmarker} you don't just subtract the {disfmarker} the estimate of the noise spectrum . You subtract th that times {">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are subtracting in the power domain. This was mentioned by Professor C and confirmed by PhD B when they were discussing the implementation of spectral subtraction. When Professor C asked &quot;And you're subtracting in the...power domain, or magnitude domain. Probably power domain, right?&quot; PhD B responded with &quot;I guess it's power domain, yeah.&quot;&#10;&#10;The reason they chose the power domain over the magnitude domain is not explicitly stated in the transcript. However, it can be inferred that they may have chosen the power domain because it is more closely aligned with the theory of spectral subtraction, as Professor C mentioned when he said &quot;if you look at the theory, it should be in the power domain.&quot; Despite this, Professor C also mentions that he has seen implementations where people do it in the magnitude domain and shrug their shoulders, implying that it may still work effectively for some use cases." target="isfmarker} and maybe {disfmarker} yeah , maybe the nonlinear discrimination isn't necessary .&#10;Speaker: PhD D&#10;Content: S maybe .&#10;Speaker: PhD B&#10;Content: Yeah . Mm - hmm .&#10;Speaker: Professor C&#10;Content: Could be .&#10;Speaker: PhD D&#10;Content: Maybe .&#10;Speaker: Professor C&#10;Content: Good {disfmarker} good to know . But the other part you were saying was the spectral subtraction , so you just kind of , uh {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: At what stage do you do that ? Do you {disfmarker} you 're doing that , um {disfmarker} ?&#10;Speaker: PhD B&#10;Content: So it would be on the um {disfmarker} on {disfmarker} on the mel frequency bands ,&#10;Speaker: PhD D&#10;Content: We was think&#10;Speaker: PhD B&#10;Content: so . Yeah , be before everything .&#10;Speaker: Professor C&#10;Content: OK ,&#10;Speaker: PhD D&#10;Content: Yeah">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are subtracting in the power domain. This was mentioned by Professor C and confirmed by PhD B when they were discussing the implementation of spectral subtraction. When Professor C asked &quot;And you're subtracting in the...power domain, or magnitude domain. Probably power domain, right?&quot; PhD B responded with &quot;I guess it's power domain, yeah.&quot;&#10;&#10;The reason they chose the power domain over the magnitude domain is not explicitly stated in the transcript. However, it can be inferred that they may have chosen the power domain because it is more closely aligned with the theory of spectral subtraction, as Professor C mentioned when he said &quot;if you look at the theory, it should be in the power domain.&quot; Despite this, Professor C also mentions that he has seen implementations where people do it in the magnitude domain and shrug their shoulders, implying that it may still work effectively for some use cases." target="Based on the transcript, PhD A requires the names, as well as other details, from Professor C in order to create the &quot;key&quot; files. The reason for needing this information is not explicitly stated in the transcript, but it can be inferred that it may be necessary for properly labeling or identifying the filters or data being used.&#10;&#10;PhD A also mentions that they need the forms filled out, even if there are no digits involved. This suggests that the forms may serve a purpose beyond simply collecting numerical data, and may instead be used to gather important information about the design and implementation of the filters.&#10;&#10;Professor C initially expresses some confusion about why the forms are necessary, but PhD A explains that it's important for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used. They also mention that stress (presumably in the context of speech recognition) may not always help distinguish between words, but that getting the sequence right is still important.&#10;&#10;Overall, while the transcript does not provide a lot of detail about why PhD A needs the information they are requesting from Professor C, it is clear that this information is necessary for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are subtracting in the power domain. This was mentioned by Professor C and confirmed by PhD B when they were discussing the implementation of spectral subtraction. When Professor C asked &quot;And you're subtracting in the...power domain, or magnitude domain. Probably power domain, right?&quot; PhD B responded with &quot;I guess it's power domain, yeah.&quot;&#10;&#10;The reason they chose the power domain over the magnitude domain is not explicitly stated in the transcript. However, it can be inferred that they may have chosen the power domain because it is more closely aligned with the theory of spectral subtraction, as Professor C mentioned when he said &quot;if you look at the theory, it should be in the power domain.&quot; Despite this, Professor C also mentions that he has seen implementations where people do it in the magnitude domain and shrug their shoulders, implying that it may still work effectively for some use cases." target="1. Some people who implement spectral subtraction theory may prefer doing it in the magnitude domain because, while the theory suggests that it should be done in the power domain, practical implementations in the magnitude domain can still work effectively for some use cases. This is mentioned by PhD B and Professor C when they discuss different implementations.&#10;2. The &quot;trick&quot; used to subtract the estimate of the noise spectrum in the magnitude domain (instead of the power domain) is not explicitly stated in the transcript. However, it can be inferred that these practitioners might be using some sort of running estimate or average of the noise spectrum and applying a scaling factor based on the signal-to-noise ratio (SNR). This is suggested when PhD B mentions &quot;you don't just subtract the estimate of the noise spectrum. You subtract that times a factor, which is dependent on the SNR.&quot; Additionally, Professor C mentions that the estimation of the power spectra of the noise should be multiplied by a factor depending on the SNR.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers are subtracting in the power domain. This was mentioned by Professor C and confirmed by PhD B when they were discussing the implementation of spectral subtraction. When Professor C asked &quot;And you're subtracting in the...power domain, or magnitude domain. Probably power domain, right?&quot; PhD B responded with &quot;I guess it's power domain, yeah.&quot;&#10;&#10;The reason they chose the power domain over the magnitude domain is not explicitly stated in the transcript. However, it can be inferred that they may have chosen the power domain because it is more closely aligned with the theory of spectral subtraction, as Professor C mentioned when he said &quot;if you look at the theory, it should be in the power domain.&quot; Despite this, Professor C also mentions that he has seen implementations where people do it in the magnitude domain and shrug their shoulders, implying that it may still work effectively for some use cases." target="The issue with obtaining a good estimate of the speech modulation spectrum in this scenario is that if the speech varies significantly in just a few seconds, even with the use of spectral subtraction, getting a decent estimate requires a few seconds of speech. However, if the speech has changed a lot within those few seconds, then using that estimate to subtract the noise could be problematic and may not accurately represent the actual noise present in the signal.&#10;&#10;This is because spectral subtraction relies on having an accurate estimate of the noise spectrum, which can be challenging to obtain if the noise is non-stationary or varying rapidly over time. In such cases, using a running estimate or average of the noise spectrum with a scaling factor based on the signal-to-noise ratio (SNR) may help improve the accuracy of the estimation. However, as mentioned by Professor C in the transcript, this approach may not always work well if there are non-stationary components present in the noise.&#10;&#10;Overall, the challenge lies in developing spectral subtraction methods that can effectively handle non-stationary noise and accurately estimate the speech modulation spectrum in real-world situations where the noise characteristics may vary significantly over time.">
      <data key="d0">1</data>
    </edge>
    <edge source=" um {disfmarker} you don't just subtract the {disfmarker} the estimate of the noise spectrum . You subtract th that times {disfmarker}&#10;Speaker: PhD B&#10;Content: A little bit more and {disfmarker} Yeah .&#10;Speaker: Professor C&#10;Content: Or {disfmarker} or less , or {disfmarker}&#10;Speaker: PhD A&#10;Content: Really ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Huh !&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And generated this {disfmarker} this ,&#10;Speaker: Professor C&#10;Content: Uh .&#10;Speaker: PhD B&#10;Content: um , so you have the estimation of the power spectra of the noise , and you multiply this by a factor which is depend dependent on the SNR . So . Well .&#10;Speaker: PhD D&#10;Content: Hmm , maybe .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: PhD B&#10;Content: When the speech lev when the signal level is more important , compared to this noise level ," target="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source=" um {disfmarker} you don't just subtract the {disfmarker} the estimate of the noise spectrum . You subtract th that times {disfmarker}&#10;Speaker: PhD B&#10;Content: A little bit more and {disfmarker} Yeah .&#10;Speaker: Professor C&#10;Content: Or {disfmarker} or less , or {disfmarker}&#10;Speaker: PhD A&#10;Content: Really ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Huh !&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And generated this {disfmarker} this ,&#10;Speaker: Professor C&#10;Content: Uh .&#10;Speaker: PhD B&#10;Content: um , so you have the estimation of the power spectra of the noise , and you multiply this by a factor which is depend dependent on the SNR . So . Well .&#10;Speaker: PhD D&#10;Content: Hmm , maybe .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: PhD B&#10;Content: When the speech lev when the signal level is more important , compared to this noise level ," target="1. Stationarity of the noise: If the noise is fairly stationary, then it is better to use as much data as possible to get a more accurate estimate of the noise and subtract it off. However, if the noise is non-stationary (which is the case for most real situations), then an online estimation with some forgetting factor needs to be used.&#10;2. Availability of good noise estimation: A good estimation of the noise is necessary for spectral subtraction methods to work effectively. However, getting a good estimate of the noise can be challenging, especially if the noise is non-stationary.&#10;3. Type of noise: The decision between using a running estimate or a stationary noise estimate may also depend on the type of noise present in the signal. For example, sinusoidally modulated amplitude modulated noise may pose a challenge in practice.&#10;4. Use of low-level detectors: The EM algorithm is used to determine the training targets for the low-level detectors used in the analysis, which suggests that these detectors play a role in the decision between using a running estimate or a stationary noise estimate.&#10;5. Signal level importance: The choice between using a running estimate or a stationary noise estimate may also depend on the relative importance of the signal level compared to the noise level. When the signal level is more important, a different approach may be necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source=" um {disfmarker} you don't just subtract the {disfmarker} the estimate of the noise spectrum . You subtract th that times {disfmarker}&#10;Speaker: PhD B&#10;Content: A little bit more and {disfmarker} Yeah .&#10;Speaker: Professor C&#10;Content: Or {disfmarker} or less , or {disfmarker}&#10;Speaker: PhD A&#10;Content: Really ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Huh !&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And generated this {disfmarker} this ,&#10;Speaker: Professor C&#10;Content: Uh .&#10;Speaker: PhD B&#10;Content: um , so you have the estimation of the power spectra of the noise , and you multiply this by a factor which is depend dependent on the SNR . So . Well .&#10;Speaker: PhD D&#10;Content: Hmm , maybe .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: PhD B&#10;Content: When the speech lev when the signal level is more important , compared to this noise level ," target="1. Some people who implement spectral subtraction theory may prefer doing it in the magnitude domain because, while the theory suggests that it should be done in the power domain, practical implementations in the magnitude domain can still work effectively for some use cases. This is mentioned by PhD B and Professor C when they discuss different implementations.&#10;2. The &quot;trick&quot; used to subtract the estimate of the noise spectrum in the magnitude domain (instead of the power domain) is not explicitly stated in the transcript. However, it can be inferred that these practitioners might be using some sort of running estimate or average of the noise spectrum and applying a scaling factor based on the signal-to-noise ratio (SNR). This is suggested when PhD B mentions &quot;you don't just subtract the estimate of the noise spectrum. You subtract that times a factor, which is dependent on the SNR.&quot; Additionally, Professor C mentions that the estimation of the power spectra of the noise should be multiplied by a factor depending on the SNR.">
      <data key="d0">1</data>
    </edge>
    <edge source=" um {disfmarker} you don't just subtract the {disfmarker} the estimate of the noise spectrum . You subtract th that times {disfmarker}&#10;Speaker: PhD B&#10;Content: A little bit more and {disfmarker} Yeah .&#10;Speaker: Professor C&#10;Content: Or {disfmarker} or less , or {disfmarker}&#10;Speaker: PhD A&#10;Content: Really ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Huh !&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And generated this {disfmarker} this ,&#10;Speaker: Professor C&#10;Content: Uh .&#10;Speaker: PhD B&#10;Content: um , so you have the estimation of the power spectra of the noise , and you multiply this by a factor which is depend dependent on the SNR . So . Well .&#10;Speaker: PhD D&#10;Content: Hmm , maybe .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: PhD B&#10;Content: When the speech lev when the signal level is more important , compared to this noise level ," target="The issue with obtaining a good estimate of the speech modulation spectrum in this scenario is that if the speech varies significantly in just a few seconds, even with the use of spectral subtraction, getting a decent estimate requires a few seconds of speech. However, if the speech has changed a lot within those few seconds, then using that estimate to subtract the noise could be problematic and may not accurately represent the actual noise present in the signal.&#10;&#10;This is because spectral subtraction relies on having an accurate estimate of the noise spectrum, which can be challenging to obtain if the noise is non-stationary or varying rapidly over time. In such cases, using a running estimate or average of the noise spectrum with a scaling factor based on the signal-to-noise ratio (SNR) may help improve the accuracy of the estimation. However, as mentioned by Professor C in the transcript, this approach may not always work well if there are non-stationary components present in the noise.&#10;&#10;Overall, the challenge lies in developing spectral subtraction methods that can effectively handle non-stationary noise and accurately estimate the speech modulation spectrum in real-world situations where the noise characteristics may vary significantly over time.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: That 's what spectral subtraction will help with , practically speaking .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: If it varies a lot , to get a If {disfmarker} if {disfmarker} to get a good estimate you need a few seconds of speech , even if it 's centered , right ?&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: if you need a few seconds to get a decent estimate but it 's changed a lot in a few seconds , then it , you know , i it 's kind of a problem .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: I mean , imagine e five hertz is the middle of the {disfmarker} of the speech modulation spectrum ,&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: right ? So imagine a jack hammer going at five hertz .&#10;Speaker: PhD B&#10;Content: Yeah , that 's {disfmarker}&#10;Speaker:" target="The issue with obtaining a good estimate of the speech modulation spectrum in this scenario is that if the speech varies significantly in just a few seconds, even with the use of spectral subtraction, getting a decent estimate requires a few seconds of speech. However, if the speech has changed a lot within those few seconds, then using that estimate to subtract the noise could be problematic and may not accurately represent the actual noise present in the signal.&#10;&#10;This is because spectral subtraction relies on having an accurate estimate of the noise spectrum, which can be challenging to obtain if the noise is non-stationary or varying rapidly over time. In such cases, using a running estimate or average of the noise spectrum with a scaling factor based on the signal-to-noise ratio (SNR) may help improve the accuracy of the estimation. However, as mentioned by Professor C in the transcript, this approach may not always work well if there are non-stationary components present in the noise.&#10;&#10;Overall, the challenge lies in developing spectral subtraction methods that can effectively handle non-stationary noise and accurately estimate the speech modulation spectrum in real-world situations where the noise characteristics may vary significantly over time.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Professor C&#10;Content: I mean , if you look at the theory , it 's {disfmarker} it should be in the power domain but {disfmarker} but , uh , I 've seen implementations where people do it in the magnitude domain&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and {disfmarker}&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: I have asked people why and they shrug their shoulders and say , &quot; oh , it works . &quot; So .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , and there 's this {disfmarker} I guess there 's this mysterious {disfmarker} I mean people who do this a lot I guess have developed little tricks of the trade . I mean , there 's {disfmarker} there 's this , um {disfmarker} you don't just subtract the {disfmarker} the estimate of the noise spectrum . You subtract th that times {" target="1. Some people who implement spectral subtraction theory may prefer doing it in the magnitude domain because, while the theory suggests that it should be done in the power domain, practical implementations in the magnitude domain can still work effectively for some use cases. This is mentioned by PhD B and Professor C when they discuss different implementations.&#10;2. The &quot;trick&quot; used to subtract the estimate of the noise spectrum in the magnitude domain (instead of the power domain) is not explicitly stated in the transcript. However, it can be inferred that these practitioners might be using some sort of running estimate or average of the noise spectrum and applying a scaling factor based on the signal-to-noise ratio (SNR). This is suggested when PhD B mentions &quot;you don't just subtract the estimate of the noise spectrum. You subtract that times a factor, which is dependent on the SNR.&quot; Additionally, Professor C mentions that the estimation of the power spectra of the noise should be multiplied by a factor depending on the SNR.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Human interaction and text-based conversation systems differ in conversational dynamics due to factors such as turn-taking, overlapping utterances, and reaction time. In human interaction, individuals often step on each other's utterances, creating a more fluid and simultaneous exchange of information. However, text-based systems typically involve distinct turns with no overlap or immediate response, which can feel abrupt or unnatural.&#10;&#10;2. To improve the flow and reduce abruptness in a text-based conversation system, implementing techniques that mimic human conversational dynamics could be beneficial. For instance, incorporating delays between responses to simulate thinking time or introducing a level of overlapping utterances by allowing multiple responses at once can help create a more natural feel.&#10;&#10;3. The proposal discussed during the video conference aims to address these issues by having a third party run Voice Activity Detection (VAD) and establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness. However, this idea needs further discussion and understanding before implementation." target=" to get back to you ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: I mean , {vocalsound} it would be f I mean , it might even be too abrupt . You might have to put in a s a s {vocalsound} a delay .&#10;Speaker: PhD A&#10;Content: Yeah . I mean , it may feel different than talking to a person&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: because when we talk to each other we tend to step on each other 's utterances . So like if I 'm asking you a question , you may start answering before I 'm even done .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So it {disfmarker} it would probably feel different&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: but I don't think it would feel slow .&#10;Speaker: Professor C&#10;Content: Right . Well , anyway , I mean , I think {disfmarker} we could cut {disfmarker} we know what else , we could cut">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Human interaction and text-based conversation systems differ in conversational dynamics due to factors such as turn-taking, overlapping utterances, and reaction time. In human interaction, individuals often step on each other's utterances, creating a more fluid and simultaneous exchange of information. However, text-based systems typically involve distinct turns with no overlap or immediate response, which can feel abrupt or unnatural.&#10;&#10;2. To improve the flow and reduce abruptness in a text-based conversation system, implementing techniques that mimic human conversational dynamics could be beneficial. For instance, incorporating delays between responses to simulate thinking time or introducing a level of overlapping utterances by allowing multiple responses at once can help create a more natural feel.&#10;&#10;3. The proposal discussed during the video conference aims to address these issues by having a third party run Voice Activity Detection (VAD) and establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness. However, this idea needs further discussion and understanding before implementation." target="Human ability to perceive the difference between various short time intervals in conversation is quite limited. As mentioned in the transcript, Dr. A stated that people might not be able to tell the difference between a quarter of a second and 100 milliseconds or even half a second. They further expressed that responses often feel so quick, which indicates that humans can perceive small time differences but may not accurately differentiate them.&#10;&#10;This perception affects human communication in several ways. It impacts turn-taking and the fluidity of conversations. In text-based systems, delays between responses can make interactions feel abrupt or unnatural; however, in human interaction, individuals tend to step on each other's utterances creating a more simultaneous exchange of information. The limitations in perceiving short time intervals contribute to these conversational dynamics and create challenges for designing natural and efficient text-based conversation systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Human interaction and text-based conversation systems differ in conversational dynamics due to factors such as turn-taking, overlapping utterances, and reaction time. In human interaction, individuals often step on each other's utterances, creating a more fluid and simultaneous exchange of information. However, text-based systems typically involve distinct turns with no overlap or immediate response, which can feel abrupt or unnatural.&#10;&#10;2. To improve the flow and reduce abruptness in a text-based conversation system, implementing techniques that mimic human conversational dynamics could be beneficial. For instance, incorporating delays between responses to simulate thinking time or introducing a level of overlapping utterances by allowing multiple responses at once can help create a more natural feel.&#10;&#10;3. The proposal discussed during the video conference aims to address these issues by having a third party run Voice Activity Detection (VAD) and establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness. However, this idea needs further discussion and understanding before implementation." target="1. Smooth spectral envelopes may not be the best option for determining sonorant or non-sonorant sounds in speech recognition, as they focus more on the overall envelope rather than the periodicity of the signal. An alternative approach could involve building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information.&#10;2. To improve speech quality and reduce reverberation in a recording, an all-pole filter can be used in conjunction with a reconstructed excitation signal that has been processed to minimize distortion caused by reverberation. This technique involves identifying the linear time-invariant system between two microphones and using echo cancellation techniques to correct for it.&#10;3. Incorporating delays between responses in text-based conversation systems can help mimic human conversational dynamics, reducing abruptness and creating a more natural feel. Additionally, implementing techniques such as Voice Activity Detection (VAD) can establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness.&#10;4. Other possible approaches to improving speech quality and reducing reverberation include adding a low-pass filter at 25 Hz to supplement existing LDA filters, incorporating a ten-millisecond filter into the system, and minimizing delay caused by online normalization using recursion on the feature stream. These techniques aim to address shortcomings in LDA filters, delay compensation, and online normalization.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the provided transcript, there was no concrete outcome or decision made during the conference talk related to Voice Activity Detection (VAD) and determining boundaries. The participants discussed the possibility of having a third party run a VAD and then doing recognition based on those boundaries, but they did not reach a conclusion. They also mentioned that they had not been informed about this proposal beforehand, which caused some confusion during the meeting. It appears that further discussions and decisions will take place in subsequent meetings or communications." target="Based on the transcript provided, the speakers conducted separate Viterbi analyses on different sub-band labels. However, they did not explicitly discuss the outcome or impact of this action on the overall training process. Therefore, it is not possible to accurately answer this question without additional information. The transcript only indicates that the separate Viterbi analysis was done but does not provide any details about the results or effects of this process.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that both PhD B and Sunil are involved in designing new IIR filters to improve the bugs and latency issues. They used LDA filters and FIR filters as a starting point for their design. This information is conveyed in several parts of the transcript, including when PhD B says &quot;we took the LDA filters and we designed new filters, using recursive filters actually&quot; and &quot;so we took the filters the FIR filters and we designed IIR filters that have the same frequency response&quot;. Additionally, when Professor C asks &quot;when you say 'we', is that something Sunil is doing or is that?&quot; PhD B responds by saying &quot;I'm sorry?&quot; which suggests that they are including Sunil in their use of the word &quot;we&quot;. This is later confirmed when Professor C asks &quot;You had a discussion with Sunil about this though?&quot; and PhD B responds by saying &quot;Uh - huh.&quot;" target=" . So what are you doing ?&#10;Speaker: PhD B&#10;Content: Mm - hmm . Uh , well , we 've {disfmarker} a little bit worked on trying to see , uh , what were the bugs and the problem with the latencies .&#10;Speaker: PhD D&#10;Content: To improve {disfmarker}&#10;Speaker: PhD B&#10;Content: So , We took {disfmarker} first we took the LDA filters and , {vocalsound} uh , we designed new filters , using uh recursive filters actually .&#10;Speaker: Professor C&#10;Content: So when you say &quot; we &quot; , is that something Sunil is doing or is that {disfmarker} ?&#10;Speaker: PhD B&#10;Content: I 'm sorry ?&#10;Speaker: Professor C&#10;Content: Who is doing that ?&#10;Speaker: PhD B&#10;Content: Uh , us . Yeah .&#10;Speaker: Professor C&#10;Content: Oh , oh . Oh , OK .&#10;Speaker: PhD B&#10;Content: So we took the filters {disfmarker} the FIR filters {vocalsound} and we {comment} designed , uh , IIR filters that">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that both PhD B and Sunil are involved in designing new IIR filters to improve the bugs and latency issues. They used LDA filters and FIR filters as a starting point for their design. This information is conveyed in several parts of the transcript, including when PhD B says &quot;we took the LDA filters and we designed new filters, using recursive filters actually&quot; and &quot;so we took the filters the FIR filters and we designed IIR filters that have the same frequency response&quot;. Additionally, when Professor C asks &quot;when you say 'we', is that something Sunil is doing or is that?&quot; PhD B responds by saying &quot;I'm sorry?&quot; which suggests that they are including Sunil in their use of the word &quot;we&quot;. This is later confirmed when Professor C asks &quot;You had a discussion with Sunil about this though?&quot; and PhD B responds by saying &quot;Uh - huh.&quot;" target=" So we took the filters {disfmarker} the FIR filters {vocalsound} and we {comment} designed , uh , IIR filters that have the same frequency response .&#10;Speaker: PhD D&#10;Content: But {disfmarker}&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Well , similar , but that have shorter delays .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So they had two filters , one for the low frequency bands and another for the high frequency bands . And so we redesigned two filters . And the low frequency band has sixty - four milliseconds of delay , and the high frequency band filter has something like eleven milliseconds compared to the two hundred milliseconds of the IIR filters . But it 's not yet test . So we have the filters but we still have to implement a routine that does recursive filtering&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Professor C&#10;Content: You {disfmarker} you had a discussion with Sunil about this though ?&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that both PhD B and Sunil are involved in designing new IIR filters to improve the bugs and latency issues. They used LDA filters and FIR filters as a starting point for their design. This information is conveyed in several parts of the transcript, including when PhD B says &quot;we took the LDA filters and we designed new filters, using recursive filters actually&quot; and &quot;so we took the filters the FIR filters and we designed IIR filters that have the same frequency response&quot;. Additionally, when Professor C asks &quot;when you say 'we', is that something Sunil is doing or is that?&quot; PhD B responds by saying &quot;I'm sorry?&quot; which suggests that they are including Sunil in their use of the word &quot;we&quot;. This is later confirmed when Professor C asks &quot;You had a discussion with Sunil about this though?&quot; and PhD B responds by saying &quot;Uh - huh.&quot;" target="&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: well , a low - pass filter at {disfmarker} at twenty - five hertz . Uh , because wh when {disfmarker} when we look at the LDA filters , well , they are basically low - pass but they leave a lot of what 's above twenty - five hertz .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Um , and so , yeah , this will be another filter which would add ten milliseconds again .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Um , yeah , and then there 's a third thing , is that , um , basically the way on - line normalization was done uh , is just using this recursion on {disfmarker} on the um , um , on the feature stream ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: and {disfmarker} but this is a filter , so it has also a delay . Uh , and when we look at this filter actually it has a delay of eighty - five milliseconds . So if">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that both PhD B and Sunil are involved in designing new IIR filters to improve the bugs and latency issues. They used LDA filters and FIR filters as a starting point for their design. This information is conveyed in several parts of the transcript, including when PhD B says &quot;we took the LDA filters and we designed new filters, using recursive filters actually&quot; and &quot;so we took the filters the FIR filters and we designed IIR filters that have the same frequency response&quot;. Additionally, when Professor C asks &quot;when you say 'we', is that something Sunil is doing or is that?&quot; PhD B responds by saying &quot;I'm sorry?&quot; which suggests that they are including Sunil in their use of the word &quot;we&quot;. This is later confirmed when Professor C asks &quot;You had a discussion with Sunil about this though?&quot; and PhD B responds by saying &quot;Uh - huh.&quot;" target=" - six milliseconds ,&#10;Speaker: PhD D&#10;Content: The low f f&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: which , uh {disfmarker} What was the total we ended up with through the whole system ?&#10;Speaker: PhD B&#10;Content: Three hundred and thirty .&#10;Speaker: Professor C&#10;Content: So that would be within {disfmarker} ?&#10;Speaker: PhD B&#10;Content: Yeah , but there are other points actually , uh , which will perhaps add some more delay . Is that some other {disfmarker} other stuff in the process were perhaps not very {disfmarker} um perf well , not very correct , like the downsampling which w was simply dropping frames .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Um , so we will try also to add a nice downsampling having a filter that {disfmarker} that {disfmarker}&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: well , a low - pass filter at {disfmarker} at twenty">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that both PhD B and Sunil are involved in designing new IIR filters to improve the bugs and latency issues. They used LDA filters and FIR filters as a starting point for their design. This information is conveyed in several parts of the transcript, including when PhD B says &quot;we took the LDA filters and we designed new filters, using recursive filters actually&quot; and &quot;so we took the filters the FIR filters and we designed IIR filters that have the same frequency response&quot;. Additionally, when Professor C asks &quot;when you say 'we', is that something Sunil is doing or is that?&quot; PhD B responds by saying &quot;I'm sorry?&quot; which suggests that they are including Sunil in their use of the word &quot;we&quot;. This is later confirmed when Professor C asks &quot;You had a discussion with Sunil about this though?&quot; and PhD B responds by saying &quot;Uh - huh.&quot;" target="1. PhD B and Sunil designed IIR filters that have the same frequency response as the FIR filters but with shorter delays.&#10;2. The low frequency band filter has a delay of 64 milliseconds, and the high frequency band filter has a delay of approximately 11 milliseconds.&#10;3. This is compared to the original IIR filters, which have a delay of 200 milliseconds in both low and high frequency bands. However, these delays for the new filters are not yet tested.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it appears that both PhD B and Sunil are involved in designing new IIR filters to improve the bugs and latency issues. They used LDA filters and FIR filters as a starting point for their design. This information is conveyed in several parts of the transcript, including when PhD B says &quot;we took the LDA filters and we designed new filters, using recursive filters actually&quot; and &quot;so we took the filters the FIR filters and we designed IIR filters that have the same frequency response&quot;. Additionally, when Professor C asks &quot;when you say 'we', is that something Sunil is doing or is that?&quot; PhD B responds by saying &quot;I'm sorry?&quot; which suggests that they are including Sunil in their use of the word &quot;we&quot;. This is later confirmed when Professor C asks &quot;You had a discussion with Sunil about this though?&quot; and PhD B responds by saying &quot;Uh - huh.&quot;" target="Based on the transcript, PhD A requires the names, as well as other details, from Professor C in order to create the &quot;key&quot; files. The reason for needing this information is not explicitly stated in the transcript, but it can be inferred that it may be necessary for properly labeling or identifying the filters or data being used.&#10;&#10;PhD A also mentions that they need the forms filled out, even if there are no digits involved. This suggests that the forms may serve a purpose beyond simply collecting numerical data, and may instead be used to gather important information about the design and implementation of the filters.&#10;&#10;Professor C initially expresses some confusion about why the forms are necessary, but PhD A explains that it's important for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used. They also mention that stress (presumably in the context of speech recognition) may not always help distinguish between words, but that getting the sequence right is still important.&#10;&#10;Overall, while the transcript does not provide a lot of detail about why PhD A needs the information they are requesting from Professor C, it is clear that this information is necessary for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used.">
      <data key="d0">1</data>
    </edge>
    <edge source=" . So what are you doing ?&#10;Speaker: PhD B&#10;Content: Mm - hmm . Uh , well , we 've {disfmarker} a little bit worked on trying to see , uh , what were the bugs and the problem with the latencies .&#10;Speaker: PhD D&#10;Content: To improve {disfmarker}&#10;Speaker: PhD B&#10;Content: So , We took {disfmarker} first we took the LDA filters and , {vocalsound} uh , we designed new filters , using uh recursive filters actually .&#10;Speaker: Professor C&#10;Content: So when you say &quot; we &quot; , is that something Sunil is doing or is that {disfmarker} ?&#10;Speaker: PhD B&#10;Content: I 'm sorry ?&#10;Speaker: Professor C&#10;Content: Who is doing that ?&#10;Speaker: PhD B&#10;Content: Uh , us . Yeah .&#10;Speaker: Professor C&#10;Content: Oh , oh . Oh , OK .&#10;Speaker: PhD B&#10;Content: So we took the filters {disfmarker} the FIR filters {vocalsound} and we {comment} designed , uh , IIR filters that" target="1. PhD B and Sunil designed IIR filters that have the same frequency response as the FIR filters but with shorter delays.&#10;2. The low frequency band filter has a delay of 64 milliseconds, and the high frequency band filter has a delay of approximately 11 milliseconds.&#10;3. This is compared to the original IIR filters, which have a delay of 200 milliseconds in both low and high frequency bands. However, these delays for the new filters are not yet tested.">
      <data key="d0">1</data>
    </edge>
    <edge source=" So we took the filters {disfmarker} the FIR filters {vocalsound} and we {comment} designed , uh , IIR filters that have the same frequency response .&#10;Speaker: PhD D&#10;Content: But {disfmarker}&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Well , similar , but that have shorter delays .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So they had two filters , one for the low frequency bands and another for the high frequency bands . And so we redesigned two filters . And the low frequency band has sixty - four milliseconds of delay , and the high frequency band filter has something like eleven milliseconds compared to the two hundred milliseconds of the IIR filters . But it 's not yet test . So we have the filters but we still have to implement a routine that does recursive filtering&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Professor C&#10;Content: You {disfmarker} you had a discussion with Sunil about this though ?&#10;Spe" target="1. PhD B and Sunil designed IIR filters that have the same frequency response as the FIR filters but with shorter delays.&#10;2. The low frequency band filter has a delay of 64 milliseconds, and the high frequency band filter has a delay of approximately 11 milliseconds.&#10;3. This is compared to the original IIR filters, which have a delay of 200 milliseconds in both low and high frequency bands. However, these delays for the new filters are not yet tested.">
      <data key="d0">1</data>
    </edge>
    <edge source=" So we took the filters {disfmarker} the FIR filters {vocalsound} and we {comment} designed , uh , IIR filters that have the same frequency response .&#10;Speaker: PhD D&#10;Content: But {disfmarker}&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: Well , similar , but that have shorter delays .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So they had two filters , one for the low frequency bands and another for the high frequency bands . And so we redesigned two filters . And the low frequency band has sixty - four milliseconds of delay , and the high frequency band filter has something like eleven milliseconds compared to the two hundred milliseconds of the IIR filters . But it 's not yet test . So we have the filters but we still have to implement a routine that does recursive filtering&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Professor C&#10;Content: You {disfmarker} you had a discussion with Sunil about this though ?&#10;Spe" target="1. The proposed solution to address the limitations of LDA filters involves adding an additional low-pass filter at 25 Hz. This filter will act as a supplementary measure to the existing LDA filters, which are essentially low-pass but allow high frequency components above 25 Hz to pass through.&#10;2. A ten-millisecond filter will be incorporated into the system in addition to the low-pass filter. However, the specifics of this filter and its purpose have not been explicitly mentioned in the transcript.&#10;3. The solution also aims to tackle the delay caused by online normalization using recursion on the feature stream. It has been noted that this method of normalization introduces a delay of 85 milliseconds, which needs to be accounted for and minimized in order to improve the system's performance.&#10;4. The transcript suggests that these changes are aimed at improving the overall efficiency and accuracy of the speech recognition or processing system by addressing the shortcomings of LDA filters, delay compensation, and online normalization. However, it is important to note that these proposed solutions have not been fully implemented or tested yet, as mentioned in the conversation between PhD B and Professor C.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: well , a low - pass filter at {disfmarker} at twenty - five hertz . Uh , because wh when {disfmarker} when we look at the LDA filters , well , they are basically low - pass but they leave a lot of what 's above twenty - five hertz .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Um , and so , yeah , this will be another filter which would add ten milliseconds again .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Um , yeah , and then there 's a third thing , is that , um , basically the way on - line normalization was done uh , is just using this recursion on {disfmarker} on the um , um , on the feature stream ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: and {disfmarker} but this is a filter , so it has also a delay . Uh , and when we look at this filter actually it has a delay of eighty - five milliseconds . So if" target="1. The proposed solution to address the limitations of LDA filters involves adding an additional low-pass filter at 25 Hz. This filter will act as a supplementary measure to the existing LDA filters, which are essentially low-pass but allow high frequency components above 25 Hz to pass through.&#10;2. A ten-millisecond filter will be incorporated into the system in addition to the low-pass filter. However, the specifics of this filter and its purpose have not been explicitly mentioned in the transcript.&#10;3. The solution also aims to tackle the delay caused by online normalization using recursion on the feature stream. It has been noted that this method of normalization introduces a delay of 85 milliseconds, which needs to be accounted for and minimized in order to improve the system's performance.&#10;4. The transcript suggests that these changes are aimed at improving the overall efficiency and accuracy of the speech recognition or processing system by addressing the shortcomings of LDA filters, delay compensation, and online normalization. However, it is important to note that these proposed solutions have not been fully implemented or tested yet, as mentioned in the conversation between PhD B and Professor C.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. PhD B and Sunil designed IIR filters that have the same frequency response as the FIR filters but with shorter delays.&#10;2. The low frequency band filter has a delay of 64 milliseconds, and the high frequency band filter has a delay of approximately 11 milliseconds.&#10;3. This is compared to the original IIR filters, which have a delay of 200 milliseconds in both low and high frequency bands. However, these delays for the new filters are not yet tested." target="Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So .&#10;Speaker: PhD B&#10;Content: Alright .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Um , Yeah . Well , there is w one , um , remark about these filters , that they don't have a linear phase . So ,&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: Well , I don't know , perhaps it {disfmarker} perhaps it doesn't hurt because the phase is almost linear but . Um , and so , yeah , for the delay I gave you here , it 's {disfmarker} it 's , uh , computed on the five hertz modulation frequency , which is the {disfmarker} mmm , well , the most important for speech so . Uh , this is the first thing .&#10;Speaker: Professor C&#10;Content: So that would be , uh , a reduction of a hundred and thirty - six milliseconds ,&#10;Speaker: PhD D&#10;Content: The low f f&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. PhD B and Sunil designed IIR filters that have the same frequency response as the FIR filters but with shorter delays.&#10;2. The low frequency band filter has a delay of 64 milliseconds, and the high frequency band filter has a delay of approximately 11 milliseconds.&#10;3. This is compared to the original IIR filters, which have a delay of 200 milliseconds in both low and high frequency bands. However, these delays for the new filters are not yet tested." target=" , all pole filter that you get out of that should be good . It 's just the , um , excitation signal {comment} that is going to be distorted by the reverberation and so you can try and reconstruct a better excitation signal and , um , feed that through the i um , all pole filter and get enhanced speech with reverberation reduced .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: There 's also this , uh , um , uh , echo cancellation stuff that we 've sort of been chasing , so , uh we have , uh {disfmarker} and when we 're saying these digits now we do have a close microphone signal and then there 's the distant microphone signal . And you could as a kind of baseline say , &quot; OK , given that we have both of these , uh , we should be able to do , uh , a cancellation . &quot; So that , uh , um , we {disfmarker} we , uh , essentially identify the system in between {disfmarker} the linear time invariant system between the microphones and {disfmarker} and {disfmarker} and">
      <data key="d0">1</data>
    </edge>
    <edge source="1. PhD B and Sunil designed IIR filters that have the same frequency response as the FIR filters but with shorter delays.&#10;2. The low frequency band filter has a delay of 64 milliseconds, and the high frequency band filter has a delay of approximately 11 milliseconds.&#10;3. This is compared to the original IIR filters, which have a delay of 200 milliseconds in both low and high frequency bands. However, these delays for the new filters are not yet tested." target="1. The proposed solution to address the limitations of LDA filters involves adding an additional low-pass filter at 25 Hz. This filter will act as a supplementary measure to the existing LDA filters, which are essentially low-pass but allow high frequency components above 25 Hz to pass through.&#10;2. A ten-millisecond filter will be incorporated into the system in addition to the low-pass filter. However, the specifics of this filter and its purpose have not been explicitly mentioned in the transcript.&#10;3. The solution also aims to tackle the delay caused by online normalization using recursion on the feature stream. It has been noted that this method of normalization introduces a delay of 85 milliseconds, which needs to be accounted for and minimized in order to improve the system's performance.&#10;4. The transcript suggests that these changes are aimed at improving the overall efficiency and accuracy of the speech recognition or processing system by addressing the shortcomings of LDA filters, delay compensation, and online normalization. However, it is important to note that these proposed solutions have not been fully implemented or tested yet, as mentioned in the conversation between PhD B and Professor C.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So .&#10;Speaker: PhD B&#10;Content: Alright .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Um , Yeah . Well , there is w one , um , remark about these filters , that they don't have a linear phase . So ,&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: Well , I don't know , perhaps it {disfmarker} perhaps it doesn't hurt because the phase is almost linear but . Um , and so , yeah , for the delay I gave you here , it 's {disfmarker} it 's , uh , computed on the five hertz modulation frequency , which is the {disfmarker} mmm , well , the most important for speech so . Uh , this is the first thing .&#10;Speaker: Professor C&#10;Content: So that would be , uh , a reduction of a hundred and thirty - six milliseconds ,&#10;Speaker: PhD D&#10;Content: The low f f&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker:" target="Human ability to perceive the difference between various short time intervals in conversation is quite limited. As mentioned in the transcript, Dr. A stated that people might not be able to tell the difference between a quarter of a second and 100 milliseconds or even half a second. They further expressed that responses often feel so quick, which indicates that humans can perceive small time differences but may not accurately differentiate them.&#10;&#10;This perception affects human communication in several ways. It impacts turn-taking and the fluidity of conversations. In text-based systems, delays between responses can make interactions feel abrupt or unnatural; however, in human interaction, individuals tend to step on each other's utterances creating a more simultaneous exchange of information. The limitations in perceiving short time intervals contribute to these conversational dynamics and create challenges for designing natural and efficient text-based conversation systems.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , all pole filter that you get out of that should be good . It 's just the , um , excitation signal {comment} that is going to be distorted by the reverberation and so you can try and reconstruct a better excitation signal and , um , feed that through the i um , all pole filter and get enhanced speech with reverberation reduced .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: There 's also this , uh , um , uh , echo cancellation stuff that we 've sort of been chasing , so , uh we have , uh {disfmarker} and when we 're saying these digits now we do have a close microphone signal and then there 's the distant microphone signal . And you could as a kind of baseline say , &quot; OK , given that we have both of these , uh , we should be able to do , uh , a cancellation . &quot; So that , uh , um , we {disfmarker} we , uh , essentially identify the system in between {disfmarker} the linear time invariant system between the microphones and {disfmarker} and {disfmarker} and" target="1. Smooth spectral envelopes may not be the best option for determining sonorant or non-sonorant sounds in speech recognition, as they focus more on the overall envelope rather than the periodicity of the signal. An alternative approach could involve building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information.&#10;2. To improve speech quality and reduce reverberation in a recording, an all-pole filter can be used in conjunction with a reconstructed excitation signal that has been processed to minimize distortion caused by reverberation. This technique involves identifying the linear time-invariant system between two microphones and using echo cancellation techniques to correct for it.&#10;3. Incorporating delays between responses in text-based conversation systems can help mimic human conversational dynamics, reducing abruptness and creating a more natural feel. Additionally, implementing techniques such as Voice Activity Detection (VAD) can establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness.&#10;4. Other possible approaches to improving speech quality and reducing reverberation include adding a low-pass filter at 25 Hz to supplement existing LDA filters, incorporating a ten-millisecond filter into the system, and minimizing delay caused by online normalization using recursion on the feature stream. These techniques aim to address shortcomings in LDA filters, delay compensation, and online normalization.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , all pole filter that you get out of that should be good . It 's just the , um , excitation signal {comment} that is going to be distorted by the reverberation and so you can try and reconstruct a better excitation signal and , um , feed that through the i um , all pole filter and get enhanced speech with reverberation reduced .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: There 's also this , uh , um , uh , echo cancellation stuff that we 've sort of been chasing , so , uh we have , uh {disfmarker} and when we 're saying these digits now we do have a close microphone signal and then there 's the distant microphone signal . And you could as a kind of baseline say , &quot; OK , given that we have both of these , uh , we should be able to do , uh , a cancellation . &quot; So that , uh , um , we {disfmarker} we , uh , essentially identify the system in between {disfmarker} the linear time invariant system between the microphones and {disfmarker} and {disfmarker} and" target="1. The focus of the individual's work is to improve far-microphone speech recognition, specifically addressing reverberation in the signal. They have experimented with methods such as MSG and PLP, which are robust against reverberation, but they are also interested in comparing these approaches with speech enhancement or de-reverberation techniques that attempt to remove reverberation from the signal directly.&#10;2. Echo cancellation is a technique being explored by the individual. By identifying the linear time-invariant system between two microphones and using echo cancellation techniques, it is possible to correct for the distortion caused by reverberation. The individual aims to use this method in conjunction with an all-pole filter and a processed excitation signal to reduce reverberation.&#10;3. The individual has also been working on training neural networks under different reverberation conditions, trying to find the best network configuration for each scenario. They have experimented with training a network using various reverberation settings and then selecting the most effective configuration based on performance.&#10;4. Lastly, there is discussion of exploring &quot;multiple mike things&quot; where all microphones are at a distance from the speaker, particularly in conditions where there may be an obstruction between them, creating a helpful shadow effect.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis." target="Content: the one o&#10;Speaker: Professor C&#10;Content: So that 's all it is . It 's a sig it 's a sigmoid ,&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: uh , with weighted sum at the input ,&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: which you train by gradient {pause} descent .&#10;Speaker: Grad E&#10;Content: Right . Yeah , so he uses , um , an EM algorithm to {disfmarker} to um train up these um parameters for the logistic regression .&#10;Speaker: Professor C&#10;Content: Well , actually , yeah ,&#10;Speaker: Grad E&#10;Content: The {disfmarker}&#10;Speaker: Professor C&#10;Content: so I was using EM to get the targets . So {disfmarker} so you have this {disfmarker} this {disfmarker} this AND gate {disfmarker} what we were calling an AND gate , but it 's a product {disfmarker} product rule thing at the output . And then he uses , uh , i u and">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis." target="1. The tricky part about calculating signal-to-noise (SNR) ratio, as discussed by the speakers, is that sometimes the signal and noise can overlap in the histogram, making it difficult to distinguish between the two. To address this issue, they suggest an alternative method for determining means only in the EM algorithm, which appears to be more reliable for estimating SNR.&#10;2. The alternative method for determining means only in the EM algorithm involves using an iterative approach similar to the EM algorithm (referred to as an &quot;EM-like thing&quot;). This method focuses on estimating the means of the signal and noise distributions without concerning itself with the variances. Once the mean values are determined, they can be used as the signal-to-noise ratio in that specific band.&#10;3. The speakers mention that this alternative method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. This is because the EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis." target="1. Stationarity of the noise: If the noise is fairly stationary, then it is better to use as much data as possible to get a more accurate estimate of the noise and subtract it off. However, if the noise is non-stationary (which is the case for most real situations), then an online estimation with some forgetting factor needs to be used.&#10;2. Availability of good noise estimation: A good estimation of the noise is necessary for spectral subtraction methods to work effectively. However, getting a good estimate of the noise can be challenging, especially if the noise is non-stationary.&#10;3. Type of noise: The decision between using a running estimate or a stationary noise estimate may also depend on the type of noise present in the signal. For example, sinusoidally modulated amplitude modulated noise may pose a challenge in practice.&#10;4. Use of low-level detectors: The EM algorithm is used to determine the training targets for the low-level detectors used in the analysis, which suggests that these detectors play a role in the decision between using a running estimate or a stationary noise estimate.&#10;5. Signal level importance: The choice between using a running estimate or a stationary noise estimate may also depend on the relative importance of the signal level compared to the noise level. When the signal level is more important, a different approach may be necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis." target="1. The proposed solution to address the limitations of LDA filters involves adding an additional low-pass filter at 25 Hz. This filter will act as a supplementary measure to the existing LDA filters, which are essentially low-pass but allow high frequency components above 25 Hz to pass through.&#10;2. A ten-millisecond filter will be incorporated into the system in addition to the low-pass filter. However, the specifics of this filter and its purpose have not been explicitly mentioned in the transcript.&#10;3. The solution also aims to tackle the delay caused by online normalization using recursion on the feature stream. It has been noted that this method of normalization introduces a delay of 85 milliseconds, which needs to be accounted for and minimized in order to improve the system's performance.&#10;4. The transcript suggests that these changes are aimed at improving the overall efficiency and accuracy of the speech recognition or processing system by addressing the shortcomings of LDA filters, delay compensation, and online normalization. However, it is important to note that these proposed solutions have not been fully implemented or tested yet, as mentioned in the conversation between PhD B and Professor C.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis." target="1. The primary goal of the conversation appears to be a comparison between different data transformation techniques, specifically focusing on the use of a neural net transformation, Linear Discriminant Analysis (LDA), and Karhunen-LoÃ¨ve Transform (KLT). The speakers aim to understand if the additional complexity of a neural net offers any significant benefits in terms of accuracy or performance compared to these other methods.&#10;2. This comparison is essential for evaluating the relative merits of each technique, especially considering that LDA provides better discrimination between classes than KLT and has a linear but discriminant representation of the data. By examining how well the neural net performs in this context, researchers can determine if there is a meaningful improvement over more traditional methods like KLT or LDA.&#10;3. In addition to comparing techniques, the speakers also discuss implementing spectral subtraction theory in different domains (magnitude versus power) and using various methods for determining signal-to-noise ratios (SNR). These discussions demonstrate the importance of understanding the nuances between different implementation strategies and their effects on performance.&#10;&#10;In summary, the purpose of comparing a neural net transformation to KLT and LDA is to determine if there are any significant benefits in terms of accuracy or performance from using the more complex neural net method over these other techniques. This comparison helps researchers understand the best methods for data transformation based on their specific application needs.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis." target="1. The speakers discuss a method of determining the signal-to-noise (SNR) ratio based on a histogram with two Gaussian approximations for signal and noise. When the distributions overlap, they suggest using the Expectation Maximization (EM) algorithm to separate them. This iterative method is more reliable for estimating SNR when the signal and noise can overlap in the histogram. The EM algorithm focuses on estimating the means of the signal and noise distributions without considering the variances. Once the mean values are determined, they can be used as the SNR in that specific band.&#10;&#10;This method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. The EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary.&#10;&#10;The use of this method may depend on factors such as:&#10;- Stationarity of the noise&#10;- Availability of good noise estimation&#10;- Type of noise present in the signal&#10;- Use of low-level detectors&#10;- Signal level importance&#10;&#10;Additionally, the speakers use the gradient descent algorithm to train the parameters for the logistic regression and utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm, likely to estimate or determine the training targets for the low-level detectors used in their analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis." target="In statistics, a stationary process is one where the joint probability distribution of the random variables remains unchanged over time. In other words, the statistical properties of the process such as mean, variance, and autocorrelation, do not change over time.&#10;&#10;In signal processing, the term &quot;stationary&quot; is often used to refer to signals or processes that change slowly compared to the processing techniques being used. This means that the statistical properties of the signal remain approximately constant over a short period of time, so that techniques such as spectral analysis can be applied effectively. For example, when analyzing a noise signal in a car, the nature of the noise is expected to change relatively slowly over time, compared to the processing techniques being used.&#10;&#10;However, it's important to note that many real-world signals are non-stationary and their statistical properties change over time. In such cases, more sophisticated signal processing techniques may be needed to accurately analyze and interpret the signals. For example, using a running estimate or an online estimation with some forgetting factor may be necessary to estimate the noise in non-stationary signals.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis." target="1. The focus of the individual's work is to improve far-microphone speech recognition, specifically addressing reverberation in the signal. They have experimented with methods such as MSG and PLP, which are robust against reverberation, but they are also interested in comparing these approaches with speech enhancement or de-reverberation techniques that attempt to remove reverberation from the signal directly.&#10;2. Echo cancellation is a technique being explored by the individual. By identifying the linear time-invariant system between two microphones and using echo cancellation techniques, it is possible to correct for the distortion caused by reverberation. The individual aims to use this method in conjunction with an all-pole filter and a processed excitation signal to reduce reverberation.&#10;3. The individual has also been working on training neural networks under different reverberation conditions, trying to find the best network configuration for each scenario. They have experimented with training a network using various reverberation settings and then selecting the most effective configuration based on performance.&#10;4. Lastly, there is discussion of exploring &quot;multiple mike things&quot; where all microphones are at a distance from the speaker, particularly in conditions where there may be an obstruction between them, creating a helpful shadow effect.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers use the gradient descent algorithm to train the parameters for the logistic regression.&#10;2. They also utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm. Specifically, they mentioned using the EM algorithm to &quot;get the targets&quot; for the logistic regression. This implies that they likely use the EM algorithm to estimate or determine the training targets for the low-level detectors used in their analysis." target="Based on the transcript provided, the speakers conducted separate Viterbi analyses on different sub-band labels. However, they did not explicitly discuss the outcome or impact of this action on the overall training process. Therefore, it is not possible to accurately answer this question without additional information. The transcript only indicates that the separate Viterbi analysis was done but does not provide any details about the results or effects of this process.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The tricky part about calculating signal-to-noise (SNR) ratio, as discussed by the speakers, is that sometimes the signal and noise can overlap in the histogram, making it difficult to distinguish between the two. To address this issue, they suggest an alternative method for determining means only in the EM algorithm, which appears to be more reliable for estimating SNR.&#10;2. The alternative method for determining means only in the EM algorithm involves using an iterative approach similar to the EM algorithm (referred to as an &quot;EM-like thing&quot;). This method focuses on estimating the means of the signal and noise distributions without concerning itself with the variances. Once the mean values are determined, they can be used as the signal-to-noise ratio in that specific band.&#10;3. The speakers mention that this alternative method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. This is because the EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary." target=" , but , you know , it 's {disfmarker} that {disfmarker} that {disfmarker} that 's tricky to do . It has mistakes . Uh , and if you 've got enough time , uh , this other method appears to be somewhat more reliable . Uh , a variant on that for just determining signal - to - noise ratio is to just , uh {disfmarker} you can do a w a uh {disfmarker} an iterative thing , EM - like thing , to determine means only . I guess it is EM still , but just {disfmarker} just determine the means only . Don't worry about the variances .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And then you just use those mean values as being the {disfmarker} the , uh uh signal - to - noise ratio in that band .&#10;Speaker: PhD A&#10;Content: But what is the {disfmarker} it seems like this kind of thing could add to the latency . I mean , depending on where the window was that you used to calculate {pause} the signal - to - noise">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The tricky part about calculating signal-to-noise (SNR) ratio, as discussed by the speakers, is that sometimes the signal and noise can overlap in the histogram, making it difficult to distinguish between the two. To address this issue, they suggest an alternative method for determining means only in the EM algorithm, which appears to be more reliable for estimating SNR.&#10;2. The alternative method for determining means only in the EM algorithm involves using an iterative approach similar to the EM algorithm (referred to as an &quot;EM-like thing&quot;). This method focuses on estimating the means of the signal and noise distributions without concerning itself with the variances. Once the mean values are determined, they can be used as the signal-to-noise ratio in that specific band.&#10;3. The speakers mention that this alternative method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. This is because the EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary." target=" spectrum a running estimate ? Or {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Well , that 's {disfmarker} I mean , that 's what differs from different {disfmarker} different tasks and different s uh , spectral subtraction methods .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: Professor C&#10;Content: I mean , if {disfmarker} if you have , uh , fair assurance that , uh , the noise is {disfmarker} is quite stationary , then the smartest thing to do is use as much data as possible to estimate the noise , get a much better estimate , and subtract it off .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: But if it 's varying at all , which is gonna be the case for almost any real situation , you have to do it on - line , uh , with some forgetting factor or something .&#10;Speaker: PhD A&#10;Content: So do you {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The tricky part about calculating signal-to-noise (SNR) ratio, as discussed by the speakers, is that sometimes the signal and noise can overlap in the histogram, making it difficult to distinguish between the two. To address this issue, they suggest an alternative method for determining means only in the EM algorithm, which appears to be more reliable for estimating SNR.&#10;2. The alternative method for determining means only in the EM algorithm involves using an iterative approach similar to the EM algorithm (referred to as an &quot;EM-like thing&quot;). This method focuses on estimating the means of the signal and noise distributions without concerning itself with the variances. Once the mean values are determined, they can be used as the signal-to-noise ratio in that specific band.&#10;3. The speakers mention that this alternative method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. This is because the EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary." target="1. The primary goal of the conversation appears to be a comparison between different data transformation techniques, specifically focusing on the use of a neural net transformation, Linear Discriminant Analysis (LDA), and Karhunen-LoÃ¨ve Transform (KLT). The speakers aim to understand if the additional complexity of a neural net offers any significant benefits in terms of accuracy or performance compared to these other methods.&#10;2. This comparison is essential for evaluating the relative merits of each technique, especially considering that LDA provides better discrimination between classes than KLT and has a linear but discriminant representation of the data. By examining how well the neural net performs in this context, researchers can determine if there is a meaningful improvement over more traditional methods like KLT or LDA.&#10;3. In addition to comparing techniques, the speakers also discuss implementing spectral subtraction theory in different domains (magnitude versus power) and using various methods for determining signal-to-noise ratios (SNR). These discussions demonstrate the importance of understanding the nuances between different implementation strategies and their effects on performance.&#10;&#10;In summary, the purpose of comparing a neural net transformation to KLT and LDA is to determine if there are any significant benefits in terms of accuracy or performance from using the more complex neural net method over these other techniques. This comparison helps researchers understand the best methods for data transformation based on their specific application needs.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The tricky part about calculating signal-to-noise (SNR) ratio, as discussed by the speakers, is that sometimes the signal and noise can overlap in the histogram, making it difficult to distinguish between the two. To address this issue, they suggest an alternative method for determining means only in the EM algorithm, which appears to be more reliable for estimating SNR.&#10;2. The alternative method for determining means only in the EM algorithm involves using an iterative approach similar to the EM algorithm (referred to as an &quot;EM-like thing&quot;). This method focuses on estimating the means of the signal and noise distributions without concerning itself with the variances. Once the mean values are determined, they can be used as the signal-to-noise ratio in that specific band.&#10;3. The speakers mention that this alternative method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. This is because the EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary." target="1. The speakers discuss a method of determining the signal-to-noise (SNR) ratio based on a histogram with two Gaussian approximations for signal and noise. When the distributions overlap, they suggest using the Expectation Maximization (EM) algorithm to separate them. This iterative method is more reliable for estimating SNR when the signal and noise can overlap in the histogram. The EM algorithm focuses on estimating the means of the signal and noise distributions without considering the variances. Once the mean values are determined, they can be used as the SNR in that specific band.&#10;&#10;This method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. The EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary.&#10;&#10;The use of this method may depend on factors such as:&#10;- Stationarity of the noise&#10;- Availability of good noise estimation&#10;- Type of noise present in the signal&#10;- Use of low-level detectors&#10;- Signal level importance&#10;&#10;Additionally, the speakers use the gradient descent algorithm to train the parameters for the logistic regression and utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm, likely to estimate or determine the training targets for the low-level detectors used in their analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The tricky part about calculating signal-to-noise (SNR) ratio, as discussed by the speakers, is that sometimes the signal and noise can overlap in the histogram, making it difficult to distinguish between the two. To address this issue, they suggest an alternative method for determining means only in the EM algorithm, which appears to be more reliable for estimating SNR.&#10;2. The alternative method for determining means only in the EM algorithm involves using an iterative approach similar to the EM algorithm (referred to as an &quot;EM-like thing&quot;). This method focuses on estimating the means of the signal and noise distributions without concerning itself with the variances. Once the mean values are determined, they can be used as the signal-to-noise ratio in that specific band.&#10;3. The speakers mention that this alternative method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. This is because the EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary." target="1. Based on the transcript, it is indicated that both Ericsson and France Telecom have implemented spectral subtraction theory in their work. However, the specific details of their implementations are not provided in the given transcript. Therefore, it cannot be definitively stated whether they used a threshold on previous noise estimates to determine what updates the estimate, with only signals below the threshold being used for updating.&#10;2. PhD B mentions a method where an estimate of the noise level is obtained and a threshold is set above it, such as six or ten DB. Any signal under this threshold is then used to update the estimate. PhD D confirms this method, but does not specify whether it was used by Ericsson or France Telecom.&#10;3. Therefore, while it is possible that both Ericsson and France Telecom have used this method, there is no explicit confirmation in the given transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , but , you know , it 's {disfmarker} that {disfmarker} that {disfmarker} that 's tricky to do . It has mistakes . Uh , and if you 've got enough time , uh , this other method appears to be somewhat more reliable . Uh , a variant on that for just determining signal - to - noise ratio is to just , uh {disfmarker} you can do a w a uh {disfmarker} an iterative thing , EM - like thing , to determine means only . I guess it is EM still , but just {disfmarker} just determine the means only . Don't worry about the variances .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: And then you just use those mean values as being the {disfmarker} the , uh uh signal - to - noise ratio in that band .&#10;Speaker: PhD A&#10;Content: But what is the {disfmarker} it seems like this kind of thing could add to the latency . I mean , depending on where the window was that you used to calculate {pause} the signal - to - noise" target="1. The speakers discuss a method of determining the signal-to-noise (SNR) ratio based on a histogram with two Gaussian approximations for signal and noise. When the distributions overlap, they suggest using the Expectation Maximization (EM) algorithm to separate them. This iterative method is more reliable for estimating SNR when the signal and noise can overlap in the histogram. The EM algorithm focuses on estimating the means of the signal and noise distributions without considering the variances. Once the mean values are determined, they can be used as the SNR in that specific band.&#10;&#10;This method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. The EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary.&#10;&#10;The use of this method may depend on factors such as:&#10;- Stationarity of the noise&#10;- Availability of good noise estimation&#10;- Type of noise present in the signal&#10;- Use of low-level detectors&#10;- Signal level importance&#10;&#10;Additionally, the speakers use the gradient descent algorithm to train the parameters for the logistic regression and utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm, likely to estimate or determine the training targets for the low-level detectors used in their analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source=" spectrum a running estimate ? Or {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Well , that 's {disfmarker} I mean , that 's what differs from different {disfmarker} different tasks and different s uh , spectral subtraction methods .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: Professor C&#10;Content: I mean , if {disfmarker} if you have , uh , fair assurance that , uh , the noise is {disfmarker} is quite stationary , then the smartest thing to do is use as much data as possible to estimate the noise , get a much better estimate , and subtract it off .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: But if it 's varying at all , which is gonna be the case for almost any real situation , you have to do it on - line , uh , with some forgetting factor or something .&#10;Speaker: PhD A&#10;Content: So do you {disfmark" target="1. Stationarity of the noise: If the noise is fairly stationary, then it is better to use as much data as possible to get a more accurate estimate of the noise and subtract it off. However, if the noise is non-stationary (which is the case for most real situations), then an online estimation with some forgetting factor needs to be used.&#10;2. Availability of good noise estimation: A good estimation of the noise is necessary for spectral subtraction methods to work effectively. However, getting a good estimate of the noise can be challenging, especially if the noise is non-stationary.&#10;3. Type of noise: The decision between using a running estimate or a stationary noise estimate may also depend on the type of noise present in the signal. For example, sinusoidally modulated amplitude modulated noise may pose a challenge in practice.&#10;4. Use of low-level detectors: The EM algorithm is used to determine the training targets for the low-level detectors used in the analysis, which suggests that these detectors play a role in the decision between using a running estimate or a stationary noise estimate.&#10;5. Signal level importance: The choice between using a running estimate or a stationary noise estimate may also depend on the relative importance of the signal level compared to the noise level. When the signal level is more important, a different approach may be necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source=" spectrum a running estimate ? Or {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Well , that 's {disfmarker} I mean , that 's what differs from different {disfmarker} different tasks and different s uh , spectral subtraction methods .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: Professor C&#10;Content: I mean , if {disfmarker} if you have , uh , fair assurance that , uh , the noise is {disfmarker} is quite stationary , then the smartest thing to do is use as much data as possible to estimate the noise , get a much better estimate , and subtract it off .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: But if it 's varying at all , which is gonna be the case for almost any real situation , you have to do it on - line , uh , with some forgetting factor or something .&#10;Speaker: PhD A&#10;Content: So do you {disfmark" target="1. Some people who implement spectral subtraction theory may prefer doing it in the magnitude domain because, while the theory suggests that it should be done in the power domain, practical implementations in the magnitude domain can still work effectively for some use cases. This is mentioned by PhD B and Professor C when they discuss different implementations.&#10;2. The &quot;trick&quot; used to subtract the estimate of the noise spectrum in the magnitude domain (instead of the power domain) is not explicitly stated in the transcript. However, it can be inferred that these practitioners might be using some sort of running estimate or average of the noise spectrum and applying a scaling factor based on the signal-to-noise ratio (SNR). This is suggested when PhD B mentions &quot;you don't just subtract the estimate of the noise spectrum. You subtract that times a factor, which is dependent on the SNR.&quot; Additionally, Professor C mentions that the estimation of the power spectra of the noise should be multiplied by a factor depending on the SNR.">
      <data key="d0">1</data>
    </edge>
    <edge source=" spectrum a running estimate ? Or {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Well , that 's {disfmarker} I mean , that 's what differs from different {disfmarker} different tasks and different s uh , spectral subtraction methods .&#10;Speaker: PhD A&#10;Content: Hmm !&#10;Speaker: Professor C&#10;Content: I mean , if {disfmarker} if you have , uh , fair assurance that , uh , the noise is {disfmarker} is quite stationary , then the smartest thing to do is use as much data as possible to estimate the noise , get a much better estimate , and subtract it off .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: But if it 's varying at all , which is gonna be the case for almost any real situation , you have to do it on - line , uh , with some forgetting factor or something .&#10;Speaker: PhD A&#10;Content: So do you {disfmark" target="The issue with obtaining a good estimate of the speech modulation spectrum in this scenario is that if the speech varies significantly in just a few seconds, even with the use of spectral subtraction, getting a decent estimate requires a few seconds of speech. However, if the speech has changed a lot within those few seconds, then using that estimate to subtract the noise could be problematic and may not accurately represent the actual noise present in the signal.&#10;&#10;This is because spectral subtraction relies on having an accurate estimate of the noise spectrum, which can be challenging to obtain if the noise is non-stationary or varying rapidly over time. In such cases, using a running estimate or average of the noise spectrum with a scaling factor based on the signal-to-noise ratio (SNR) may help improve the accuracy of the estimation. However, as mentioned by Professor C in the transcript, this approach may not always work well if there are non-stationary components present in the noise.&#10;&#10;Overall, the challenge lies in developing spectral subtraction methods that can effectively handle non-stationary noise and accurately estimate the speech modulation spectrum in real-world situations where the noise characteristics may vary significantly over time.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Stationarity of the noise: If the noise is fairly stationary, then it is better to use as much data as possible to get a more accurate estimate of the noise and subtract it off. However, if the noise is non-stationary (which is the case for most real situations), then an online estimation with some forgetting factor needs to be used.&#10;2. Availability of good noise estimation: A good estimation of the noise is necessary for spectral subtraction methods to work effectively. However, getting a good estimate of the noise can be challenging, especially if the noise is non-stationary.&#10;3. Type of noise: The decision between using a running estimate or a stationary noise estimate may also depend on the type of noise present in the signal. For example, sinusoidally modulated amplitude modulated noise may pose a challenge in practice.&#10;4. Use of low-level detectors: The EM algorithm is used to determine the training targets for the low-level detectors used in the analysis, which suggests that these detectors play a role in the decision between using a running estimate or a stationary noise estimate.&#10;5. Signal level importance: The choice between using a running estimate or a stationary noise estimate may also depend on the relative importance of the signal level compared to the noise level. When the signal level is more important, a different approach may be necessary." target=" for non - stationary noise ,&#10;Speaker: PhD B&#10;Content: Well , if y if you have a good estimation of the noise ,&#10;Speaker: Professor C&#10;Content: you know ?&#10;Speaker: PhD B&#10;Content: yeah , because well it it has to work .&#10;Speaker: Professor C&#10;Content: But it 's hard to {disfmarker}&#10;Speaker: PhD B&#10;Content: i&#10;Speaker: Professor C&#10;Content: but that 's hard to do .&#10;Speaker: PhD B&#10;Content: Yeah , that 's hard to do . Yeah .&#10;Speaker: Professor C&#10;Content: Yeah . So {disfmarker} so I think that {disfmarker} that what {disfmarker} what is {disfmarker} wh what 's more common is that you 're going to be helped with r slowly varying or stationary noise .&#10;Speaker: PhD B&#10;Content: But {disfmarker} Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's what spectral subtraction will help with , practically speaking .&#10;Speaker: PhD B&#10;Content: Mm - hmm . M">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Stationarity of the noise: If the noise is fairly stationary, then it is better to use as much data as possible to get a more accurate estimate of the noise and subtract it off. However, if the noise is non-stationary (which is the case for most real situations), then an online estimation with some forgetting factor needs to be used.&#10;2. Availability of good noise estimation: A good estimation of the noise is necessary for spectral subtraction methods to work effectively. However, getting a good estimate of the noise can be challenging, especially if the noise is non-stationary.&#10;3. Type of noise: The decision between using a running estimate or a stationary noise estimate may also depend on the type of noise present in the signal. For example, sinusoidally modulated amplitude modulated noise may pose a challenge in practice.&#10;4. Use of low-level detectors: The EM algorithm is used to determine the training targets for the low-level detectors used in the analysis, which suggests that these detectors play a role in the decision between using a running estimate or a stationary noise estimate.&#10;5. Signal level importance: The choice between using a running estimate or a stationary noise estimate may also depend on the relative importance of the signal level compared to the noise level. When the signal level is more important, a different approach may be necessary." target=" can always pick something that {disfmarker} that falls between your methods ,&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: uh , uh , but I don't know if , you know , if sinusoidally , uh , modul amplitude modulated noise is {disfmarker} is sort of a big problem in {disfmarker} in in {disfmarker} practice .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: I think that {vocalsound} it 's uh {disfmarker}&#10;Speaker: PhD A&#10;Content: We could probably get a really good estimate of the noise if we just went to the noise files , and built the averages from them .&#10;Speaker: Professor C&#10;Content: Yeah . Well .&#10;Speaker: PhD B&#10;Content: What {disfmarker} What do you mean ?&#10;Speaker: Professor C&#10;Content: Just cheat {disfmarker} You 're saying , cheat .&#10;Speaker: PhD B&#10;Content: But if the {disfmarker} if the noise is stationary perhaps you don't even need">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Stationarity of the noise: If the noise is fairly stationary, then it is better to use as much data as possible to get a more accurate estimate of the noise and subtract it off. However, if the noise is non-stationary (which is the case for most real situations), then an online estimation with some forgetting factor needs to be used.&#10;2. Availability of good noise estimation: A good estimation of the noise is necessary for spectral subtraction methods to work effectively. However, getting a good estimate of the noise can be challenging, especially if the noise is non-stationary.&#10;3. Type of noise: The decision between using a running estimate or a stationary noise estimate may also depend on the type of noise present in the signal. For example, sinusoidally modulated amplitude modulated noise may pose a challenge in practice.&#10;4. Use of low-level detectors: The EM algorithm is used to determine the training targets for the low-level detectors used in the analysis, which suggests that these detectors play a role in the decision between using a running estimate or a stationary noise estimate.&#10;5. Signal level importance: The choice between using a running estimate or a stationary noise estimate may also depend on the relative importance of the signal level compared to the noise level. When the signal level is more important, a different approach may be necessary." target="1. The speakers discuss a method of determining the signal-to-noise (SNR) ratio based on a histogram with two Gaussian approximations for signal and noise. When the distributions overlap, they suggest using the Expectation Maximization (EM) algorithm to separate them. This iterative method is more reliable for estimating SNR when the signal and noise can overlap in the histogram. The EM algorithm focuses on estimating the means of the signal and noise distributions without considering the variances. Once the mean values are determined, they can be used as the SNR in that specific band.&#10;&#10;This method could potentially reduce latency, as it does not require looking into the future to calculate the SNR ratio. The EM algorithm can make an initial guess about the noise and then refine it over time using an online approach with a forgetting factor, making it more suitable for real-world situations where noise might vary.&#10;&#10;The use of this method may depend on factors such as:&#10;- Stationarity of the noise&#10;- Availability of good noise estimation&#10;- Type of noise present in the signal&#10;- Use of low-level detectors&#10;- Signal level importance&#10;&#10;Additionally, the speakers use the gradient descent algorithm to train the parameters for the logistic regression and utilize an EM (Expectation-Maximization) algorithm in conjunction with the gradient descent algorithm, likely to estimate or determine the training targets for the low-level detectors used in their analysis.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Stationarity of the noise: If the noise is fairly stationary, then it is better to use as much data as possible to get a more accurate estimate of the noise and subtract it off. However, if the noise is non-stationary (which is the case for most real situations), then an online estimation with some forgetting factor needs to be used.&#10;2. Availability of good noise estimation: A good estimation of the noise is necessary for spectral subtraction methods to work effectively. However, getting a good estimate of the noise can be challenging, especially if the noise is non-stationary.&#10;3. Type of noise: The decision between using a running estimate or a stationary noise estimate may also depend on the type of noise present in the signal. For example, sinusoidally modulated amplitude modulated noise may pose a challenge in practice.&#10;4. Use of low-level detectors: The EM algorithm is used to determine the training targets for the low-level detectors used in the analysis, which suggests that these detectors play a role in the decision between using a running estimate or a stationary noise estimate.&#10;5. Signal level importance: The choice between using a running estimate or a stationary noise estimate may also depend on the relative importance of the signal level compared to the noise level. When the signal level is more important, a different approach may be necessary." target="In statistics, a stationary process is one where the joint probability distribution of the random variables remains unchanged over time. In other words, the statistical properties of the process such as mean, variance, and autocorrelation, do not change over time.&#10;&#10;In signal processing, the term &quot;stationary&quot; is often used to refer to signals or processes that change slowly compared to the processing techniques being used. This means that the statistical properties of the signal remain approximately constant over a short period of time, so that techniques such as spectral analysis can be applied effectively. For example, when analyzing a noise signal in a car, the nature of the noise is expected to change relatively slowly over time, compared to the processing techniques being used.&#10;&#10;However, it's important to note that many real-world signals are non-stationary and their statistical properties change over time. In such cases, more sophisticated signal processing techniques may be needed to accurately analyze and interpret the signals. For example, using a running estimate or an online estimation with some forgetting factor may be necessary to estimate the noise in non-stationary signals.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Stationarity of the noise: If the noise is fairly stationary, then it is better to use as much data as possible to get a more accurate estimate of the noise and subtract it off. However, if the noise is non-stationary (which is the case for most real situations), then an online estimation with some forgetting factor needs to be used.&#10;2. Availability of good noise estimation: A good estimation of the noise is necessary for spectral subtraction methods to work effectively. However, getting a good estimate of the noise can be challenging, especially if the noise is non-stationary.&#10;3. Type of noise: The decision between using a running estimate or a stationary noise estimate may also depend on the type of noise present in the signal. For example, sinusoidally modulated amplitude modulated noise may pose a challenge in practice.&#10;4. Use of low-level detectors: The EM algorithm is used to determine the training targets for the low-level detectors used in the analysis, which suggests that these detectors play a role in the decision between using a running estimate or a stationary noise estimate.&#10;5. Signal level importance: The choice between using a running estimate or a stationary noise estimate may also depend on the relative importance of the signal level compared to the noise level. When the signal level is more important, a different approach may be necessary." target="1. Based on the transcript, it is indicated that both Ericsson and France Telecom have implemented spectral subtraction theory in their work. However, the specific details of their implementations are not provided in the given transcript. Therefore, it cannot be definitively stated whether they used a threshold on previous noise estimates to determine what updates the estimate, with only signals below the threshold being used for updating.&#10;2. PhD B mentions a method where an estimate of the noise level is obtained and a threshold is set above it, such as six or ten DB. Any signal under this threshold is then used to update the estimate. PhD D confirms this method, but does not specify whether it was used by Ericsson or France Telecom.&#10;3. Therefore, while it is possible that both Ericsson and France Telecom have used this method, there is no explicit confirmation in the given transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source=" for non - stationary noise ,&#10;Speaker: PhD B&#10;Content: Well , if y if you have a good estimation of the noise ,&#10;Speaker: Professor C&#10;Content: you know ?&#10;Speaker: PhD B&#10;Content: yeah , because well it it has to work .&#10;Speaker: Professor C&#10;Content: But it 's hard to {disfmarker}&#10;Speaker: PhD B&#10;Content: i&#10;Speaker: Professor C&#10;Content: but that 's hard to do .&#10;Speaker: PhD B&#10;Content: Yeah , that 's hard to do . Yeah .&#10;Speaker: Professor C&#10;Content: Yeah . So {disfmarker} so I think that {disfmarker} that what {disfmarker} what is {disfmarker} wh what 's more common is that you 're going to be helped with r slowly varying or stationary noise .&#10;Speaker: PhD B&#10;Content: But {disfmarker} Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's what spectral subtraction will help with , practically speaking .&#10;Speaker: PhD B&#10;Content: Mm - hmm . M" target="1. Some people who implement spectral subtraction theory may prefer doing it in the magnitude domain because, while the theory suggests that it should be done in the power domain, practical implementations in the magnitude domain can still work effectively for some use cases. This is mentioned by PhD B and Professor C when they discuss different implementations.&#10;2. The &quot;trick&quot; used to subtract the estimate of the noise spectrum in the magnitude domain (instead of the power domain) is not explicitly stated in the transcript. However, it can be inferred that these practitioners might be using some sort of running estimate or average of the noise spectrum and applying a scaling factor based on the signal-to-noise ratio (SNR). This is suggested when PhD B mentions &quot;you don't just subtract the estimate of the noise spectrum. You subtract that times a factor, which is dependent on the SNR.&quot; Additionally, Professor C mentions that the estimation of the power spectra of the noise should be multiplied by a factor depending on the SNR.">
      <data key="d0">1</data>
    </edge>
    <edge source=" for non - stationary noise ,&#10;Speaker: PhD B&#10;Content: Well , if y if you have a good estimation of the noise ,&#10;Speaker: Professor C&#10;Content: you know ?&#10;Speaker: PhD B&#10;Content: yeah , because well it it has to work .&#10;Speaker: Professor C&#10;Content: But it 's hard to {disfmarker}&#10;Speaker: PhD B&#10;Content: i&#10;Speaker: Professor C&#10;Content: but that 's hard to do .&#10;Speaker: PhD B&#10;Content: Yeah , that 's hard to do . Yeah .&#10;Speaker: Professor C&#10;Content: Yeah . So {disfmarker} so I think that {disfmarker} that what {disfmarker} what is {disfmarker} wh what 's more common is that you 're going to be helped with r slowly varying or stationary noise .&#10;Speaker: PhD B&#10;Content: But {disfmarker} Mm - hmm .&#10;Speaker: Professor C&#10;Content: That 's what spectral subtraction will help with , practically speaking .&#10;Speaker: PhD B&#10;Content: Mm - hmm . M" target="In statistics, a stationary process is one where the joint probability distribution of the random variables remains unchanged over time. In other words, the statistical properties of the process such as mean, variance, and autocorrelation, do not change over time.&#10;&#10;In signal processing, the term &quot;stationary&quot; is often used to refer to signals or processes that change slowly compared to the processing techniques being used. This means that the statistical properties of the signal remain approximately constant over a short period of time, so that techniques such as spectral analysis can be applied effectively. For example, when analyzing a noise signal in a car, the nature of the noise is expected to change relatively slowly over time, compared to the processing techniques being used.&#10;&#10;However, it's important to note that many real-world signals are non-stationary and their statistical properties change over time. In such cases, more sophisticated signal processing techniques may be needed to accurately analyze and interpret the signals. For example, using a running estimate or an online estimation with some forgetting factor may be necessary to estimate the noise in non-stationary signals.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A requires the names, as well as other details, from Professor C in order to create the &quot;key&quot; files. The reason for needing this information is not explicitly stated in the transcript, but it can be inferred that it may be necessary for properly labeling or identifying the filters or data being used.&#10;&#10;PhD A also mentions that they need the forms filled out, even if there are no digits involved. This suggests that the forms may serve a purpose beyond simply collecting numerical data, and may instead be used to gather important information about the design and implementation of the filters.&#10;&#10;Professor C initially expresses some confusion about why the forms are necessary, but PhD A explains that it's important for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used. They also mention that stress (presumably in the context of speech recognition) may not always help distinguish between words, but that getting the sequence right is still important.&#10;&#10;Overall, while the transcript does not provide a lot of detail about why PhD A needs the information they are requesting from Professor C, it is clear that this information is necessary for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used." target=" ?&#10;Speaker: Grad E&#10;Content: Oh , no {disfmarker} ?&#10;Speaker: Professor C&#10;Content: Or do {disfmarker} did any {disfmarker} do we need the names for the other stuff ,&#10;Speaker: PhD A&#10;Content: Uh , yeah , I do need your names and {disfmarker} and the time , and all that ,&#10;Speaker: Professor C&#10;Content: or {disfmarker} ? Oh , OK .&#10;Speaker: PhD A&#10;Content: cuz we put that into the &quot; key &quot; files .&#10;Speaker: Professor C&#10;Content: Oh , OK .&#10;Speaker: PhD A&#10;Content: Um . But w&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: That 's why we have the forms , uh , even if there are no digits .&#10;Speaker: Professor C&#10;Content: OK , yeah , I didn't notice this . I 'm sitting here and I was {disfmarker} I was about to read them too . It 's a , uh , blank sheet of paper .&#10;Speaker: PhD A&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A requires the names, as well as other details, from Professor C in order to create the &quot;key&quot; files. The reason for needing this information is not explicitly stated in the transcript, but it can be inferred that it may be necessary for properly labeling or identifying the filters or data being used.&#10;&#10;PhD A also mentions that they need the forms filled out, even if there are no digits involved. This suggests that the forms may serve a purpose beyond simply collecting numerical data, and may instead be used to gather important information about the design and implementation of the filters.&#10;&#10;Professor C initially expresses some confusion about why the forms are necessary, but PhD A explains that it's important for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used. They also mention that stress (presumably in the context of speech recognition) may not always help distinguish between words, but that getting the sequence right is still important.&#10;&#10;Overall, while the transcript does not provide a lot of detail about why PhD A needs the information they are requesting from Professor C, it is clear that this information is necessary for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used." target=" . Anyway , O K . Uh , I think that 's {disfmarker} that 's all we have this week .&#10;Speaker: Grad E&#10;Content: Oh .&#10;Speaker: Professor C&#10;Content:  And , uh , I think it 's digit time .&#10;Speaker: PhD A&#10;Content: Actually the , um {disfmarker} For some reason the digit forms are blank .&#10;Speaker: Professor C&#10;Content: Yeah ?&#10;Speaker: PhD A&#10;Content: Uh , I think th that may be due to the fact that {comment} Adam ran out of digits , {comment} uh , and didn't have time to regenerate any .&#10;Speaker: Professor C&#10;Content: Oh ! Oh ! I guess it 's {disfmarker} Well there 's no real reason to write our names on here then ,&#10;Speaker: PhD A&#10;Content: Yeah , if you want to put your credit card numbers and , uh {disfmarker}&#10;Speaker: Professor C&#10;Content: is there ?&#10;Speaker: Grad E&#10;Content: Oh , no {disfmarker} ?&#10;Speaker: Professor C&#10;Content: Or do {">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A requires the names, as well as other details, from Professor C in order to create the &quot;key&quot; files. The reason for needing this information is not explicitly stated in the transcript, but it can be inferred that it may be necessary for properly labeling or identifying the filters or data being used.&#10;&#10;PhD A also mentions that they need the forms filled out, even if there are no digits involved. This suggests that the forms may serve a purpose beyond simply collecting numerical data, and may instead be used to gather important information about the design and implementation of the filters.&#10;&#10;Professor C initially expresses some confusion about why the forms are necessary, but PhD A explains that it's important for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used. They also mention that stress (presumably in the context of speech recognition) may not always help distinguish between words, but that getting the sequence right is still important.&#10;&#10;Overall, while the transcript does not provide a lot of detail about why PhD A needs the information they are requesting from Professor C, it is clear that this information is necessary for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used." target=" E&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: right ? It 's a {disfmarker} it 's a good thing to mark and will probably help us ultimate with recognition&#10;Speaker: PhD A&#10;Content: Yeah , there 's a few cases where it can like permit {comment} and permit .&#10;Speaker: Professor C&#10;Content: but {disfmarker}&#10;Speaker: PhD A&#10;Content: But {disfmarker} that 's not very common in English . In other languages it 's more uh , important .&#10;Speaker: Professor C&#10;Content: Well , yeah , but i either case you 'd write PERMIT , right ? So you 'd get the word right .&#10;Speaker: PhD A&#10;Content: No , I 'm saying , i i e I thought you were saying that stress doesn't help you distinguish between words .&#10;Speaker: Professor C&#10;Content: Um ,&#10;Speaker: PhD A&#10;Content: Oh , I see what you 're saying . As long as you get {disfmarker} The sequence ,&#10;Speaker: Professor C&#10;Content: We 're g if we '">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A requires the names, as well as other details, from Professor C in order to create the &quot;key&quot; files. The reason for needing this information is not explicitly stated in the transcript, but it can be inferred that it may be necessary for properly labeling or identifying the filters or data being used.&#10;&#10;PhD A also mentions that they need the forms filled out, even if there are no digits involved. This suggests that the forms may serve a purpose beyond simply collecting numerical data, and may instead be used to gather important information about the design and implementation of the filters.&#10;&#10;Professor C initially expresses some confusion about why the forms are necessary, but PhD A explains that it's important for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used. They also mention that stress (presumably in the context of speech recognition) may not always help distinguish between words, but that getting the sequence right is still important.&#10;&#10;Overall, while the transcript does not provide a lot of detail about why PhD A needs the information they are requesting from Professor C, it is clear that this information is necessary for creating the &quot;key&quot; files and ensuring proper recognition of the filters or data being used." target="The transcript does not provide direct information about why &quot;Adam ran out of digits.&quot; However, it is mentioned that the digit forms are blank, which may be a result of running out of digits. There is also no explicit mention of why this makes it unnecessary to write names on the forms.&#10;&#10;Based on the context, it appears that the group is discussing some sort of data collection or input process involving numerical values and forms requiring names. The discussion between PhD A and Professor C suggests that the forms serve a purpose beyond just collecting numerical data, possibly for associating specific details with each set of numbers.&#10;&#10;Since the digit forms are blank due to running out of digits, it could imply that there is a limitation on how many sets of digits can be generated within a certain time frame or under certain conditions (such as Adam's situation). This limitation might have made writing names on the forms unnecessary because there were no digits available for association.&#10;&#10;Nonetheless, since the transcript does not provide direct information about why Adam ran out of digits, any answer to this question can only be speculative and based on inferences drawn from the context.">
      <data key="d0">1</data>
    </edge>
    <edge source=" ?&#10;Speaker: Grad E&#10;Content: Oh , no {disfmarker} ?&#10;Speaker: Professor C&#10;Content: Or do {disfmarker} did any {disfmarker} do we need the names for the other stuff ,&#10;Speaker: PhD A&#10;Content: Uh , yeah , I do need your names and {disfmarker} and the time , and all that ,&#10;Speaker: Professor C&#10;Content: or {disfmarker} ? Oh , OK .&#10;Speaker: PhD A&#10;Content: cuz we put that into the &quot; key &quot; files .&#10;Speaker: Professor C&#10;Content: Oh , OK .&#10;Speaker: PhD A&#10;Content: Um . But w&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: That 's why we have the forms , uh , even if there are no digits .&#10;Speaker: Professor C&#10;Content: OK , yeah , I didn't notice this . I 'm sitting here and I was {disfmarker} I was about to read them too . It 's a , uh , blank sheet of paper .&#10;Speaker: PhD A&#10;Content:" target="The transcript does not provide direct information about why &quot;Adam ran out of digits.&quot; However, it is mentioned that the digit forms are blank, which may be a result of running out of digits. There is also no explicit mention of why this makes it unnecessary to write names on the forms.&#10;&#10;Based on the context, it appears that the group is discussing some sort of data collection or input process involving numerical values and forms requiring names. The discussion between PhD A and Professor C suggests that the forms serve a purpose beyond just collecting numerical data, possibly for associating specific details with each set of numbers.&#10;&#10;Since the digit forms are blank due to running out of digits, it could imply that there is a limitation on how many sets of digits can be generated within a certain time frame or under certain conditions (such as Adam's situation). This limitation might have made writing names on the forms unnecessary because there were no digits available for association.&#10;&#10;Nonetheless, since the transcript does not provide direct information about why Adam ran out of digits, any answer to this question can only be speculative and based on inferences drawn from the context.">
      <data key="d0">1</data>
    </edge>
    <edge source=" . Anyway , O K . Uh , I think that 's {disfmarker} that 's all we have this week .&#10;Speaker: Grad E&#10;Content: Oh .&#10;Speaker: Professor C&#10;Content:  And , uh , I think it 's digit time .&#10;Speaker: PhD A&#10;Content: Actually the , um {disfmarker} For some reason the digit forms are blank .&#10;Speaker: Professor C&#10;Content: Yeah ?&#10;Speaker: PhD A&#10;Content: Uh , I think th that may be due to the fact that {comment} Adam ran out of digits , {comment} uh , and didn't have time to regenerate any .&#10;Speaker: Professor C&#10;Content: Oh ! Oh ! I guess it 's {disfmarker} Well there 's no real reason to write our names on here then ,&#10;Speaker: PhD A&#10;Content: Yeah , if you want to put your credit card numbers and , uh {disfmarker}&#10;Speaker: Professor C&#10;Content: is there ?&#10;Speaker: Grad E&#10;Content: Oh , no {disfmarker} ?&#10;Speaker: Professor C&#10;Content: Or do {" target="The transcript does not provide direct information about why &quot;Adam ran out of digits.&quot; However, it is mentioned that the digit forms are blank, which may be a result of running out of digits. There is also no explicit mention of why this makes it unnecessary to write names on the forms.&#10;&#10;Based on the context, it appears that the group is discussing some sort of data collection or input process involving numerical values and forms requiring names. The discussion between PhD A and Professor C suggests that the forms serve a purpose beyond just collecting numerical data, possibly for associating specific details with each set of numbers.&#10;&#10;Since the digit forms are blank due to running out of digits, it could imply that there is a limitation on how many sets of digits can be generated within a certain time frame or under certain conditions (such as Adam's situation). This limitation might have made writing names on the forms unnecessary because there were no digits available for association.&#10;&#10;Nonetheless, since the transcript does not provide direct information about why Adam ran out of digits, any answer to this question can only be speculative and based on inferences drawn from the context.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Grad E's plan for their qualifying proposal involves using Voice Activity Detection (VAD) to determine boundaries and have everyone do recognition based on those boundaries. This topic was not discussed in previous meetings because it was proposed during the video conference yesterday morning and had not been communicated to the group beforehand." target="Speaker: Professor C&#10;Content: Does France Telecom do this {disfmarker}&#10;Speaker: PhD B&#10;Content: and {disfmarker}&#10;Speaker: Grad E&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: Does France Telecom do th do the same thing ? More or less ?&#10;Speaker: PhD B&#10;Content: I d I {disfmarker} Y you know , perhaps ?&#10;Speaker: PhD D&#10;Content: No . I do I have not here the proposal .&#10;Speaker: Professor C&#10;Content: OK . Um , OK , if we 're {disfmarker} we 're done {disfmarker} done with that , uh , let 's see . Uh , maybe we can talk about a couple other things briefly , just , uh , things that {disfmarker} that we 've been chatting about but haven't made it into these meetings yet . So you 're coming up with your quals proposal , and , uh {disfmarker} Wanna just give a two three minute summary of what you 're planning on doing ?&#10;Speaker: Grad E&#10;Content: Oh , um , two , three">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Grad E's plan for their qualifying proposal involves using Voice Activity Detection (VAD) to determine boundaries and have everyone do recognition based on those boundaries. This topic was not discussed in previous meetings because it was proposed during the video conference yesterday morning and had not been communicated to the group beforehand." target="1. Frication: This is a type of consonantal sound produced by forcing air through a narrow channel in the vocal tract, causing turbulence. Examples include the 's' sound in &quot;snake&quot; or the 'f' sound in &quot;fish.&quot;&#10;2. Abrupt closure: This refers to the way some consonants are formed with a rapid closing and then releasing of the articulators (tongue, lips, etc.), creating a sharp attack and release, such as the 'p' sound in &quot;spot&quot; or the 't' sound in &quot;tot.&quot;&#10;3. R-coloring: This is a phonetic process where a vowel near an /r/ sound takes on some of the characteristics of the /r/ sound, often resulting in a more rounded or backed quality. For instance, in many American English accents, the word &quot;bird&quot; might have an 'r-colored' vowel, making it sound like &quot;[bÉd].&quot;&#10;4. Nasality: This occurs when part of the airflow from speech is directed through the nasal cavity instead of only through the oral cavity. This creates distinct resonances in the sound produced, and results in nasal consonants such as 'm' or 'n.' Additionally, vowels can become nasalized if they appear before or after a nasal consonant.&#10;5. Voicing: This refers to the vibration of the vocal cords during speech production, which distinguishes voiced sounds (e.g., /v/ in &quot;vase&quot; or /z/ in &quot;zip&quot;) from their voiceless counterparts (e.g., /f/ in &quot;fife&quot; or /s/ in &quot;zip&quot;).">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Grad E's plan for their qualifying proposal involves using Voice Activity Detection (VAD) to determine boundaries and have everyone do recognition based on those boundaries. This topic was not discussed in previous meetings because it was proposed during the video conference yesterday morning and had not been communicated to the group beforehand." target="The transcript does not provide direct information about why &quot;Adam ran out of digits.&quot; However, it is mentioned that the digit forms are blank, which may be a result of running out of digits. There is also no explicit mention of why this makes it unnecessary to write names on the forms.&#10;&#10;Based on the context, it appears that the group is discussing some sort of data collection or input process involving numerical values and forms requiring names. The discussion between PhD A and Professor C suggests that the forms serve a purpose beyond just collecting numerical data, possibly for associating specific details with each set of numbers.&#10;&#10;Since the digit forms are blank due to running out of digits, it could imply that there is a limitation on how many sets of digits can be generated within a certain time frame or under certain conditions (such as Adam's situation). This limitation might have made writing names on the forms unnecessary because there were no digits available for association.&#10;&#10;Nonetheless, since the transcript does not provide direct information about why Adam ran out of digits, any answer to this question can only be speculative and based on inferences drawn from the context.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Frication: This is a type of consonantal sound produced by forcing air through a narrow channel in the vocal tract, causing turbulence. Examples include the 's' sound in &quot;snake&quot; or the 'f' sound in &quot;fish.&quot;&#10;2. Abrupt closure: This refers to the way some consonants are formed with a rapid closing and then releasing of the articulators (tongue, lips, etc.), creating a sharp attack and release, such as the 'p' sound in &quot;spot&quot; or the 't' sound in &quot;tot.&quot;&#10;3. R-coloring: This is a phonetic process where a vowel near an /r/ sound takes on some of the characteristics of the /r/ sound, often resulting in a more rounded or backed quality. For instance, in many American English accents, the word &quot;bird&quot; might have an 'r-colored' vowel, making it sound like &quot;[bÉd].&quot;&#10;4. Nasality: This occurs when part of the airflow from speech is directed through the nasal cavity instead of only through the oral cavity. This creates distinct resonances in the sound produced, and results in nasal consonants such as 'm' or 'n.' Additionally, vowels can become nasalized if they appear before or after a nasal consonant.&#10;5. Voicing: This refers to the vibration of the vocal cords during speech production, which distinguishes voiced sounds (e.g., /v/ in &quot;vase&quot; or /z/ in &quot;zip&quot;) from their voiceless counterparts (e.g., /f/ in &quot;fife&quot; or /s/ in &quot;zip&quot;)." target=" but that 's very similar to sonorance .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: Um ,&#10;Speaker: PhD A&#10;Content: When we {disfmarker} when we talked with John Ohala the other day we made a list of some of the things that w&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: like frication ,&#10;Speaker: Grad E&#10;Content: Oh ! OK .&#10;Speaker: PhD A&#10;Content: abrupt closure ,&#10;Speaker: Grad E&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD A&#10;Content: R - coloring , nasality , voicing {disfmarker} Uh .&#10;Speaker: Professor C&#10;Content: Yeah , so there 's a half dozen like that that are {disfmarker}&#10;Speaker: Grad E&#10;Content: Yeah , nasality .&#10;Speaker: Professor C&#10;Content: Now this was coming at it from a different angle but maybe it 's a good way to start . Uh , these are things which , uh , John felt that a">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Frication: This is a type of consonantal sound produced by forcing air through a narrow channel in the vocal tract, causing turbulence. Examples include the 's' sound in &quot;snake&quot; or the 'f' sound in &quot;fish.&quot;&#10;2. Abrupt closure: This refers to the way some consonants are formed with a rapid closing and then releasing of the articulators (tongue, lips, etc.), creating a sharp attack and release, such as the 'p' sound in &quot;spot&quot; or the 't' sound in &quot;tot.&quot;&#10;3. R-coloring: This is a phonetic process where a vowel near an /r/ sound takes on some of the characteristics of the /r/ sound, often resulting in a more rounded or backed quality. For instance, in many American English accents, the word &quot;bird&quot; might have an 'r-colored' vowel, making it sound like &quot;[bÉd].&quot;&#10;4. Nasality: This occurs when part of the airflow from speech is directed through the nasal cavity instead of only through the oral cavity. This creates distinct resonances in the sound produced, and results in nasal consonants such as 'm' or 'n.' Additionally, vowels can become nasalized if they appear before or after a nasal consonant.&#10;5. Voicing: This refers to the vibration of the vocal cords during speech production, which distinguishes voiced sounds (e.g., /v/ in &quot;vase&quot; or /z/ in &quot;zip&quot;) from their voiceless counterparts (e.g., /f/ in &quot;fife&quot; or /s/ in &quot;zip&quot;)." target="aker: PhD B&#10;Content: And the de - reverberation algorithm , do you have {disfmarker} can you give some more details on this or {disfmarker} ? Does it use one microphone ?&#10;Speaker: Grad F&#10;Content: o o&#10;Speaker: PhD B&#10;Content: Several microphones ? Does it {disfmarker} ?&#10;Speaker: Grad F&#10;Content: OK , well , um , there was something that was done by , um , a guy named Carlos , I forget his last name , {comment} who worked with Hynek , who , um ,&#10;Speaker: Professor C&#10;Content: Avendano .&#10;Speaker: Grad F&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad F&#10;Content: Who , um ,&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad F&#10;Content: um , it was like RASTA in the sense that of it was , um , de - convolution by filtering um , except he used a longer time window ,&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad F&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution to address the limitations of LDA filters involves adding an additional low-pass filter at 25 Hz. This filter will act as a supplementary measure to the existing LDA filters, which are essentially low-pass but allow high frequency components above 25 Hz to pass through.&#10;2. A ten-millisecond filter will be incorporated into the system in addition to the low-pass filter. However, the specifics of this filter and its purpose have not been explicitly mentioned in the transcript.&#10;3. The solution also aims to tackle the delay caused by online normalization using recursion on the feature stream. It has been noted that this method of normalization introduces a delay of 85 milliseconds, which needs to be accounted for and minimized in order to improve the system's performance.&#10;4. The transcript suggests that these changes are aimed at improving the overall efficiency and accuracy of the speech recognition or processing system by addressing the shortcomings of LDA filters, delay compensation, and online normalization. However, it is important to note that these proposed solutions have not been fully implemented or tested yet, as mentioned in the conversation between PhD B and Professor C." target="Human ability to perceive the difference between various short time intervals in conversation is quite limited. As mentioned in the transcript, Dr. A stated that people might not be able to tell the difference between a quarter of a second and 100 milliseconds or even half a second. They further expressed that responses often feel so quick, which indicates that humans can perceive small time differences but may not accurately differentiate them.&#10;&#10;This perception affects human communication in several ways. It impacts turn-taking and the fluidity of conversations. In text-based systems, delays between responses can make interactions feel abrupt or unnatural; however, in human interaction, individuals tend to step on each other's utterances creating a more simultaneous exchange of information. The limitations in perceiving short time intervals contribute to these conversational dynamics and create challenges for designing natural and efficient text-based conversation systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed solution to address the limitations of LDA filters involves adding an additional low-pass filter at 25 Hz. This filter will act as a supplementary measure to the existing LDA filters, which are essentially low-pass but allow high frequency components above 25 Hz to pass through.&#10;2. A ten-millisecond filter will be incorporated into the system in addition to the low-pass filter. However, the specifics of this filter and its purpose have not been explicitly mentioned in the transcript.&#10;3. The solution also aims to tackle the delay caused by online normalization using recursion on the feature stream. It has been noted that this method of normalization introduces a delay of 85 milliseconds, which needs to be accounted for and minimized in order to improve the system's performance.&#10;4. The transcript suggests that these changes are aimed at improving the overall efficiency and accuracy of the speech recognition or processing system by addressing the shortcomings of LDA filters, delay compensation, and online normalization. However, it is important to note that these proposed solutions have not been fully implemented or tested yet, as mentioned in the conversation between PhD B and Professor C." target="1. Smooth spectral envelopes may not be the best option for determining sonorant or non-sonorant sounds in speech recognition, as they focus more on the overall envelope rather than the periodicity of the signal. An alternative approach could involve building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information.&#10;2. To improve speech quality and reduce reverberation in a recording, an all-pole filter can be used in conjunction with a reconstructed excitation signal that has been processed to minimize distortion caused by reverberation. This technique involves identifying the linear time-invariant system between two microphones and using echo cancellation techniques to correct for it.&#10;3. Incorporating delays between responses in text-based conversation systems can help mimic human conversational dynamics, reducing abruptness and creating a more natural feel. Additionally, implementing techniques such as Voice Activity Detection (VAD) can establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness.&#10;4. Other possible approaches to improving speech quality and reducing reverberation include adding a low-pass filter at 25 Hz to supplement existing LDA filters, incorporating a ten-millisecond filter into the system, and minimizing delay caused by online normalization using recursion on the feature stream. These techniques aim to address shortcomings in LDA filters, delay compensation, and online normalization.">
      <data key="d0">1</data>
    </edge>
    <edge source="Human ability to perceive the difference between various short time intervals in conversation is quite limited. As mentioned in the transcript, Dr. A stated that people might not be able to tell the difference between a quarter of a second and 100 milliseconds or even half a second. They further expressed that responses often feel so quick, which indicates that humans can perceive small time differences but may not accurately differentiate them.&#10;&#10;This perception affects human communication in several ways. It impacts turn-taking and the fluidity of conversations. In text-based systems, delays between responses can make interactions feel abrupt or unnatural; however, in human interaction, individuals tend to step on each other's utterances creating a more simultaneous exchange of information. The limitations in perceiving short time intervals contribute to these conversational dynamics and create challenges for designing natural and efficient text-based conversation systems." target=" Professor C&#10;Content: but .&#10;Speaker: PhD A&#10;Content: I don't remember the exact numbers but it was something like that .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: I don't think you can really tell . A person {disfmarker} I don't think a person can tell the difference between , uh , you know , a quarter of a second and a hundred milliseconds , and {disfmarker} I 'm not even sure if we can tell the difference between a quarter of a second and half a second .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: I mean it just {disfmarker} it feels so quick .&#10;Speaker: Professor C&#10;Content: Yeah . I mean , basically if you {disfmarker} yeah , if you said , uh , um , &quot; what 's the , uh , uh {disfmarker} what 's the shortest route to the opera ? &quot; and it took half a second to get back to you ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: I mean , {vocals">
      <data key="d0">1</data>
    </edge>
    <edge source="Human ability to perceive the difference between various short time intervals in conversation is quite limited. As mentioned in the transcript, Dr. A stated that people might not be able to tell the difference between a quarter of a second and 100 milliseconds or even half a second. They further expressed that responses often feel so quick, which indicates that humans can perceive small time differences but may not accurately differentiate them.&#10;&#10;This perception affects human communication in several ways. It impacts turn-taking and the fluidity of conversations. In text-based systems, delays between responses can make interactions feel abrupt or unnatural; however, in human interaction, individuals tend to step on each other's utterances creating a more simultaneous exchange of information. The limitations in perceiving short time intervals contribute to these conversational dynamics and create challenges for designing natural and efficient text-based conversation systems." target=" of latency .&#10;Speaker: Professor C&#10;Content: So . No . What {disfmarker} what does {disfmarker} wa was your experience when you were doing this stuff with , uh , the {disfmarker} the {disfmarker} the surgical , uh , uh , microscopes and so forth . Um , how long was it from when somebody , uh , finished an utterance to when , uh , something started happening ?&#10;Speaker: PhD A&#10;Content: Um , we had a silence detector , so we would look for the end of an utterance based on the silence detector .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: And I {disfmarker} I can't remember now off the top of my head how many frames of silence we had to detect before we would declare it to be the end of an utterance .&#10;Speaker: Professor C&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD A&#10;Content: Um , but it was , uh , I would say it was probably around the order of two hundred and fifty milliseconds .&#10;Speaker: Professor">
      <data key="d0">1</data>
    </edge>
    <edge source="The transcript does not provide direct information about why &quot;Adam ran out of digits.&quot; However, it is mentioned that the digit forms are blank, which may be a result of running out of digits. There is also no explicit mention of why this makes it unnecessary to write names on the forms.&#10;&#10;Based on the context, it appears that the group is discussing some sort of data collection or input process involving numerical values and forms requiring names. The discussion between PhD A and Professor C suggests that the forms serve a purpose beyond just collecting numerical data, possibly for associating specific details with each set of numbers.&#10;&#10;Since the digit forms are blank due to running out of digits, it could imply that there is a limitation on how many sets of digits can be generated within a certain time frame or under certain conditions (such as Adam's situation). This limitation might have made writing names on the forms unnecessary because there were no digits available for association.&#10;&#10;Nonetheless, since the transcript does not provide direct information about why Adam ran out of digits, any answer to this question can only be speculative and based on inferences drawn from the context." target=" normalization . So it 's&#10;Speaker: Professor C&#10;Content: Uh ,&#10;Speaker: PhD B&#10;Content: plus {disfmarker} plus eighty for the neural net and PCA .&#10;Speaker: Professor C&#10;Content: yeah , but then there 's {disfmarker} Oh .&#10;Speaker: PhD B&#10;Content: So it would be around two hundred and forty {disfmarker} so , well ,&#10;Speaker: Professor C&#10;Content: Just {disfmarker} just barely in there .&#10;Speaker: PhD B&#10;Content: plus {disfmarker} plus the frames , but it 's OK .&#10;Speaker: PhD A&#10;Content: What 's the allowable ?&#10;Speaker: Professor C&#10;Content: Two - fifty , unless they changed the rules .&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: Which there is {disfmarker} there 's some discussion of .&#10;Speaker: PhD A&#10;Content: What were they thinking of changing it to ?&#10;Speaker: Professor C&#10;Content: But {disfmarker}&#10;Speaker: PhD B&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Some people who implement spectral subtraction theory may prefer doing it in the magnitude domain because, while the theory suggests that it should be done in the power domain, practical implementations in the magnitude domain can still work effectively for some use cases. This is mentioned by PhD B and Professor C when they discuss different implementations.&#10;2. The &quot;trick&quot; used to subtract the estimate of the noise spectrum in the magnitude domain (instead of the power domain) is not explicitly stated in the transcript. However, it can be inferred that these practitioners might be using some sort of running estimate or average of the noise spectrum and applying a scaling factor based on the signal-to-noise ratio (SNR). This is suggested when PhD B mentions &quot;you don't just subtract the estimate of the noise spectrum. You subtract that times a factor, which is dependent on the SNR.&quot; Additionally, Professor C mentions that the estimation of the power spectra of the noise should be multiplied by a factor depending on the SNR." target="1. The primary goal of the conversation appears to be a comparison between different data transformation techniques, specifically focusing on the use of a neural net transformation, Linear Discriminant Analysis (LDA), and Karhunen-LoÃ¨ve Transform (KLT). The speakers aim to understand if the additional complexity of a neural net offers any significant benefits in terms of accuracy or performance compared to these other methods.&#10;2. This comparison is essential for evaluating the relative merits of each technique, especially considering that LDA provides better discrimination between classes than KLT and has a linear but discriminant representation of the data. By examining how well the neural net performs in this context, researchers can determine if there is a meaningful improvement over more traditional methods like KLT or LDA.&#10;3. In addition to comparing techniques, the speakers also discuss implementing spectral subtraction theory in different domains (magnitude versus power) and using various methods for determining signal-to-noise ratios (SNR). These discussions demonstrate the importance of understanding the nuances between different implementation strategies and their effects on performance.&#10;&#10;In summary, the purpose of comparing a neural net transformation to KLT and LDA is to determine if there are any significant benefits in terms of accuracy or performance from using the more complex neural net method over these other techniques. This comparison helps researchers understand the best methods for data transformation based on their specific application needs.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Some people who implement spectral subtraction theory may prefer doing it in the magnitude domain because, while the theory suggests that it should be done in the power domain, practical implementations in the magnitude domain can still work effectively for some use cases. This is mentioned by PhD B and Professor C when they discuss different implementations.&#10;2. The &quot;trick&quot; used to subtract the estimate of the noise spectrum in the magnitude domain (instead of the power domain) is not explicitly stated in the transcript. However, it can be inferred that these practitioners might be using some sort of running estimate or average of the noise spectrum and applying a scaling factor based on the signal-to-noise ratio (SNR). This is suggested when PhD B mentions &quot;you don't just subtract the estimate of the noise spectrum. You subtract that times a factor, which is dependent on the SNR.&quot; Additionally, Professor C mentions that the estimation of the power spectra of the noise should be multiplied by a factor depending on the SNR." target="The issue with obtaining a good estimate of the speech modulation spectrum in this scenario is that if the speech varies significantly in just a few seconds, even with the use of spectral subtraction, getting a decent estimate requires a few seconds of speech. However, if the speech has changed a lot within those few seconds, then using that estimate to subtract the noise could be problematic and may not accurately represent the actual noise present in the signal.&#10;&#10;This is because spectral subtraction relies on having an accurate estimate of the noise spectrum, which can be challenging to obtain if the noise is non-stationary or varying rapidly over time. In such cases, using a running estimate or average of the noise spectrum with a scaling factor based on the signal-to-noise ratio (SNR) may help improve the accuracy of the estimation. However, as mentioned by Professor C in the transcript, this approach may not always work well if there are non-stationary components present in the noise.&#10;&#10;Overall, the challenge lies in developing spectral subtraction methods that can effectively handle non-stationary noise and accurately estimate the speech modulation spectrum in real-world situations where the noise characteristics may vary significantly over time.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Some people who implement spectral subtraction theory may prefer doing it in the magnitude domain because, while the theory suggests that it should be done in the power domain, practical implementations in the magnitude domain can still work effectively for some use cases. This is mentioned by PhD B and Professor C when they discuss different implementations.&#10;2. The &quot;trick&quot; used to subtract the estimate of the noise spectrum in the magnitude domain (instead of the power domain) is not explicitly stated in the transcript. However, it can be inferred that these practitioners might be using some sort of running estimate or average of the noise spectrum and applying a scaling factor based on the signal-to-noise ratio (SNR). This is suggested when PhD B mentions &quot;you don't just subtract the estimate of the noise spectrum. You subtract that times a factor, which is dependent on the SNR.&quot; Additionally, Professor C mentions that the estimation of the power spectra of the noise should be multiplied by a factor depending on the SNR." target="1. Based on the transcript, it is indicated that both Ericsson and France Telecom have implemented spectral subtraction theory in their work. However, the specific details of their implementations are not provided in the given transcript. Therefore, it cannot be definitively stated whether they used a threshold on previous noise estimates to determine what updates the estimate, with only signals below the threshold being used for updating.&#10;2. PhD B mentions a method where an estimate of the noise level is obtained and a threshold is set above it, such as six or ten DB. Any signal under this threshold is then used to update the estimate. PhD D confirms this method, but does not specify whether it was used by Ericsson or France Telecom.&#10;3. Therefore, while it is possible that both Ericsson and France Telecom have used this method, there is no explicit confirmation in the given transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Smooth spectral envelopes may not be the best option for determining sonorant or non-sonorant sounds in speech recognition, as they focus more on the overall envelope rather than the periodicity of the signal. An alternative approach could involve building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information.&#10;2. To improve speech quality and reduce reverberation in a recording, an all-pole filter can be used in conjunction with a reconstructed excitation signal that has been processed to minimize distortion caused by reverberation. This technique involves identifying the linear time-invariant system between two microphones and using echo cancellation techniques to correct for it.&#10;3. Incorporating delays between responses in text-based conversation systems can help mimic human conversational dynamics, reducing abruptness and creating a more natural feel. Additionally, implementing techniques such as Voice Activity Detection (VAD) can establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness.&#10;4. Other possible approaches to improving speech quality and reducing reverberation include adding a low-pass filter at 25 Hz to supplement existing LDA filters, incorporating a ten-millisecond filter into the system, and minimizing delay caused by online normalization using recursion on the feature stream. These techniques aim to address shortcomings in LDA filters, delay compensation, and online normalization." target=" a c two or three minutes about what we 've been talking about today and other days ?&#10;Speaker: Grad F&#10;Content: Ri Yeah , OK , so , um , we 're interested in , um , methods for far mike speech recognition , um , {pause} mainly , uh , methods that deal with the reverberation {pause} in the far mike signal . So , um , one approach would be , um , say MSG and PLP , like was used in Aurora one and , um , there are other approaches which actually attempt to {pause} remove the reverberation , instead of being robust to it like MSG . And so we 're interested in , um , comparing the performance of {pause} um , a robust approach like MSG with these , um , speech enhancement or de - reverber de - reverberation approaches .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad F&#10;Content: And , um , {vocalsound} it looks like we 're gonna use the Meeting Recorder digits data for that .&#10;Speaker: PhD B&#10;Content: And the de - reverberation algorithm , do you have {disfmarker} can you give some more details on this">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Smooth spectral envelopes may not be the best option for determining sonorant or non-sonorant sounds in speech recognition, as they focus more on the overall envelope rather than the periodicity of the signal. An alternative approach could involve building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information.&#10;2. To improve speech quality and reduce reverberation in a recording, an all-pole filter can be used in conjunction with a reconstructed excitation signal that has been processed to minimize distortion caused by reverberation. This technique involves identifying the linear time-invariant system between two microphones and using echo cancellation techniques to correct for it.&#10;3. Incorporating delays between responses in text-based conversation systems can help mimic human conversational dynamics, reducing abruptness and creating a more natural feel. Additionally, implementing techniques such as Voice Activity Detection (VAD) can establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness.&#10;4. Other possible approaches to improving speech quality and reducing reverberation include adding a low-pass filter at 25 Hz to supplement existing LDA filters, incorporating a ten-millisecond filter into the system, and minimizing delay caused by online normalization using recursion on the feature stream. These techniques aim to address shortcomings in LDA filters, delay compensation, and online normalization." target=" someone working on this on i in Mons&#10;Speaker: Professor C&#10;Content: Yeah , OK .&#10;Speaker: PhD B&#10;Content: So perhaps , yeah , we should try t to {disfmarker} He 's working on this , on trying to {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: on re reverberation , um {disfmarker}&#10;Speaker: Professor C&#10;Content: The first paper on this is gonna have great references , I can tell already .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: It 's always good to have references , especially when reviewers read it or {disfmarker} or one of the authors and , {vocalsound} feel they 'll &quot; You 're OK , you 've r You cited me . &quot;&#10;Speaker: PhD B&#10;Content: So , yeah . Well , he did echo cancellation and he did some fancier things like , uh , {vocalsound} {vocalsound} uh , training different network on different reverberation conditions and then trying to find the best one , but .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Smooth spectral envelopes may not be the best option for determining sonorant or non-sonorant sounds in speech recognition, as they focus more on the overall envelope rather than the periodicity of the signal. An alternative approach could involve building other detectors based on different phonetic features, such as voicing, which is related to sonorance but distinct enough to provide additional information.&#10;2. To improve speech quality and reduce reverberation in a recording, an all-pole filter can be used in conjunction with a reconstructed excitation signal that has been processed to minimize distortion caused by reverberation. This technique involves identifying the linear time-invariant system between two microphones and using echo cancellation techniques to correct for it.&#10;3. Incorporating delays between responses in text-based conversation systems can help mimic human conversational dynamics, reducing abruptness and creating a more natural feel. Additionally, implementing techniques such as Voice Activity Detection (VAD) can establish boundaries for the conversation, making recognition smoother and potentially reducing abruptness.&#10;4. Other possible approaches to improving speech quality and reducing reverberation include adding a low-pass filter at 25 Hz to supplement existing LDA filters, incorporating a ten-millisecond filter into the system, and minimizing delay caused by online normalization using recursion on the feature stream. These techniques aim to address shortcomings in LDA filters, delay compensation, and online normalization." target="1. The focus of the individual's work is to improve far-microphone speech recognition, specifically addressing reverberation in the signal. They have experimented with methods such as MSG and PLP, which are robust against reverberation, but they are also interested in comparing these approaches with speech enhancement or de-reverberation techniques that attempt to remove reverberation from the signal directly.&#10;2. Echo cancellation is a technique being explored by the individual. By identifying the linear time-invariant system between two microphones and using echo cancellation techniques, it is possible to correct for the distortion caused by reverberation. The individual aims to use this method in conjunction with an all-pole filter and a processed excitation signal to reduce reverberation.&#10;3. The individual has also been working on training neural networks under different reverberation conditions, trying to find the best network configuration for each scenario. They have experimented with training a network using various reverberation settings and then selecting the most effective configuration based on performance.&#10;4. Lastly, there is discussion of exploring &quot;multiple mike things&quot; where all microphones are at a distance from the speaker, particularly in conditions where there may be an obstruction between them, creating a helpful shadow effect.">
      <data key="d0">1</data>
    </edge>
    <edge source=" a c two or three minutes about what we 've been talking about today and other days ?&#10;Speaker: Grad F&#10;Content: Ri Yeah , OK , so , um , we 're interested in , um , methods for far mike speech recognition , um , {pause} mainly , uh , methods that deal with the reverberation {pause} in the far mike signal . So , um , one approach would be , um , say MSG and PLP , like was used in Aurora one and , um , there are other approaches which actually attempt to {pause} remove the reverberation , instead of being robust to it like MSG . And so we 're interested in , um , comparing the performance of {pause} um , a robust approach like MSG with these , um , speech enhancement or de - reverber de - reverberation approaches .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Grad F&#10;Content: And , um , {vocalsound} it looks like we 're gonna use the Meeting Recorder digits data for that .&#10;Speaker: PhD B&#10;Content: And the de - reverberation algorithm , do you have {disfmarker} can you give some more details on this" target="1. The focus of the individual's work is to improve far-microphone speech recognition, specifically addressing reverberation in the signal. They have experimented with methods such as MSG and PLP, which are robust against reverberation, but they are also interested in comparing these approaches with speech enhancement or de-reverberation techniques that attempt to remove reverberation from the signal directly.&#10;2. Echo cancellation is a technique being explored by the individual. By identifying the linear time-invariant system between two microphones and using echo cancellation techniques, it is possible to correct for the distortion caused by reverberation. The individual aims to use this method in conjunction with an all-pole filter and a processed excitation signal to reduce reverberation.&#10;3. The individual has also been working on training neural networks under different reverberation conditions, trying to find the best network configuration for each scenario. They have experimented with training a network using various reverberation settings and then selecting the most effective configuration based on performance.&#10;4. Lastly, there is discussion of exploring &quot;multiple mike things&quot; where all microphones are at a distance from the speaker, particularly in conditions where there may be an obstruction between them, creating a helpful shadow effect.">
      <data key="d0">1</data>
    </edge>
    <edge source=" someone working on this on i in Mons&#10;Speaker: Professor C&#10;Content: Yeah , OK .&#10;Speaker: PhD B&#10;Content: So perhaps , yeah , we should try t to {disfmarker} He 's working on this , on trying to {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: on re reverberation , um {disfmarker}&#10;Speaker: Professor C&#10;Content: The first paper on this is gonna have great references , I can tell already .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: It 's always good to have references , especially when reviewers read it or {disfmarker} or one of the authors and , {vocalsound} feel they 'll &quot; You 're OK , you 've r You cited me . &quot;&#10;Speaker: PhD B&#10;Content: So , yeah . Well , he did echo cancellation and he did some fancier things like , uh , {vocalsound} {vocalsound} uh , training different network on different reverberation conditions and then trying to find the best one , but ." target="1. The focus of the individual's work is to improve far-microphone speech recognition, specifically addressing reverberation in the signal. They have experimented with methods such as MSG and PLP, which are robust against reverberation, but they are also interested in comparing these approaches with speech enhancement or de-reverberation techniques that attempt to remove reverberation from the signal directly.&#10;2. Echo cancellation is a technique being explored by the individual. By identifying the linear time-invariant system between two microphones and using echo cancellation techniques, it is possible to correct for the distortion caused by reverberation. The individual aims to use this method in conjunction with an all-pole filter and a processed excitation signal to reduce reverberation.&#10;3. The individual has also been working on training neural networks under different reverberation conditions, trying to find the best network configuration for each scenario. They have experimented with training a network using various reverberation settings and then selecting the most effective configuration based on performance.&#10;4. Lastly, there is discussion of exploring &quot;multiple mike things&quot; where all microphones are at a distance from the speaker, particularly in conditions where there may be an obstruction between them, creating a helpful shadow effect.">
      <data key="d0">1</data>
    </edge>
    <edge source="In statistics, a stationary process is one where the joint probability distribution of the random variables remains unchanged over time. In other words, the statistical properties of the process such as mean, variance, and autocorrelation, do not change over time.&#10;&#10;In signal processing, the term &quot;stationary&quot; is often used to refer to signals or processes that change slowly compared to the processing techniques being used. This means that the statistical properties of the signal remain approximately constant over a short period of time, so that techniques such as spectral analysis can be applied effectively. For example, when analyzing a noise signal in a car, the nature of the noise is expected to change relatively slowly over time, compared to the processing techniques being used.&#10;&#10;However, it's important to note that many real-world signals are non-stationary and their statistical properties change over time. In such cases, more sophisticated signal processing techniques may be needed to accurately analyze and interpret the signals. For example, using a running estimate or an online estimation with some forgetting factor may be necessary to estimate the noise in non-stationary signals." target=": Professor C&#10;Content: you know , stationary {disfmarker}&#10;Speaker: PhD A&#10;Content: Very slow adaptation .&#10;Speaker: PhD B&#10;Content: and is the c&#10;Speaker: PhD A&#10;Content: th&#10;Speaker: Professor C&#10;Content: Right , the word &quot; stationary &quot; is {disfmarker} has a very precise statistical meaning . But , you know , in {disfmarker} in signal - processing really what we 're talking about I think is things that change slowly , uh , compared with our {disfmarker} our processing techniques .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So if you 're driving along in a car I {disfmarker} I would think that most of the time the nature of the noise is going to change relatively slowly . It 's not gonna stay absolute the same . If you {disfmarker} if you check it out , uh , five minutes later you may be in a different part of the road&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: or whatever . But">
      <data key="d0">1</data>
    </edge>
    <edge source="The issue with obtaining a good estimate of the speech modulation spectrum in this scenario is that if the speech varies significantly in just a few seconds, even with the use of spectral subtraction, getting a decent estimate requires a few seconds of speech. However, if the speech has changed a lot within those few seconds, then using that estimate to subtract the noise could be problematic and may not accurately represent the actual noise present in the signal.&#10;&#10;This is because spectral subtraction relies on having an accurate estimate of the noise spectrum, which can be challenging to obtain if the noise is non-stationary or varying rapidly over time. In such cases, using a running estimate or average of the noise spectrum with a scaling factor based on the signal-to-noise ratio (SNR) may help improve the accuracy of the estimation. However, as mentioned by Professor C in the transcript, this approach may not always work well if there are non-stationary components present in the noise.&#10;&#10;Overall, the challenge lies in developing spectral subtraction methods that can effectively handle non-stationary noise and accurately estimate the speech modulation spectrum in real-world situations where the noise characteristics may vary significantly over time." target="1. Based on the transcript, it is indicated that both Ericsson and France Telecom have implemented spectral subtraction theory in their work. However, the specific details of their implementations are not provided in the given transcript. Therefore, it cannot be definitively stated whether they used a threshold on previous noise estimates to determine what updates the estimate, with only signals below the threshold being used for updating.&#10;2. PhD B mentions a method where an estimate of the noise level is obtained and a threshold is set above it, such as six or ten DB. Any signal under this threshold is then used to update the estimate. PhD D confirms this method, but does not specify whether it was used by Ericsson or France Telecom.&#10;3. Therefore, while it is possible that both Ericsson and France Telecom have used this method, there is no explicit confirmation in the given transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The focus of the individual's work is to improve far-microphone speech recognition, specifically addressing reverberation in the signal. They have experimented with methods such as MSG and PLP, which are robust against reverberation, but they are also interested in comparing these approaches with speech enhancement or de-reverberation techniques that attempt to remove reverberation from the signal directly.&#10;2. Echo cancellation is a technique being explored by the individual. By identifying the linear time-invariant system between two microphones and using echo cancellation techniques, it is possible to correct for the distortion caused by reverberation. The individual aims to use this method in conjunction with an all-pole filter and a processed excitation signal to reduce reverberation.&#10;3. The individual has also been working on training neural networks under different reverberation conditions, trying to find the best network configuration for each scenario. They have experimented with training a network using various reverberation settings and then selecting the most effective configuration based on performance.&#10;4. Lastly, there is discussion of exploring &quot;multiple mike things&quot; where all microphones are at a distance from the speaker, particularly in conditions where there may be an obstruction between them, creating a helpful shadow effect." target=" , {vocalsound} {vocalsound} uh , training different network on different reverberation conditions and then trying to find the best one , but . Well .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: The oth the other thing , uh , that Dave was talking about earlier was , uh , uh , multiple mike things , uh , where they 're all distant . So , um , I mean , there 's {disfmarker} there 's all this work on arrays , but the other thing is , uh , {pause} what can we do that 's cleverer that can take some advantage of only two mikes , uh , particularly if there 's an obstruction between them , as we {disfmarker} as we have over there .&#10;Speaker: PhD B&#10;Content: If there is {disfmarker} ?&#10;Speaker: Professor C&#10;Content: An obstruction between them .&#10;Speaker: PhD B&#10;Content: Ah , yeah .&#10;Speaker: Professor C&#10;Content: It creates a shadow which is {disfmarker} is helpful . It 's">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on the transcript, it is indicated that both Ericsson and France Telecom have implemented spectral subtraction theory in their work. However, the specific details of their implementations are not provided in the given transcript. Therefore, it cannot be definitively stated whether they used a threshold on previous noise estimates to determine what updates the estimate, with only signals below the threshold being used for updating.&#10;2. PhD B mentions a method where an estimate of the noise level is obtained and a threshold is set above it, such as six or ten DB. Any signal under this threshold is then used to update the estimate. PhD D confirms this method, but does not specify whether it was used by Ericsson or France Telecom.&#10;3. Therefore, while it is possible that both Ericsson and France Telecom have used this method, there is no explicit confirmation in the given transcript." target=" , uh {disfmarker}&#10;Speaker: PhD B&#10;Content: Yeah , they have some kind of threshold on {disfmarker} on the previous estimate , and {disfmarker} So . Yeah . I think . Yeah , I think Ericsson used this kind of threshold . Yeah , so , they h they have an estimate of the noise level and they put a threshold like six or ten DB above , and what 's under this threshold is used to update the estimate . Is {disfmarker} is that right&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: or {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I think so .&#10;Speaker: PhD B&#10;Content: So it 's {disfmarker} it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: I have not here the proposal .&#10;Speaker: PhD B&#10;Content: Yeah . It 's like saying what 's under the threshold is silence ,&#10;Speaker: Professor C&#10;Content: Does France Telecom do this {disfmarker}&#10;Speaker: PhD B&#10;Content: and {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, the speakers conducted separate Viterbi analyses on different sub-band labels. However, they did not explicitly discuss the outcome or impact of this action on the overall training process. Therefore, it is not possible to accurately answer this question without additional information. The transcript only indicates that the separate Viterbi analysis was done but does not provide any details about the results or effects of this process." target="disfmarker} a person to make that determination . So , um , um , we don't really know how those should be labeled . It could sh be that you should , um , not be paying that much attention to , uh , certain bands for certain sounds , uh , in order to get the best result .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So , um , what we have been doing there , just sort of mixing it all together , is certainly much {disfmarker} much cruder than that . We trained these things up on the {disfmarker} on the , uh the final label . Now we have I guess done experiments {disfmarker} you 've probably done stuff where you have , um , done separate , uh , Viterbis on the different {disfmarker}&#10;Speaker: Grad E&#10;Content: Yeah . Forced alignment on the sub - band labels ?&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad E&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: You 've done that . Did {disfmarker} did that help at all ?">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, the speakers conducted separate Viterbi analyses on different sub-band labels. However, they did not explicitly discuss the outcome or impact of this action on the overall training process. Therefore, it is not possible to accurately answer this question without additional information. The transcript only indicates that the separate Viterbi analysis was done but does not provide any details about the results or effects of this process." target=" issue of where the targets come from . So in our case when we 've been training it multi - band things , the way we get the targets for the individual bands is , uh , that we get the phonetic label {disfmarker} for the sound there&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: and we say , &quot; OK , we train every {disfmarker} &quot; What this is saying is , OK , that 's maybe what our ultimate goal is {disfmarker} or not ultimate but penultimate {vocalsound} goal is getting these {disfmarker} these small sound units . But {disfmarker} but , um , along the way how much should we , uh {disfmarker} uh , what should we be training these intermediate things for ? I mean , because , uh , we don't know uh , that this is a particularly good feature . I mean , there 's no way , uh {disfmarker} someone in the audience yesterday was asking , &quot; well couldn't you have people go through and mark the individual bands and say where the {disfmarker} where it was son">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
