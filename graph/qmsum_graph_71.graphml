<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="1. The importance of this part of the evaluation in relation to the overall outcome is not explicitly stated in the transcript. However, it is mentioned that there will be debates about it afterwards, indicating that it may have some significance.&#10;2. The conceptual significance of this aspect in the assessment is related to the emphasis on well-matched and highly mismatched evaluations. It seems that this part of the evaluation focuses on how well the system performs under various levels of matching and mismatching. The goal is to ensure that even if there are significant differences between the expected and actual inputs, the system should still be able to produce reasonable results. This is similar to how human brains can fill in gaps or imagine what should be there when we are having a conversation with noisy backgrounds." />
    <node id=" Yeah , I think that 'll be helpful . There 's {disfmarker} there 's not anybody OGI currently who 's {disfmarker} who 's , uh , working with this and {disfmarker} and&#10;Speaker: PhD E&#10;Content: Is {disfmarker} is this part of the evaluation just a small part , or ho how important is this to the overall {disfmarker} ?&#10;Speaker: Professor B&#10;Content: I {disfmarker} I think it 's {disfmarker} it 's , um {disfmarker} it depends how badly {vocalsound} you do . I mean , I think that it {disfmarker} it is {disfmarker} Uh .&#10;Speaker: PhD D&#10;Content: b&#10;Speaker: PhD E&#10;Content: This is one of those things that will be debated afterwards ?&#10;Speaker: Professor B&#10;Content: Yeah . Well , I mean , it 's {disfmarker} it 's {disfmarker} Conceptually , it {disfmarker} my impression , again , you guys correct me if I '" />
    <node id=" D&#10;Content: Oh , you already have it ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: OK , so just figure how to take the features from the final {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Um . But , yeah , I think there are plenty of issues to work on for the feature net @ @ .&#10;Speaker: Grad C&#10;Content: Feature net .&#10;Speaker: PhD E&#10;Content: What about the , um {disfmarker} uh , the new part of the evaluation , the , uh , Wall Street Journal part ?&#10;Speaker: Professor B&#10;Content: Right . Right . Um . Have you ever {disfmarker} ? Very good question . Have you ever worked with the Mississippi State h uh , software ?&#10;Speaker: PhD A&#10;Content: Sorry .&#10;Speaker: PhD E&#10;Content: No . Not yet .&#10;Speaker: Professor B&#10;Content: Oh . Well you {disfmarker} you may be called upon to help , uh , uh ," />
    <node id=" should give you a one or a zero . Y you passed the threshold or you didn't pass the threshold , and they shouldn't even care about what the score is .&#10;Speaker: Professor B&#10;Content: Yeah . But , I mean , we 'll {disfmarker} we 'll {disfmarker} we 'll see what they come up with . Uh , but in {disfmarker} in the current thing , for instance , where you have this well - matched , moderately - matched , and {disfmarker} and mis highly - mismatched , uh , the emphasis is somewhat on the {disfmarker} on the well - matched , but it 's only a {disfmarker} a marginal ,&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: right ? It 's like forty , thirty - five , twenty - five , or something like that . So you still {disfmarker} if you were way , way off on the highly - mismatched , it would have a big effect .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Spe" />
    <node id=" used to do {disfmarker} I was saying this before . I think he used to do mel , uh , spectra and mel cepstra . He used them as alternate features . Put them together .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Uh .&#10;Speaker: PhD E&#10;Content: So if you took the system the way it is now , the way it 's fro you 're gonna freeze it , and it ran it on the last evaluation , where it would it be ?&#10;Speaker: PhD A&#10;Content: Mm - hmm . It , uh ,&#10;Speaker: PhD E&#10;Content: In terms of ranking ?&#10;Speaker: PhD A&#10;Content: Ri - right now it 's second .&#10;Speaker: PhD D&#10;Content: Second . &#10;Speaker: PhD A&#10;Content: Um .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Although you {disfmarker} you know , you haven't tested it actually on the German and Danish , have you ?&#10;Speaker: PhD A&#10;Content: No , we didn't . No ," />
    <node id=" this much effect &quot; {disfmarker} I mean , you don't want to change six things and then see what happens . You want to change them one at a time .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So adding this other stream in , that 's simple in some way . And then {pause} saying , oh {disfmarker} uh {disfmarker} particularly because we 've found in the past there 's all these {disfmarker} these {disfmarker} these different results you get with slight modifications of how you do normalization . Normalization 's a very tricky , sensitive thing and {pause} you learn a lot . So , I would think you would wanna {pause} have some baseline that says , &quot; OK , we don't normalize , this is what we get &quot; , when we do this normalization , when we do that normalization . But {disfmarker} but the other question is {disfmarker} So I think ultimately we 'll wind up doing some normalization . I agree .&#10;Speaker: PhD E&#10;Content: So this second stream , will it" />
    <node id=": W I don't know . Anyway , that 's {disfmarker}&#10;Speaker: PhD E&#10;Content: I think part of the difficulty is that a l a lot of the robustness that we have is probably coming from a much higher level .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: You know , we understand the context of the situation when we 're having a conversation . And so if there 's noise in there , you know , our brain fills in and imagines what {disfmarker} what should be there .&#10;Speaker: Professor B&#10;Content: Well that {disfmarker}&#10;Speaker: Grad C&#10;Content: Yeah . We 're {disfmarker} we 're doing some sort of prediction of what {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah , exactly .&#10;Speaker: Professor B&#10;Content: Oh , sure , that 's really big .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh , but I mean , even if you do um , uh , diagnostic rhyme test kind of things ," />
    <node id="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." />
    <node id=" , wh what I called a useless comments because I 'm not really telling you how to do it . But I mean , it 's a {disfmarker} {vocalsound} it 's {disfmarker} it 's , you know {disfmarker} it&#10;Speaker: PhD E&#10;Content: No , but I think the important part in there is that , you know , if you want to be discriminative , you have to have uh , you know , categories .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: And I think this {disfmarker} the important categories are the words , and {pause} not the phones .&#10;Speaker: Professor B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD E&#10;Content: Maybe . And so {disfmarker} Right . If you can put the words in to the loop somehow for determining goodness of your sets of clusters {disfmarker} Uh {disfmarker}&#10;Speaker: Professor B&#10;Content: Now , that being said , I think that {disfmarker} that if you have something that is , um {disfmarker" />
    <node id="&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Since we 're here ?&#10;Speaker: PhD E&#10;Content: Go ahead , Morgan .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: That 's all folks ." />
    <node id=" B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: And and so I guess what I don't understand is how to do that and still be discriminative , because to be discriminative you have to have categories and the only categories that we know of to use are sort of these human {disfmarker} human sig significant {disfmarker} categories that are significant to humans , like phonemes , things like that .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: But that 's sort of what you want to avoid . And so that feels {disfmarker} I don't know how to get out of this .&#10;Speaker: Professor B&#10;Content: Well , here 's a {disfmarker} here 's a , uh , uh Here 's a generic and possibly useless thought , which is , {vocalsound} um , what do you really {disfmarker} I mean , in a sense the only s s systems that make sense , uh , are ones that {disfmarker} that have something from top - down in th in them . Right ? Because if e even the smallest" />
    <node id=" , uh , are ones that {disfmarker} that have something from top - down in th in them . Right ? Because if e even the smallest organism that 's trying to learn to do anything , if it doesn't have any kind of reward for doing {disfmarker} or penal penalty for doing anything , then it 's just going to behave randomly .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So whether you 're talking about something being learned through evolution or being learned through experience , it 's gotta have something come down to it that gives its reward or , you know , at least some reinforcement learning ,&#10;Speaker: PhD E&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: right ?&#10;Speaker: PhD E&#10;Content: So the question is , how far down ?&#10;Speaker: Professor B&#10;Content: And&#10;Speaker: PhD E&#10;Content: We could stop at words , but we don't , right ? We go all the way down to phonemes .&#10;Speaker: Professor B&#10;Content: Right , but I me I {disfmarker} I think that maybe in some ways part" />
    <node id="&#10;Content: Now , that being said , I think that {disfmarker} that if you have something that is , um {disfmarker} i Once you start dealing with spontaneous speech , all the things you 're saying are {disfmarker} are really true .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: If you {pause} have read speech that 's been manually annotated , like TIMIT , then , you know , i i you the phones are gonna be right , actually , {vocalsound} for the most part .&#10;Speaker: PhD E&#10;Content: Yeah . Yeah ,&#10;Speaker: Professor B&#10;Content: So {disfmarker} so , uh , it doesn't really hurt them to {disfmarker} to do that , to put in discrimination at that level .&#10;Speaker: PhD E&#10;Content: yeah .&#10;Speaker: Professor B&#10;Content: Um , if you go to spontaneous speech then it 's {disfmarker} it 's trickier and {disfmarker} and {disfmarker} and , uh , the phones are {disf" />
    <node id=" so everybody goes to phonemes . But the problem is that we {disfmarker} we build models of words in terms of phonemes and these models are {disfmarker} are really cartoon - ish , right ? So when you look at conversational speech , for example , you don't see the phonemes that you {disfmarker} that you have in your word models .&#10;Speaker: Professor B&#10;Content: Yeah . But {disfmarker} but {disfmarker} but we 're not trying for models of words here . See , so her here 's maybe where {disfmarker} If the issue is that we 're trying to come up with , um , some sort of intermediate categories which will then be useful for later stuff , uh , then {pause} maybe it doesn't matter that we can't have enough {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: I mean , what you wanna do is {disfmarker} is build up these categories that are {disfmarker} that are best for word recognition .&#10;" />
    <node id="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." />
    <node id="Content: Hmm . Yeah , um . In fact , I mean , if you look into it a little bit , it might be reasonable {disfmarker} You know Joe , right ? Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Just to sort of ask him about the issue of , um , different features having different kinds of , uh , scaling characteristics and so on . So that , you know , w w possibly having entirely different optimal values for {disfmarker} for the usual twiddle factors and what 's {disfmarker} what 's the plan about that ?&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: So sh shall we , like , add Chuck also to the mailing lists ? It may be better , I mean , in that case if he 's going to {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Because there 's a mailing list for this .&#10;Speaker: Professor B&#10;Content: Is that OK ?&#10;Speaker: PhD E&#10;Content: Yeah , that 'd be great ." />
    <node id=" So these sugges these {disfmarker} this , uh , period during which people are gonna make suggestions is to know whether it is actually biased towards any set of features or {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , so I th th certainly the thing that I would want to know about is whether we get really hurt , uh , on in insertion penalty , language model , scaling , sorts of things .&#10;Speaker: PhD E&#10;Content: Using our features .&#10;Speaker: Professor B&#10;Content: Yeah , yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh , in which case , um , H Hari or Hynek will need to , you know , push the case {pause} more about {disfmarker} about this .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um .&#10;Speaker: PhD E&#10;Content: And we may be able to revisit this idea about , you know , somehow modifying our features to work with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yes . In this case , that 's" />
    <node id="} I guess it 's almost ready .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: So {disfmarker} That 's what {disfmarker} So they have released their , uh , document , describing the system .&#10;Speaker: Professor B&#10;Content: Maybe you could , uh , point it {pause} at Chuck ,&#10;Speaker: PhD E&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: because , I mean {disfmarker}&#10;Speaker: PhD D&#10;Content: Sure .&#10;Speaker: PhD E&#10;Content: So we 'll have to grab this over CVS or something ?&#10;Speaker: PhD D&#10;Content: It - no , it 's just downloadable from their {disfmarker} from their web site .&#10;Speaker: PhD E&#10;Content: Is that how they do it ? OK .&#10;Speaker: Professor B&#10;Content: Cuz one of the things that might be helpful , if you 've {disfmarker} if you 've got time in all of this is , is if {disfmarker} if these guys are really focusing on" />
    <node id="&#10;Speaker: Professor B&#10;Content: I think th the biggest we 've run into for storage is the neural net . Right ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . Um . And so I guess the issue there is , are we {disfmarker} are we using neural - net - based TRAPS , and {disfmarker} and how big are they ? So that 'll {disfmarker} that 'll be , you know , an issue .&#10;Speaker: Grad C&#10;Content: Oh , right .&#10;Speaker: Professor B&#10;Content: Maybe they can be little ones .&#10;Speaker: Grad C&#10;Content: Yeah . Cuz sh Right .&#10;Speaker: Professor B&#10;Content: Mini - TRAPS .&#10;Speaker: Grad C&#10;Content: Cuz she also does the , uh {disfmarker} the correlation - based , uh , TRAPS , with without the neural net , just looking at the correlation between {disfmarker}&#10;Speaker: Professor B&#10;Content: Right . And maybe for VAD they would be OK . Yeah . Yeah .&#10;Speaker: Grad C&#10;" />
    <node id="er} to the set of phonemes that you already have . Um , whereas maybe we want to just take {disfmarker} take a look at , um , arbitrary windows in time , um , of varying length , um , and cluster those .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: And I 'm thinking if we {disfmarker} if we do that , then we would probably , um , at some point in the clustering algorithm find that we 've clustered things like , OK , thi this is a transition , um , this is a relatively stable {disfmarker} stable point .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: Um , and I 'm hoping to find other things of {disfmarker} of similarity and maybe use these things as the intermediate , um {disfmarker} intermediate categories that , uh , um , I 'll later classify .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Are you looking at these in narrow bands ?&#10;Speaker: Grad" />
    <node id="According to the conversation between Professor B and Grad C, creating a simple neural network for performing correlation using learned weights from data does not require a complex function. Professor B states, &quot;And so, uh, the question is, how complex a function do you need? Do you need to have an added layer or something?&quot; This implies that the initial assumption was that a simple neural network would be sufficient, but they are considering whether a more complex model with additional layers might be necessary. However, Professor B's tone of voice and follow-up statement suggest that they believe a simpler model will likely be sufficient. Therefore, based on this conversation, it seems that the level of complexity required for the function in creating a simple neural net for performing correlation using learned weights from data is not high, and an additional layer may not be necessary." />
    <node id="&#10;Speaker: Professor B&#10;Content: Right . And maybe for VAD they would be OK . Yeah . Yeah .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: That 's true .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Or a simple neural net , right ? I mean , the thing is , if you 're doing correlation , you 're just doing a simple {disfmarker} uh , uh {disfmarker} uh , dot product , you know , with some weights which you happened to learn from this {disfmarker} learn from the data .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And so , uh , putting a nonlinearity on it is , {pause} you know , not that big a deal . It certainly doesn't take much space .&#10;Speaker: Grad C&#10;Content: Mm - hmm . Right .&#10;Speaker: Professor B&#10;Content: So , uh , the question is , how complex a function do you need ? Do you need to have an added layer or something ? In which" />
    <node id=" PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , OK . And then the mel and then the log , and then the&#10;Speaker: PhD A&#10;Content: Then the LDA filter ,&#10;Speaker: Professor B&#10;Content: LDA filter .&#10;Speaker: PhD A&#10;Content: mmm , then the downsampling ,&#10;Speaker: Professor B&#10;Content: And then uh downsample ,&#10;Speaker: PhD A&#10;Content: DCT ,&#10;Speaker: Professor B&#10;Content: DCT ,&#10;Speaker: PhD A&#10;Content: then , um , on - line normalization ,&#10;Speaker: Professor B&#10;Content: on - line norm ,&#10;Speaker: PhD A&#10;Content: followed by {pause} upsampling . Then finally , we compute delta and we put the neural network also .&#10;Speaker: Professor B&#10;Content: Right , and then in parallel with {disfmarker} an {disfmarker} a neural net . And then following neural net , some {disfmarker} probably some orthogonalization .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh" />
    <node id="isfmarker} probably some orthogonalization .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh {disfmarker} Um .&#10;Speaker: PhD A&#10;Content: And finally frame dropping , which um , {vocalsound} would be a neural network also , used for estimated silence probabilities . And the input of this neural network would be somewhere between log {pause} mel bands or one of the earlier stages of the processing .&#10;Speaker: Professor B&#10;Content: Mm - hmm . So that 's sort of {disfmarker} most of this stuff is {disfmarker} yeah , is operating parallel with this other stuff .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . So the things that we , um , uh , I guess we sort of {disfmarker} uh , There 's {disfmarker} there 's some , uh , neat ideas for {vocalsound} V A So , I mean , in {disfmarker} I think there 's sort of like {disfmarker} There 's a bunch" />
    <node id=" orthogonalizing .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: But , you know {disfmarker} and {disfmarker} and , um , this is a little harder because you 're not just trying to find parameters . You 're actually trying to find the {disfmarker} the {disfmarker} the {disfmarker} the categories themselves . Uh , so a little more like brain surgery , I think on yourself . Uh . So , uh&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Um , anyway . That 's my {pause} thought .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: You 've been thinking about this problem for a long time actually . I mean , well {disfmarker} W actually , you stopped thinking about it for a long time , but you used to think about it {vocalsound} a lot .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: And you 've been thinking about it more now ,&#10;Speaker: PhD D" />
    <node id="Grad C's suggestion for discovering intermediate categories for classifying data is to use arbitrary windows in time of varying length and cluster those. This method differs from Sangita's approach, which focuses on specific temporal patterns for a certain set of phonemes. By clustering arbitrary windows in time, Grad C aims to find similarities and create intermediate categories that can be used in later classification stages. These intermediate categories may include transitions or relatively stable points in speech. This approach aligns with the idea of unsupervised learning as it allows for the discovery of potential patterns without relying on predefined human-significant categories such as phonemes." />
    <node id=" Professor B&#10;Content: I ju you guys are Well , y anyway , you don't have to decide this second but thi think about it {disfmarker} about what {disfmarker} what you would think would be the {disfmarker} the best way to work it . I 'll&#10;Speaker: PhD A&#10;Content: But , uh {pause} Huh . Mm - hmm .&#10;Speaker: Professor B&#10;Content: support it either way , so .&#10;Speaker: PhD A&#10;Content: Mm - hmm Right .&#10;Speaker: Professor B&#10;Content: OK . Uh . Got anything to tell us ?&#10;Speaker: Grad C&#10;Content: Um . Well , I 've been reading some literature about clustering of data . Just , um , I guess , let me put it in context . OK , so we 're talking about discovering intermediate categories to , um {disfmarker} to classify . And , uh , I was looking at some of the work that , uh , Sangita was doing on these TRAPS things . So she has , um {disfmarker} she has temporal patterns for , um , a certain set of phonemes ," />
    <node id=" mej metric which , uh , measures how {disfmarker} how closely related they are . And you start , um {vocalsound} by merging the patterns that are most closely related .&#10;Speaker: PhD E&#10;Content: And you create a tree .&#10;Speaker: Grad C&#10;Content: And y yeah , yeah , a dendrogram tree .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: Um .&#10;Speaker: PhD E&#10;Content: And then you can pick , uh , values anywhere along that tree to fix your set of clusters .&#10;Speaker: Grad C&#10;Content: Right , usually it 's when , um {disfmarker} when the sol similarity measures , um , don't go down as much .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: And so , uh {disfmarker} so you stop at that point . And what she found was , sh um , was there were five broad , um {disfmarker} broad categories , uh , corresponding to , uh , things like , uh , fricatives and , uh ," />
    <node id=" representation ,&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: I mean , if you 're going for this sort of thing where you have {pause} uh , little detectors that are looking at narrow bands , then what you 're going to be looking for should be some category that you can find with the narrow bands .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: That {disfmarker} that seems to be kind of fundamental to it . Um , and then the other thing , uh , is {disfmarker} that I wonder about with it , and {disfmarker} and don't take this in the wrong way , like I {disfmarker} I know what I 'm doing or anything ,&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: but , I mean . {vocalsound} Um , just wondering really .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um , the sort of standard answer about this sort of thing is that if you 're trying to find" />
    <node id="The difficulty in considering reverberation (echoes) in isolation when examining the real case is that there is a significant interaction between reverberation and noise. In real-world situations, background noise combined with reverberation can significantly impact the performance of various techniques designed to address these issues.&#10;&#10;When reverberation and noise are considered separately, certain methods may work well for artificial or controlled environments. However, when they occur together in a real situation, those same techniques might not perform as expected. This is because noise can make it more difficult to separate the desired signal from the reverberation, making it essential to consider both factors simultaneously.&#10;&#10;By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques could be minimized, resulting in better overall performance." />
    <node id=" PhD E&#10;Content: I guess he 's busy with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , prelims , right .&#10;Speaker: Grad C&#10;Content: Pre - prelim hell .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh , but {disfmarker} but , you know , that 'll {disfmarker} uh , it 's clear that we , uh {disfmarker} we are not {disfmarker} with the real case that we 're looking at , we can't just look at reverberation in isolation because the interaction between that and noise is {disfmarker} is considerable . And that 's I mean , in the past we 've looked at , uh , and this is hard enough , the interaction between channel effects and {disfmarker} and , uh {disfmarker} and additive noise , uh , so convolutional effects and {disfmarker} and additive effects . And that" />
    <node id="er} well , we do , but we don't {disfmarker} don't re - synthesize . In {disfmarker} in the program we don't re - synthesize and then re - analyze once again . We just use the clean FFT bins .&#10;Speaker: Professor B&#10;Content: But you have a re - synthesized thing that you {disfmarker} that 's an {disfmarker} an option here .&#10;Speaker: PhD A&#10;Content: This is an option that {disfmarker} then you can {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , I gu I guess my point is that , um , i in some of the work he 's doing in reverberation , one of the things that we 're finding is that , uh , it 's {disfmarker} it 's {disfmarker} for the {disfmarker} for an artificial situation , we can just deal with the reverberation and his techniques work really well . But for the real situation uh , problem is , is that you don't just have reverberation , you have reverberation in noise . And if you don" />
    <node id=" . But for the real situation uh , problem is , is that you don't just have reverberation , you have reverberation in noise . And if you don't include that in the model , it doesn't work very well . So in fact it might be a very nice thing to do , to just take the noise removal part of it and put that in front of what he 's looking at . And , uh , generate new files or whatever , and {disfmarker} and , uh , uh {disfmarker} and then do the reverberation part .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Anyway .&#10;Speaker: PhD E&#10;Content: So Dave hasn't {pause} tried that yet ?&#10;Speaker: Professor B&#10;Content: No , no . He 's {disfmarker} I mean , e&#10;Speaker: PhD E&#10;Content: I guess he 's busy with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , prelims ," />
    <node id=" , uh {disfmarker} and additive noise , uh , so convolutional effects and {disfmarker} and additive effects . And that 's hard enough . I mean , I don't think we really {disfmarker} I mean , we 're trying to deal with that . In a sense that 's what we 're trying to deal with in this Aurora task . And we have , uh , the , uh , uh , LDA stuff that in principle is doing something about convolutional effects . And we have the noise suppression that 's doing something about noise . Uh , even that 's hard enough . And {disfmarker} and the on - line normalization as well , in that s category . i i There 's all these interactions between these two and that 's part of why these guys had to work so hard on {disfmarker} on juggling everything around . But now when you throw in the reverberation , it 's even worse , because not only do you have these effects , but you also have some long time effects . And , um , so Dave has something which , uh , is doing some nice things under some conditions with {disfmarker} with long time" />
    <node id=" effects . And , um , so Dave has something which , uh , is doing some nice things under some conditions with {disfmarker} with long time effects but when it 's {disfmarker} when there 's noise there too , it 's {disfmarker} it 's {disfmarker} it 's pretty hard . So we have to start {disfmarker} Since any {disfmarker} almost any real situation is gonna have {disfmarker} uh , where you have the microphone distant , is going to have both things , we {disfmarker} we actually have to think about both at the same time .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: So , um {disfmarker} So there 's this noise suppression thing , which is sort of worked out and then , uh , maybe you should just continue telling what {disfmarker} what else is in the {disfmarker} the form we have .&#10;Speaker: PhD A&#10;Content: Yeah , well , {vocalsound} the , um , the other parts of the system are the {disfmarker" />
    <node id=" A So , I mean , in {disfmarker} I think there 's sort of like {disfmarker} There 's a bunch of tuning things to improve stuff . There 's questions about {pause} various places where there 's an exponent , if it 's the right exponent , or {pause} ways that we 're estimating noise , that we can improve estimating noise . And there 's gonna be a host of those . But structurally it seemed like the things {disfmarker} the main things that {disfmarker} that we brought up that , uh , are {disfmarker} are gonna need to get worked on seriously are , uh , uh , a {disfmarker} {vocalsound} a significantly better VAD , uh , putting the neural net on , um , which , you know , we haven't been doing anything with , the , uh , neural net at the end there , and , uh , the , uh , {vocalsound} opening up the second front . Uh .&#10;Speaker: PhD E&#10;Content: The other half of the channel ?&#10;Speaker: Professor B&#10;Content: Yeah , yeah , I mean , cuz we {" />
    <node id="According to the conversation, while open-source projects often use anonymous CVS mechanisms, Professor B states that they're &quot;still so much in development&quot; and want to restrict access to insiders for their project. PhD E initially brought up the idea of using an anonymous mechanism for their transcripts, but after considering the project's stage and the desire to limit access, they decided against it. Therefore, based on the conversation, the anonymous mechanism is not currently available or feasible for their transcripts, given the project's development stage and the group's decision to limit access to insiders." />
    <node id=" server but then , yeah , you can access it .&#10;Speaker: PhD E&#10;Content: So that {disfmarker} Oh , OK .&#10;Speaker: PhD A&#10;Content: you {disfmarker} you can set up priorities .&#10;Speaker: PhD E&#10;Content: So the anonymous mechanism {disfmarker}&#10;Speaker: PhD A&#10;Content: You can access them and mostly if you {disfmarker} if y the set the server is set up like this .&#10;Speaker: PhD E&#10;Content: OK . Because a lot of the open source stuff works with anonymous CVS and I 'm just wondering {disfmarker} Uh , I mean , for our transcripts we may want to do that .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Uh .&#10;Speaker: Professor B&#10;Content: Yeah , for this stuff I don't think we 're {pause} quite up to that . I mean , we 're still so much in development .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Yeah ,&#10;Spe" />
    <node id=" . I mean , we 're still so much in development .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Yeah ,&#10;Speaker: Professor B&#10;Content: We want to have just the insiders .&#10;Speaker: PhD E&#10;Content: yeah , yeah . Oh , I wasn't suggesting for this . I 'm {pause} thinking of the Meeting Recorder {comment} stuff&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: but . Yeah . OK . Cool .&#10;Speaker: Professor B&#10;Content: Yeah . So , uh {disfmarker}&#10;Speaker: PhD E&#10;Content: What 's new ?&#10;Speaker: Professor B&#10;Content: Well , I mean , I think maybe the thing to me might be {disfmarker} I me I 'm sure you 've just been working on {disfmarker} on , uh , details of that since the meeting , right ? And so {disfmarker}&#10;Speaker: PhD A&#10;Content: Mmm , since the meeting , well , I {disfmarker} I 've been {disfmarker} I 've been train" />
    <node id=" .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: You probably received the mail .&#10;Speaker: PhD E&#10;Content: Oh , right , I saw {disfmarker} I saw the note .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: What was the update ?&#10;Speaker: PhD A&#10;Content: What was the update ? So there is th then {disfmarker} the {disfmarker} all the new features that go in .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: The , um , noise suppression , the re - synthesis of speech after suppression . These are the {disfmarker}&#10;Speaker: PhD E&#10;Content: Is the , um {disfmarker} the CVS mechanism working {pause} well ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Are {disfmarker} are people , uh , up at OGI grabbing code uh , via that ?&#10;Speaker: PhD D" />
    <node id=": Yeah .&#10;Speaker: PhD A&#10;Content: I logged in there and I tried {pause} to import {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah ? It worked good ?&#10;Speaker: PhD A&#10;Content: Yeah , it works .&#10;Speaker: PhD E&#10;Content: Oh , good !&#10;Speaker: PhD A&#10;Content: But it 's {disfmarker} So , right now it 's the mechanism with SSH .&#10;Speaker: PhD D&#10;Content: Oh .&#10;Speaker: PhD E&#10;Content: Great !&#10;Speaker: PhD A&#10;Content: I don't {pause} s I didn't set up {disfmarker} You can also set up a CVS server {pause} on a new port . It 's like well {pause} uh , a main server , or d You can do a CVS server .&#10;Speaker: PhD E&#10;Content: Yeah . Right . Then that 's using the CVS password mechanism and all that ,&#10;Speaker: PhD A&#10;Content: But . Yeah , right .&#10;Speaker: PhD E&#10;Content: right ?&#10;Speaker: PhD A&#10;Content: But" />
    <node id="1. Challenges in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;2. Previous difficulties with channel effects and additive noise: Similar challenges have been experienced when examining the interaction between channel effects (e.g., distortions caused by transmission channels) and additive noise (unwanted signals added to the desired signal). Addressing these interactions is difficult, as convolutional effects (related to channel characteristics) and additive noise need to be considered concurrently. This complexity has been a challenge in past research efforts, such as the Aurora task, which aims to develop solutions for handling these issues.&#10;3. Impact on performance: When reverberation is not taken into account in conjunction with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. As a result, this could lead to suboptimal overall performance.&#10;4. Proposed solution: In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in real-world situations where both noise and reverberation are present." />
    <node id="Based on the conversation provided, it appears that PhD A has successfully set up a CVS server using the existing SSH mechanism for authentication. There is no clear mention of setting up a CVS server on a new port in addition to the current SSH mechanism. Therefore, as of this conversation, the proposed setup with a CVS password mechanism for authentication on a new port has not been implemented or discussed further." />
    <node id=" PhD A&#10;Content: But . Yeah , right .&#10;Speaker: PhD E&#10;Content: right ?&#10;Speaker: PhD A&#10;Content: But I didn't do that because I was not sure about {pause} security problems . I {disfmarker} I would have to {disfmarker}&#10;Speaker: PhD E&#10;Content: So w when you came in from Belgian {disfmarker} {comment} Belgium , using SSH , uh , was it asking you for your own {pause} password into ICSI ? So if yo you can only do that if you have an account at ICSI ?&#10;Speaker: PhD A&#10;Content: Right . Yeah .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Cuz there is an {disfmarker} a way to set up anonymous CVS right ?&#10;Speaker: PhD A&#10;Content: Yeah , you ha in this way you ca you have to set up a CVS server but then , yeah , you can access it .&#10;Speaker: PhD E&#10;Content: So that {disfmarker} Oh , OK ." />
    <node id=" E&#10;Content: So has {disfmarker} Has anybody tried remotely accessing the CVS using , uh , uh , SSH ?&#10;Speaker: Professor B&#10;Content: Yeah . &#10;Speaker: PhD A&#10;Content: Um , I don't know if Hari did that or {disfmarker} You d&#10;Speaker: PhD D&#10;Content: I {comment} can actually do it today . I mean , I can just log into {disfmarker}&#10;Speaker: PhD E&#10;Content: Have you tried it yet ?&#10;Speaker: PhD D&#10;Content: No , I didn't . So I I 'll try it today .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Good idea .&#10;Speaker: PhD A&#10;Content: Actually I {disfmarker} I tried wh while {disfmarker} when I installed the {pause} repository , I tried from Belgium .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: I logged in there and I tried {pause} to import {disfmarker}&#10;Spe" />
    <node id="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns." />
    <node id="1. The significance of November fifteenth is that it is the date when the evaluation is scheduled to take place. There is a possibility that the evaluation date may be pushed off, as Professor B mentions that some people are going away and it would be awkward if the evaluation was pushed off for a long while. However, they are hoping it won't be delayed for too long." />
    <node id="er} and so forth . It {disfmarker} it {disfmarker} it seems pretty tight to me .&#10;Speaker: PhD E&#10;Content: So wha what 's the significance of November fifteenth ?&#10;Speaker: Professor B&#10;Content: That 's when the evaluation is .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah . So , yeah , so after {disfmarker} But , you know , they may even decide in the end to push it off . It wouldn't , you know , entirely surprise me . But , uh , due to other reasons , like some people are going away , I 'm {disfmarker} I 'm hoping it 's not pushed off for {vocalsound} a l a long while . That would be , uh {disfmarker} put us in an awkward position . But {disfmarker} Anyway .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Great . Yeah , I think that 'll be helpful . There 's {disfmarker} there 's not anybody OGI currently who 's {d" />
    <node id=" , somehow modifying our features to work with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yes . In this case , that 's right .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: That 's right . Um , some of that may be , uh , a last minute rush thing because if the {disfmarker} if our features are changing {disfmarker} Uh .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh . But , um . Yeah , the other thing is that even though it 's months away , uh , it 's starting to seem to me now like November fifteenth is right around the corner . And , um , if they haven't decided things like this , like what the parameters are gonna be for this , uh , when &quot; deciding &quot; is not just somebody deciding . I mean , in fact there should be some understanding behind the , uh , {vocalsound} deciding , which means some experiments and {disfmarker} and so forth . It {disfmarker} it {disfmarker} it seems pretty tight to me .&#10;Speaker: PhD" />
    <node id="1. Based on the meeting transcript, it appears that no one is currently working on the Aurora code at OGI. The consensus among the participants is that someone, possibly Sunil, will &quot;grab&quot; or work on the Aurora code when they are at OGI for a different task. This suggests that while there isn't active development on Aurora at OGI right now, it is expected to be addressed as part of other tasks in the future." />
    <node id="&#10;Content: Are {disfmarker} are people , uh , up at OGI grabbing code uh , via that ?&#10;Speaker: PhD D&#10;Content: Uh , I don't think {disfmarker} I don't think {disfmarker}&#10;Speaker: PhD E&#10;Content: Or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: I don't know if they use it , but .&#10;Speaker: PhD D&#10;Content: Yeah , I I don't think anybody up there is like {pause} working on it right now .&#10;Speaker: PhD E&#10;Content: Uh - huh . Mmm .&#10;Speaker: Professor B&#10;Content: I think it more likely that what it means is that when Sunil is up there {vocalsound} he will grab it .&#10;Speaker: PhD D&#10;Content: Yeah . Yeah . So right now nobody 's working on Aurora there .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: They 're {disfmarker} Yeah . They 're working on a different task .&#10;Speaker: PhD E&#10;Content: I see . I see .&#10;" />
    <node id=" PhD D&#10;Content: So , like {disfmarker} like , it 's , like {disfmarker}&#10;Speaker: Professor B&#10;Content: Have you thought about {pause} how long {pause} would be uh , most useful for you to go up to OGI ?&#10;Speaker: PhD A&#10;Content: I don't know , uh . We can {disfmarker} {vocalsound} For September , we can set up a work schedule and we can maybe work independently . And then at some point it maybe be better to work together again .&#10;Speaker: Professor B&#10;Content: Oh , so you 're {disfmarker} you 're imagining more that you would come back here first for a while and then {disfmarker} and then go up there ?&#10;Speaker: PhD A&#10;Content: I {disfmarker}&#10;Speaker: Professor B&#10;Content: I mean , it 's to you .&#10;Speaker: PhD A&#10;Content: Maybe , yeah .&#10;Speaker: Professor B&#10;Content: I ju you guys are Well , y anyway , you don't have to decide this second but thi think about it {disf" />
    <node id="1. Difficulties in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;   &#10;2. Importance of taking into account the interaction between reverberation and noise: If reverberation is not taken into account along with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in situations where both noise and reverberation are present." />
    <node id="1. Phone-based speech recognition systems struggle with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined.&#10;2. Humans have a mechanism that allows them to understand munged word models in conversional speech because they possess some kind of cognitive ability that enables them to deal with missing or distorted phonemes within words. This mechanism doesn't seem to affect human comprehension as much, indicating an inherent flexibility in the way humans process and interpret spoken language compared to rigid phone-based systems." />
    <node id="marker} it 's trickier and {disfmarker} and {disfmarker} and , uh , the phones are {disfmarker} uh , you know , it 's gonna be based on bad pronunciation models that you have of {disfmarker}&#10;Speaker: PhD E&#10;Content: &#10;Speaker: Professor B&#10;Content: and , um {disfmarker} And it won't allow for the overlapping phenomenon&#10;Speaker: PhD E&#10;Content: Mmm . So it 's almost like there 's this mechanism that we have that , you know , when {disfmarker} when we 're hearing read speech and all the phonemes are there you know , we {disfmarker} we deal with that , but {disfmarker} but when we go to conversational , and then all of a sudden not all the phonemes are there , it doesn't really matter that much to us as humans because we have some kind of mechanism that allows for these word models , whatever those models are , to be {pause} munged , you know , and {disfmarker} and it doesn't really hurt , and I 'm not sure" />
    <node id="The fundamental category to look for with narrow band detectors, as mentioned by Professor B, refers to the specific type of information or signal that the detectors are designed to identify within the narrow frequency bands. However, the transcript does not provide explicit details on what this &quot;category&quot; refers to in the given context.&#10;&#10;Professor B wonders about it when trying to find a standard answer because they are contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands. The professor's tone and use of phrases like &quot;in the wrong way&quot; and &quot;just wondering really&quot; suggest that this question is not meant as criticism, but rather an invitation for Grad C to consider standard practices in their work." />
    <node id="&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Are you looking at these in narrow bands ?&#10;Speaker: Grad C&#10;Content: Um , right . F um , I 'm {disfmarker}&#10;Speaker: Professor B&#10;Content: Cuz that 's what you 're gonna be using , right ?&#10;Speaker: Grad C&#10;Content: Yeah , yeah . I {disfmarker} I haven't exactly figured out , um , the exact details for that but , uh , the {disfmarker} the representation of the data that I was thinking of , was using , um , critical band , um , energies , {vocalsound} um , over different lengths of time . So {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , I mean , it seems somehow that needs th uh , there 's a couple things that I wonder about with this . I mean , so one is {disfmarker} is , {pause} again , looking at the same representation ,&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: I mean , if you 're going for this" />
    <node id=" see .&#10;Speaker: PhD D&#10;Content: So {disfmarker}&#10;Speaker: PhD E&#10;Content: About other things , but .&#10;Speaker: Professor B&#10;Content: Do you have Hari 's , uh {disfmarker} ?&#10;Speaker: PhD E&#10;Content: I have Hari 's {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , so maybe just CC Hari and say that you 've just been asked to handle the large vocabulary part here , and , uh , you know ,&#10;Speaker: PhD E&#10;Content: OK . Would it be better if I asked Hari to ask Joe ?&#10;Speaker: Professor B&#10;Content: Uh . Why don't you just ask Joe but CC Hari , and then in the note say , &quot; Hari , hopefully this is OK with you &quot; .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: And then if Joe feels like he needs a confirmation , Hari can answer it .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: That way" />
    <node id="1. The meeting with Hynek was attended by PhD E, Professor B, Sunil, and Stephane. It appears that they discussed the current progress and future plans regarding their work.&#10;2. No specific details about what was discussed in the meeting with Hynek are provided in the transcript. However, it is mentioned that Sunil and Stephane summarized where they were and talked about where they were going.&#10;3. There is no direct mention of the status of the code update in the meeting with Hynek. The conversation focus is on other ongoing tasks, such as training new VAD (Voice Activity Detection) and feature net by PhD A since Tuesday.&#10;4. Before the meeting with Hynek, it seems that PhD D had updated their code the previous day. PhD E acknowledged this update, but it's not explicitly linked to the meeting with Hynek.&#10;5. It can be inferred that no major decisions or actions related to the Aurora code were made during the meeting with Hynek, as there is no further discussion about it among the participants after the meeting." />
    <node id="Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: OK , so {pause} We {disfmarker} we had a meeting with , uh {disfmarker} with Hynek , um , in {disfmarker} in which , uh , uh , Sunil and Stephane , uh {vocalsound} summarized where they were and {disfmarker} and , uh , talked about where we were gonna go . So that {disfmarker} that happened sort of mid - week . Uh .&#10;Speaker: PhD E&#10;Content: D did {disfmarker} did you guys get your code pushed together ?&#10;Speaker: PhD D&#10;Content: Oh , yeah . Yeah . It 's {disfmarker} it 's {disfmarker} it 's {disfmarker} it was updated yesterday ,&#10;Speaker: PhD E&#10;Content: Cool .&#10;Speaker: PhD D&#10;Content: right ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: You probably received the mail .&#10;Speaker: PhD" />
    <node id=": Mmm , since the meeting , well , I {disfmarker} I 've been {disfmarker} I 've been train training a new VAD and a new {pause} feature net .&#10;Speaker: Professor B&#10;Content: That was {disfmarker} that was Tuesday . OK .&#10;Speaker: PhD A&#10;Content: So they should be ready . Um .&#10;Speaker: Professor B&#10;Content: But I guess maybe the thing {disfmarker} since you weren't {disfmarker} yo you guys weren't at that {disfmarker} that meeting , might be just {disfmarker} just to , um , sort of recap , uh , the {disfmarker} the conclusions of the meeting .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Oh , great .&#10;Speaker: Professor B&#10;Content: So .&#10;Speaker: PhD E&#10;Content: You 're talking about the meeting with Hynek ?&#10;Speaker: Professor B&#10;Content: Yeah . Cuz that was sort of , uh {disfmarker} we {disfmark" />
    <node id="&#10;Content: So , uh , the question is , how complex a function do you need ? Do you need to have an added layer or something ? In which case , uh , potentially , you know , it could be big . So .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So , uh , uh {disfmarker} So what 's next ? Maybe s s remind us .&#10;Speaker: PhD E&#10;Content: So the meeting with Hynek that you guys just had was to decide exactly what you were gonna freeze in this system ? Is that {disfmarker} ? Or was there {disfmarker} ? Were you talking about what t new stuff , or {disfmarker} ?&#10;Speaker: Professor B&#10;Content: What to freeze and then what to do after we froze .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Yeah . And like I was saying , I think the {disfmarker} you know , the basic directions are , uh , uh {disfmarker} I mean , there 's lots of little things , such as improve the" />
    <node id="1. The configuration applied for spectral subtraction in the Wiener filter that works well is using two steps of smoothing: recursion along time and a sliding window of twenty FFT bins along frequency, before any mel scaling has been done.&#10;2. The aspects of the system they plan to work on next are improving the noise estimator, adding a neural net at the end of the process, and opening up a second stream or &quot;front&quot; for processing." />
    <node id="aker: Professor B&#10;Content: This {disfmarker} this smoothing is done on the estimate , um , of what you 're going to subtract ? Or on the thing that has already had something subtracted ?&#10;Speaker: PhD A&#10;Content: Yeah . Uh , {vocalsound} it 's on the transfer function . So {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , it 's on the transfer function for the Wiener filter .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , OK .&#10;Speaker: PhD A&#10;Content: Yeah , so basically we tried {vocalsound} different configuration within this idea . We tried u u applying this on mel bands , having spectral subtraction instead of wiener filtering . Um . Well , finally we end up with {pause} this configuration that works , uh , quite well . So we are going to fix this for the moment and work on the other aspects of {vocalsound} the whole system .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So {disfmarker}&#10;Speaker:" />
    <node id=": PhD A&#10;Content: Yeah , well , {vocalsound} the , um , the other parts of the system are the {disfmarker} the blocks that were already present before and that we did not modify a lot .&#10;Speaker: Professor B&#10;Content: So that 's {disfmarker} again , that {disfmarker} that 's the Wiener filtering , followed by , uh {disfmarker} uh , that 's done at the FFT level . Then {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah , th then the mel filter bank ,&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: then the log operation ,&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: The {disfmarker} the {disfmarker} the filtering is done in the frequency domain ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , OK . And then the mel and then the log , and then the" />
    <node id=" noise compensation and we end up with something that seems reasonable . Um .&#10;Speaker: PhD E&#10;Content: Are you gonna use {disfmarker} which of the two techniques ?&#10;Speaker: PhD A&#10;Content: So finally it 's {disfmarker} it 's , um , Wiener filtering on FFT bins . And it uses , uh , two steps , smoothing of the transfer function , the first step , that 's along time , which use recursion . And {vocalsound} after this step there is a further smoothing along frequency , which use a sliding window of twenty FFT bins . Mmm . And , uh {disfmarker}&#10;Speaker: PhD E&#10;Content: So this is on the {disfmarker} uh , before any mel scaling has been done ?&#10;Speaker: PhD A&#10;Content: Yeah , yeah .&#10;Speaker: PhD E&#10;Content: This is {disfmarker}&#10;Speaker: PhD A&#10;Content: It was {disfmarker}&#10;Speaker: Professor B&#10;Content: This {disfmarker} this smoothing is done on the estimate , um , of what you 're going to" />
    <node id=" you know , the basic directions are , uh , uh {disfmarker} I mean , there 's lots of little things , such as improve the noise estimator but the bigger things are adding on the neural net and , uh , the second stream . And then , uh , improving the VAD . Uh . So .&#10;Speaker: PhD D&#10;Content: So , I 'll , um {disfmarker} I 'll actually {disfmarker} after the meeting I 'll add the second stream to the VAD and maybe I 'll start with the feature net in that case . It 's like , you 're looking at the VAD , right ?&#10;Speaker: PhD A&#10;Content: Uh , yeah . I I 've a new feature net ready also .&#10;Speaker: PhD D&#10;Content: I 'll {disfmarker} For the VAD ?&#10;Speaker: PhD A&#10;Content: No , uh . Well p two network , one VAD and one {pause} feature net .&#10;Speaker: PhD D&#10;Content: Oh , you already have it ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D" />
    <node id="The speakers, Professor B and PhD E, mention that some kind of normalization is being applied in both cases to deal with convolutional effects. In the speakers' project, they use online normalization and LDA RASTA, which seem to help but do so by throwing away high modulation frequencies, a technique not used in the other case. It is suggested to try normalization, particularly if logs are taken, and then compare the results, as changing multiple factors simultaneously might make it difficult to understand the impact of each change. The specific type of normalization being applied in both cases remains unspecified in the transcript." />
    <node id=" , in some sense we 're all doing fairly similar things . Uh , I mean , one could argue about the LDA and so forth but I {disfmarker} I think , you know , in a lot of ways we 're doing very similar things . But what {disfmarker} what {disfmarker}&#10;Speaker: PhD E&#10;Content: So how did they fill up this {disfmarker} all these {disfmarker} these bits ? I mean , if we 're u&#10;Speaker: Professor B&#10;Content: Um , why are we using half ? Well , so you could {disfmarker} you c&#10;Speaker: PhD E&#10;Content: Yeah . Or how are they using more than half , I guess maybe is what I {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , so I {disfmarker} I think {disfmarker} uh , you guys are closer to it than me , so correct me if I 'm wrong , but I {disfmarker} I think that what 's going on is that in {disfmarker} in both cases , some kind of normalization is done" />
    <node id="isfmarker} I think that what 's going on is that in {disfmarker} in both cases , some kind of normalization is done to deal with convola convolutional effects . Uh , they have some cepstral {pause} modification ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: right ? In our case we have a couple things . We have the on - line normalization and then we have the LDA RASTA . And {pause} they seem to comple complement each other enough and be different enough that they both seem to help {disfmarker} help us . But in any event , they 're both doing the same sort of thing . But there 's one difference . The LDA RASTA , uh , throws away high modulation frequencies . And they 're not doing that .&#10;Speaker: PhD E&#10;Content: So th So {disfmarker}&#10;Speaker: Professor B&#10;Content: So that if you throw away high modulation frequencies , then you can downsample .&#10;Speaker: Grad C&#10;Content: Get down .&#10;Speaker: PhD E&#10;Content: I see . I see" />
    <node id=" the noise removal stuff ? or after ?&#10;Speaker: Professor B&#10;Content: Well , that 's a question . I mean , we were talking about that . It looks like it 'd be straightforward to {disfmarker} to , uh , remove the noise , um , and , uh ,&#10;Speaker: PhD E&#10;Content: Cuz that happens before the mel conversion , right ?&#10;Speaker: Professor B&#10;Content: Yeah . So , I mean , to do it after the mel conversion {disfmarker} uh , after the noise removal , after the mel conversion . There 's even a question in my mind anyhow of whether th you should take the log or not . Uh . I sort of think you should , but I don't know .&#10;Speaker: PhD A&#10;Content: What about norm normalizing also ?&#10;Speaker: Professor B&#10;Content: Right . Uh . Well , but normalizing spectra instead of cepstra ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , probably . Some kind would be good . You know ? I would think .&#10;Speaker: PhD D&#10;Content: Well , it {disfmark" />
    <node id=" , probably . Some kind would be good . You know ? I would think .&#10;Speaker: PhD D&#10;Content: Well , it {disfmarker} it {disfmarker} it {disfmarker} it {disfmarker} so it actually makes it dependent on the overall energy of the {disfmarker} uh , the frame .&#10;Speaker: Professor B&#10;Content: If you do or don't normalize ?&#10;Speaker: PhD D&#10;Content: If yo if you don't normalize and {disfmarker} if {disfmarker} if you don't normalize .&#10;Speaker: Professor B&#10;Content: Right . Yes , so I mean , one would think that you would want to normalize . But I {disfmarker} I {disfmarker} w w My thought is , uh , particularly if you take the log , try it . And then if {disfmarker} if normalization helps , then y you have something to compare against , and say , &quot; OK , this much effect &quot; {disfmarker} I mean , you don't want to change six things and then see what happens . You want to change them" />
    <node id="1. The proposed plan for handling the large vocabulary part of the discussion between Professor B, PhD E, Hari, and Joe involves PhD E taking charge of this aspect. According to the conversation, PhD E will be asked to handle the large vocabulary part and should communicate this responsibility to Joe and CC Hari in a note. The plan is to have Joe involved in the discussion while keeping Hari informed, so that if Joe requires confirmation on any decisions, Hari can provide it. This approach aims to ensure both Joe's involvement and Hari's approval for the large vocabulary part of the project." />
    <node id=" this .&#10;Speaker: Professor B&#10;Content: Is that OK ?&#10;Speaker: PhD E&#10;Content: Yeah , that 'd be great .&#10;Speaker: PhD D&#10;Content: Yeah , I guess maybe Hari or Hynek , one of them , has to {pause} send a mail to Joe . Or maybe if you {disfmarker}&#10;Speaker: PhD E&#10;Content: I {disfmarker} I could send him an email .&#10;Speaker: PhD D&#10;Content: Well , yeah , to add or maybe wh&#10;Speaker: PhD E&#10;Content: I {disfmarker} I know him really well .&#10;Speaker: PhD D&#10;Content: Yeah , so that 's just fine .&#10;Speaker: PhD E&#10;Content: I {disfmarker} I was just talking with him on email the other day actually .&#10;Speaker: PhD D&#10;Content: So {disfmarker}&#10;Speaker: Professor B&#10;Content: Uh , yeah , and just , um , se maybe see .&#10;Speaker: PhD D&#10;Content: So {disfmarker}&#10;Speaker: PhD E&#10;Content: About other things , but" />
    <node id="The speech recognizer being discussed in the transcripts is a phone-based speech recognition system, which struggles with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined. The researchers mentioned in the transcripts commonly use phone-based systems, as they build models of words in terms of phonemes. However, these models can be oversimplified, creating a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;The proposed solution to improve the system is to consider using words instead of phones as categories for discriminative learning. This approach might allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." />
    <node id="Researchers are considering creating intermediate categories, such as words instead of phones (phonemes), to improve word recognition in a speech system because phone-based models can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech. Human listeners can understand munged word models in conversational speech due to cognitive abilities that enable them to deal with missing or distorted phonemes, indicating an inherent flexibility in human spoken language processing. By using words as categories, researchers aim to create a more robust and accurate speech recognition system by focusing on recognizing and distinguishing whole words rather than individual phonemes.&#10;&#10;The idea of discovering intermediate categories like phonemes comes from Grad C's suggestion to use arbitrary windows in time of varying length and cluster those, potentially revealing transitions or relatively stable points in speech. This method aligns with the concept of unsupervised learning, allowing for pattern discovery without relying on predefined human-significant categories such as phonemes.&#10;&#10;While conversational speech may not clearly exhibit these intermediate categories like phones, researchers are exploring this approach to overcome limitations associated with phone-based models and improve overall system performance." />
    <node id="The current classification system puts some emphasis on well-matched categories, but it's only marginal with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively. Being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome because the evaluation criteria give more weight to well-matched categories, so performing poorly in this area could negatively affect the overall assessment.&#10;&#10;The importance of this aspect in relation to the overall outcome is not explicitly stated in the transcript, but it's mentioned that there will be debates about it afterwards, indicating that it may have some significance. Additionally, finding a standard answer to what the detectors should focus on within narrow bands and exploring unsupervised learning approaches like Grad C's suggestion of discovering intermediate categories can help improve performance in matching various inputs accurately.&#10;&#10;The use of more suitable categories, such as words instead of phones, could also benefit discriminative learning for speech recognition tasks, as suggested by PhD E. This approach focuses on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." />
    <node id="Yes, the speakers are discussing the use of various machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM). They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, as indicated by PhD E's question: &quot;they're providing scripts and everything so that basically, you push a button and it does training, and then it does test, and everything?&quot; They also mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models." />
    <node id=" . Oh , OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: It 's just a HMM , Gaussian mixture model .&#10;Speaker: Grad C&#10;Content: Gaussian mixture HMM .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah , the SVM thing was an HMM also . It was just a {disfmarker} it {disfmarker} it {disfmarker} it was like a hybrid , like {disfmarker}&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Yeah , this is a g yeah , this i&#10;Speaker: Professor B&#10;Content: what ?&#10;Speaker: PhD D&#10;Content: yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: So , just so that I understand , they 're providing scripts and everything so that basically , uh , you {disfmarker} you push a button and it does training , and then it does test , and everything ? Is" />
    <node id=" They probably put training {disfmarker} uh , almost certain they put training data there too . Maybe not . So . That 's that . Anybody have anything else ?&#10;Speaker: PhD E&#10;Content: Uh , one {disfmarker} one last question on that . When did they estimate that they would have that system available for download ?&#10;Speaker: PhD D&#10;Content: Um , I guess {disfmarker} I guess one {disfmarker} some preliminary version is already there .&#10;Speaker: PhD E&#10;Content: Oh , so there 's w something you can download to just learn ?&#10;Speaker: PhD D&#10;Content: Yeah , it 's already there . Yeah .&#10;Speaker: PhD E&#10;Content: OK ,&#10;Speaker: PhD D&#10;Content: But they 're actually parallel - y doing some modifications also , I think .&#10;Speaker: PhD E&#10;Content: good .&#10;Speaker: PhD D&#10;Content: So I guess the f final system will be frozen by middle of , like , one more week maybe .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Oh , well that 's" />
    <node id=" I think ultimately we 'll wind up doing some normalization . I agree .&#10;Speaker: PhD E&#10;Content: So this second stream , will it add latency to the system&#10;Speaker: Professor B&#10;Content: No , it 's in parallel .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ?&#10;Speaker: Grad C&#10;Content: Para&#10;Speaker: Professor B&#10;Content: We 're not talking about computation time here .&#10;Speaker: PhD E&#10;Content: S&#10;Speaker: Professor B&#10;Content: We 're ta I think we 're pretty far out .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So it 's just in terms of what data it 's depending on . It 's depending on the same data as the other .&#10;Speaker: PhD E&#10;Content: Same data .&#10;Speaker: Professor B&#10;Content: So it 's in parallel .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: Grad C&#10;Content: So with this , uh , new stream would you train up a V" />
    <node id=" so that basically , uh , you {disfmarker} you push a button and it does training , and then it does test , and everything ? Is that {pause} the idea ?&#10;Speaker: PhD D&#10;Content: I {disfmarker} I {disfmarker} I think {disfmarker} yeah , I {disfmarker} I guess something like that . It 's like {vocalsound} {disfmarker} as painless as possible ,&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: is what {disfmarker} Do they provide all the scripts , everything , and then {disfmarker} Just ,&#10;Speaker: PhD E&#10;Content: I see . Hmm . Somehow yo there 's hooks to put your features in and {disfmarker}&#10;Speaker: PhD D&#10;Content: ju Yeah , I th I think .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Hmm . Yeah , um . In fact , I mean , if you look into it a little bit , it might be reasonable {disfmarker" />
    <node id="Yes, there is a possibility to automate the integration of error units like words or syllables into a larger unit for processing and feedback in speech recognition systems. The approach discussed by the researchers involves using words instead of phones as categories for discriminative learning in speech recognition systems. This method focuses on recognizing and distinguishing whole words rather than individual phonemes, which can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;In this context, automating the integration of error units could mean using word-level units directly in the speech recognition system, bypassing phone-based models entirely. By employing words as categories, the system can potentially improve its robustness and accuracy in processing spontaneous speech with variable phoneme appearances.&#10;&#10;Additionally, the researchers mentioned exploring unsupervised learning methods like Grad C's suggestion to use arbitrary windows in time of varying length and cluster those. This approach could help discover intermediate categories that align with transitions or relatively stable points in speech, further aiding in the accurate recognition and processing of larger units such as words or syllables for feedback in the system." />
    <node id="The fundamental category that narrow band detectors should look for refers to the specific type of information or signal these detectors are designed to identify within the narrow frequency bands. However, the text does not provide explicit details on what this &quot;category&quot; refers to in the given context. The speakers seem to be contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands.&#10;&#10;The concerns regarding the standard approach to finding this category include the current classification system's marginal emphasis on well-matched categories (with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively). The speakers mention that being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome. They also discuss debates about the importance of this aspect in relation to the overall outcome and suggest that finding a standard answer to what detectors should focus on within narrow bands could help improve performance in matching various inputs accurately.&#10;&#10;The speakers are discussing the use of machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), for tasks such as speech recognition. They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, and they mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models." />
    <node id="The brief conversation between PhD E and Professor B seems to be related to a presentation or discussion, where Professor B is asking for permission to go ahead with his point. They also discuss a modification idea brought up by PhD E, which may involve adjusting certain features for better compatibility with a language model. However, the specific topic or context of their conversation is not provided in the transcript." />
    <node id="1. Parallel processing: The conversation does not explicitly mention the specific techniques or neural networks being used in parallel. However, it is stated that a noise removal process would be applied before addressing reverberation, which could be one of the parallel processes.&#10;&#10;2. Orthogonalization: This term generally refers to transforming data to make its dimensions orthogonal (uncorrelated). In the context of signal processing and neural networks, it might involve decorrelating features or weight vectors to improve model performance. The specific method or model for orthogonalization is not mentioned in the conversation.&#10;&#10;3. Frame dropping for estimated silence probabilities: The neural network model used for frame dropping is a neural network also, with its input being somewhere between log mel bands or one of the earlier stages of processing. This neural network is likely responsible for estimating silence probabilities and deciding which frames to drop based on those estimates.&#10;&#10;While the specific techniques and models are not explicitly mentioned, the conversation touches upon parallel processing, orthogonalization, and frame dropping using a neural network for estimated silence probabilities in an audio processing pipeline that involves log mel bands, LDA filtering, downsampling, DCT, on-line normalization, upsampling, delta computation, and possibly other unspecified techniques." />
    <node id="Based on the provided transcripts, the system for improving digital processes and handling large vocabulary runs is not explicitly stated as ready. However, PhD E is assigned to take charge of the large vocabulary part and communicate this responsibility to Joe and CC Hari. It would be beneficial to have Professor B use his experience to iron out potential issues, as he has shown concern about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. His expertise could help address these concerns and improve the system's performance." />
    <node id="To create a discriminative system without using significant human categories and avoiding top-down approaches, one could consider the following strategies:&#10;&#10;1. Explore unsupervised learning methods to discover underlying patterns or structures within the data. For example, Grad C's suggestion of using arbitrary windows in time of varying length and clustering them can help identify intermediate categories that are not predefined by human significance.&#10;2. Utilize a bottom-up approach where the system learns from raw data without relying on handcrafted features or predetermined categories. This allows the model to discover its own relevant features and patterns, potentially leading to novel insights about the structure of the data.&#10;3. Investigate the use of context-dependent models that consider sequential information in the data, rather than focusing on individual segments (e.g., phones or words). These models can capture the variability present in speech by taking into account how different contexts influence the acoustic signal.&#10;4. Consider incorporating additional modalities, such as visual or semantic information, to supplement the audio input and provide alternative cues for distinguishing between categories. This can help reduce reliance on human-significant categories while still maintaining a high level of accuracy in the system.&#10;5. Develop hybrid systems that combine unsupervised learning techniques with weakly supervised methods, where some minimal guidance is provided to the model but without explicitly defining the categories. This approach could strike a balance between data-driven discovery and human expertise." />
    <node id="Based on the transcript, Professors B and E are discussing the current machine learning projects they are working on and the challenges of coordinating their efforts given their upcoming travel schedules. They mention several models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), and discuss the need to normalize data and create new streams for the models.&#10;&#10;Regarding the specific focus of their attention, Professors B and E mention a software created by &quot;these guys&quot; that is a key part of their work, and they plan to prioritize this software by keeping certain options constant and focusing on experimenting with specific variables. PhD A mentions that they have been working on noise compensation for six weeks and have made progress.&#10;&#10;Therefore, the specific focus of their attention will likely be to keep certain options constant in the machine learning models and prioritize the use and further development of the software created by &quot;these guys,&quot; possibly involving noise compensation work done by PhD A." />
    <node id="k ?&#10;Speaker: Professor B&#10;Content: Yeah . Cuz that was sort of , uh {disfmarker} we {disfmarker} we 'd sort of been working up to that , that {disfmarker} that , uh , he would come here this week and {disfmarker} and we would sort of {disfmarker}&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Since he 's going out of town like now , and I 'm going out town in a couple weeks , uh , and time is marching , sort of , given all the mu many wonderful things we could be working on , what {disfmarker} what will we actually focus on ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh {disfmarker} and what do we freeze ? And , you know , what do we {disfmarker} ? So , um . I mean , this {pause} software that these guys created was certainly a {disfmarker} a key part . So then there 's something central and there aren't" />
    <node id=" {pause} software that these guys created was certainly a {disfmarker} a key part . So then there 's something central and there aren't at least a bunch of different versions going off in {disfmarker} in ways that {pause} differ {pause} trivially . Uh , um , and , um ,&#10;Speaker: PhD E&#10;Content: Yeah . That 's {disfmarker} that 's nice .&#10;Speaker: Professor B&#10;Content: and then within that , I guess the idea was to freeze a certain set of options for now , to run it , uh , a particular way , and decide on what things are gonna be experimented with , as opposed to just experimenting with everything . So keep a certain set of things constant . So , um . Uh , maybe describe roughly what {disfmarker} what we are keeping constant for now , or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Yeah . Well . So we 've been working like six weeks on {disfmarker} on the noise compensation and we end up with something that seems reasonable . Um .&#10;Speaker: PhD E&#10;Content: Are you gonna use {disfmarker" />
    <node id=" .&#10;Speaker: Professor B&#10;Content: Oh . Well you {disfmarker} you may be called upon to help , uh , uh , on account of , uh , all the work in this stuff here has been , uh , with small vocabulary .&#10;Speaker: PhD E&#10;Content: OK . Mm - hmm . So what {disfmarker} how is the , uh , interaction supposed to happen ? Uh , I remember the last time we talked about this , it was sort of up in the air whether they were going to be taking , uh , people 's features and then running them or they were gonna give the system out or {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: PhD E&#10;Content: Oh , so they 're gonna just deliver a system basically .&#10;Speaker: PhD D&#10;Content: Yeah , yeah .&#10;Speaker: Professor B&#10;Content: Do we already have it ?&#10;Speaker: PhD D&#10;Content: Yeah , th I {disfmarker} I guess it 's almost ready .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: So" />
    <edge source="1. The importance of this part of the evaluation in relation to the overall outcome is not explicitly stated in the transcript. However, it is mentioned that there will be debates about it afterwards, indicating that it may have some significance.&#10;2. The conceptual significance of this aspect in the assessment is related to the emphasis on well-matched and highly mismatched evaluations. It seems that this part of the evaluation focuses on how well the system performs under various levels of matching and mismatching. The goal is to ensure that even if there are significant differences between the expected and actual inputs, the system should still be able to produce reasonable results. This is similar to how human brains can fill in gaps or imagine what should be there when we are having a conversation with noisy backgrounds." target=" Yeah , I think that 'll be helpful . There 's {disfmarker} there 's not anybody OGI currently who 's {disfmarker} who 's , uh , working with this and {disfmarker} and&#10;Speaker: PhD E&#10;Content: Is {disfmarker} is this part of the evaluation just a small part , or ho how important is this to the overall {disfmarker} ?&#10;Speaker: Professor B&#10;Content: I {disfmarker} I think it 's {disfmarker} it 's , um {disfmarker} it depends how badly {vocalsound} you do . I mean , I think that it {disfmarker} it is {disfmarker} Uh .&#10;Speaker: PhD D&#10;Content: b&#10;Speaker: PhD E&#10;Content: This is one of those things that will be debated afterwards ?&#10;Speaker: Professor B&#10;Content: Yeah . Well , I mean , it 's {disfmarker} it 's {disfmarker} Conceptually , it {disfmarker} my impression , again , you guys correct me if I '">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The importance of this part of the evaluation in relation to the overall outcome is not explicitly stated in the transcript. However, it is mentioned that there will be debates about it afterwards, indicating that it may have some significance.&#10;2. The conceptual significance of this aspect in the assessment is related to the emphasis on well-matched and highly mismatched evaluations. It seems that this part of the evaluation focuses on how well the system performs under various levels of matching and mismatching. The goal is to ensure that even if there are significant differences between the expected and actual inputs, the system should still be able to produce reasonable results. This is similar to how human brains can fill in gaps or imagine what should be there when we are having a conversation with noisy backgrounds." target=" D&#10;Content: Oh , you already have it ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: OK , so just figure how to take the features from the final {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Um . But , yeah , I think there are plenty of issues to work on for the feature net @ @ .&#10;Speaker: Grad C&#10;Content: Feature net .&#10;Speaker: PhD E&#10;Content: What about the , um {disfmarker} uh , the new part of the evaluation , the , uh , Wall Street Journal part ?&#10;Speaker: Professor B&#10;Content: Right . Right . Um . Have you ever {disfmarker} ? Very good question . Have you ever worked with the Mississippi State h uh , software ?&#10;Speaker: PhD A&#10;Content: Sorry .&#10;Speaker: PhD E&#10;Content: No . Not yet .&#10;Speaker: Professor B&#10;Content: Oh . Well you {disfmarker} you may be called upon to help , uh , uh ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The importance of this part of the evaluation in relation to the overall outcome is not explicitly stated in the transcript. However, it is mentioned that there will be debates about it afterwards, indicating that it may have some significance.&#10;2. The conceptual significance of this aspect in the assessment is related to the emphasis on well-matched and highly mismatched evaluations. It seems that this part of the evaluation focuses on how well the system performs under various levels of matching and mismatching. The goal is to ensure that even if there are significant differences between the expected and actual inputs, the system should still be able to produce reasonable results. This is similar to how human brains can fill in gaps or imagine what should be there when we are having a conversation with noisy backgrounds." target=" should give you a one or a zero . Y you passed the threshold or you didn't pass the threshold , and they shouldn't even care about what the score is .&#10;Speaker: Professor B&#10;Content: Yeah . But , I mean , we 'll {disfmarker} we 'll {disfmarker} we 'll see what they come up with . Uh , but in {disfmarker} in the current thing , for instance , where you have this well - matched , moderately - matched , and {disfmarker} and mis highly - mismatched , uh , the emphasis is somewhat on the {disfmarker} on the well - matched , but it 's only a {disfmarker} a marginal ,&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: right ? It 's like forty , thirty - five , twenty - five , or something like that . So you still {disfmarker} if you were way , way off on the highly - mismatched , it would have a big effect .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The importance of this part of the evaluation in relation to the overall outcome is not explicitly stated in the transcript. However, it is mentioned that there will be debates about it afterwards, indicating that it may have some significance.&#10;2. The conceptual significance of this aspect in the assessment is related to the emphasis on well-matched and highly mismatched evaluations. It seems that this part of the evaluation focuses on how well the system performs under various levels of matching and mismatching. The goal is to ensure that even if there are significant differences between the expected and actual inputs, the system should still be able to produce reasonable results. This is similar to how human brains can fill in gaps or imagine what should be there when we are having a conversation with noisy backgrounds." target=" used to do {disfmarker} I was saying this before . I think he used to do mel , uh , spectra and mel cepstra . He used them as alternate features . Put them together .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Uh .&#10;Speaker: PhD E&#10;Content: So if you took the system the way it is now , the way it 's fro you 're gonna freeze it , and it ran it on the last evaluation , where it would it be ?&#10;Speaker: PhD A&#10;Content: Mm - hmm . It , uh ,&#10;Speaker: PhD E&#10;Content: In terms of ranking ?&#10;Speaker: PhD A&#10;Content: Ri - right now it 's second .&#10;Speaker: PhD D&#10;Content: Second . &#10;Speaker: PhD A&#10;Content: Um .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Although you {disfmarker} you know , you haven't tested it actually on the German and Danish , have you ?&#10;Speaker: PhD A&#10;Content: No , we didn't . No ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The importance of this part of the evaluation in relation to the overall outcome is not explicitly stated in the transcript. However, it is mentioned that there will be debates about it afterwards, indicating that it may have some significance.&#10;2. The conceptual significance of this aspect in the assessment is related to the emphasis on well-matched and highly mismatched evaluations. It seems that this part of the evaluation focuses on how well the system performs under various levels of matching and mismatching. The goal is to ensure that even if there are significant differences between the expected and actual inputs, the system should still be able to produce reasonable results. This is similar to how human brains can fill in gaps or imagine what should be there when we are having a conversation with noisy backgrounds." target=" this much effect &quot; {disfmarker} I mean , you don't want to change six things and then see what happens . You want to change them one at a time .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So adding this other stream in , that 's simple in some way . And then {pause} saying , oh {disfmarker} uh {disfmarker} particularly because we 've found in the past there 's all these {disfmarker} these {disfmarker} these different results you get with slight modifications of how you do normalization . Normalization 's a very tricky , sensitive thing and {pause} you learn a lot . So , I would think you would wanna {pause} have some baseline that says , &quot; OK , we don't normalize , this is what we get &quot; , when we do this normalization , when we do that normalization . But {disfmarker} but the other question is {disfmarker} So I think ultimately we 'll wind up doing some normalization . I agree .&#10;Speaker: PhD E&#10;Content: So this second stream , will it">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The importance of this part of the evaluation in relation to the overall outcome is not explicitly stated in the transcript. However, it is mentioned that there will be debates about it afterwards, indicating that it may have some significance.&#10;2. The conceptual significance of this aspect in the assessment is related to the emphasis on well-matched and highly mismatched evaluations. It seems that this part of the evaluation focuses on how well the system performs under various levels of matching and mismatching. The goal is to ensure that even if there are significant differences between the expected and actual inputs, the system should still be able to produce reasonable results. This is similar to how human brains can fill in gaps or imagine what should be there when we are having a conversation with noisy backgrounds." target=": W I don't know . Anyway , that 's {disfmarker}&#10;Speaker: PhD E&#10;Content: I think part of the difficulty is that a l a lot of the robustness that we have is probably coming from a much higher level .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: You know , we understand the context of the situation when we 're having a conversation . And so if there 's noise in there , you know , our brain fills in and imagines what {disfmarker} what should be there .&#10;Speaker: Professor B&#10;Content: Well that {disfmarker}&#10;Speaker: Grad C&#10;Content: Yeah . We 're {disfmarker} we 're doing some sort of prediction of what {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah , exactly .&#10;Speaker: Professor B&#10;Content: Oh , sure , that 's really big .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh , but I mean , even if you do um , uh , diagnostic rhyme test kind of things ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The importance of this part of the evaluation in relation to the overall outcome is not explicitly stated in the transcript. However, it is mentioned that there will be debates about it afterwards, indicating that it may have some significance.&#10;2. The conceptual significance of this aspect in the assessment is related to the emphasis on well-matched and highly mismatched evaluations. It seems that this part of the evaluation focuses on how well the system performs under various levels of matching and mismatching. The goal is to ensure that even if there are significant differences between the expected and actual inputs, the system should still be able to produce reasonable results. This is similar to how human brains can fill in gaps or imagine what should be there when we are having a conversation with noisy backgrounds." target="1. The significance of November fifteenth is that it is the date when the evaluation is scheduled to take place. There is a possibility that the evaluation date may be pushed off, as Professor B mentions that some people are going away and it would be awkward if the evaluation was pushed off for a long while. However, they are hoping it won't be delayed for too long.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The importance of this part of the evaluation in relation to the overall outcome is not explicitly stated in the transcript. However, it is mentioned that there will be debates about it afterwards, indicating that it may have some significance.&#10;2. The conceptual significance of this aspect in the assessment is related to the emphasis on well-matched and highly mismatched evaluations. It seems that this part of the evaluation focuses on how well the system performs under various levels of matching and mismatching. The goal is to ensure that even if there are significant differences between the expected and actual inputs, the system should still be able to produce reasonable results. This is similar to how human brains can fill in gaps or imagine what should be there when we are having a conversation with noisy backgrounds." target="The current classification system puts some emphasis on well-matched categories, but it's only marginal with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively. Being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome because the evaluation criteria give more weight to well-matched categories, so performing poorly in this area could negatively affect the overall assessment.&#10;&#10;The importance of this aspect in relation to the overall outcome is not explicitly stated in the transcript, but it's mentioned that there will be debates about it afterwards, indicating that it may have some significance. Additionally, finding a standard answer to what the detectors should focus on within narrow bands and exploring unsupervised learning approaches like Grad C's suggestion of discovering intermediate categories can help improve performance in matching various inputs accurately.&#10;&#10;The use of more suitable categories, such as words instead of phones, could also benefit discriminative learning for speech recognition tasks, as suggested by PhD E. This approach focuses on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Yeah , I think that 'll be helpful . There 's {disfmarker} there 's not anybody OGI currently who 's {disfmarker} who 's , uh , working with this and {disfmarker} and&#10;Speaker: PhD E&#10;Content: Is {disfmarker} is this part of the evaluation just a small part , or ho how important is this to the overall {disfmarker} ?&#10;Speaker: Professor B&#10;Content: I {disfmarker} I think it 's {disfmarker} it 's , um {disfmarker} it depends how badly {vocalsound} you do . I mean , I think that it {disfmarker} it is {disfmarker} Uh .&#10;Speaker: PhD D&#10;Content: b&#10;Speaker: PhD E&#10;Content: This is one of those things that will be debated afterwards ?&#10;Speaker: Professor B&#10;Content: Yeah . Well , I mean , it 's {disfmarker} it 's {disfmarker} Conceptually , it {disfmarker} my impression , again , you guys correct me if I '" target="1. Based on the meeting transcript, it appears that no one is currently working on the Aurora code at OGI. The consensus among the participants is that someone, possibly Sunil, will &quot;grab&quot; or work on the Aurora code when they are at OGI for a different task. This suggests that while there isn't active development on Aurora at OGI right now, it is expected to be addressed as part of other tasks in the future.">
      <data key="d0">1</data>
    </edge>
    <edge source=" D&#10;Content: Oh , you already have it ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: OK , so just figure how to take the features from the final {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Um . But , yeah , I think there are plenty of issues to work on for the feature net @ @ .&#10;Speaker: Grad C&#10;Content: Feature net .&#10;Speaker: PhD E&#10;Content: What about the , um {disfmarker} uh , the new part of the evaluation , the , uh , Wall Street Journal part ?&#10;Speaker: Professor B&#10;Content: Right . Right . Um . Have you ever {disfmarker} ? Very good question . Have you ever worked with the Mississippi State h uh , software ?&#10;Speaker: PhD A&#10;Content: Sorry .&#10;Speaker: PhD E&#10;Content: No . Not yet .&#10;Speaker: Professor B&#10;Content: Oh . Well you {disfmarker} you may be called upon to help , uh , uh ," target="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion.">
      <data key="d0">1</data>
    </edge>
    <edge source=" D&#10;Content: Oh , you already have it ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: OK , so just figure how to take the features from the final {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Um . But , yeah , I think there are plenty of issues to work on for the feature net @ @ .&#10;Speaker: Grad C&#10;Content: Feature net .&#10;Speaker: PhD E&#10;Content: What about the , um {disfmarker} uh , the new part of the evaluation , the , uh , Wall Street Journal part ?&#10;Speaker: Professor B&#10;Content: Right . Right . Um . Have you ever {disfmarker} ? Very good question . Have you ever worked with the Mississippi State h uh , software ?&#10;Speaker: PhD A&#10;Content: Sorry .&#10;Speaker: PhD E&#10;Content: No . Not yet .&#10;Speaker: Professor B&#10;Content: Oh . Well you {disfmarker} you may be called upon to help , uh , uh ," target="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns.">
      <data key="d0">1</data>
    </edge>
    <edge source=" D&#10;Content: Oh , you already have it ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: OK , so just figure how to take the features from the final {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Um . But , yeah , I think there are plenty of issues to work on for the feature net @ @ .&#10;Speaker: Grad C&#10;Content: Feature net .&#10;Speaker: PhD E&#10;Content: What about the , um {disfmarker} uh , the new part of the evaluation , the , uh , Wall Street Journal part ?&#10;Speaker: Professor B&#10;Content: Right . Right . Um . Have you ever {disfmarker} ? Very good question . Have you ever worked with the Mississippi State h uh , software ?&#10;Speaker: PhD A&#10;Content: Sorry .&#10;Speaker: PhD E&#10;Content: No . Not yet .&#10;Speaker: Professor B&#10;Content: Oh . Well you {disfmarker} you may be called upon to help , uh , uh ," target="1. The significance of November fifteenth is that it is the date when the evaluation is scheduled to take place. There is a possibility that the evaluation date may be pushed off, as Professor B mentions that some people are going away and it would be awkward if the evaluation was pushed off for a long while. However, they are hoping it won't be delayed for too long.">
      <data key="d0">1</data>
    </edge>
    <edge source=" should give you a one or a zero . Y you passed the threshold or you didn't pass the threshold , and they shouldn't even care about what the score is .&#10;Speaker: Professor B&#10;Content: Yeah . But , I mean , we 'll {disfmarker} we 'll {disfmarker} we 'll see what they come up with . Uh , but in {disfmarker} in the current thing , for instance , where you have this well - matched , moderately - matched , and {disfmarker} and mis highly - mismatched , uh , the emphasis is somewhat on the {disfmarker} on the well - matched , but it 's only a {disfmarker} a marginal ,&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: right ? It 's like forty , thirty - five , twenty - five , or something like that . So you still {disfmarker} if you were way , way off on the highly - mismatched , it would have a big effect .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Spe" target="The current classification system puts some emphasis on well-matched categories, but it's only marginal with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively. Being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome because the evaluation criteria give more weight to well-matched categories, so performing poorly in this area could negatively affect the overall assessment.&#10;&#10;The importance of this aspect in relation to the overall outcome is not explicitly stated in the transcript, but it's mentioned that there will be debates about it afterwards, indicating that it may have some significance. Additionally, finding a standard answer to what the detectors should focus on within narrow bands and exploring unsupervised learning approaches like Grad C's suggestion of discovering intermediate categories can help improve performance in matching various inputs accurately.&#10;&#10;The use of more suitable categories, such as words instead of phones, could also benefit discriminative learning for speech recognition tasks, as suggested by PhD E. This approach focuses on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source=" this much effect &quot; {disfmarker} I mean , you don't want to change six things and then see what happens . You want to change them one at a time .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So adding this other stream in , that 's simple in some way . And then {pause} saying , oh {disfmarker} uh {disfmarker} particularly because we 've found in the past there 's all these {disfmarker} these {disfmarker} these different results you get with slight modifications of how you do normalization . Normalization 's a very tricky , sensitive thing and {pause} you learn a lot . So , I would think you would wanna {pause} have some baseline that says , &quot; OK , we don't normalize , this is what we get &quot; , when we do this normalization , when we do that normalization . But {disfmarker} but the other question is {disfmarker} So I think ultimately we 'll wind up doing some normalization . I agree .&#10;Speaker: PhD E&#10;Content: So this second stream , will it" target="The speakers, Professor B and PhD E, mention that some kind of normalization is being applied in both cases to deal with convolutional effects. In the speakers' project, they use online normalization and LDA RASTA, which seem to help but do so by throwing away high modulation frequencies, a technique not used in the other case. It is suggested to try normalization, particularly if logs are taken, and then compare the results, as changing multiple factors simultaneously might make it difficult to understand the impact of each change. The specific type of normalization being applied in both cases remains unspecified in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target=" , wh what I called a useless comments because I 'm not really telling you how to do it . But I mean , it 's a {disfmarker} {vocalsound} it 's {disfmarker} it 's , you know {disfmarker} it&#10;Speaker: PhD E&#10;Content: No , but I think the important part in there is that , you know , if you want to be discriminative , you have to have uh , you know , categories .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: And I think this {disfmarker} the important categories are the words , and {pause} not the phones .&#10;Speaker: Professor B&#10;Content: Yeah . Yeah .&#10;Speaker: PhD E&#10;Content: Maybe . And so {disfmarker} Right . If you can put the words in to the loop somehow for determining goodness of your sets of clusters {disfmarker} Uh {disfmarker}&#10;Speaker: Professor B&#10;Content: Now , that being said , I think that {disfmarker} that if you have something that is , um {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Since we 're here ?&#10;Speaker: PhD E&#10;Content: Go ahead , Morgan .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: That 's all folks .">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target=" B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: And and so I guess what I don't understand is how to do that and still be discriminative , because to be discriminative you have to have categories and the only categories that we know of to use are sort of these human {disfmarker} human sig significant {disfmarker} categories that are significant to humans , like phonemes , things like that .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: But that 's sort of what you want to avoid . And so that feels {disfmarker} I don't know how to get out of this .&#10;Speaker: Professor B&#10;Content: Well , here 's a {disfmarker} here 's a , uh , uh Here 's a generic and possibly useless thought , which is , {vocalsound} um , what do you really {disfmarker} I mean , in a sense the only s s systems that make sense , uh , are ones that {disfmarker} that have something from top - down in th in them . Right ? Because if e even the smallest">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target=" , uh , are ones that {disfmarker} that have something from top - down in th in them . Right ? Because if e even the smallest organism that 's trying to learn to do anything , if it doesn't have any kind of reward for doing {disfmarker} or penal penalty for doing anything , then it 's just going to behave randomly .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So whether you 're talking about something being learned through evolution or being learned through experience , it 's gotta have something come down to it that gives its reward or , you know , at least some reinforcement learning ,&#10;Speaker: PhD E&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: right ?&#10;Speaker: PhD E&#10;Content: So the question is , how far down ?&#10;Speaker: Professor B&#10;Content: And&#10;Speaker: PhD E&#10;Content: We could stop at words , but we don't , right ? We go all the way down to phonemes .&#10;Speaker: Professor B&#10;Content: Right , but I me I {disfmarker} I think that maybe in some ways part">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="&#10;Content: Now , that being said , I think that {disfmarker} that if you have something that is , um {disfmarker} i Once you start dealing with spontaneous speech , all the things you 're saying are {disfmarker} are really true .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: If you {pause} have read speech that 's been manually annotated , like TIMIT , then , you know , i i you the phones are gonna be right , actually , {vocalsound} for the most part .&#10;Speaker: PhD E&#10;Content: Yeah . Yeah ,&#10;Speaker: Professor B&#10;Content: So {disfmarker} so , uh , it doesn't really hurt them to {disfmarker} to do that , to put in discrimination at that level .&#10;Speaker: PhD E&#10;Content: yeah .&#10;Speaker: Professor B&#10;Content: Um , if you go to spontaneous speech then it 's {disfmarker} it 's trickier and {disfmarker} and {disfmarker} and , uh , the phones are {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target=" so everybody goes to phonemes . But the problem is that we {disfmarker} we build models of words in terms of phonemes and these models are {disfmarker} are really cartoon - ish , right ? So when you look at conversational speech , for example , you don't see the phonemes that you {disfmarker} that you have in your word models .&#10;Speaker: Professor B&#10;Content: Yeah . But {disfmarker} but {disfmarker} but we 're not trying for models of words here . See , so her here 's maybe where {disfmarker} If the issue is that we 're trying to come up with , um , some sort of intermediate categories which will then be useful for later stuff , uh , then {pause} maybe it doesn't matter that we can't have enough {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: I mean , what you wanna do is {disfmarker} is build up these categories that are {disfmarker} that are best for word recognition .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="Grad C's suggestion for discovering intermediate categories for classifying data is to use arbitrary windows in time of varying length and cluster those. This method differs from Sangita's approach, which focuses on specific temporal patterns for a certain set of phonemes. By clustering arbitrary windows in time, Grad C aims to find similarities and create intermediate categories that can be used in later classification stages. These intermediate categories may include transitions or relatively stable points in speech. This approach aligns with the idea of unsupervised learning as it allows for the discovery of potential patterns without relying on predefined human-significant categories such as phonemes.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="1. Phone-based speech recognition systems struggle with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined.&#10;2. Humans have a mechanism that allows them to understand munged word models in conversional speech because they possess some kind of cognitive ability that enables them to deal with missing or distorted phonemes within words. This mechanism doesn't seem to affect human comprehension as much, indicating an inherent flexibility in the way humans process and interpret spoken language compared to rigid phone-based systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="The speech recognizer being discussed in the transcripts is a phone-based speech recognition system, which struggles with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined. The researchers mentioned in the transcripts commonly use phone-based systems, as they build models of words in terms of phonemes. However, these models can be oversimplified, creating a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;The proposed solution to improve the system is to consider using words instead of phones as categories for discriminative learning. This approach might allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="Researchers are considering creating intermediate categories, such as words instead of phones (phonemes), to improve word recognition in a speech system because phone-based models can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech. Human listeners can understand munged word models in conversational speech due to cognitive abilities that enable them to deal with missing or distorted phonemes, indicating an inherent flexibility in human spoken language processing. By using words as categories, researchers aim to create a more robust and accurate speech recognition system by focusing on recognizing and distinguishing whole words rather than individual phonemes.&#10;&#10;The idea of discovering intermediate categories like phonemes comes from Grad C's suggestion to use arbitrary windows in time of varying length and cluster those, potentially revealing transitions or relatively stable points in speech. This method aligns with the concept of unsupervised learning, allowing for pattern discovery without relying on predefined human-significant categories such as phonemes.&#10;&#10;While conversational speech may not clearly exhibit these intermediate categories like phones, researchers are exploring this approach to overcome limitations associated with phone-based models and improve overall system performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="The current classification system puts some emphasis on well-matched categories, but it's only marginal with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively. Being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome because the evaluation criteria give more weight to well-matched categories, so performing poorly in this area could negatively affect the overall assessment.&#10;&#10;The importance of this aspect in relation to the overall outcome is not explicitly stated in the transcript, but it's mentioned that there will be debates about it afterwards, indicating that it may have some significance. Additionally, finding a standard answer to what the detectors should focus on within narrow bands and exploring unsupervised learning approaches like Grad C's suggestion of discovering intermediate categories can help improve performance in matching various inputs accurately.&#10;&#10;The use of more suitable categories, such as words instead of phones, could also benefit discriminative learning for speech recognition tasks, as suggested by PhD E. This approach focuses on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="Yes, there is a possibility to automate the integration of error units like words or syllables into a larger unit for processing and feedback in speech recognition systems. The approach discussed by the researchers involves using words instead of phones as categories for discriminative learning in speech recognition systems. This method focuses on recognizing and distinguishing whole words rather than individual phonemes, which can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;In this context, automating the integration of error units could mean using word-level units directly in the speech recognition system, bypassing phone-based models entirely. By employing words as categories, the system can potentially improve its robustness and accuracy in processing spontaneous speech with variable phoneme appearances.&#10;&#10;Additionally, the researchers mentioned exploring unsupervised learning methods like Grad C's suggestion to use arbitrary windows in time of varying length and cluster those. This approach could help discover intermediate categories that align with transitions or relatively stable points in speech, further aiding in the accurate recognition and processing of larger units such as words or syllables for feedback in the system.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to speaker PhD E, the important aspect in discriminative learning is that you need categories to be able to distinguish between different things. PhD E believes that for speech recognition, the traditional categories used have been &quot;human significant&quot; categories such as phonemes, which is what one would want to avoid because it goes against the idea of having an unsupervised learning approach. However, they mention that these human-significant categories are the only ones we know of so far.&#10;&#10;In their discussion, PhD E suggests that a more suitable category for discriminative learning could be words instead of phones. This is because phones can create difficulties when trying to create a system that generalizes well due to the variability in how phones can appear in different contexts. By using words as categories, it might be possible to create a more robust and accurate speech recognition system. However, this idea still presents challenges, particularly when dealing with spontaneous speech where word boundaries may not always be clear.&#10;&#10;In summary, PhD E believes that the key aspect of discriminative learning is the use of categories, and they propose that words might make better categories than phones for speech recognition tasks. This would allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="To create a discriminative system without using significant human categories and avoiding top-down approaches, one could consider the following strategies:&#10;&#10;1. Explore unsupervised learning methods to discover underlying patterns or structures within the data. For example, Grad C's suggestion of using arbitrary windows in time of varying length and clustering them can help identify intermediate categories that are not predefined by human significance.&#10;2. Utilize a bottom-up approach where the system learns from raw data without relying on handcrafted features or predetermined categories. This allows the model to discover its own relevant features and patterns, potentially leading to novel insights about the structure of the data.&#10;3. Investigate the use of context-dependent models that consider sequential information in the data, rather than focusing on individual segments (e.g., phones or words). These models can capture the variability present in speech by taking into account how different contexts influence the acoustic signal.&#10;4. Consider incorporating additional modalities, such as visual or semantic information, to supplement the audio input and provide alternative cues for distinguishing between categories. This can help reduce reliance on human-significant categories while still maintaining a high level of accuracy in the system.&#10;5. Develop hybrid systems that combine unsupervised learning techniques with weakly supervised methods, where some minimal guidance is provided to the model but without explicitly defining the categories. This approach could strike a balance between data-driven discovery and human expertise.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Since we 're here ?&#10;Speaker: PhD E&#10;Content: Go ahead , Morgan .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: That 's all folks ." target="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Since we 're here ?&#10;Speaker: PhD E&#10;Content: Go ahead , Morgan .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: That 's all folks ." target="Yes, the speakers are discussing the use of various machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM). They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, as indicated by PhD E's question: &quot;they're providing scripts and everything so that basically, you push a button and it does training, and then it does test, and everything?&quot; They also mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Since we 're here ?&#10;Speaker: PhD E&#10;Content: Go ahead , Morgan .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: That 's all folks ." target="The brief conversation between PhD E and Professor B seems to be related to a presentation or discussion, where Professor B is asking for permission to go ahead with his point. They also discuss a modification idea brought up by PhD E, which may involve adjusting certain features for better compatibility with a language model. However, the specific topic or context of their conversation is not provided in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Since we 're here ?&#10;Speaker: PhD E&#10;Content: Go ahead , Morgan .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: That 's all folks ." target="Based on the transcript, Professors B and E are discussing the current machine learning projects they are working on and the challenges of coordinating their efforts given their upcoming travel schedules. They mention several models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), and discuss the need to normalize data and create new streams for the models.&#10;&#10;Regarding the specific focus of their attention, Professors B and E mention a software created by &quot;these guys&quot; that is a key part of their work, and they plan to prioritize this software by keeping certain options constant and focusing on experimenting with specific variables. PhD A mentions that they have been working on noise compensation for six weeks and have made progress.&#10;&#10;Therefore, the specific focus of their attention will likely be to keep certain options constant in the machine learning models and prioritize the use and further development of the software created by &quot;these guys,&quot; possibly involving noise compensation work done by PhD A.">
      <data key="d0">1</data>
    </edge>
    <edge source=" B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: And and so I guess what I don't understand is how to do that and still be discriminative , because to be discriminative you have to have categories and the only categories that we know of to use are sort of these human {disfmarker} human sig significant {disfmarker} categories that are significant to humans , like phonemes , things like that .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: But that 's sort of what you want to avoid . And so that feels {disfmarker} I don't know how to get out of this .&#10;Speaker: Professor B&#10;Content: Well , here 's a {disfmarker} here 's a , uh , uh Here 's a generic and possibly useless thought , which is , {vocalsound} um , what do you really {disfmarker} I mean , in a sense the only s s systems that make sense , uh , are ones that {disfmarker} that have something from top - down in th in them . Right ? Because if e even the smallest" target="To create a discriminative system without using significant human categories and avoiding top-down approaches, one could consider the following strategies:&#10;&#10;1. Explore unsupervised learning methods to discover underlying patterns or structures within the data. For example, Grad C's suggestion of using arbitrary windows in time of varying length and clustering them can help identify intermediate categories that are not predefined by human significance.&#10;2. Utilize a bottom-up approach where the system learns from raw data without relying on handcrafted features or predetermined categories. This allows the model to discover its own relevant features and patterns, potentially leading to novel insights about the structure of the data.&#10;3. Investigate the use of context-dependent models that consider sequential information in the data, rather than focusing on individual segments (e.g., phones or words). These models can capture the variability present in speech by taking into account how different contexts influence the acoustic signal.&#10;4. Consider incorporating additional modalities, such as visual or semantic information, to supplement the audio input and provide alternative cues for distinguishing between categories. This can help reduce reliance on human-significant categories while still maintaining a high level of accuracy in the system.&#10;5. Develop hybrid systems that combine unsupervised learning techniques with weakly supervised methods, where some minimal guidance is provided to the model but without explicitly defining the categories. This approach could strike a balance between data-driven discovery and human expertise.">
      <data key="d0">1</data>
    </edge>
    <edge source=" so everybody goes to phonemes . But the problem is that we {disfmarker} we build models of words in terms of phonemes and these models are {disfmarker} are really cartoon - ish , right ? So when you look at conversational speech , for example , you don't see the phonemes that you {disfmarker} that you have in your word models .&#10;Speaker: Professor B&#10;Content: Yeah . But {disfmarker} but {disfmarker} but we 're not trying for models of words here . See , so her here 's maybe where {disfmarker} If the issue is that we 're trying to come up with , um , some sort of intermediate categories which will then be useful for later stuff , uh , then {pause} maybe it doesn't matter that we can't have enough {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: I mean , what you wanna do is {disfmarker} is build up these categories that are {disfmarker} that are best for word recognition .&#10;" target="Grad C's suggestion for discovering intermediate categories for classifying data is to use arbitrary windows in time of varying length and cluster those. This method differs from Sangita's approach, which focuses on specific temporal patterns for a certain set of phonemes. By clustering arbitrary windows in time, Grad C aims to find similarities and create intermediate categories that can be used in later classification stages. These intermediate categories may include transitions or relatively stable points in speech. This approach aligns with the idea of unsupervised learning as it allows for the discovery of potential patterns without relying on predefined human-significant categories such as phonemes.">
      <data key="d0">1</data>
    </edge>
    <edge source=" so everybody goes to phonemes . But the problem is that we {disfmarker} we build models of words in terms of phonemes and these models are {disfmarker} are really cartoon - ish , right ? So when you look at conversational speech , for example , you don't see the phonemes that you {disfmarker} that you have in your word models .&#10;Speaker: Professor B&#10;Content: Yeah . But {disfmarker} but {disfmarker} but we 're not trying for models of words here . See , so her here 's maybe where {disfmarker} If the issue is that we 're trying to come up with , um , some sort of intermediate categories which will then be useful for later stuff , uh , then {pause} maybe it doesn't matter that we can't have enough {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: I mean , what you wanna do is {disfmarker} is build up these categories that are {disfmarker} that are best for word recognition .&#10;" target="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns.">
      <data key="d0">1</data>
    </edge>
    <edge source=" so everybody goes to phonemes . But the problem is that we {disfmarker} we build models of words in terms of phonemes and these models are {disfmarker} are really cartoon - ish , right ? So when you look at conversational speech , for example , you don't see the phonemes that you {disfmarker} that you have in your word models .&#10;Speaker: Professor B&#10;Content: Yeah . But {disfmarker} but {disfmarker} but we 're not trying for models of words here . See , so her here 's maybe where {disfmarker} If the issue is that we 're trying to come up with , um , some sort of intermediate categories which will then be useful for later stuff , uh , then {pause} maybe it doesn't matter that we can't have enough {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: I mean , what you wanna do is {disfmarker} is build up these categories that are {disfmarker} that are best for word recognition .&#10;" target="1. Phone-based speech recognition systems struggle with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined.&#10;2. Humans have a mechanism that allows them to understand munged word models in conversional speech because they possess some kind of cognitive ability that enables them to deal with missing or distorted phonemes within words. This mechanism doesn't seem to affect human comprehension as much, indicating an inherent flexibility in the way humans process and interpret spoken language compared to rigid phone-based systems.">
      <data key="d0">1</data>
    </edge>
    <edge source=" so everybody goes to phonemes . But the problem is that we {disfmarker} we build models of words in terms of phonemes and these models are {disfmarker} are really cartoon - ish , right ? So when you look at conversational speech , for example , you don't see the phonemes that you {disfmarker} that you have in your word models .&#10;Speaker: Professor B&#10;Content: Yeah . But {disfmarker} but {disfmarker} but we 're not trying for models of words here . See , so her here 's maybe where {disfmarker} If the issue is that we 're trying to come up with , um , some sort of intermediate categories which will then be useful for later stuff , uh , then {pause} maybe it doesn't matter that we can't have enough {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: I mean , what you wanna do is {disfmarker} is build up these categories that are {disfmarker} that are best for word recognition .&#10;" target="The speech recognizer being discussed in the transcripts is a phone-based speech recognition system, which struggles with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined. The researchers mentioned in the transcripts commonly use phone-based systems, as they build models of words in terms of phonemes. However, these models can be oversimplified, creating a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;The proposed solution to improve the system is to consider using words instead of phones as categories for discriminative learning. This approach might allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source=" so everybody goes to phonemes . But the problem is that we {disfmarker} we build models of words in terms of phonemes and these models are {disfmarker} are really cartoon - ish , right ? So when you look at conversational speech , for example , you don't see the phonemes that you {disfmarker} that you have in your word models .&#10;Speaker: Professor B&#10;Content: Yeah . But {disfmarker} but {disfmarker} but we 're not trying for models of words here . See , so her here 's maybe where {disfmarker} If the issue is that we 're trying to come up with , um , some sort of intermediate categories which will then be useful for later stuff , uh , then {pause} maybe it doesn't matter that we can't have enough {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor B&#10;Content: I mean , what you wanna do is {disfmarker} is build up these categories that are {disfmarker} that are best for word recognition .&#10;" target="Researchers are considering creating intermediate categories, such as words instead of phones (phonemes), to improve word recognition in a speech system because phone-based models can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech. Human listeners can understand munged word models in conversational speech due to cognitive abilities that enable them to deal with missing or distorted phonemes, indicating an inherent flexibility in human spoken language processing. By using words as categories, researchers aim to create a more robust and accurate speech recognition system by focusing on recognizing and distinguishing whole words rather than individual phonemes.&#10;&#10;The idea of discovering intermediate categories like phonemes comes from Grad C's suggestion to use arbitrary windows in time of varying length and cluster those, potentially revealing transitions or relatively stable points in speech. This method aligns with the concept of unsupervised learning, allowing for pattern discovery without relying on predefined human-significant categories such as phonemes.&#10;&#10;While conversational speech may not clearly exhibit these intermediate categories like phones, researchers are exploring this approach to overcome limitations associated with phone-based models and improve overall system performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target="Content: Hmm . Yeah , um . In fact , I mean , if you look into it a little bit , it might be reasonable {disfmarker} You know Joe , right ? Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Just to sort of ask him about the issue of , um , different features having different kinds of , uh , scaling characteristics and so on . So that , you know , w w possibly having entirely different optimal values for {disfmarker} for the usual twiddle factors and what 's {disfmarker} what 's the plan about that ?&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: So sh shall we , like , add Chuck also to the mailing lists ? It may be better , I mean , in that case if he 's going to {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Because there 's a mailing list for this .&#10;Speaker: Professor B&#10;Content: Is that OK ?&#10;Speaker: PhD E&#10;Content: Yeah , that 'd be great .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target=" So these sugges these {disfmarker} this , uh , period during which people are gonna make suggestions is to know whether it is actually biased towards any set of features or {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , so I th th certainly the thing that I would want to know about is whether we get really hurt , uh , on in insertion penalty , language model , scaling , sorts of things .&#10;Speaker: PhD E&#10;Content: Using our features .&#10;Speaker: Professor B&#10;Content: Yeah , yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh , in which case , um , H Hari or Hynek will need to , you know , push the case {pause} more about {disfmarker} about this .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um .&#10;Speaker: PhD E&#10;Content: And we may be able to revisit this idea about , you know , somehow modifying our features to work with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yes . In this case , that 's">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target="} I guess it 's almost ready .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: So {disfmarker} That 's what {disfmarker} So they have released their , uh , document , describing the system .&#10;Speaker: Professor B&#10;Content: Maybe you could , uh , point it {pause} at Chuck ,&#10;Speaker: PhD E&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: because , I mean {disfmarker}&#10;Speaker: PhD D&#10;Content: Sure .&#10;Speaker: PhD E&#10;Content: So we 'll have to grab this over CVS or something ?&#10;Speaker: PhD D&#10;Content: It - no , it 's just downloadable from their {disfmarker} from their web site .&#10;Speaker: PhD E&#10;Content: Is that how they do it ? OK .&#10;Speaker: Professor B&#10;Content: Cuz one of the things that might be helpful , if you 've {disfmarker} if you 've got time in all of this is , is if {disfmarker} if these guys are really focusing on">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target="&#10;Speaker: Professor B&#10;Content: I think th the biggest we 've run into for storage is the neural net . Right ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . Um . And so I guess the issue there is , are we {disfmarker} are we using neural - net - based TRAPS , and {disfmarker} and how big are they ? So that 'll {disfmarker} that 'll be , you know , an issue .&#10;Speaker: Grad C&#10;Content: Oh , right .&#10;Speaker: Professor B&#10;Content: Maybe they can be little ones .&#10;Speaker: Grad C&#10;Content: Yeah . Cuz sh Right .&#10;Speaker: Professor B&#10;Content: Mini - TRAPS .&#10;Speaker: Grad C&#10;Content: Cuz she also does the , uh {disfmarker} the correlation - based , uh , TRAPS , with without the neural net , just looking at the correlation between {disfmarker}&#10;Speaker: Professor B&#10;Content: Right . And maybe for VAD they would be OK . Yeah . Yeah .&#10;Speaker: Grad C&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target="er} to the set of phonemes that you already have . Um , whereas maybe we want to just take {disfmarker} take a look at , um , arbitrary windows in time , um , of varying length , um , and cluster those .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: And I 'm thinking if we {disfmarker} if we do that , then we would probably , um , at some point in the clustering algorithm find that we 've clustered things like , OK , thi this is a transition , um , this is a relatively stable {disfmarker} stable point .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: Um , and I 'm hoping to find other things of {disfmarker} of similarity and maybe use these things as the intermediate , um {disfmarker} intermediate categories that , uh , um , I 'll later classify .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Are you looking at these in narrow bands ?&#10;Speaker: Grad">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target="According to the conversation between Professor B and Grad C, creating a simple neural network for performing correlation using learned weights from data does not require a complex function. Professor B states, &quot;And so, uh, the question is, how complex a function do you need? Do you need to have an added layer or something?&quot; This implies that the initial assumption was that a simple neural network would be sufficient, but they are considering whether a more complex model with additional layers might be necessary. However, Professor B's tone of voice and follow-up statement suggest that they believe a simpler model will likely be sufficient. Therefore, based on this conversation, it seems that the level of complexity required for the function in creating a simple neural net for performing correlation using learned weights from data is not high, and an additional layer may not be necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target="Based on the conversation provided, it appears that PhD A has successfully set up a CVS server using the existing SSH mechanism for authentication. There is no clear mention of setting up a CVS server on a new port in addition to the current SSH mechanism. Therefore, as of this conversation, the proposed setup with a CVS password mechanism for authentication on a new port has not been implemented or discussed further.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target="1. The significance of November fifteenth is that it is the date when the evaluation is scheduled to take place. There is a possibility that the evaluation date may be pushed off, as Professor B mentions that some people are going away and it would be awkward if the evaluation was pushed off for a long while. However, they are hoping it won't be delayed for too long.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target="1. Based on the meeting transcript, it appears that no one is currently working on the Aurora code at OGI. The consensus among the participants is that someone, possibly Sunil, will &quot;grab&quot; or work on the Aurora code when they are at OGI for a different task. This suggests that while there isn't active development on Aurora at OGI right now, it is expected to be addressed as part of other tasks in the future.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target="1. The proposed plan for handling the large vocabulary part of the discussion between Professor B, PhD E, Hari, and Joe involves PhD E taking charge of this aspect. According to the conversation, PhD E will be asked to handle the large vocabulary part and should communicate this responsibility to Joe and CC Hari in a note. The plan is to have Joe involved in the discussion while keeping Hari informed, so that if Joe requires confirmation on any decisions, Hari can provide it. This approach aims to ensure both Joe's involvement and Hari's approval for the large vocabulary part of the project.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target="The brief conversation between PhD E and Professor B seems to be related to a presentation or discussion, where Professor B is asking for permission to go ahead with his point. They also discuss a modification idea brought up by PhD E, which may involve adjusting certain features for better compatibility with a language model. However, the specific topic or context of their conversation is not provided in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The plan for addressing the issue of different features having varying scaling characteristics and optimal values for twiddle factors involves discussing the matter with Joe and considering the possibility of having different optimal values for the usual twiddle factors. It is not explicitly stated if Chuck should be added to the mailing list, but it is mentioned that he might be relevant to the discussion about releasing a document describing a system.&#10;2. Based on the context provided, it would be appropriate to add Chuck to the mailing list for this discussion, as it involves considering the features and optimal values for twiddle factors, which may be related to the document describing the system that he is working on. Additionally, Professor B suggests having PhD D send the relevant information to Chuck, indicating that he should be involved in the discussion." target="Based on the provided transcripts, the system for improving digital processes and handling large vocabulary runs is not explicitly stated as ready. However, PhD E is assigned to take charge of the large vocabulary part and communicate this responsibility to Joe and CC Hari. It would be beneficial to have Professor B use his experience to iron out potential issues, as he has shown concern about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. His expertise could help address these concerns and improve the system's performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Hmm . Yeah , um . In fact , I mean , if you look into it a little bit , it might be reasonable {disfmarker} You know Joe , right ? Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Just to sort of ask him about the issue of , um , different features having different kinds of , uh , scaling characteristics and so on . So that , you know , w w possibly having entirely different optimal values for {disfmarker} for the usual twiddle factors and what 's {disfmarker} what 's the plan about that ?&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: So sh shall we , like , add Chuck also to the mailing lists ? It may be better , I mean , in that case if he 's going to {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Because there 's a mailing list for this .&#10;Speaker: Professor B&#10;Content: Is that OK ?&#10;Speaker: PhD E&#10;Content: Yeah , that 'd be great ." target="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns.">
      <data key="d0">1</data>
    </edge>
    <edge source=" So these sugges these {disfmarker} this , uh , period during which people are gonna make suggestions is to know whether it is actually biased towards any set of features or {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , so I th th certainly the thing that I would want to know about is whether we get really hurt , uh , on in insertion penalty , language model , scaling , sorts of things .&#10;Speaker: PhD E&#10;Content: Using our features .&#10;Speaker: Professor B&#10;Content: Yeah , yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh , in which case , um , H Hari or Hynek will need to , you know , push the case {pause} more about {disfmarker} about this .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um .&#10;Speaker: PhD E&#10;Content: And we may be able to revisit this idea about , you know , somehow modifying our features to work with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yes . In this case , that 's" target="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns.">
      <data key="d0">1</data>
    </edge>
    <edge source=" So these sugges these {disfmarker} this , uh , period during which people are gonna make suggestions is to know whether it is actually biased towards any set of features or {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , so I th th certainly the thing that I would want to know about is whether we get really hurt , uh , on in insertion penalty , language model , scaling , sorts of things .&#10;Speaker: PhD E&#10;Content: Using our features .&#10;Speaker: Professor B&#10;Content: Yeah , yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh , in which case , um , H Hari or Hynek will need to , you know , push the case {pause} more about {disfmarker} about this .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um .&#10;Speaker: PhD E&#10;Content: And we may be able to revisit this idea about , you know , somehow modifying our features to work with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yes . In this case , that 's" target="1. The proposed plan for handling the large vocabulary part of the discussion between Professor B, PhD E, Hari, and Joe involves PhD E taking charge of this aspect. According to the conversation, PhD E will be asked to handle the large vocabulary part and should communicate this responsibility to Joe and CC Hari in a note. The plan is to have Joe involved in the discussion while keeping Hari informed, so that if Joe requires confirmation on any decisions, Hari can provide it. This approach aims to ensure both Joe's involvement and Hari's approval for the large vocabulary part of the project.">
      <data key="d0">1</data>
    </edge>
    <edge source="} I guess it 's almost ready .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: So {disfmarker} That 's what {disfmarker} So they have released their , uh , document , describing the system .&#10;Speaker: Professor B&#10;Content: Maybe you could , uh , point it {pause} at Chuck ,&#10;Speaker: PhD E&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: because , I mean {disfmarker}&#10;Speaker: PhD D&#10;Content: Sure .&#10;Speaker: PhD E&#10;Content: So we 'll have to grab this over CVS or something ?&#10;Speaker: PhD D&#10;Content: It - no , it 's just downloadable from their {disfmarker} from their web site .&#10;Speaker: PhD E&#10;Content: Is that how they do it ? OK .&#10;Speaker: Professor B&#10;Content: Cuz one of the things that might be helpful , if you 've {disfmarker} if you 've got time in all of this is , is if {disfmarker} if these guys are really focusing on" target="According to the conversation, while open-source projects often use anonymous CVS mechanisms, Professor B states that they're &quot;still so much in development&quot; and want to restrict access to insiders for their project. PhD E initially brought up the idea of using an anonymous mechanism for their transcripts, but after considering the project's stage and the desire to limit access, they decided against it. Therefore, based on the conversation, the anonymous mechanism is not currently available or feasible for their transcripts, given the project's development stage and the group's decision to limit access to insiders.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: Professor B&#10;Content: I think th the biggest we 've run into for storage is the neural net . Right ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . Um . And so I guess the issue there is , are we {disfmarker} are we using neural - net - based TRAPS , and {disfmarker} and how big are they ? So that 'll {disfmarker} that 'll be , you know , an issue .&#10;Speaker: Grad C&#10;Content: Oh , right .&#10;Speaker: Professor B&#10;Content: Maybe they can be little ones .&#10;Speaker: Grad C&#10;Content: Yeah . Cuz sh Right .&#10;Speaker: Professor B&#10;Content: Mini - TRAPS .&#10;Speaker: Grad C&#10;Content: Cuz she also does the , uh {disfmarker} the correlation - based , uh , TRAPS , with without the neural net , just looking at the correlation between {disfmarker}&#10;Speaker: Professor B&#10;Content: Right . And maybe for VAD they would be OK . Yeah . Yeah .&#10;Speaker: Grad C&#10;" target="According to the conversation between Professor B and Grad C, creating a simple neural network for performing correlation using learned weights from data does not require a complex function. Professor B states, &quot;And so, uh, the question is, how complex a function do you need? Do you need to have an added layer or something?&quot; This implies that the initial assumption was that a simple neural network would be sufficient, but they are considering whether a more complex model with additional layers might be necessary. However, Professor B's tone of voice and follow-up statement suggest that they believe a simpler model will likely be sufficient. Therefore, based on this conversation, it seems that the level of complexity required for the function in creating a simple neural net for performing correlation using learned weights from data is not high, and an additional layer may not be necessary.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: Professor B&#10;Content: I think th the biggest we 've run into for storage is the neural net . Right ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah . Um . And so I guess the issue there is , are we {disfmarker} are we using neural - net - based TRAPS , and {disfmarker} and how big are they ? So that 'll {disfmarker} that 'll be , you know , an issue .&#10;Speaker: Grad C&#10;Content: Oh , right .&#10;Speaker: Professor B&#10;Content: Maybe they can be little ones .&#10;Speaker: Grad C&#10;Content: Yeah . Cuz sh Right .&#10;Speaker: Professor B&#10;Content: Mini - TRAPS .&#10;Speaker: Grad C&#10;Content: Cuz she also does the , uh {disfmarker} the correlation - based , uh , TRAPS , with without the neural net , just looking at the correlation between {disfmarker}&#10;Speaker: Professor B&#10;Content: Right . And maybe for VAD they would be OK . Yeah . Yeah .&#10;Speaker: Grad C&#10;" target="The fundamental category that narrow band detectors should look for refers to the specific type of information or signal these detectors are designed to identify within the narrow frequency bands. However, the text does not provide explicit details on what this &quot;category&quot; refers to in the given context. The speakers seem to be contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands.&#10;&#10;The concerns regarding the standard approach to finding this category include the current classification system's marginal emphasis on well-matched categories (with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively). The speakers mention that being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome. They also discuss debates about the importance of this aspect in relation to the overall outcome and suggest that finding a standard answer to what detectors should focus on within narrow bands could help improve performance in matching various inputs accurately.&#10;&#10;The speakers are discussing the use of machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), for tasks such as speech recognition. They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, and they mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models.">
      <data key="d0">1</data>
    </edge>
    <edge source="er} to the set of phonemes that you already have . Um , whereas maybe we want to just take {disfmarker} take a look at , um , arbitrary windows in time , um , of varying length , um , and cluster those .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: And I 'm thinking if we {disfmarker} if we do that , then we would probably , um , at some point in the clustering algorithm find that we 've clustered things like , OK , thi this is a transition , um , this is a relatively stable {disfmarker} stable point .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: Um , and I 'm hoping to find other things of {disfmarker} of similarity and maybe use these things as the intermediate , um {disfmarker} intermediate categories that , uh , um , I 'll later classify .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Are you looking at these in narrow bands ?&#10;Speaker: Grad" target="Grad C's suggestion for discovering intermediate categories for classifying data is to use arbitrary windows in time of varying length and cluster those. This method differs from Sangita's approach, which focuses on specific temporal patterns for a certain set of phonemes. By clustering arbitrary windows in time, Grad C aims to find similarities and create intermediate categories that can be used in later classification stages. These intermediate categories may include transitions or relatively stable points in speech. This approach aligns with the idea of unsupervised learning as it allows for the discovery of potential patterns without relying on predefined human-significant categories such as phonemes.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation between Professor B and Grad C, creating a simple neural network for performing correlation using learned weights from data does not require a complex function. Professor B states, &quot;And so, uh, the question is, how complex a function do you need? Do you need to have an added layer or something?&quot; This implies that the initial assumption was that a simple neural network would be sufficient, but they are considering whether a more complex model with additional layers might be necessary. However, Professor B's tone of voice and follow-up statement suggest that they believe a simpler model will likely be sufficient. Therefore, based on this conversation, it seems that the level of complexity required for the function in creating a simple neural net for performing correlation using learned weights from data is not high, and an additional layer may not be necessary." target="&#10;Speaker: Professor B&#10;Content: Right . And maybe for VAD they would be OK . Yeah . Yeah .&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: That 's true .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Or a simple neural net , right ? I mean , the thing is , if you 're doing correlation , you 're just doing a simple {disfmarker} uh , uh {disfmarker} uh , dot product , you know , with some weights which you happened to learn from this {disfmarker} learn from the data .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And so , uh , putting a nonlinearity on it is , {pause} you know , not that big a deal . It certainly doesn't take much space .&#10;Speaker: Grad C&#10;Content: Mm - hmm . Right .&#10;Speaker: Professor B&#10;Content: So , uh , the question is , how complex a function do you need ? Do you need to have an added layer or something ? In which">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation between Professor B and Grad C, creating a simple neural network for performing correlation using learned weights from data does not require a complex function. Professor B states, &quot;And so, uh, the question is, how complex a function do you need? Do you need to have an added layer or something?&quot; This implies that the initial assumption was that a simple neural network would be sufficient, but they are considering whether a more complex model with additional layers might be necessary. However, Professor B's tone of voice and follow-up statement suggest that they believe a simpler model will likely be sufficient. Therefore, based on this conversation, it seems that the level of complexity required for the function in creating a simple neural net for performing correlation using learned weights from data is not high, and an additional layer may not be necessary." target=" PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , OK . And then the mel and then the log , and then the&#10;Speaker: PhD A&#10;Content: Then the LDA filter ,&#10;Speaker: Professor B&#10;Content: LDA filter .&#10;Speaker: PhD A&#10;Content: mmm , then the downsampling ,&#10;Speaker: Professor B&#10;Content: And then uh downsample ,&#10;Speaker: PhD A&#10;Content: DCT ,&#10;Speaker: Professor B&#10;Content: DCT ,&#10;Speaker: PhD A&#10;Content: then , um , on - line normalization ,&#10;Speaker: Professor B&#10;Content: on - line norm ,&#10;Speaker: PhD A&#10;Content: followed by {pause} upsampling . Then finally , we compute delta and we put the neural network also .&#10;Speaker: Professor B&#10;Content: Right , and then in parallel with {disfmarker} an {disfmarker} a neural net . And then following neural net , some {disfmarker} probably some orthogonalization .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation between Professor B and Grad C, creating a simple neural network for performing correlation using learned weights from data does not require a complex function. Professor B states, &quot;And so, uh, the question is, how complex a function do you need? Do you need to have an added layer or something?&quot; This implies that the initial assumption was that a simple neural network would be sufficient, but they are considering whether a more complex model with additional layers might be necessary. However, Professor B's tone of voice and follow-up statement suggest that they believe a simpler model will likely be sufficient. Therefore, based on this conversation, it seems that the level of complexity required for the function in creating a simple neural net for performing correlation using learned weights from data is not high, and an additional layer may not be necessary." target="isfmarker} probably some orthogonalization .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh {disfmarker} Um .&#10;Speaker: PhD A&#10;Content: And finally frame dropping , which um , {vocalsound} would be a neural network also , used for estimated silence probabilities . And the input of this neural network would be somewhere between log {pause} mel bands or one of the earlier stages of the processing .&#10;Speaker: Professor B&#10;Content: Mm - hmm . So that 's sort of {disfmarker} most of this stuff is {disfmarker} yeah , is operating parallel with this other stuff .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . So the things that we , um , uh , I guess we sort of {disfmarker} uh , There 's {disfmarker} there 's some , uh , neat ideas for {vocalsound} V A So , I mean , in {disfmarker} I think there 's sort of like {disfmarker} There 's a bunch">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation between Professor B and Grad C, creating a simple neural network for performing correlation using learned weights from data does not require a complex function. Professor B states, &quot;And so, uh, the question is, how complex a function do you need? Do you need to have an added layer or something?&quot; This implies that the initial assumption was that a simple neural network would be sufficient, but they are considering whether a more complex model with additional layers might be necessary. However, Professor B's tone of voice and follow-up statement suggest that they believe a simpler model will likely be sufficient. Therefore, based on this conversation, it seems that the level of complexity required for the function in creating a simple neural net for performing correlation using learned weights from data is not high, and an additional layer may not be necessary." target=" orthogonalizing .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: But , you know {disfmarker} and {disfmarker} and , um , this is a little harder because you 're not just trying to find parameters . You 're actually trying to find the {disfmarker} the {disfmarker} the {disfmarker} the categories themselves . Uh , so a little more like brain surgery , I think on yourself . Uh . So , uh&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Um , anyway . That 's my {pause} thought .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: You 've been thinking about this problem for a long time actually . I mean , well {disfmarker} W actually , you stopped thinking about it for a long time , but you used to think about it {vocalsound} a lot .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: And you 've been thinking about it more now ,&#10;Speaker: PhD D">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation between Professor B and Grad C, creating a simple neural network for performing correlation using learned weights from data does not require a complex function. Professor B states, &quot;And so, uh, the question is, how complex a function do you need? Do you need to have an added layer or something?&quot; This implies that the initial assumption was that a simple neural network would be sufficient, but they are considering whether a more complex model with additional layers might be necessary. However, Professor B's tone of voice and follow-up statement suggest that they believe a simpler model will likely be sufficient. Therefore, based on this conversation, it seems that the level of complexity required for the function in creating a simple neural net for performing correlation using learned weights from data is not high, and an additional layer may not be necessary." target="According to the conversation, while open-source projects often use anonymous CVS mechanisms, Professor B states that they're &quot;still so much in development&quot; and want to restrict access to insiders for their project. PhD E initially brought up the idea of using an anonymous mechanism for their transcripts, but after considering the project's stage and the desire to limit access, they decided against it. Therefore, based on the conversation, the anonymous mechanism is not currently available or feasible for their transcripts, given the project's development stage and the group's decision to limit access to insiders.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation between Professor B and Grad C, creating a simple neural network for performing correlation using learned weights from data does not require a complex function. Professor B states, &quot;And so, uh, the question is, how complex a function do you need? Do you need to have an added layer or something?&quot; This implies that the initial assumption was that a simple neural network would be sufficient, but they are considering whether a more complex model with additional layers might be necessary. However, Professor B's tone of voice and follow-up statement suggest that they believe a simpler model will likely be sufficient. Therefore, based on this conversation, it seems that the level of complexity required for the function in creating a simple neural net for performing correlation using learned weights from data is not high, and an additional layer may not be necessary." target="1. Phone-based speech recognition systems struggle with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined.&#10;2. Humans have a mechanism that allows them to understand munged word models in conversional speech because they possess some kind of cognitive ability that enables them to deal with missing or distorted phonemes within words. This mechanism doesn't seem to affect human comprehension as much, indicating an inherent flexibility in the way humans process and interpret spoken language compared to rigid phone-based systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation between Professor B and Grad C, creating a simple neural network for performing correlation using learned weights from data does not require a complex function. Professor B states, &quot;And so, uh, the question is, how complex a function do you need? Do you need to have an added layer or something?&quot; This implies that the initial assumption was that a simple neural network would be sufficient, but they are considering whether a more complex model with additional layers might be necessary. However, Professor B's tone of voice and follow-up statement suggest that they believe a simpler model will likely be sufficient. Therefore, based on this conversation, it seems that the level of complexity required for the function in creating a simple neural net for performing correlation using learned weights from data is not high, and an additional layer may not be necessary." target="The fundamental category to look for with narrow band detectors, as mentioned by Professor B, refers to the specific type of information or signal that the detectors are designed to identify within the narrow frequency bands. However, the transcript does not provide explicit details on what this &quot;category&quot; refers to in the given context.&#10;&#10;Professor B wonders about it when trying to find a standard answer because they are contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands. The professor's tone and use of phrases like &quot;in the wrong way&quot; and &quot;just wondering really&quot; suggest that this question is not meant as criticism, but rather an invitation for Grad C to consider standard practices in their work.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation between Professor B and Grad C, creating a simple neural network for performing correlation using learned weights from data does not require a complex function. Professor B states, &quot;And so, uh, the question is, how complex a function do you need? Do you need to have an added layer or something?&quot; This implies that the initial assumption was that a simple neural network would be sufficient, but they are considering whether a more complex model with additional layers might be necessary. However, Professor B's tone of voice and follow-up statement suggest that they believe a simpler model will likely be sufficient. Therefore, based on this conversation, it seems that the level of complexity required for the function in creating a simple neural net for performing correlation using learned weights from data is not high, and an additional layer may not be necessary." target="Yes, the speakers are discussing the use of various machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM). They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, as indicated by PhD E's question: &quot;they're providing scripts and everything so that basically, you push a button and it does training, and then it does test, and everything?&quot; They also mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation between Professor B and Grad C, creating a simple neural network for performing correlation using learned weights from data does not require a complex function. Professor B states, &quot;And so, uh, the question is, how complex a function do you need? Do you need to have an added layer or something?&quot; This implies that the initial assumption was that a simple neural network would be sufficient, but they are considering whether a more complex model with additional layers might be necessary. However, Professor B's tone of voice and follow-up statement suggest that they believe a simpler model will likely be sufficient. Therefore, based on this conversation, it seems that the level of complexity required for the function in creating a simple neural net for performing correlation using learned weights from data is not high, and an additional layer may not be necessary." target="1. Parallel processing: The conversation does not explicitly mention the specific techniques or neural networks being used in parallel. However, it is stated that a noise removal process would be applied before addressing reverberation, which could be one of the parallel processes.&#10;&#10;2. Orthogonalization: This term generally refers to transforming data to make its dimensions orthogonal (uncorrelated). In the context of signal processing and neural networks, it might involve decorrelating features or weight vectors to improve model performance. The specific method or model for orthogonalization is not mentioned in the conversation.&#10;&#10;3. Frame dropping for estimated silence probabilities: The neural network model used for frame dropping is a neural network also, with its input being somewhere between log mel bands or one of the earlier stages of processing. This neural network is likely responsible for estimating silence probabilities and deciding which frames to drop based on those estimates.&#10;&#10;While the specific techniques and models are not explicitly mentioned, the conversation touches upon parallel processing, orthogonalization, and frame dropping using a neural network for estimated silence probabilities in an audio processing pipeline that involves log mel bands, LDA filtering, downsampling, DCT, on-line normalization, upsampling, delta computation, and possibly other unspecified techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source=" PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , OK . And then the mel and then the log , and then the&#10;Speaker: PhD A&#10;Content: Then the LDA filter ,&#10;Speaker: Professor B&#10;Content: LDA filter .&#10;Speaker: PhD A&#10;Content: mmm , then the downsampling ,&#10;Speaker: Professor B&#10;Content: And then uh downsample ,&#10;Speaker: PhD A&#10;Content: DCT ,&#10;Speaker: Professor B&#10;Content: DCT ,&#10;Speaker: PhD A&#10;Content: then , um , on - line normalization ,&#10;Speaker: Professor B&#10;Content: on - line norm ,&#10;Speaker: PhD A&#10;Content: followed by {pause} upsampling . Then finally , we compute delta and we put the neural network also .&#10;Speaker: Professor B&#10;Content: Right , and then in parallel with {disfmarker} an {disfmarker} a neural net . And then following neural net , some {disfmarker} probably some orthogonalization .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh" target="The speakers, Professor B and PhD E, mention that some kind of normalization is being applied in both cases to deal with convolutional effects. In the speakers' project, they use online normalization and LDA RASTA, which seem to help but do so by throwing away high modulation frequencies, a technique not used in the other case. It is suggested to try normalization, particularly if logs are taken, and then compare the results, as changing multiple factors simultaneously might make it difficult to understand the impact of each change. The specific type of normalization being applied in both cases remains unspecified in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source=" PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , OK . And then the mel and then the log , and then the&#10;Speaker: PhD A&#10;Content: Then the LDA filter ,&#10;Speaker: Professor B&#10;Content: LDA filter .&#10;Speaker: PhD A&#10;Content: mmm , then the downsampling ,&#10;Speaker: Professor B&#10;Content: And then uh downsample ,&#10;Speaker: PhD A&#10;Content: DCT ,&#10;Speaker: Professor B&#10;Content: DCT ,&#10;Speaker: PhD A&#10;Content: then , um , on - line normalization ,&#10;Speaker: Professor B&#10;Content: on - line norm ,&#10;Speaker: PhD A&#10;Content: followed by {pause} upsampling . Then finally , we compute delta and we put the neural network also .&#10;Speaker: Professor B&#10;Content: Right , and then in parallel with {disfmarker} an {disfmarker} a neural net . And then following neural net , some {disfmarker} probably some orthogonalization .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh" target="1. Parallel processing: The conversation does not explicitly mention the specific techniques or neural networks being used in parallel. However, it is stated that a noise removal process would be applied before addressing reverberation, which could be one of the parallel processes.&#10;&#10;2. Orthogonalization: This term generally refers to transforming data to make its dimensions orthogonal (uncorrelated). In the context of signal processing and neural networks, it might involve decorrelating features or weight vectors to improve model performance. The specific method or model for orthogonalization is not mentioned in the conversation.&#10;&#10;3. Frame dropping for estimated silence probabilities: The neural network model used for frame dropping is a neural network also, with its input being somewhere between log mel bands or one of the earlier stages of processing. This neural network is likely responsible for estimating silence probabilities and deciding which frames to drop based on those estimates.&#10;&#10;While the specific techniques and models are not explicitly mentioned, the conversation touches upon parallel processing, orthogonalization, and frame dropping using a neural network for estimated silence probabilities in an audio processing pipeline that involves log mel bands, LDA filtering, downsampling, DCT, on-line normalization, upsampling, delta computation, and possibly other unspecified techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker} probably some orthogonalization .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh {disfmarker} Um .&#10;Speaker: PhD A&#10;Content: And finally frame dropping , which um , {vocalsound} would be a neural network also , used for estimated silence probabilities . And the input of this neural network would be somewhere between log {pause} mel bands or one of the earlier stages of the processing .&#10;Speaker: Professor B&#10;Content: Mm - hmm . So that 's sort of {disfmarker} most of this stuff is {disfmarker} yeah , is operating parallel with this other stuff .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah . So the things that we , um , uh , I guess we sort of {disfmarker} uh , There 's {disfmarker} there 's some , uh , neat ideas for {vocalsound} V A So , I mean , in {disfmarker} I think there 's sort of like {disfmarker} There 's a bunch" target="1. Parallel processing: The conversation does not explicitly mention the specific techniques or neural networks being used in parallel. However, it is stated that a noise removal process would be applied before addressing reverberation, which could be one of the parallel processes.&#10;&#10;2. Orthogonalization: This term generally refers to transforming data to make its dimensions orthogonal (uncorrelated). In the context of signal processing and neural networks, it might involve decorrelating features or weight vectors to improve model performance. The specific method or model for orthogonalization is not mentioned in the conversation.&#10;&#10;3. Frame dropping for estimated silence probabilities: The neural network model used for frame dropping is a neural network also, with its input being somewhere between log mel bands or one of the earlier stages of processing. This neural network is likely responsible for estimating silence probabilities and deciding which frames to drop based on those estimates.&#10;&#10;While the specific techniques and models are not explicitly mentioned, the conversation touches upon parallel processing, orthogonalization, and frame dropping using a neural network for estimated silence probabilities in an audio processing pipeline that involves log mel bands, LDA filtering, downsampling, DCT, on-line normalization, upsampling, delta computation, and possibly other unspecified techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source=" orthogonalizing .&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: But , you know {disfmarker} and {disfmarker} and , um , this is a little harder because you 're not just trying to find parameters . You 're actually trying to find the {disfmarker} the {disfmarker} the {disfmarker} the categories themselves . Uh , so a little more like brain surgery , I think on yourself . Uh . So , uh&#10;Speaker: Grad C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Um , anyway . That 's my {pause} thought .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: You 've been thinking about this problem for a long time actually . I mean , well {disfmarker} W actually , you stopped thinking about it for a long time , but you used to think about it {vocalsound} a lot .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: And you 've been thinking about it more now ,&#10;Speaker: PhD D" target="The fundamental category to look for with narrow band detectors, as mentioned by Professor B, refers to the specific type of information or signal that the detectors are designed to identify within the narrow frequency bands. However, the transcript does not provide explicit details on what this &quot;category&quot; refers to in the given context.&#10;&#10;Professor B wonders about it when trying to find a standard answer because they are contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands. The professor's tone and use of phrases like &quot;in the wrong way&quot; and &quot;just wondering really&quot; suggest that this question is not meant as criticism, but rather an invitation for Grad C to consider standard practices in their work.">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C's suggestion for discovering intermediate categories for classifying data is to use arbitrary windows in time of varying length and cluster those. This method differs from Sangita's approach, which focuses on specific temporal patterns for a certain set of phonemes. By clustering arbitrary windows in time, Grad C aims to find similarities and create intermediate categories that can be used in later classification stages. These intermediate categories may include transitions or relatively stable points in speech. This approach aligns with the idea of unsupervised learning as it allows for the discovery of potential patterns without relying on predefined human-significant categories such as phonemes." target=" Professor B&#10;Content: I ju you guys are Well , y anyway , you don't have to decide this second but thi think about it {disfmarker} about what {disfmarker} what you would think would be the {disfmarker} the best way to work it . I 'll&#10;Speaker: PhD A&#10;Content: But , uh {pause} Huh . Mm - hmm .&#10;Speaker: Professor B&#10;Content: support it either way , so .&#10;Speaker: PhD A&#10;Content: Mm - hmm Right .&#10;Speaker: Professor B&#10;Content: OK . Uh . Got anything to tell us ?&#10;Speaker: Grad C&#10;Content: Um . Well , I 've been reading some literature about clustering of data . Just , um , I guess , let me put it in context . OK , so we 're talking about discovering intermediate categories to , um {disfmarker} to classify . And , uh , I was looking at some of the work that , uh , Sangita was doing on these TRAPS things . So she has , um {disfmarker} she has temporal patterns for , um , a certain set of phonemes ,">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C's suggestion for discovering intermediate categories for classifying data is to use arbitrary windows in time of varying length and cluster those. This method differs from Sangita's approach, which focuses on specific temporal patterns for a certain set of phonemes. By clustering arbitrary windows in time, Grad C aims to find similarities and create intermediate categories that can be used in later classification stages. These intermediate categories may include transitions or relatively stable points in speech. This approach aligns with the idea of unsupervised learning as it allows for the discovery of potential patterns without relying on predefined human-significant categories such as phonemes." target=" mej metric which , uh , measures how {disfmarker} how closely related they are . And you start , um {vocalsound} by merging the patterns that are most closely related .&#10;Speaker: PhD E&#10;Content: And you create a tree .&#10;Speaker: Grad C&#10;Content: And y yeah , yeah , a dendrogram tree .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: Um .&#10;Speaker: PhD E&#10;Content: And then you can pick , uh , values anywhere along that tree to fix your set of clusters .&#10;Speaker: Grad C&#10;Content: Right , usually it 's when , um {disfmarker} when the sol similarity measures , um , don't go down as much .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Grad C&#10;Content: And so , uh {disfmarker} so you stop at that point . And what she found was , sh um , was there were five broad , um {disfmarker} broad categories , uh , corresponding to , uh , things like , uh , fricatives and , uh ,">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C's suggestion for discovering intermediate categories for classifying data is to use arbitrary windows in time of varying length and cluster those. This method differs from Sangita's approach, which focuses on specific temporal patterns for a certain set of phonemes. By clustering arbitrary windows in time, Grad C aims to find similarities and create intermediate categories that can be used in later classification stages. These intermediate categories may include transitions or relatively stable points in speech. This approach aligns with the idea of unsupervised learning as it allows for the discovery of potential patterns without relying on predefined human-significant categories such as phonemes." target=" representation ,&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: I mean , if you 're going for this sort of thing where you have {pause} uh , little detectors that are looking at narrow bands , then what you 're going to be looking for should be some category that you can find with the narrow bands .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: That {disfmarker} that seems to be kind of fundamental to it . Um , and then the other thing , uh , is {disfmarker} that I wonder about with it , and {disfmarker} and don't take this in the wrong way , like I {disfmarker} I know what I 'm doing or anything ,&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: but , I mean . {vocalsound} Um , just wondering really .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um , the sort of standard answer about this sort of thing is that if you 're trying to find">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C's suggestion for discovering intermediate categories for classifying data is to use arbitrary windows in time of varying length and cluster those. This method differs from Sangita's approach, which focuses on specific temporal patterns for a certain set of phonemes. By clustering arbitrary windows in time, Grad C aims to find similarities and create intermediate categories that can be used in later classification stages. These intermediate categories may include transitions or relatively stable points in speech. This approach aligns with the idea of unsupervised learning as it allows for the discovery of potential patterns without relying on predefined human-significant categories such as phonemes." target="1. Phone-based speech recognition systems struggle with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined.&#10;2. Humans have a mechanism that allows them to understand munged word models in conversional speech because they possess some kind of cognitive ability that enables them to deal with missing or distorted phonemes within words. This mechanism doesn't seem to affect human comprehension as much, indicating an inherent flexibility in the way humans process and interpret spoken language compared to rigid phone-based systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C's suggestion for discovering intermediate categories for classifying data is to use arbitrary windows in time of varying length and cluster those. This method differs from Sangita's approach, which focuses on specific temporal patterns for a certain set of phonemes. By clustering arbitrary windows in time, Grad C aims to find similarities and create intermediate categories that can be used in later classification stages. These intermediate categories may include transitions or relatively stable points in speech. This approach aligns with the idea of unsupervised learning as it allows for the discovery of potential patterns without relying on predefined human-significant categories such as phonemes." target="Researchers are considering creating intermediate categories, such as words instead of phones (phonemes), to improve word recognition in a speech system because phone-based models can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech. Human listeners can understand munged word models in conversational speech due to cognitive abilities that enable them to deal with missing or distorted phonemes, indicating an inherent flexibility in human spoken language processing. By using words as categories, researchers aim to create a more robust and accurate speech recognition system by focusing on recognizing and distinguishing whole words rather than individual phonemes.&#10;&#10;The idea of discovering intermediate categories like phonemes comes from Grad C's suggestion to use arbitrary windows in time of varying length and cluster those, potentially revealing transitions or relatively stable points in speech. This method aligns with the concept of unsupervised learning, allowing for pattern discovery without relying on predefined human-significant categories such as phonemes.&#10;&#10;While conversational speech may not clearly exhibit these intermediate categories like phones, researchers are exploring this approach to overcome limitations associated with phone-based models and improve overall system performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C's suggestion for discovering intermediate categories for classifying data is to use arbitrary windows in time of varying length and cluster those. This method differs from Sangita's approach, which focuses on specific temporal patterns for a certain set of phonemes. By clustering arbitrary windows in time, Grad C aims to find similarities and create intermediate categories that can be used in later classification stages. These intermediate categories may include transitions or relatively stable points in speech. This approach aligns with the idea of unsupervised learning as it allows for the discovery of potential patterns without relying on predefined human-significant categories such as phonemes." target="The current classification system puts some emphasis on well-matched categories, but it's only marginal with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively. Being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome because the evaluation criteria give more weight to well-matched categories, so performing poorly in this area could negatively affect the overall assessment.&#10;&#10;The importance of this aspect in relation to the overall outcome is not explicitly stated in the transcript, but it's mentioned that there will be debates about it afterwards, indicating that it may have some significance. Additionally, finding a standard answer to what the detectors should focus on within narrow bands and exploring unsupervised learning approaches like Grad C's suggestion of discovering intermediate categories can help improve performance in matching various inputs accurately.&#10;&#10;The use of more suitable categories, such as words instead of phones, could also benefit discriminative learning for speech recognition tasks, as suggested by PhD E. This approach focuses on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C's suggestion for discovering intermediate categories for classifying data is to use arbitrary windows in time of varying length and cluster those. This method differs from Sangita's approach, which focuses on specific temporal patterns for a certain set of phonemes. By clustering arbitrary windows in time, Grad C aims to find similarities and create intermediate categories that can be used in later classification stages. These intermediate categories may include transitions or relatively stable points in speech. This approach aligns with the idea of unsupervised learning as it allows for the discovery of potential patterns without relying on predefined human-significant categories such as phonemes." target="Yes, there is a possibility to automate the integration of error units like words or syllables into a larger unit for processing and feedback in speech recognition systems. The approach discussed by the researchers involves using words instead of phones as categories for discriminative learning in speech recognition systems. This method focuses on recognizing and distinguishing whole words rather than individual phonemes, which can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;In this context, automating the integration of error units could mean using word-level units directly in the speech recognition system, bypassing phone-based models entirely. By employing words as categories, the system can potentially improve its robustness and accuracy in processing spontaneous speech with variable phoneme appearances.&#10;&#10;Additionally, the researchers mentioned exploring unsupervised learning methods like Grad C's suggestion to use arbitrary windows in time of varying length and cluster those. This approach could help discover intermediate categories that align with transitions or relatively stable points in speech, further aiding in the accurate recognition and processing of larger units such as words or syllables for feedback in the system.">
      <data key="d0">1</data>
    </edge>
    <edge source="Grad C's suggestion for discovering intermediate categories for classifying data is to use arbitrary windows in time of varying length and cluster those. This method differs from Sangita's approach, which focuses on specific temporal patterns for a certain set of phonemes. By clustering arbitrary windows in time, Grad C aims to find similarities and create intermediate categories that can be used in later classification stages. These intermediate categories may include transitions or relatively stable points in speech. This approach aligns with the idea of unsupervised learning as it allows for the discovery of potential patterns without relying on predefined human-significant categories such as phonemes." target="To create a discriminative system without using significant human categories and avoiding top-down approaches, one could consider the following strategies:&#10;&#10;1. Explore unsupervised learning methods to discover underlying patterns or structures within the data. For example, Grad C's suggestion of using arbitrary windows in time of varying length and clustering them can help identify intermediate categories that are not predefined by human significance.&#10;2. Utilize a bottom-up approach where the system learns from raw data without relying on handcrafted features or predetermined categories. This allows the model to discover its own relevant features and patterns, potentially leading to novel insights about the structure of the data.&#10;3. Investigate the use of context-dependent models that consider sequential information in the data, rather than focusing on individual segments (e.g., phones or words). These models can capture the variability present in speech by taking into account how different contexts influence the acoustic signal.&#10;4. Consider incorporating additional modalities, such as visual or semantic information, to supplement the audio input and provide alternative cues for distinguishing between categories. This can help reduce reliance on human-significant categories while still maintaining a high level of accuracy in the system.&#10;5. Develop hybrid systems that combine unsupervised learning techniques with weakly supervised methods, where some minimal guidance is provided to the model but without explicitly defining the categories. This approach could strike a balance between data-driven discovery and human expertise.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Professor B&#10;Content: I ju you guys are Well , y anyway , you don't have to decide this second but thi think about it {disfmarker} about what {disfmarker} what you would think would be the {disfmarker} the best way to work it . I 'll&#10;Speaker: PhD A&#10;Content: But , uh {pause} Huh . Mm - hmm .&#10;Speaker: Professor B&#10;Content: support it either way , so .&#10;Speaker: PhD A&#10;Content: Mm - hmm Right .&#10;Speaker: Professor B&#10;Content: OK . Uh . Got anything to tell us ?&#10;Speaker: Grad C&#10;Content: Um . Well , I 've been reading some literature about clustering of data . Just , um , I guess , let me put it in context . OK , so we 're talking about discovering intermediate categories to , um {disfmarker} to classify . And , uh , I was looking at some of the work that , uh , Sangita was doing on these TRAPS things . So she has , um {disfmarker} she has temporal patterns for , um , a certain set of phonemes ," target="The fundamental category to look for with narrow band detectors, as mentioned by Professor B, refers to the specific type of information or signal that the detectors are designed to identify within the narrow frequency bands. However, the transcript does not provide explicit details on what this &quot;category&quot; refers to in the given context.&#10;&#10;Professor B wonders about it when trying to find a standard answer because they are contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands. The professor's tone and use of phrases like &quot;in the wrong way&quot; and &quot;just wondering really&quot; suggest that this question is not meant as criticism, but rather an invitation for Grad C to consider standard practices in their work.">
      <data key="d0">1</data>
    </edge>
    <edge source=" representation ,&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: I mean , if you 're going for this sort of thing where you have {pause} uh , little detectors that are looking at narrow bands , then what you 're going to be looking for should be some category that you can find with the narrow bands .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: That {disfmarker} that seems to be kind of fundamental to it . Um , and then the other thing , uh , is {disfmarker} that I wonder about with it , and {disfmarker} and don't take this in the wrong way , like I {disfmarker} I know what I 'm doing or anything ,&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: but , I mean . {vocalsound} Um , just wondering really .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um , the sort of standard answer about this sort of thing is that if you 're trying to find" target="The fundamental category to look for with narrow band detectors, as mentioned by Professor B, refers to the specific type of information or signal that the detectors are designed to identify within the narrow frequency bands. However, the transcript does not provide explicit details on what this &quot;category&quot; refers to in the given context.&#10;&#10;Professor B wonders about it when trying to find a standard answer because they are contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands. The professor's tone and use of phrases like &quot;in the wrong way&quot; and &quot;just wondering really&quot; suggest that this question is not meant as criticism, but rather an invitation for Grad C to consider standard practices in their work.">
      <data key="d0">1</data>
    </edge>
    <edge source=" representation ,&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: I mean , if you 're going for this sort of thing where you have {pause} uh , little detectors that are looking at narrow bands , then what you 're going to be looking for should be some category that you can find with the narrow bands .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: That {disfmarker} that seems to be kind of fundamental to it . Um , and then the other thing , uh , is {disfmarker} that I wonder about with it , and {disfmarker} and don't take this in the wrong way , like I {disfmarker} I know what I 'm doing or anything ,&#10;Speaker: Grad C&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: but , I mean . {vocalsound} Um , just wondering really .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um , the sort of standard answer about this sort of thing is that if you 're trying to find" target="The fundamental category that narrow band detectors should look for refers to the specific type of information or signal these detectors are designed to identify within the narrow frequency bands. However, the text does not provide explicit details on what this &quot;category&quot; refers to in the given context. The speakers seem to be contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands.&#10;&#10;The concerns regarding the standard approach to finding this category include the current classification system's marginal emphasis on well-matched categories (with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively). The speakers mention that being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome. They also discuss debates about the importance of this aspect in relation to the overall outcome and suggest that finding a standard answer to what detectors should focus on within narrow bands could help improve performance in matching various inputs accurately.&#10;&#10;The speakers are discussing the use of machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), for tasks such as speech recognition. They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, and they mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difficulty in considering reverberation (echoes) in isolation when examining the real case is that there is a significant interaction between reverberation and noise. In real-world situations, background noise combined with reverberation can significantly impact the performance of various techniques designed to address these issues.&#10;&#10;When reverberation and noise are considered separately, certain methods may work well for artificial or controlled environments. However, when they occur together in a real situation, those same techniques might not perform as expected. This is because noise can make it more difficult to separate the desired signal from the reverberation, making it essential to consider both factors simultaneously.&#10;&#10;By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques could be minimized, resulting in better overall performance." target=" PhD E&#10;Content: I guess he 's busy with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , prelims , right .&#10;Speaker: Grad C&#10;Content: Pre - prelim hell .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh , but {disfmarker} but , you know , that 'll {disfmarker} uh , it 's clear that we , uh {disfmarker} we are not {disfmarker} with the real case that we 're looking at , we can't just look at reverberation in isolation because the interaction between that and noise is {disfmarker} is considerable . And that 's I mean , in the past we 've looked at , uh , and this is hard enough , the interaction between channel effects and {disfmarker} and , uh {disfmarker} and additive noise , uh , so convolutional effects and {disfmarker} and additive effects . And that">
      <data key="d0">1</data>
    </edge>
    <edge source="The difficulty in considering reverberation (echoes) in isolation when examining the real case is that there is a significant interaction between reverberation and noise. In real-world situations, background noise combined with reverberation can significantly impact the performance of various techniques designed to address these issues.&#10;&#10;When reverberation and noise are considered separately, certain methods may work well for artificial or controlled environments. However, when they occur together in a real situation, those same techniques might not perform as expected. This is because noise can make it more difficult to separate the desired signal from the reverberation, making it essential to consider both factors simultaneously.&#10;&#10;By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques could be minimized, resulting in better overall performance." target="er} well , we do , but we don't {disfmarker} don't re - synthesize . In {disfmarker} in the program we don't re - synthesize and then re - analyze once again . We just use the clean FFT bins .&#10;Speaker: Professor B&#10;Content: But you have a re - synthesized thing that you {disfmarker} that 's an {disfmarker} an option here .&#10;Speaker: PhD A&#10;Content: This is an option that {disfmarker} then you can {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , I gu I guess my point is that , um , i in some of the work he 's doing in reverberation , one of the things that we 're finding is that , uh , it 's {disfmarker} it 's {disfmarker} for the {disfmarker} for an artificial situation , we can just deal with the reverberation and his techniques work really well . But for the real situation uh , problem is , is that you don't just have reverberation , you have reverberation in noise . And if you don">
      <data key="d0">1</data>
    </edge>
    <edge source="The difficulty in considering reverberation (echoes) in isolation when examining the real case is that there is a significant interaction between reverberation and noise. In real-world situations, background noise combined with reverberation can significantly impact the performance of various techniques designed to address these issues.&#10;&#10;When reverberation and noise are considered separately, certain methods may work well for artificial or controlled environments. However, when they occur together in a real situation, those same techniques might not perform as expected. This is because noise can make it more difficult to separate the desired signal from the reverberation, making it essential to consider both factors simultaneously.&#10;&#10;By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques could be minimized, resulting in better overall performance." target=" . But for the real situation uh , problem is , is that you don't just have reverberation , you have reverberation in noise . And if you don't include that in the model , it doesn't work very well . So in fact it might be a very nice thing to do , to just take the noise removal part of it and put that in front of what he 's looking at . And , uh , generate new files or whatever , and {disfmarker} and , uh , uh {disfmarker} and then do the reverberation part .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Anyway .&#10;Speaker: PhD E&#10;Content: So Dave hasn't {pause} tried that yet ?&#10;Speaker: Professor B&#10;Content: No , no . He 's {disfmarker} I mean , e&#10;Speaker: PhD E&#10;Content: I guess he 's busy with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , prelims ,">
      <data key="d0">1</data>
    </edge>
    <edge source="The difficulty in considering reverberation (echoes) in isolation when examining the real case is that there is a significant interaction between reverberation and noise. In real-world situations, background noise combined with reverberation can significantly impact the performance of various techniques designed to address these issues.&#10;&#10;When reverberation and noise are considered separately, certain methods may work well for artificial or controlled environments. However, when they occur together in a real situation, those same techniques might not perform as expected. This is because noise can make it more difficult to separate the desired signal from the reverberation, making it essential to consider both factors simultaneously.&#10;&#10;By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques could be minimized, resulting in better overall performance." target=" , uh {disfmarker} and additive noise , uh , so convolutional effects and {disfmarker} and additive effects . And that 's hard enough . I mean , I don't think we really {disfmarker} I mean , we 're trying to deal with that . In a sense that 's what we 're trying to deal with in this Aurora task . And we have , uh , the , uh , uh , LDA stuff that in principle is doing something about convolutional effects . And we have the noise suppression that 's doing something about noise . Uh , even that 's hard enough . And {disfmarker} and the on - line normalization as well , in that s category . i i There 's all these interactions between these two and that 's part of why these guys had to work so hard on {disfmarker} on juggling everything around . But now when you throw in the reverberation , it 's even worse , because not only do you have these effects , but you also have some long time effects . And , um , so Dave has something which , uh , is doing some nice things under some conditions with {disfmarker} with long time">
      <data key="d0">1</data>
    </edge>
    <edge source="The difficulty in considering reverberation (echoes) in isolation when examining the real case is that there is a significant interaction between reverberation and noise. In real-world situations, background noise combined with reverberation can significantly impact the performance of various techniques designed to address these issues.&#10;&#10;When reverberation and noise are considered separately, certain methods may work well for artificial or controlled environments. However, when they occur together in a real situation, those same techniques might not perform as expected. This is because noise can make it more difficult to separate the desired signal from the reverberation, making it essential to consider both factors simultaneously.&#10;&#10;By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques could be minimized, resulting in better overall performance." target=" effects . And , um , so Dave has something which , uh , is doing some nice things under some conditions with {disfmarker} with long time effects but when it 's {disfmarker} when there 's noise there too , it 's {disfmarker} it 's {disfmarker} it 's pretty hard . So we have to start {disfmarker} Since any {disfmarker} almost any real situation is gonna have {disfmarker} uh , where you have the microphone distant , is going to have both things , we {disfmarker} we actually have to think about both at the same time .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: So , um {disfmarker} So there 's this noise suppression thing , which is sort of worked out and then , uh , maybe you should just continue telling what {disfmarker} what else is in the {disfmarker} the form we have .&#10;Speaker: PhD A&#10;Content: Yeah , well , {vocalsound} the , um , the other parts of the system are the {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="The difficulty in considering reverberation (echoes) in isolation when examining the real case is that there is a significant interaction between reverberation and noise. In real-world situations, background noise combined with reverberation can significantly impact the performance of various techniques designed to address these issues.&#10;&#10;When reverberation and noise are considered separately, certain methods may work well for artificial or controlled environments. However, when they occur together in a real situation, those same techniques might not perform as expected. This is because noise can make it more difficult to separate the desired signal from the reverberation, making it essential to consider both factors simultaneously.&#10;&#10;By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques could be minimized, resulting in better overall performance." target=" A So , I mean , in {disfmarker} I think there 's sort of like {disfmarker} There 's a bunch of tuning things to improve stuff . There 's questions about {pause} various places where there 's an exponent , if it 's the right exponent , or {pause} ways that we 're estimating noise , that we can improve estimating noise . And there 's gonna be a host of those . But structurally it seemed like the things {disfmarker} the main things that {disfmarker} that we brought up that , uh , are {disfmarker} are gonna need to get worked on seriously are , uh , uh , a {disfmarker} {vocalsound} a significantly better VAD , uh , putting the neural net on , um , which , you know , we haven't been doing anything with , the , uh , neural net at the end there , and , uh , the , uh , {vocalsound} opening up the second front . Uh .&#10;Speaker: PhD E&#10;Content: The other half of the channel ?&#10;Speaker: Professor B&#10;Content: Yeah , yeah , I mean , cuz we {">
      <data key="d0">1</data>
    </edge>
    <edge source="The difficulty in considering reverberation (echoes) in isolation when examining the real case is that there is a significant interaction between reverberation and noise. In real-world situations, background noise combined with reverberation can significantly impact the performance of various techniques designed to address these issues.&#10;&#10;When reverberation and noise are considered separately, certain methods may work well for artificial or controlled environments. However, when they occur together in a real situation, those same techniques might not perform as expected. This is because noise can make it more difficult to separate the desired signal from the reverberation, making it essential to consider both factors simultaneously.&#10;&#10;By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques could be minimized, resulting in better overall performance." target="1. Challenges in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;2. Previous difficulties with channel effects and additive noise: Similar challenges have been experienced when examining the interaction between channel effects (e.g., distortions caused by transmission channels) and additive noise (unwanted signals added to the desired signal). Addressing these interactions is difficult, as convolutional effects (related to channel characteristics) and additive noise need to be considered concurrently. This complexity has been a challenge in past research efforts, such as the Aurora task, which aims to develop solutions for handling these issues.&#10;3. Impact on performance: When reverberation is not taken into account in conjunction with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. As a result, this could lead to suboptimal overall performance.&#10;4. Proposed solution: In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in real-world situations where both noise and reverberation are present.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difficulty in considering reverberation (echoes) in isolation when examining the real case is that there is a significant interaction between reverberation and noise. In real-world situations, background noise combined with reverberation can significantly impact the performance of various techniques designed to address these issues.&#10;&#10;When reverberation and noise are considered separately, certain methods may work well for artificial or controlled environments. However, when they occur together in a real situation, those same techniques might not perform as expected. This is because noise can make it more difficult to separate the desired signal from the reverberation, making it essential to consider both factors simultaneously.&#10;&#10;By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques could be minimized, resulting in better overall performance." target="1. Difficulties in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;   &#10;2. Importance of taking into account the interaction between reverberation and noise: If reverberation is not taken into account along with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in situations where both noise and reverberation are present.">
      <data key="d0">1</data>
    </edge>
    <edge source="The difficulty in considering reverberation (echoes) in isolation when examining the real case is that there is a significant interaction between reverberation and noise. In real-world situations, background noise combined with reverberation can significantly impact the performance of various techniques designed to address these issues.&#10;&#10;When reverberation and noise are considered separately, certain methods may work well for artificial or controlled environments. However, when they occur together in a real situation, those same techniques might not perform as expected. This is because noise can make it more difficult to separate the desired signal from the reverberation, making it essential to consider both factors simultaneously.&#10;&#10;By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques could be minimized, resulting in better overall performance." target="1. Parallel processing: The conversation does not explicitly mention the specific techniques or neural networks being used in parallel. However, it is stated that a noise removal process would be applied before addressing reverberation, which could be one of the parallel processes.&#10;&#10;2. Orthogonalization: This term generally refers to transforming data to make its dimensions orthogonal (uncorrelated). In the context of signal processing and neural networks, it might involve decorrelating features or weight vectors to improve model performance. The specific method or model for orthogonalization is not mentioned in the conversation.&#10;&#10;3. Frame dropping for estimated silence probabilities: The neural network model used for frame dropping is a neural network also, with its input being somewhere between log mel bands or one of the earlier stages of processing. This neural network is likely responsible for estimating silence probabilities and deciding which frames to drop based on those estimates.&#10;&#10;While the specific techniques and models are not explicitly mentioned, the conversation touches upon parallel processing, orthogonalization, and frame dropping using a neural network for estimated silence probabilities in an audio processing pipeline that involves log mel bands, LDA filtering, downsampling, DCT, on-line normalization, upsampling, delta computation, and possibly other unspecified techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source=" PhD E&#10;Content: I guess he 's busy with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , prelims , right .&#10;Speaker: Grad C&#10;Content: Pre - prelim hell .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh , but {disfmarker} but , you know , that 'll {disfmarker} uh , it 's clear that we , uh {disfmarker} we are not {disfmarker} with the real case that we 're looking at , we can't just look at reverberation in isolation because the interaction between that and noise is {disfmarker} is considerable . And that 's I mean , in the past we 've looked at , uh , and this is hard enough , the interaction between channel effects and {disfmarker} and , uh {disfmarker} and additive noise , uh , so convolutional effects and {disfmarker} and additive effects . And that" target="1. Challenges in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;2. Previous difficulties with channel effects and additive noise: Similar challenges have been experienced when examining the interaction between channel effects (e.g., distortions caused by transmission channels) and additive noise (unwanted signals added to the desired signal). Addressing these interactions is difficult, as convolutional effects (related to channel characteristics) and additive noise need to be considered concurrently. This complexity has been a challenge in past research efforts, such as the Aurora task, which aims to develop solutions for handling these issues.&#10;3. Impact on performance: When reverberation is not taken into account in conjunction with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. As a result, this could lead to suboptimal overall performance.&#10;4. Proposed solution: In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in real-world situations where both noise and reverberation are present.">
      <data key="d0">1</data>
    </edge>
    <edge source=" PhD E&#10;Content: I guess he 's busy with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , prelims , right .&#10;Speaker: Grad C&#10;Content: Pre - prelim hell .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh , but {disfmarker} but , you know , that 'll {disfmarker} uh , it 's clear that we , uh {disfmarker} we are not {disfmarker} with the real case that we 're looking at , we can't just look at reverberation in isolation because the interaction between that and noise is {disfmarker} is considerable . And that 's I mean , in the past we 've looked at , uh , and this is hard enough , the interaction between channel effects and {disfmarker} and , uh {disfmarker} and additive noise , uh , so convolutional effects and {disfmarker} and additive effects . And that" target="1. Difficulties in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;   &#10;2. Importance of taking into account the interaction between reverberation and noise: If reverberation is not taken into account along with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in situations where both noise and reverberation are present.">
      <data key="d0">1</data>
    </edge>
    <edge source="er} well , we do , but we don't {disfmarker} don't re - synthesize . In {disfmarker} in the program we don't re - synthesize and then re - analyze once again . We just use the clean FFT bins .&#10;Speaker: Professor B&#10;Content: But you have a re - synthesized thing that you {disfmarker} that 's an {disfmarker} an option here .&#10;Speaker: PhD A&#10;Content: This is an option that {disfmarker} then you can {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , I gu I guess my point is that , um , i in some of the work he 's doing in reverberation , one of the things that we 're finding is that , uh , it 's {disfmarker} it 's {disfmarker} for the {disfmarker} for an artificial situation , we can just deal with the reverberation and his techniques work really well . But for the real situation uh , problem is , is that you don't just have reverberation , you have reverberation in noise . And if you don" target="1. Challenges in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;2. Previous difficulties with channel effects and additive noise: Similar challenges have been experienced when examining the interaction between channel effects (e.g., distortions caused by transmission channels) and additive noise (unwanted signals added to the desired signal). Addressing these interactions is difficult, as convolutional effects (related to channel characteristics) and additive noise need to be considered concurrently. This complexity has been a challenge in past research efforts, such as the Aurora task, which aims to develop solutions for handling these issues.&#10;3. Impact on performance: When reverberation is not taken into account in conjunction with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. As a result, this could lead to suboptimal overall performance.&#10;4. Proposed solution: In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in real-world situations where both noise and reverberation are present.">
      <data key="d0">1</data>
    </edge>
    <edge source="er} well , we do , but we don't {disfmarker} don't re - synthesize . In {disfmarker} in the program we don't re - synthesize and then re - analyze once again . We just use the clean FFT bins .&#10;Speaker: Professor B&#10;Content: But you have a re - synthesized thing that you {disfmarker} that 's an {disfmarker} an option here .&#10;Speaker: PhD A&#10;Content: This is an option that {disfmarker} then you can {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , I gu I guess my point is that , um , i in some of the work he 's doing in reverberation , one of the things that we 're finding is that , uh , it 's {disfmarker} it 's {disfmarker} for the {disfmarker} for an artificial situation , we can just deal with the reverberation and his techniques work really well . But for the real situation uh , problem is , is that you don't just have reverberation , you have reverberation in noise . And if you don" target="1. Difficulties in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;   &#10;2. Importance of taking into account the interaction between reverberation and noise: If reverberation is not taken into account along with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in situations where both noise and reverberation are present.">
      <data key="d0">1</data>
    </edge>
    <edge source=" . But for the real situation uh , problem is , is that you don't just have reverberation , you have reverberation in noise . And if you don't include that in the model , it doesn't work very well . So in fact it might be a very nice thing to do , to just take the noise removal part of it and put that in front of what he 's looking at . And , uh , generate new files or whatever , and {disfmarker} and , uh , uh {disfmarker} and then do the reverberation part .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Anyway .&#10;Speaker: PhD E&#10;Content: So Dave hasn't {pause} tried that yet ?&#10;Speaker: Professor B&#10;Content: No , no . He 's {disfmarker} I mean , e&#10;Speaker: PhD E&#10;Content: I guess he 's busy with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , prelims ," target="1. Challenges in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;2. Previous difficulties with channel effects and additive noise: Similar challenges have been experienced when examining the interaction between channel effects (e.g., distortions caused by transmission channels) and additive noise (unwanted signals added to the desired signal). Addressing these interactions is difficult, as convolutional effects (related to channel characteristics) and additive noise need to be considered concurrently. This complexity has been a challenge in past research efforts, such as the Aurora task, which aims to develop solutions for handling these issues.&#10;3. Impact on performance: When reverberation is not taken into account in conjunction with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. As a result, this could lead to suboptimal overall performance.&#10;4. Proposed solution: In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in real-world situations where both noise and reverberation are present.">
      <data key="d0">1</data>
    </edge>
    <edge source=" . But for the real situation uh , problem is , is that you don't just have reverberation , you have reverberation in noise . And if you don't include that in the model , it doesn't work very well . So in fact it might be a very nice thing to do , to just take the noise removal part of it and put that in front of what he 's looking at . And , uh , generate new files or whatever , and {disfmarker} and , uh , uh {disfmarker} and then do the reverberation part .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Anyway .&#10;Speaker: PhD E&#10;Content: So Dave hasn't {pause} tried that yet ?&#10;Speaker: Professor B&#10;Content: No , no . He 's {disfmarker} I mean , e&#10;Speaker: PhD E&#10;Content: I guess he 's busy with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , prelims ," target="1. Difficulties in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;   &#10;2. Importance of taking into account the interaction between reverberation and noise: If reverberation is not taken into account along with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in situations where both noise and reverberation are present.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , uh {disfmarker} and additive noise , uh , so convolutional effects and {disfmarker} and additive effects . And that 's hard enough . I mean , I don't think we really {disfmarker} I mean , we 're trying to deal with that . In a sense that 's what we 're trying to deal with in this Aurora task . And we have , uh , the , uh , uh , LDA stuff that in principle is doing something about convolutional effects . And we have the noise suppression that 's doing something about noise . Uh , even that 's hard enough . And {disfmarker} and the on - line normalization as well , in that s category . i i There 's all these interactions between these two and that 's part of why these guys had to work so hard on {disfmarker} on juggling everything around . But now when you throw in the reverberation , it 's even worse , because not only do you have these effects , but you also have some long time effects . And , um , so Dave has something which , uh , is doing some nice things under some conditions with {disfmarker} with long time" target="1. Challenges in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;2. Previous difficulties with channel effects and additive noise: Similar challenges have been experienced when examining the interaction between channel effects (e.g., distortions caused by transmission channels) and additive noise (unwanted signals added to the desired signal). Addressing these interactions is difficult, as convolutional effects (related to channel characteristics) and additive noise need to be considered concurrently. This complexity has been a challenge in past research efforts, such as the Aurora task, which aims to develop solutions for handling these issues.&#10;3. Impact on performance: When reverberation is not taken into account in conjunction with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. As a result, this could lead to suboptimal overall performance.&#10;4. Proposed solution: In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in real-world situations where both noise and reverberation are present.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , uh {disfmarker} and additive noise , uh , so convolutional effects and {disfmarker} and additive effects . And that 's hard enough . I mean , I don't think we really {disfmarker} I mean , we 're trying to deal with that . In a sense that 's what we 're trying to deal with in this Aurora task . And we have , uh , the , uh , uh , LDA stuff that in principle is doing something about convolutional effects . And we have the noise suppression that 's doing something about noise . Uh , even that 's hard enough . And {disfmarker} and the on - line normalization as well , in that s category . i i There 's all these interactions between these two and that 's part of why these guys had to work so hard on {disfmarker} on juggling everything around . But now when you throw in the reverberation , it 's even worse , because not only do you have these effects , but you also have some long time effects . And , um , so Dave has something which , uh , is doing some nice things under some conditions with {disfmarker} with long time" target="1. Difficulties in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;   &#10;2. Importance of taking into account the interaction between reverberation and noise: If reverberation is not taken into account along with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in situations where both noise and reverberation are present.">
      <data key="d0">1</data>
    </edge>
    <edge source=" A So , I mean , in {disfmarker} I think there 's sort of like {disfmarker} There 's a bunch of tuning things to improve stuff . There 's questions about {pause} various places where there 's an exponent , if it 's the right exponent , or {pause} ways that we 're estimating noise , that we can improve estimating noise . And there 's gonna be a host of those . But structurally it seemed like the things {disfmarker} the main things that {disfmarker} that we brought up that , uh , are {disfmarker} are gonna need to get worked on seriously are , uh , uh , a {disfmarker} {vocalsound} a significantly better VAD , uh , putting the neural net on , um , which , you know , we haven't been doing anything with , the , uh , neural net at the end there , and , uh , the , uh , {vocalsound} opening up the second front . Uh .&#10;Speaker: PhD E&#10;Content: The other half of the channel ?&#10;Speaker: Professor B&#10;Content: Yeah , yeah , I mean , cuz we {" target="1. Challenges in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;2. Previous difficulties with channel effects and additive noise: Similar challenges have been experienced when examining the interaction between channel effects (e.g., distortions caused by transmission channels) and additive noise (unwanted signals added to the desired signal). Addressing these interactions is difficult, as convolutional effects (related to channel characteristics) and additive noise need to be considered concurrently. This complexity has been a challenge in past research efforts, such as the Aurora task, which aims to develop solutions for handling these issues.&#10;3. Impact on performance: When reverberation is not taken into account in conjunction with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. As a result, this could lead to suboptimal overall performance.&#10;4. Proposed solution: In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in real-world situations where both noise and reverberation are present.">
      <data key="d0">1</data>
    </edge>
    <edge source=" A So , I mean , in {disfmarker} I think there 's sort of like {disfmarker} There 's a bunch of tuning things to improve stuff . There 's questions about {pause} various places where there 's an exponent , if it 's the right exponent , or {pause} ways that we 're estimating noise , that we can improve estimating noise . And there 's gonna be a host of those . But structurally it seemed like the things {disfmarker} the main things that {disfmarker} that we brought up that , uh , are {disfmarker} are gonna need to get worked on seriously are , uh , uh , a {disfmarker} {vocalsound} a significantly better VAD , uh , putting the neural net on , um , which , you know , we haven't been doing anything with , the , uh , neural net at the end there , and , uh , the , uh , {vocalsound} opening up the second front . Uh .&#10;Speaker: PhD E&#10;Content: The other half of the channel ?&#10;Speaker: Professor B&#10;Content: Yeah , yeah , I mean , cuz we {" target="1. The configuration applied for spectral subtraction in the Wiener filter that works well is using two steps of smoothing: recursion along time and a sliding window of twenty FFT bins along frequency, before any mel scaling has been done.&#10;2. The aspects of the system they plan to work on next are improving the noise estimator, adding a neural net at the end of the process, and opening up a second stream or &quot;front&quot; for processing.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation, while open-source projects often use anonymous CVS mechanisms, Professor B states that they're &quot;still so much in development&quot; and want to restrict access to insiders for their project. PhD E initially brought up the idea of using an anonymous mechanism for their transcripts, but after considering the project's stage and the desire to limit access, they decided against it. Therefore, based on the conversation, the anonymous mechanism is not currently available or feasible for their transcripts, given the project's development stage and the group's decision to limit access to insiders." target=" server but then , yeah , you can access it .&#10;Speaker: PhD E&#10;Content: So that {disfmarker} Oh , OK .&#10;Speaker: PhD A&#10;Content: you {disfmarker} you can set up priorities .&#10;Speaker: PhD E&#10;Content: So the anonymous mechanism {disfmarker}&#10;Speaker: PhD A&#10;Content: You can access them and mostly if you {disfmarker} if y the set the server is set up like this .&#10;Speaker: PhD E&#10;Content: OK . Because a lot of the open source stuff works with anonymous CVS and I 'm just wondering {disfmarker} Uh , I mean , for our transcripts we may want to do that .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Uh .&#10;Speaker: Professor B&#10;Content: Yeah , for this stuff I don't think we 're {pause} quite up to that . I mean , we 're still so much in development .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Yeah ,&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation, while open-source projects often use anonymous CVS mechanisms, Professor B states that they're &quot;still so much in development&quot; and want to restrict access to insiders for their project. PhD E initially brought up the idea of using an anonymous mechanism for their transcripts, but after considering the project's stage and the desire to limit access, they decided against it. Therefore, based on the conversation, the anonymous mechanism is not currently available or feasible for their transcripts, given the project's development stage and the group's decision to limit access to insiders." target=" . I mean , we 're still so much in development .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Yeah ,&#10;Speaker: Professor B&#10;Content: We want to have just the insiders .&#10;Speaker: PhD E&#10;Content: yeah , yeah . Oh , I wasn't suggesting for this . I 'm {pause} thinking of the Meeting Recorder {comment} stuff&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: but . Yeah . OK . Cool .&#10;Speaker: Professor B&#10;Content: Yeah . So , uh {disfmarker}&#10;Speaker: PhD E&#10;Content: What 's new ?&#10;Speaker: Professor B&#10;Content: Well , I mean , I think maybe the thing to me might be {disfmarker} I me I 'm sure you 've just been working on {disfmarker} on , uh , details of that since the meeting , right ? And so {disfmarker}&#10;Speaker: PhD A&#10;Content: Mmm , since the meeting , well , I {disfmarker} I 've been {disfmarker} I 've been train">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation, while open-source projects often use anonymous CVS mechanisms, Professor B states that they're &quot;still so much in development&quot; and want to restrict access to insiders for their project. PhD E initially brought up the idea of using an anonymous mechanism for their transcripts, but after considering the project's stage and the desire to limit access, they decided against it. Therefore, based on the conversation, the anonymous mechanism is not currently available or feasible for their transcripts, given the project's development stage and the group's decision to limit access to insiders." target=" .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: You probably received the mail .&#10;Speaker: PhD E&#10;Content: Oh , right , I saw {disfmarker} I saw the note .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: What was the update ?&#10;Speaker: PhD A&#10;Content: What was the update ? So there is th then {disfmarker} the {disfmarker} all the new features that go in .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: The , um , noise suppression , the re - synthesis of speech after suppression . These are the {disfmarker}&#10;Speaker: PhD E&#10;Content: Is the , um {disfmarker} the CVS mechanism working {pause} well ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Are {disfmarker} are people , uh , up at OGI grabbing code uh , via that ?&#10;Speaker: PhD D">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation, while open-source projects often use anonymous CVS mechanisms, Professor B states that they're &quot;still so much in development&quot; and want to restrict access to insiders for their project. PhD E initially brought up the idea of using an anonymous mechanism for their transcripts, but after considering the project's stage and the desire to limit access, they decided against it. Therefore, based on the conversation, the anonymous mechanism is not currently available or feasible for their transcripts, given the project's development stage and the group's decision to limit access to insiders." target=": Yeah .&#10;Speaker: PhD A&#10;Content: I logged in there and I tried {pause} to import {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah ? It worked good ?&#10;Speaker: PhD A&#10;Content: Yeah , it works .&#10;Speaker: PhD E&#10;Content: Oh , good !&#10;Speaker: PhD A&#10;Content: But it 's {disfmarker} So , right now it 's the mechanism with SSH .&#10;Speaker: PhD D&#10;Content: Oh .&#10;Speaker: PhD E&#10;Content: Great !&#10;Speaker: PhD A&#10;Content: I don't {pause} s I didn't set up {disfmarker} You can also set up a CVS server {pause} on a new port . It 's like well {pause} uh , a main server , or d You can do a CVS server .&#10;Speaker: PhD E&#10;Content: Yeah . Right . Then that 's using the CVS password mechanism and all that ,&#10;Speaker: PhD A&#10;Content: But . Yeah , right .&#10;Speaker: PhD E&#10;Content: right ?&#10;Speaker: PhD A&#10;Content: But">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation, while open-source projects often use anonymous CVS mechanisms, Professor B states that they're &quot;still so much in development&quot; and want to restrict access to insiders for their project. PhD E initially brought up the idea of using an anonymous mechanism for their transcripts, but after considering the project's stage and the desire to limit access, they decided against it. Therefore, based on the conversation, the anonymous mechanism is not currently available or feasible for their transcripts, given the project's development stage and the group's decision to limit access to insiders." target="Based on the conversation provided, it appears that PhD A has successfully set up a CVS server using the existing SSH mechanism for authentication. There is no clear mention of setting up a CVS server on a new port in addition to the current SSH mechanism. Therefore, as of this conversation, the proposed setup with a CVS password mechanism for authentication on a new port has not been implemented or discussed further.">
      <data key="d0">1</data>
    </edge>
    <edge source="According to the conversation, while open-source projects often use anonymous CVS mechanisms, Professor B states that they're &quot;still so much in development&quot; and want to restrict access to insiders for their project. PhD E initially brought up the idea of using an anonymous mechanism for their transcripts, but after considering the project's stage and the desire to limit access, they decided against it. Therefore, based on the conversation, the anonymous mechanism is not currently available or feasible for their transcripts, given the project's development stage and the group's decision to limit access to insiders." target="Based on the provided transcripts, the system for improving digital processes and handling large vocabulary runs is not explicitly stated as ready. However, PhD E is assigned to take charge of the large vocabulary part and communicate this responsibility to Joe and CC Hari. It would be beneficial to have Professor B use his experience to iron out potential issues, as he has shown concern about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. His expertise could help address these concerns and improve the system's performance.">
      <data key="d0">1</data>
    </edge>
    <edge source=" server but then , yeah , you can access it .&#10;Speaker: PhD E&#10;Content: So that {disfmarker} Oh , OK .&#10;Speaker: PhD A&#10;Content: you {disfmarker} you can set up priorities .&#10;Speaker: PhD E&#10;Content: So the anonymous mechanism {disfmarker}&#10;Speaker: PhD A&#10;Content: You can access them and mostly if you {disfmarker} if y the set the server is set up like this .&#10;Speaker: PhD E&#10;Content: OK . Because a lot of the open source stuff works with anonymous CVS and I 'm just wondering {disfmarker} Uh , I mean , for our transcripts we may want to do that .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Uh .&#10;Speaker: Professor B&#10;Content: Yeah , for this stuff I don't think we 're {pause} quite up to that . I mean , we 're still so much in development .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Yeah ,&#10;Spe" target="Based on the conversation provided, it appears that PhD A has successfully set up a CVS server using the existing SSH mechanism for authentication. There is no clear mention of setting up a CVS server on a new port in addition to the current SSH mechanism. Therefore, as of this conversation, the proposed setup with a CVS password mechanism for authentication on a new port has not been implemented or discussed further.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: PhD A&#10;Content: I logged in there and I tried {pause} to import {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah ? It worked good ?&#10;Speaker: PhD A&#10;Content: Yeah , it works .&#10;Speaker: PhD E&#10;Content: Oh , good !&#10;Speaker: PhD A&#10;Content: But it 's {disfmarker} So , right now it 's the mechanism with SSH .&#10;Speaker: PhD D&#10;Content: Oh .&#10;Speaker: PhD E&#10;Content: Great !&#10;Speaker: PhD A&#10;Content: I don't {pause} s I didn't set up {disfmarker} You can also set up a CVS server {pause} on a new port . It 's like well {pause} uh , a main server , or d You can do a CVS server .&#10;Speaker: PhD E&#10;Content: Yeah . Right . Then that 's using the CVS password mechanism and all that ,&#10;Speaker: PhD A&#10;Content: But . Yeah , right .&#10;Speaker: PhD E&#10;Content: right ?&#10;Speaker: PhD A&#10;Content: But" target="Based on the conversation provided, it appears that PhD A has successfully set up a CVS server using the existing SSH mechanism for authentication. There is no clear mention of setting up a CVS server on a new port in addition to the current SSH mechanism. Therefore, as of this conversation, the proposed setup with a CVS password mechanism for authentication on a new port has not been implemented or discussed further.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Challenges in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;2. Previous difficulties with channel effects and additive noise: Similar challenges have been experienced when examining the interaction between channel effects (e.g., distortions caused by transmission channels) and additive noise (unwanted signals added to the desired signal). Addressing these interactions is difficult, as convolutional effects (related to channel characteristics) and additive noise need to be considered concurrently. This complexity has been a challenge in past research efforts, such as the Aurora task, which aims to develop solutions for handling these issues.&#10;3. Impact on performance: When reverberation is not taken into account in conjunction with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. As a result, this could lead to suboptimal overall performance.&#10;4. Proposed solution: In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in real-world situations where both noise and reverberation are present." target="1. Difficulties in considering reverberation in isolation: Reverberation is the persistence of sound after a sound is produced, resulting in multiple echoes in an enclosed space. When considered independently, techniques to address reverberation may work well for artificial or controlled environments. However, when reverberation occurs together with background noise in real-world situations, these same techniques might not perform as expected. This is because noise makes it more difficult to separate the desired signal from the reverberation, making both factors essential to consider simultaneously.&#10;   &#10;2. Importance of taking into account the interaction between reverberation and noise: If reverberation is not taken into account along with noise, techniques designed to address reverberation may fail to function effectively in real-world situations due to the degrading impact of background noise on reverberation reduction methods. By including both reverberation and noise in the model, researchers can develop more effective solutions for real-world situations. In this meeting, one suggestion was to apply a noise removal process before addressing reverberation. This way, the negative impact of background noise on reverberation reduction techniques would be minimized, leading to better overall performance in situations where both noise and reverberation are present.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the conversation provided, it appears that PhD A has successfully set up a CVS server using the existing SSH mechanism for authentication. There is no clear mention of setting up a CVS server on a new port in addition to the current SSH mechanism. Therefore, as of this conversation, the proposed setup with a CVS password mechanism for authentication on a new port has not been implemented or discussed further." target=" PhD A&#10;Content: But . Yeah , right .&#10;Speaker: PhD E&#10;Content: right ?&#10;Speaker: PhD A&#10;Content: But I didn't do that because I was not sure about {pause} security problems . I {disfmarker} I would have to {disfmarker}&#10;Speaker: PhD E&#10;Content: So w when you came in from Belgian {disfmarker} {comment} Belgium , using SSH , uh , was it asking you for your own {pause} password into ICSI ? So if yo you can only do that if you have an account at ICSI ?&#10;Speaker: PhD A&#10;Content: Right . Yeah .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Cuz there is an {disfmarker} a way to set up anonymous CVS right ?&#10;Speaker: PhD A&#10;Content: Yeah , you ha in this way you ca you have to set up a CVS server but then , yeah , you can access it .&#10;Speaker: PhD E&#10;Content: So that {disfmarker} Oh , OK .">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the conversation provided, it appears that PhD A has successfully set up a CVS server using the existing SSH mechanism for authentication. There is no clear mention of setting up a CVS server on a new port in addition to the current SSH mechanism. Therefore, as of this conversation, the proposed setup with a CVS password mechanism for authentication on a new port has not been implemented or discussed further." target=" E&#10;Content: So has {disfmarker} Has anybody tried remotely accessing the CVS using , uh , uh , SSH ?&#10;Speaker: Professor B&#10;Content: Yeah . &#10;Speaker: PhD A&#10;Content: Um , I don't know if Hari did that or {disfmarker} You d&#10;Speaker: PhD D&#10;Content: I {comment} can actually do it today . I mean , I can just log into {disfmarker}&#10;Speaker: PhD E&#10;Content: Have you tried it yet ?&#10;Speaker: PhD D&#10;Content: No , I didn't . So I I 'll try it today .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Good idea .&#10;Speaker: PhD A&#10;Content: Actually I {disfmarker} I tried wh while {disfmarker} when I installed the {pause} repository , I tried from Belgium .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: I logged in there and I tried {pause} to import {disfmarker}&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the conversation provided, it appears that PhD A has successfully set up a CVS server using the existing SSH mechanism for authentication. There is no clear mention of setting up a CVS server on a new port in addition to the current SSH mechanism. Therefore, as of this conversation, the proposed setup with a CVS password mechanism for authentication on a new port has not been implemented or discussed further." target="1. The meeting with Hynek was attended by PhD E, Professor B, Sunil, and Stephane. It appears that they discussed the current progress and future plans regarding their work.&#10;2. No specific details about what was discussed in the meeting with Hynek are provided in the transcript. However, it is mentioned that Sunil and Stephane summarized where they were and talked about where they were going.&#10;3. There is no direct mention of the status of the code update in the meeting with Hynek. The conversation focus is on other ongoing tasks, such as training new VAD (Voice Activity Detection) and feature net by PhD A since Tuesday.&#10;4. Before the meeting with Hynek, it seems that PhD D had updated their code the previous day. PhD E acknowledged this update, but it's not explicitly linked to the meeting with Hynek.&#10;5. It can be inferred that no major decisions or actions related to the Aurora code were made during the meeting with Hynek, as there is no further discussion about it among the participants after the meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the conversation provided, it appears that PhD A has successfully set up a CVS server using the existing SSH mechanism for authentication. There is no clear mention of setting up a CVS server on a new port in addition to the current SSH mechanism. Therefore, as of this conversation, the proposed setup with a CVS password mechanism for authentication on a new port has not been implemented or discussed further." target="The brief conversation between PhD E and Professor B seems to be related to a presentation or discussion, where Professor B is asking for permission to go ahead with his point. They also discuss a modification idea brought up by PhD E, which may involve adjusting certain features for better compatibility with a language model. However, the specific topic or context of their conversation is not provided in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns." target="1. The significance of November fifteenth is that it is the date when the evaluation is scheduled to take place. There is a possibility that the evaluation date may be pushed off, as Professor B mentions that some people are going away and it would be awkward if the evaluation was pushed off for a long while. However, they are hoping it won't be delayed for too long.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns." target="1. Based on the meeting transcript, it appears that no one is currently working on the Aurora code at OGI. The consensus among the participants is that someone, possibly Sunil, will &quot;grab&quot; or work on the Aurora code when they are at OGI for a different task. This suggests that while there isn't active development on Aurora at OGI right now, it is expected to be addressed as part of other tasks in the future.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns." target="1. Phone-based speech recognition systems struggle with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined.&#10;2. Humans have a mechanism that allows them to understand munged word models in conversional speech because they possess some kind of cognitive ability that enables them to deal with missing or distorted phonemes within words. This mechanism doesn't seem to affect human comprehension as much, indicating an inherent flexibility in the way humans process and interpret spoken language compared to rigid phone-based systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns." target="1. The meeting with Hynek was attended by PhD E, Professor B, Sunil, and Stephane. It appears that they discussed the current progress and future plans regarding their work.&#10;2. No specific details about what was discussed in the meeting with Hynek are provided in the transcript. However, it is mentioned that Sunil and Stephane summarized where they were and talked about where they were going.&#10;3. There is no direct mention of the status of the code update in the meeting with Hynek. The conversation focus is on other ongoing tasks, such as training new VAD (Voice Activity Detection) and feature net by PhD A since Tuesday.&#10;4. Before the meeting with Hynek, it seems that PhD D had updated their code the previous day. PhD E acknowledged this update, but it's not explicitly linked to the meeting with Hynek.&#10;5. It can be inferred that no major decisions or actions related to the Aurora code were made during the meeting with Hynek, as there is no further discussion about it among the participants after the meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns." target="1. The proposed plan for handling the large vocabulary part of the discussion between Professor B, PhD E, Hari, and Joe involves PhD E taking charge of this aspect. According to the conversation, PhD E will be asked to handle the large vocabulary part and should communicate this responsibility to Joe and CC Hari in a note. The plan is to have Joe involved in the discussion while keeping Hari informed, so that if Joe requires confirmation on any decisions, Hari can provide it. This approach aims to ensure both Joe's involvement and Hari's approval for the large vocabulary part of the project.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns." target="The speech recognizer being discussed in the transcripts is a phone-based speech recognition system, which struggles with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined. The researchers mentioned in the transcripts commonly use phone-based systems, as they build models of words in terms of phonemes. However, these models can be oversimplified, creating a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;The proposed solution to improve the system is to consider using words instead of phones as categories for discriminative learning. This approach might allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns." target="Researchers are considering creating intermediate categories, such as words instead of phones (phonemes), to improve word recognition in a speech system because phone-based models can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech. Human listeners can understand munged word models in conversational speech due to cognitive abilities that enable them to deal with missing or distorted phonemes, indicating an inherent flexibility in human spoken language processing. By using words as categories, researchers aim to create a more robust and accurate speech recognition system by focusing on recognizing and distinguishing whole words rather than individual phonemes.&#10;&#10;The idea of discovering intermediate categories like phonemes comes from Grad C's suggestion to use arbitrary windows in time of varying length and cluster those, potentially revealing transitions or relatively stable points in speech. This method aligns with the concept of unsupervised learning, allowing for pattern discovery without relying on predefined human-significant categories such as phonemes.&#10;&#10;While conversational speech may not clearly exhibit these intermediate categories like phones, researchers are exploring this approach to overcome limitations associated with phone-based models and improve overall system performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns." target="The brief conversation between PhD E and Professor B seems to be related to a presentation or discussion, where Professor B is asking for permission to go ahead with his point. They also discuss a modification idea brought up by PhD E, which may involve adjusting certain features for better compatibility with a language model. However, the specific topic or context of their conversation is not provided in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B's concern is about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. This suggests that there may be issues with how well the current language model performs with different features or settings, leading to potential biases in performance.&#10;2. To address these concerns, PhD E brings up the idea of revisiting the possibility of modifying features to better work with the language model. This implies that some features might not be optimized for use with the current language model and could benefit from adjustments or enhancements. No specific modifications are mentioned in the conversation, but it is clear that there is a desire to explore potential improvements to address the identified concerns." target="Based on the provided transcripts, the system for improving digital processes and handling large vocabulary runs is not explicitly stated as ready. However, PhD E is assigned to take charge of the large vocabulary part and communicate this responsibility to Joe and CC Hari. It would be beneficial to have Professor B use his experience to iron out potential issues, as he has shown concern about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. His expertise could help address these concerns and improve the system's performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significance of November fifteenth is that it is the date when the evaluation is scheduled to take place. There is a possibility that the evaluation date may be pushed off, as Professor B mentions that some people are going away and it would be awkward if the evaluation was pushed off for a long while. However, they are hoping it won't be delayed for too long." target="er} and so forth . It {disfmarker} it {disfmarker} it seems pretty tight to me .&#10;Speaker: PhD E&#10;Content: So wha what 's the significance of November fifteenth ?&#10;Speaker: Professor B&#10;Content: That 's when the evaluation is .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah . So , yeah , so after {disfmarker} But , you know , they may even decide in the end to push it off . It wouldn't , you know , entirely surprise me . But , uh , due to other reasons , like some people are going away , I 'm {disfmarker} I 'm hoping it 's not pushed off for {vocalsound} a l a long while . That would be , uh {disfmarker} put us in an awkward position . But {disfmarker} Anyway .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Great . Yeah , I think that 'll be helpful . There 's {disfmarker} there 's not anybody OGI currently who 's {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significance of November fifteenth is that it is the date when the evaluation is scheduled to take place. There is a possibility that the evaluation date may be pushed off, as Professor B mentions that some people are going away and it would be awkward if the evaluation was pushed off for a long while. However, they are hoping it won't be delayed for too long." target=" , somehow modifying our features to work with {disfmarker}&#10;Speaker: Professor B&#10;Content: Yes . In this case , that 's right .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: That 's right . Um , some of that may be , uh , a last minute rush thing because if the {disfmarker} if our features are changing {disfmarker} Uh .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Uh . But , um . Yeah , the other thing is that even though it 's months away , uh , it 's starting to seem to me now like November fifteenth is right around the corner . And , um , if they haven't decided things like this , like what the parameters are gonna be for this , uh , when &quot; deciding &quot; is not just somebody deciding . I mean , in fact there should be some understanding behind the , uh , {vocalsound} deciding , which means some experiments and {disfmarker} and so forth . It {disfmarker} it {disfmarker} it seems pretty tight to me .&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significance of November fifteenth is that it is the date when the evaluation is scheduled to take place. There is a possibility that the evaluation date may be pushed off, as Professor B mentions that some people are going away and it would be awkward if the evaluation was pushed off for a long while. However, they are hoping it won't be delayed for too long." target="1. Based on the meeting transcript, it appears that no one is currently working on the Aurora code at OGI. The consensus among the participants is that someone, possibly Sunil, will &quot;grab&quot; or work on the Aurora code when they are at OGI for a different task. This suggests that while there isn't active development on Aurora at OGI right now, it is expected to be addressed as part of other tasks in the future.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significance of November fifteenth is that it is the date when the evaluation is scheduled to take place. There is a possibility that the evaluation date may be pushed off, as Professor B mentions that some people are going away and it would be awkward if the evaluation was pushed off for a long while. However, they are hoping it won't be delayed for too long." target="The current classification system puts some emphasis on well-matched categories, but it's only marginal with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively. Being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome because the evaluation criteria give more weight to well-matched categories, so performing poorly in this area could negatively affect the overall assessment.&#10;&#10;The importance of this aspect in relation to the overall outcome is not explicitly stated in the transcript, but it's mentioned that there will be debates about it afterwards, indicating that it may have some significance. Additionally, finding a standard answer to what the detectors should focus on within narrow bands and exploring unsupervised learning approaches like Grad C's suggestion of discovering intermediate categories can help improve performance in matching various inputs accurately.&#10;&#10;The use of more suitable categories, such as words instead of phones, could also benefit discriminative learning for speech recognition tasks, as suggested by PhD E. This approach focuses on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The significance of November fifteenth is that it is the date when the evaluation is scheduled to take place. There is a possibility that the evaluation date may be pushed off, as Professor B mentions that some people are going away and it would be awkward if the evaluation was pushed off for a long while. However, they are hoping it won't be delayed for too long." target="Based on the provided transcripts, the system for improving digital processes and handling large vocabulary runs is not explicitly stated as ready. However, PhD E is assigned to take charge of the large vocabulary part and communicate this responsibility to Joe and CC Hari. It would be beneficial to have Professor B use his experience to iron out potential issues, as he has shown concern about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. His expertise could help address these concerns and improve the system's performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on the meeting transcript, it appears that no one is currently working on the Aurora code at OGI. The consensus among the participants is that someone, possibly Sunil, will &quot;grab&quot; or work on the Aurora code when they are at OGI for a different task. This suggests that while there isn't active development on Aurora at OGI right now, it is expected to be addressed as part of other tasks in the future." target="&#10;Content: Are {disfmarker} are people , uh , up at OGI grabbing code uh , via that ?&#10;Speaker: PhD D&#10;Content: Uh , I don't think {disfmarker} I don't think {disfmarker}&#10;Speaker: PhD E&#10;Content: Or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: I don't know if they use it , but .&#10;Speaker: PhD D&#10;Content: Yeah , I I don't think anybody up there is like {pause} working on it right now .&#10;Speaker: PhD E&#10;Content: Uh - huh . Mmm .&#10;Speaker: Professor B&#10;Content: I think it more likely that what it means is that when Sunil is up there {vocalsound} he will grab it .&#10;Speaker: PhD D&#10;Content: Yeah . Yeah . So right now nobody 's working on Aurora there .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: They 're {disfmarker} Yeah . They 're working on a different task .&#10;Speaker: PhD E&#10;Content: I see . I see .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on the meeting transcript, it appears that no one is currently working on the Aurora code at OGI. The consensus among the participants is that someone, possibly Sunil, will &quot;grab&quot; or work on the Aurora code when they are at OGI for a different task. This suggests that while there isn't active development on Aurora at OGI right now, it is expected to be addressed as part of other tasks in the future." target=" PhD D&#10;Content: So , like {disfmarker} like , it 's , like {disfmarker}&#10;Speaker: Professor B&#10;Content: Have you thought about {pause} how long {pause} would be uh , most useful for you to go up to OGI ?&#10;Speaker: PhD A&#10;Content: I don't know , uh . We can {disfmarker} {vocalsound} For September , we can set up a work schedule and we can maybe work independently . And then at some point it maybe be better to work together again .&#10;Speaker: Professor B&#10;Content: Oh , so you 're {disfmarker} you 're imagining more that you would come back here first for a while and then {disfmarker} and then go up there ?&#10;Speaker: PhD A&#10;Content: I {disfmarker}&#10;Speaker: Professor B&#10;Content: I mean , it 's to you .&#10;Speaker: PhD A&#10;Content: Maybe , yeah .&#10;Speaker: Professor B&#10;Content: I ju you guys are Well , y anyway , you don't have to decide this second but thi think about it {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on the meeting transcript, it appears that no one is currently working on the Aurora code at OGI. The consensus among the participants is that someone, possibly Sunil, will &quot;grab&quot; or work on the Aurora code when they are at OGI for a different task. This suggests that while there isn't active development on Aurora at OGI right now, it is expected to be addressed as part of other tasks in the future." target="1. The meeting with Hynek was attended by PhD E, Professor B, Sunil, and Stephane. It appears that they discussed the current progress and future plans regarding their work.&#10;2. No specific details about what was discussed in the meeting with Hynek are provided in the transcript. However, it is mentioned that Sunil and Stephane summarized where they were and talked about where they were going.&#10;3. There is no direct mention of the status of the code update in the meeting with Hynek. The conversation focus is on other ongoing tasks, such as training new VAD (Voice Activity Detection) and feature net by PhD A since Tuesday.&#10;4. Before the meeting with Hynek, it seems that PhD D had updated their code the previous day. PhD E acknowledged this update, but it's not explicitly linked to the meeting with Hynek.&#10;5. It can be inferred that no major decisions or actions related to the Aurora code were made during the meeting with Hynek, as there is no further discussion about it among the participants after the meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on the meeting transcript, it appears that no one is currently working on the Aurora code at OGI. The consensus among the participants is that someone, possibly Sunil, will &quot;grab&quot; or work on the Aurora code when they are at OGI for a different task. This suggests that while there isn't active development on Aurora at OGI right now, it is expected to be addressed as part of other tasks in the future." target="Based on the transcript, Professors B and E are discussing the current machine learning projects they are working on and the challenges of coordinating their efforts given their upcoming travel schedules. They mention several models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), and discuss the need to normalize data and create new streams for the models.&#10;&#10;Regarding the specific focus of their attention, Professors B and E mention a software created by &quot;these guys&quot; that is a key part of their work, and they plan to prioritize this software by keeping certain options constant and focusing on experimenting with specific variables. PhD A mentions that they have been working on noise compensation for six weeks and have made progress.&#10;&#10;Therefore, the specific focus of their attention will likely be to keep certain options constant in the machine learning models and prioritize the use and further development of the software created by &quot;these guys,&quot; possibly involving noise compensation work done by PhD A.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Phone-based speech recognition systems struggle with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined.&#10;2. Humans have a mechanism that allows them to understand munged word models in conversional speech because they possess some kind of cognitive ability that enables them to deal with missing or distorted phonemes within words. This mechanism doesn't seem to affect human comprehension as much, indicating an inherent flexibility in the way humans process and interpret spoken language compared to rigid phone-based systems." target="marker} it 's trickier and {disfmarker} and {disfmarker} and , uh , the phones are {disfmarker} uh , you know , it 's gonna be based on bad pronunciation models that you have of {disfmarker}&#10;Speaker: PhD E&#10;Content: &#10;Speaker: Professor B&#10;Content: and , um {disfmarker} And it won't allow for the overlapping phenomenon&#10;Speaker: PhD E&#10;Content: Mmm . So it 's almost like there 's this mechanism that we have that , you know , when {disfmarker} when we 're hearing read speech and all the phonemes are there you know , we {disfmarker} we deal with that , but {disfmarker} but when we go to conversational , and then all of a sudden not all the phonemes are there , it doesn't really matter that much to us as humans because we have some kind of mechanism that allows for these word models , whatever those models are , to be {pause} munged , you know , and {disfmarker} and it doesn't really hurt , and I 'm not sure">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Phone-based speech recognition systems struggle with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined.&#10;2. Humans have a mechanism that allows them to understand munged word models in conversional speech because they possess some kind of cognitive ability that enables them to deal with missing or distorted phonemes within words. This mechanism doesn't seem to affect human comprehension as much, indicating an inherent flexibility in the way humans process and interpret spoken language compared to rigid phone-based systems." target="The speech recognizer being discussed in the transcripts is a phone-based speech recognition system, which struggles with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined. The researchers mentioned in the transcripts commonly use phone-based systems, as they build models of words in terms of phonemes. However, these models can be oversimplified, creating a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;The proposed solution to improve the system is to consider using words instead of phones as categories for discriminative learning. This approach might allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Phone-based speech recognition systems struggle with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined.&#10;2. Humans have a mechanism that allows them to understand munged word models in conversional speech because they possess some kind of cognitive ability that enables them to deal with missing or distorted phonemes within words. This mechanism doesn't seem to affect human comprehension as much, indicating an inherent flexibility in the way humans process and interpret spoken language compared to rigid phone-based systems." target="Researchers are considering creating intermediate categories, such as words instead of phones (phonemes), to improve word recognition in a speech system because phone-based models can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech. Human listeners can understand munged word models in conversational speech due to cognitive abilities that enable them to deal with missing or distorted phonemes, indicating an inherent flexibility in human spoken language processing. By using words as categories, researchers aim to create a more robust and accurate speech recognition system by focusing on recognizing and distinguishing whole words rather than individual phonemes.&#10;&#10;The idea of discovering intermediate categories like phonemes comes from Grad C's suggestion to use arbitrary windows in time of varying length and cluster those, potentially revealing transitions or relatively stable points in speech. This method aligns with the concept of unsupervised learning, allowing for pattern discovery without relying on predefined human-significant categories such as phonemes.&#10;&#10;While conversational speech may not clearly exhibit these intermediate categories like phones, researchers are exploring this approach to overcome limitations associated with phone-based models and improve overall system performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Phone-based speech recognition systems struggle with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined.&#10;2. Humans have a mechanism that allows them to understand munged word models in conversional speech because they possess some kind of cognitive ability that enables them to deal with missing or distorted phonemes within words. This mechanism doesn't seem to affect human comprehension as much, indicating an inherent flexibility in the way humans process and interpret spoken language compared to rigid phone-based systems." target="Yes, there is a possibility to automate the integration of error units like words or syllables into a larger unit for processing and feedback in speech recognition systems. The approach discussed by the researchers involves using words instead of phones as categories for discriminative learning in speech recognition systems. This method focuses on recognizing and distinguishing whole words rather than individual phonemes, which can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;In this context, automating the integration of error units could mean using word-level units directly in the speech recognition system, bypassing phone-based models entirely. By employing words as categories, the system can potentially improve its robustness and accuracy in processing spontaneous speech with variable phoneme appearances.&#10;&#10;Additionally, the researchers mentioned exploring unsupervised learning methods like Grad C's suggestion to use arbitrary windows in time of varying length and cluster those. This approach could help discover intermediate categories that align with transitions or relatively stable points in speech, further aiding in the accurate recognition and processing of larger units such as words or syllables for feedback in the system.">
      <data key="d0">1</data>
    </edge>
    <edge source="The fundamental category to look for with narrow band detectors, as mentioned by Professor B, refers to the specific type of information or signal that the detectors are designed to identify within the narrow frequency bands. However, the transcript does not provide explicit details on what this &quot;category&quot; refers to in the given context.&#10;&#10;Professor B wonders about it when trying to find a standard answer because they are contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands. The professor's tone and use of phrases like &quot;in the wrong way&quot; and &quot;just wondering really&quot; suggest that this question is not meant as criticism, but rather an invitation for Grad C to consider standard practices in their work." target="&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Are you looking at these in narrow bands ?&#10;Speaker: Grad C&#10;Content: Um , right . F um , I 'm {disfmarker}&#10;Speaker: Professor B&#10;Content: Cuz that 's what you 're gonna be using , right ?&#10;Speaker: Grad C&#10;Content: Yeah , yeah . I {disfmarker} I haven't exactly figured out , um , the exact details for that but , uh , the {disfmarker} the representation of the data that I was thinking of , was using , um , critical band , um , energies , {vocalsound} um , over different lengths of time . So {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , I mean , it seems somehow that needs th uh , there 's a couple things that I wonder about with this . I mean , so one is {disfmarker} is , {pause} again , looking at the same representation ,&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: I mean , if you 're going for this">
      <data key="d0">1</data>
    </edge>
    <edge source="The fundamental category to look for with narrow band detectors, as mentioned by Professor B, refers to the specific type of information or signal that the detectors are designed to identify within the narrow frequency bands. However, the transcript does not provide explicit details on what this &quot;category&quot; refers to in the given context.&#10;&#10;Professor B wonders about it when trying to find a standard answer because they are contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands. The professor's tone and use of phrases like &quot;in the wrong way&quot; and &quot;just wondering really&quot; suggest that this question is not meant as criticism, but rather an invitation for Grad C to consider standard practices in their work." target=" see .&#10;Speaker: PhD D&#10;Content: So {disfmarker}&#10;Speaker: PhD E&#10;Content: About other things , but .&#10;Speaker: Professor B&#10;Content: Do you have Hari 's , uh {disfmarker} ?&#10;Speaker: PhD E&#10;Content: I have Hari 's {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , so maybe just CC Hari and say that you 've just been asked to handle the large vocabulary part here , and , uh , you know ,&#10;Speaker: PhD E&#10;Content: OK . Would it be better if I asked Hari to ask Joe ?&#10;Speaker: Professor B&#10;Content: Uh . Why don't you just ask Joe but CC Hari , and then in the note say , &quot; Hari , hopefully this is OK with you &quot; .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: And then if Joe feels like he needs a confirmation , Hari can answer it .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: That way">
      <data key="d0">1</data>
    </edge>
    <edge source="The fundamental category to look for with narrow band detectors, as mentioned by Professor B, refers to the specific type of information or signal that the detectors are designed to identify within the narrow frequency bands. However, the transcript does not provide explicit details on what this &quot;category&quot; refers to in the given context.&#10;&#10;Professor B wonders about it when trying to find a standard answer because they are contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands. The professor's tone and use of phrases like &quot;in the wrong way&quot; and &quot;just wondering really&quot; suggest that this question is not meant as criticism, but rather an invitation for Grad C to consider standard practices in their work." target="1. The proposed plan for handling the large vocabulary part of the discussion between Professor B, PhD E, Hari, and Joe involves PhD E taking charge of this aspect. According to the conversation, PhD E will be asked to handle the large vocabulary part and should communicate this responsibility to Joe and CC Hari in a note. The plan is to have Joe involved in the discussion while keeping Hari informed, so that if Joe requires confirmation on any decisions, Hari can provide it. This approach aims to ensure both Joe's involvement and Hari's approval for the large vocabulary part of the project.">
      <data key="d0">1</data>
    </edge>
    <edge source="The fundamental category to look for with narrow band detectors, as mentioned by Professor B, refers to the specific type of information or signal that the detectors are designed to identify within the narrow frequency bands. However, the transcript does not provide explicit details on what this &quot;category&quot; refers to in the given context.&#10;&#10;Professor B wonders about it when trying to find a standard answer because they are contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands. The professor's tone and use of phrases like &quot;in the wrong way&quot; and &quot;just wondering really&quot; suggest that this question is not meant as criticism, but rather an invitation for Grad C to consider standard practices in their work." target="The speech recognizer being discussed in the transcripts is a phone-based speech recognition system, which struggles with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined. The researchers mentioned in the transcripts commonly use phone-based systems, as they build models of words in terms of phonemes. However, these models can be oversimplified, creating a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;The proposed solution to improve the system is to consider using words instead of phones as categories for discriminative learning. This approach might allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="The fundamental category to look for with narrow band detectors, as mentioned by Professor B, refers to the specific type of information or signal that the detectors are designed to identify within the narrow frequency bands. However, the transcript does not provide explicit details on what this &quot;category&quot; refers to in the given context.&#10;&#10;Professor B wonders about it when trying to find a standard answer because they are contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands. The professor's tone and use of phrases like &quot;in the wrong way&quot; and &quot;just wondering really&quot; suggest that this question is not meant as criticism, but rather an invitation for Grad C to consider standard practices in their work." target="The current classification system puts some emphasis on well-matched categories, but it's only marginal with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively. Being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome because the evaluation criteria give more weight to well-matched categories, so performing poorly in this area could negatively affect the overall assessment.&#10;&#10;The importance of this aspect in relation to the overall outcome is not explicitly stated in the transcript, but it's mentioned that there will be debates about it afterwards, indicating that it may have some significance. Additionally, finding a standard answer to what the detectors should focus on within narrow bands and exploring unsupervised learning approaches like Grad C's suggestion of discovering intermediate categories can help improve performance in matching various inputs accurately.&#10;&#10;The use of more suitable categories, such as words instead of phones, could also benefit discriminative learning for speech recognition tasks, as suggested by PhD E. This approach focuses on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="The fundamental category to look for with narrow band detectors, as mentioned by Professor B, refers to the specific type of information or signal that the detectors are designed to identify within the narrow frequency bands. However, the transcript does not provide explicit details on what this &quot;category&quot; refers to in the given context.&#10;&#10;Professor B wonders about it when trying to find a standard answer because they are contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands. The professor's tone and use of phrases like &quot;in the wrong way&quot; and &quot;just wondering really&quot; suggest that this question is not meant as criticism, but rather an invitation for Grad C to consider standard practices in their work." target="The fundamental category that narrow band detectors should look for refers to the specific type of information or signal these detectors are designed to identify within the narrow frequency bands. However, the text does not provide explicit details on what this &quot;category&quot; refers to in the given context. The speakers seem to be contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands.&#10;&#10;The concerns regarding the standard approach to finding this category include the current classification system's marginal emphasis on well-matched categories (with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively). The speakers mention that being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome. They also discuss debates about the importance of this aspect in relation to the overall outcome and suggest that finding a standard answer to what detectors should focus on within narrow bands could help improve performance in matching various inputs accurately.&#10;&#10;The speakers are discussing the use of machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), for tasks such as speech recognition. They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, and they mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models.">
      <data key="d0">1</data>
    </edge>
    <edge source="The fundamental category to look for with narrow band detectors, as mentioned by Professor B, refers to the specific type of information or signal that the detectors are designed to identify within the narrow frequency bands. However, the transcript does not provide explicit details on what this &quot;category&quot; refers to in the given context.&#10;&#10;Professor B wonders about it when trying to find a standard answer because they are contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands. The professor's tone and use of phrases like &quot;in the wrong way&quot; and &quot;just wondering really&quot; suggest that this question is not meant as criticism, but rather an invitation for Grad C to consider standard practices in their work." target="The brief conversation between PhD E and Professor B seems to be related to a presentation or discussion, where Professor B is asking for permission to go ahead with his point. They also discuss a modification idea brought up by PhD E, which may involve adjusting certain features for better compatibility with a language model. However, the specific topic or context of their conversation is not provided in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Are you looking at these in narrow bands ?&#10;Speaker: Grad C&#10;Content: Um , right . F um , I 'm {disfmarker}&#10;Speaker: Professor B&#10;Content: Cuz that 's what you 're gonna be using , right ?&#10;Speaker: Grad C&#10;Content: Yeah , yeah . I {disfmarker} I haven't exactly figured out , um , the exact details for that but , uh , the {disfmarker} the representation of the data that I was thinking of , was using , um , critical band , um , energies , {vocalsound} um , over different lengths of time . So {disfmarker} Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , I mean , it seems somehow that needs th uh , there 's a couple things that I wonder about with this . I mean , so one is {disfmarker} is , {pause} again , looking at the same representation ,&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: I mean , if you 're going for this" target="1. The configuration applied for spectral subtraction in the Wiener filter that works well is using two steps of smoothing: recursion along time and a sliding window of twenty FFT bins along frequency, before any mel scaling has been done.&#10;2. The aspects of the system they plan to work on next are improving the noise estimator, adding a neural net at the end of the process, and opening up a second stream or &quot;front&quot; for processing.">
      <data key="d0">1</data>
    </edge>
    <edge source=" see .&#10;Speaker: PhD D&#10;Content: So {disfmarker}&#10;Speaker: PhD E&#10;Content: About other things , but .&#10;Speaker: Professor B&#10;Content: Do you have Hari 's , uh {disfmarker} ?&#10;Speaker: PhD E&#10;Content: I have Hari 's {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , so maybe just CC Hari and say that you 've just been asked to handle the large vocabulary part here , and , uh , you know ,&#10;Speaker: PhD E&#10;Content: OK . Would it be better if I asked Hari to ask Joe ?&#10;Speaker: Professor B&#10;Content: Uh . Why don't you just ask Joe but CC Hari , and then in the note say , &quot; Hari , hopefully this is OK with you &quot; .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: And then if Joe feels like he needs a confirmation , Hari can answer it .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: That way" target="1. The proposed plan for handling the large vocabulary part of the discussion between Professor B, PhD E, Hari, and Joe involves PhD E taking charge of this aspect. According to the conversation, PhD E will be asked to handle the large vocabulary part and should communicate this responsibility to Joe and CC Hari in a note. The plan is to have Joe involved in the discussion while keeping Hari informed, so that if Joe requires confirmation on any decisions, Hari can provide it. This approach aims to ensure both Joe's involvement and Hari's approval for the large vocabulary part of the project.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The meeting with Hynek was attended by PhD E, Professor B, Sunil, and Stephane. It appears that they discussed the current progress and future plans regarding their work.&#10;2. No specific details about what was discussed in the meeting with Hynek are provided in the transcript. However, it is mentioned that Sunil and Stephane summarized where they were and talked about where they were going.&#10;3. There is no direct mention of the status of the code update in the meeting with Hynek. The conversation focus is on other ongoing tasks, such as training new VAD (Voice Activity Detection) and feature net by PhD A since Tuesday.&#10;4. Before the meeting with Hynek, it seems that PhD D had updated their code the previous day. PhD E acknowledged this update, but it's not explicitly linked to the meeting with Hynek.&#10;5. It can be inferred that no major decisions or actions related to the Aurora code were made during the meeting with Hynek, as there is no further discussion about it among the participants after the meeting." target="Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: OK , so {pause} We {disfmarker} we had a meeting with , uh {disfmarker} with Hynek , um , in {disfmarker} in which , uh , uh , Sunil and Stephane , uh {vocalsound} summarized where they were and {disfmarker} and , uh , talked about where we were gonna go . So that {disfmarker} that happened sort of mid - week . Uh .&#10;Speaker: PhD E&#10;Content: D did {disfmarker} did you guys get your code pushed together ?&#10;Speaker: PhD D&#10;Content: Oh , yeah . Yeah . It 's {disfmarker} it 's {disfmarker} it 's {disfmarker} it was updated yesterday ,&#10;Speaker: PhD E&#10;Content: Cool .&#10;Speaker: PhD D&#10;Content: right ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: You probably received the mail .&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The meeting with Hynek was attended by PhD E, Professor B, Sunil, and Stephane. It appears that they discussed the current progress and future plans regarding their work.&#10;2. No specific details about what was discussed in the meeting with Hynek are provided in the transcript. However, it is mentioned that Sunil and Stephane summarized where they were and talked about where they were going.&#10;3. There is no direct mention of the status of the code update in the meeting with Hynek. The conversation focus is on other ongoing tasks, such as training new VAD (Voice Activity Detection) and feature net by PhD A since Tuesday.&#10;4. Before the meeting with Hynek, it seems that PhD D had updated their code the previous day. PhD E acknowledged this update, but it's not explicitly linked to the meeting with Hynek.&#10;5. It can be inferred that no major decisions or actions related to the Aurora code were made during the meeting with Hynek, as there is no further discussion about it among the participants after the meeting." target=": Mmm , since the meeting , well , I {disfmarker} I 've been {disfmarker} I 've been train training a new VAD and a new {pause} feature net .&#10;Speaker: Professor B&#10;Content: That was {disfmarker} that was Tuesday . OK .&#10;Speaker: PhD A&#10;Content: So they should be ready . Um .&#10;Speaker: Professor B&#10;Content: But I guess maybe the thing {disfmarker} since you weren't {disfmarker} yo you guys weren't at that {disfmarker} that meeting , might be just {disfmarker} just to , um , sort of recap , uh , the {disfmarker} the conclusions of the meeting .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Oh , great .&#10;Speaker: Professor B&#10;Content: So .&#10;Speaker: PhD E&#10;Content: You 're talking about the meeting with Hynek ?&#10;Speaker: Professor B&#10;Content: Yeah . Cuz that was sort of , uh {disfmarker} we {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The meeting with Hynek was attended by PhD E, Professor B, Sunil, and Stephane. It appears that they discussed the current progress and future plans regarding their work.&#10;2. No specific details about what was discussed in the meeting with Hynek are provided in the transcript. However, it is mentioned that Sunil and Stephane summarized where they were and talked about where they were going.&#10;3. There is no direct mention of the status of the code update in the meeting with Hynek. The conversation focus is on other ongoing tasks, such as training new VAD (Voice Activity Detection) and feature net by PhD A since Tuesday.&#10;4. Before the meeting with Hynek, it seems that PhD D had updated their code the previous day. PhD E acknowledged this update, but it's not explicitly linked to the meeting with Hynek.&#10;5. It can be inferred that no major decisions or actions related to the Aurora code were made during the meeting with Hynek, as there is no further discussion about it among the participants after the meeting." target="&#10;Content: So , uh , the question is , how complex a function do you need ? Do you need to have an added layer or something ? In which case , uh , potentially , you know , it could be big . So .&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So , uh , uh {disfmarker} So what 's next ? Maybe s s remind us .&#10;Speaker: PhD E&#10;Content: So the meeting with Hynek that you guys just had was to decide exactly what you were gonna freeze in this system ? Is that {disfmarker} ? Or was there {disfmarker} ? Were you talking about what t new stuff , or {disfmarker} ?&#10;Speaker: Professor B&#10;Content: What to freeze and then what to do after we froze .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Yeah . And like I was saying , I think the {disfmarker} you know , the basic directions are , uh , uh {disfmarker} I mean , there 's lots of little things , such as improve the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The configuration applied for spectral subtraction in the Wiener filter that works well is using two steps of smoothing: recursion along time and a sliding window of twenty FFT bins along frequency, before any mel scaling has been done.&#10;2. The aspects of the system they plan to work on next are improving the noise estimator, adding a neural net at the end of the process, and opening up a second stream or &quot;front&quot; for processing." target="aker: Professor B&#10;Content: This {disfmarker} this smoothing is done on the estimate , um , of what you 're going to subtract ? Or on the thing that has already had something subtracted ?&#10;Speaker: PhD A&#10;Content: Yeah . Uh , {vocalsound} it 's on the transfer function . So {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , it 's on the transfer function for the Wiener filter .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , OK .&#10;Speaker: PhD A&#10;Content: Yeah , so basically we tried {vocalsound} different configuration within this idea . We tried u u applying this on mel bands , having spectral subtraction instead of wiener filtering . Um . Well , finally we end up with {pause} this configuration that works , uh , quite well . So we are going to fix this for the moment and work on the other aspects of {vocalsound} the whole system .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So {disfmarker}&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The configuration applied for spectral subtraction in the Wiener filter that works well is using two steps of smoothing: recursion along time and a sliding window of twenty FFT bins along frequency, before any mel scaling has been done.&#10;2. The aspects of the system they plan to work on next are improving the noise estimator, adding a neural net at the end of the process, and opening up a second stream or &quot;front&quot; for processing." target=": PhD A&#10;Content: Yeah , well , {vocalsound} the , um , the other parts of the system are the {disfmarker} the blocks that were already present before and that we did not modify a lot .&#10;Speaker: Professor B&#10;Content: So that 's {disfmarker} again , that {disfmarker} that 's the Wiener filtering , followed by , uh {disfmarker} uh , that 's done at the FFT level . Then {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah , th then the mel filter bank ,&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: then the log operation ,&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: The {disfmarker} the {disfmarker} the filtering is done in the frequency domain ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , OK . And then the mel and then the log , and then the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The configuration applied for spectral subtraction in the Wiener filter that works well is using two steps of smoothing: recursion along time and a sliding window of twenty FFT bins along frequency, before any mel scaling has been done.&#10;2. The aspects of the system they plan to work on next are improving the noise estimator, adding a neural net at the end of the process, and opening up a second stream or &quot;front&quot; for processing." target=" noise compensation and we end up with something that seems reasonable . Um .&#10;Speaker: PhD E&#10;Content: Are you gonna use {disfmarker} which of the two techniques ?&#10;Speaker: PhD A&#10;Content: So finally it 's {disfmarker} it 's , um , Wiener filtering on FFT bins . And it uses , uh , two steps , smoothing of the transfer function , the first step , that 's along time , which use recursion . And {vocalsound} after this step there is a further smoothing along frequency , which use a sliding window of twenty FFT bins . Mmm . And , uh {disfmarker}&#10;Speaker: PhD E&#10;Content: So this is on the {disfmarker} uh , before any mel scaling has been done ?&#10;Speaker: PhD A&#10;Content: Yeah , yeah .&#10;Speaker: PhD E&#10;Content: This is {disfmarker}&#10;Speaker: PhD A&#10;Content: It was {disfmarker}&#10;Speaker: Professor B&#10;Content: This {disfmarker} this smoothing is done on the estimate , um , of what you 're going to">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The configuration applied for spectral subtraction in the Wiener filter that works well is using two steps of smoothing: recursion along time and a sliding window of twenty FFT bins along frequency, before any mel scaling has been done.&#10;2. The aspects of the system they plan to work on next are improving the noise estimator, adding a neural net at the end of the process, and opening up a second stream or &quot;front&quot; for processing." target=" you know , the basic directions are , uh , uh {disfmarker} I mean , there 's lots of little things , such as improve the noise estimator but the bigger things are adding on the neural net and , uh , the second stream . And then , uh , improving the VAD . Uh . So .&#10;Speaker: PhD D&#10;Content: So , I 'll , um {disfmarker} I 'll actually {disfmarker} after the meeting I 'll add the second stream to the VAD and maybe I 'll start with the feature net in that case . It 's like , you 're looking at the VAD , right ?&#10;Speaker: PhD A&#10;Content: Uh , yeah . I I 've a new feature net ready also .&#10;Speaker: PhD D&#10;Content: I 'll {disfmarker} For the VAD ?&#10;Speaker: PhD A&#10;Content: No , uh . Well p two network , one VAD and one {pause} feature net .&#10;Speaker: PhD D&#10;Content: Oh , you already have it ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The configuration applied for spectral subtraction in the Wiener filter that works well is using two steps of smoothing: recursion along time and a sliding window of twenty FFT bins along frequency, before any mel scaling has been done.&#10;2. The aspects of the system they plan to work on next are improving the noise estimator, adding a neural net at the end of the process, and opening up a second stream or &quot;front&quot; for processing." target="1. Parallel processing: The conversation does not explicitly mention the specific techniques or neural networks being used in parallel. However, it is stated that a noise removal process would be applied before addressing reverberation, which could be one of the parallel processes.&#10;&#10;2. Orthogonalization: This term generally refers to transforming data to make its dimensions orthogonal (uncorrelated). In the context of signal processing and neural networks, it might involve decorrelating features or weight vectors to improve model performance. The specific method or model for orthogonalization is not mentioned in the conversation.&#10;&#10;3. Frame dropping for estimated silence probabilities: The neural network model used for frame dropping is a neural network also, with its input being somewhere between log mel bands or one of the earlier stages of processing. This neural network is likely responsible for estimating silence probabilities and deciding which frames to drop based on those estimates.&#10;&#10;While the specific techniques and models are not explicitly mentioned, the conversation touches upon parallel processing, orthogonalization, and frame dropping using a neural network for estimated silence probabilities in an audio processing pipeline that involves log mel bands, LDA filtering, downsampling, DCT, on-line normalization, upsampling, delta computation, and possibly other unspecified techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source=" you know , the basic directions are , uh , uh {disfmarker} I mean , there 's lots of little things , such as improve the noise estimator but the bigger things are adding on the neural net and , uh , the second stream . And then , uh , improving the VAD . Uh . So .&#10;Speaker: PhD D&#10;Content: So , I 'll , um {disfmarker} I 'll actually {disfmarker} after the meeting I 'll add the second stream to the VAD and maybe I 'll start with the feature net in that case . It 's like , you 're looking at the VAD , right ?&#10;Speaker: PhD A&#10;Content: Uh , yeah . I I 've a new feature net ready also .&#10;Speaker: PhD D&#10;Content: I 'll {disfmarker} For the VAD ?&#10;Speaker: PhD A&#10;Content: No , uh . Well p two network , one VAD and one {pause} feature net .&#10;Speaker: PhD D&#10;Content: Oh , you already have it ?&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D" target="The fundamental category that narrow band detectors should look for refers to the specific type of information or signal these detectors are designed to identify within the narrow frequency bands. However, the text does not provide explicit details on what this &quot;category&quot; refers to in the given context. The speakers seem to be contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands.&#10;&#10;The concerns regarding the standard approach to finding this category include the current classification system's marginal emphasis on well-matched categories (with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively). The speakers mention that being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome. They also discuss debates about the importance of this aspect in relation to the overall outcome and suggest that finding a standard answer to what detectors should focus on within narrow bands could help improve performance in matching various inputs accurately.&#10;&#10;The speakers are discussing the use of machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), for tasks such as speech recognition. They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, and they mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, Professor B and PhD E, mention that some kind of normalization is being applied in both cases to deal with convolutional effects. In the speakers' project, they use online normalization and LDA RASTA, which seem to help but do so by throwing away high modulation frequencies, a technique not used in the other case. It is suggested to try normalization, particularly if logs are taken, and then compare the results, as changing multiple factors simultaneously might make it difficult to understand the impact of each change. The specific type of normalization being applied in both cases remains unspecified in the transcript." target=" , in some sense we 're all doing fairly similar things . Uh , I mean , one could argue about the LDA and so forth but I {disfmarker} I think , you know , in a lot of ways we 're doing very similar things . But what {disfmarker} what {disfmarker}&#10;Speaker: PhD E&#10;Content: So how did they fill up this {disfmarker} all these {disfmarker} these bits ? I mean , if we 're u&#10;Speaker: Professor B&#10;Content: Um , why are we using half ? Well , so you could {disfmarker} you c&#10;Speaker: PhD E&#10;Content: Yeah . Or how are they using more than half , I guess maybe is what I {disfmarker}&#10;Speaker: Professor B&#10;Content: Yeah , so I {disfmarker} I think {disfmarker} uh , you guys are closer to it than me , so correct me if I 'm wrong , but I {disfmarker} I think that what 's going on is that in {disfmarker} in both cases , some kind of normalization is done">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, Professor B and PhD E, mention that some kind of normalization is being applied in both cases to deal with convolutional effects. In the speakers' project, they use online normalization and LDA RASTA, which seem to help but do so by throwing away high modulation frequencies, a technique not used in the other case. It is suggested to try normalization, particularly if logs are taken, and then compare the results, as changing multiple factors simultaneously might make it difficult to understand the impact of each change. The specific type of normalization being applied in both cases remains unspecified in the transcript." target="isfmarker} I think that what 's going on is that in {disfmarker} in both cases , some kind of normalization is done to deal with convola convolutional effects . Uh , they have some cepstral {pause} modification ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: right ? In our case we have a couple things . We have the on - line normalization and then we have the LDA RASTA . And {pause} they seem to comple complement each other enough and be different enough that they both seem to help {disfmarker} help us . But in any event , they 're both doing the same sort of thing . But there 's one difference . The LDA RASTA , uh , throws away high modulation frequencies . And they 're not doing that .&#10;Speaker: PhD E&#10;Content: So th So {disfmarker}&#10;Speaker: Professor B&#10;Content: So that if you throw away high modulation frequencies , then you can downsample .&#10;Speaker: Grad C&#10;Content: Get down .&#10;Speaker: PhD E&#10;Content: I see . I see">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, Professor B and PhD E, mention that some kind of normalization is being applied in both cases to deal with convolutional effects. In the speakers' project, they use online normalization and LDA RASTA, which seem to help but do so by throwing away high modulation frequencies, a technique not used in the other case. It is suggested to try normalization, particularly if logs are taken, and then compare the results, as changing multiple factors simultaneously might make it difficult to understand the impact of each change. The specific type of normalization being applied in both cases remains unspecified in the transcript." target=" the noise removal stuff ? or after ?&#10;Speaker: Professor B&#10;Content: Well , that 's a question . I mean , we were talking about that . It looks like it 'd be straightforward to {disfmarker} to , uh , remove the noise , um , and , uh ,&#10;Speaker: PhD E&#10;Content: Cuz that happens before the mel conversion , right ?&#10;Speaker: Professor B&#10;Content: Yeah . So , I mean , to do it after the mel conversion {disfmarker} uh , after the noise removal , after the mel conversion . There 's even a question in my mind anyhow of whether th you should take the log or not . Uh . I sort of think you should , but I don't know .&#10;Speaker: PhD A&#10;Content: What about norm normalizing also ?&#10;Speaker: Professor B&#10;Content: Right . Uh . Well , but normalizing spectra instead of cepstra ?&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Yeah , probably . Some kind would be good . You know ? I would think .&#10;Speaker: PhD D&#10;Content: Well , it {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, Professor B and PhD E, mention that some kind of normalization is being applied in both cases to deal with convolutional effects. In the speakers' project, they use online normalization and LDA RASTA, which seem to help but do so by throwing away high modulation frequencies, a technique not used in the other case. It is suggested to try normalization, particularly if logs are taken, and then compare the results, as changing multiple factors simultaneously might make it difficult to understand the impact of each change. The specific type of normalization being applied in both cases remains unspecified in the transcript." target=" , probably . Some kind would be good . You know ? I would think .&#10;Speaker: PhD D&#10;Content: Well , it {disfmarker} it {disfmarker} it {disfmarker} it {disfmarker} so it actually makes it dependent on the overall energy of the {disfmarker} uh , the frame .&#10;Speaker: Professor B&#10;Content: If you do or don't normalize ?&#10;Speaker: PhD D&#10;Content: If yo if you don't normalize and {disfmarker} if {disfmarker} if you don't normalize .&#10;Speaker: Professor B&#10;Content: Right . Yes , so I mean , one would think that you would want to normalize . But I {disfmarker} I {disfmarker} w w My thought is , uh , particularly if you take the log , try it . And then if {disfmarker} if normalization helps , then y you have something to compare against , and say , &quot; OK , this much effect &quot; {disfmarker} I mean , you don't want to change six things and then see what happens . You want to change them">
      <data key="d0">1</data>
    </edge>
    <edge source="The speakers, Professor B and PhD E, mention that some kind of normalization is being applied in both cases to deal with convolutional effects. In the speakers' project, they use online normalization and LDA RASTA, which seem to help but do so by throwing away high modulation frequencies, a technique not used in the other case. It is suggested to try normalization, particularly if logs are taken, and then compare the results, as changing multiple factors simultaneously might make it difficult to understand the impact of each change. The specific type of normalization being applied in both cases remains unspecified in the transcript." target="The speech recognizer being discussed in the transcripts is a phone-based speech recognition system, which struggles with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined. The researchers mentioned in the transcripts commonly use phone-based systems, as they build models of words in terms of phonemes. However, these models can be oversimplified, creating a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;The proposed solution to improve the system is to consider using words instead of phones as categories for discriminative learning. This approach might allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed plan for handling the large vocabulary part of the discussion between Professor B, PhD E, Hari, and Joe involves PhD E taking charge of this aspect. According to the conversation, PhD E will be asked to handle the large vocabulary part and should communicate this responsibility to Joe and CC Hari in a note. The plan is to have Joe involved in the discussion while keeping Hari informed, so that if Joe requires confirmation on any decisions, Hari can provide it. This approach aims to ensure both Joe's involvement and Hari's approval for the large vocabulary part of the project." target=" this .&#10;Speaker: Professor B&#10;Content: Is that OK ?&#10;Speaker: PhD E&#10;Content: Yeah , that 'd be great .&#10;Speaker: PhD D&#10;Content: Yeah , I guess maybe Hari or Hynek , one of them , has to {pause} send a mail to Joe . Or maybe if you {disfmarker}&#10;Speaker: PhD E&#10;Content: I {disfmarker} I could send him an email .&#10;Speaker: PhD D&#10;Content: Well , yeah , to add or maybe wh&#10;Speaker: PhD E&#10;Content: I {disfmarker} I know him really well .&#10;Speaker: PhD D&#10;Content: Yeah , so that 's just fine .&#10;Speaker: PhD E&#10;Content: I {disfmarker} I was just talking with him on email the other day actually .&#10;Speaker: PhD D&#10;Content: So {disfmarker}&#10;Speaker: Professor B&#10;Content: Uh , yeah , and just , um , se maybe see .&#10;Speaker: PhD D&#10;Content: So {disfmarker}&#10;Speaker: PhD E&#10;Content: About other things , but">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed plan for handling the large vocabulary part of the discussion between Professor B, PhD E, Hari, and Joe involves PhD E taking charge of this aspect. According to the conversation, PhD E will be asked to handle the large vocabulary part and should communicate this responsibility to Joe and CC Hari in a note. The plan is to have Joe involved in the discussion while keeping Hari informed, so that if Joe requires confirmation on any decisions, Hari can provide it. This approach aims to ensure both Joe's involvement and Hari's approval for the large vocabulary part of the project." target="The brief conversation between PhD E and Professor B seems to be related to a presentation or discussion, where Professor B is asking for permission to go ahead with his point. They also discuss a modification idea brought up by PhD E, which may involve adjusting certain features for better compatibility with a language model. However, the specific topic or context of their conversation is not provided in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The proposed plan for handling the large vocabulary part of the discussion between Professor B, PhD E, Hari, and Joe involves PhD E taking charge of this aspect. According to the conversation, PhD E will be asked to handle the large vocabulary part and should communicate this responsibility to Joe and CC Hari in a note. The plan is to have Joe involved in the discussion while keeping Hari informed, so that if Joe requires confirmation on any decisions, Hari can provide it. This approach aims to ensure both Joe's involvement and Hari's approval for the large vocabulary part of the project." target="Based on the provided transcripts, the system for improving digital processes and handling large vocabulary runs is not explicitly stated as ready. However, PhD E is assigned to take charge of the large vocabulary part and communicate this responsibility to Joe and CC Hari. It would be beneficial to have Professor B use his experience to iron out potential issues, as he has shown concern about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. His expertise could help address these concerns and improve the system's performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speech recognizer being discussed in the transcripts is a phone-based speech recognition system, which struggles with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined. The researchers mentioned in the transcripts commonly use phone-based systems, as they build models of words in terms of phonemes. However, these models can be oversimplified, creating a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;The proposed solution to improve the system is to consider using words instead of phones as categories for discriminative learning. This approach might allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="Researchers are considering creating intermediate categories, such as words instead of phones (phonemes), to improve word recognition in a speech system because phone-based models can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech. Human listeners can understand munged word models in conversational speech due to cognitive abilities that enable them to deal with missing or distorted phonemes, indicating an inherent flexibility in human spoken language processing. By using words as categories, researchers aim to create a more robust and accurate speech recognition system by focusing on recognizing and distinguishing whole words rather than individual phonemes.&#10;&#10;The idea of discovering intermediate categories like phonemes comes from Grad C's suggestion to use arbitrary windows in time of varying length and cluster those, potentially revealing transitions or relatively stable points in speech. This method aligns with the concept of unsupervised learning, allowing for pattern discovery without relying on predefined human-significant categories such as phonemes.&#10;&#10;While conversational speech may not clearly exhibit these intermediate categories like phones, researchers are exploring this approach to overcome limitations associated with phone-based models and improve overall system performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speech recognizer being discussed in the transcripts is a phone-based speech recognition system, which struggles with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined. The researchers mentioned in the transcripts commonly use phone-based systems, as they build models of words in terms of phonemes. However, these models can be oversimplified, creating a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;The proposed solution to improve the system is to consider using words instead of phones as categories for discriminative learning. This approach might allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="Yes, there is a possibility to automate the integration of error units like words or syllables into a larger unit for processing and feedback in speech recognition systems. The approach discussed by the researchers involves using words instead of phones as categories for discriminative learning in speech recognition systems. This method focuses on recognizing and distinguishing whole words rather than individual phonemes, which can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;In this context, automating the integration of error units could mean using word-level units directly in the speech recognition system, bypassing phone-based models entirely. By employing words as categories, the system can potentially improve its robustness and accuracy in processing spontaneous speech with variable phoneme appearances.&#10;&#10;Additionally, the researchers mentioned exploring unsupervised learning methods like Grad C's suggestion to use arbitrary windows in time of varying length and cluster those. This approach could help discover intermediate categories that align with transitions or relatively stable points in speech, further aiding in the accurate recognition and processing of larger units such as words or syllables for feedback in the system.">
      <data key="d0">1</data>
    </edge>
    <edge source="The speech recognizer being discussed in the transcripts is a phone-based speech recognition system, which struggles with conversational speech due to the variability in phoneme appearance within different contexts and the lack of clear word boundaries. This is in contrast to read speech, where all phonemes are present and clearly defined. The researchers mentioned in the transcripts commonly use phone-based systems, as they build models of words in terms of phonemes. However, these models can be oversimplified, creating a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;The proposed solution to improve the system is to consider using words instead of phones as categories for discriminative learning. This approach might allow the system to focus on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="To create a discriminative system without using significant human categories and avoiding top-down approaches, one could consider the following strategies:&#10;&#10;1. Explore unsupervised learning methods to discover underlying patterns or structures within the data. For example, Grad C's suggestion of using arbitrary windows in time of varying length and clustering them can help identify intermediate categories that are not predefined by human significance.&#10;2. Utilize a bottom-up approach where the system learns from raw data without relying on handcrafted features or predetermined categories. This allows the model to discover its own relevant features and patterns, potentially leading to novel insights about the structure of the data.&#10;3. Investigate the use of context-dependent models that consider sequential information in the data, rather than focusing on individual segments (e.g., phones or words). These models can capture the variability present in speech by taking into account how different contexts influence the acoustic signal.&#10;4. Consider incorporating additional modalities, such as visual or semantic information, to supplement the audio input and provide alternative cues for distinguishing between categories. This can help reduce reliance on human-significant categories while still maintaining a high level of accuracy in the system.&#10;5. Develop hybrid systems that combine unsupervised learning techniques with weakly supervised methods, where some minimal guidance is provided to the model but without explicitly defining the categories. This approach could strike a balance between data-driven discovery and human expertise.">
      <data key="d0">1</data>
    </edge>
    <edge source="Researchers are considering creating intermediate categories, such as words instead of phones (phonemes), to improve word recognition in a speech system because phone-based models can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech. Human listeners can understand munged word models in conversational speech due to cognitive abilities that enable them to deal with missing or distorted phonemes, indicating an inherent flexibility in human spoken language processing. By using words as categories, researchers aim to create a more robust and accurate speech recognition system by focusing on recognizing and distinguishing whole words rather than individual phonemes.&#10;&#10;The idea of discovering intermediate categories like phonemes comes from Grad C's suggestion to use arbitrary windows in time of varying length and cluster those, potentially revealing transitions or relatively stable points in speech. This method aligns with the concept of unsupervised learning, allowing for pattern discovery without relying on predefined human-significant categories such as phonemes.&#10;&#10;While conversational speech may not clearly exhibit these intermediate categories like phones, researchers are exploring this approach to overcome limitations associated with phone-based models and improve overall system performance." target="Yes, there is a possibility to automate the integration of error units like words or syllables into a larger unit for processing and feedback in speech recognition systems. The approach discussed by the researchers involves using words instead of phones as categories for discriminative learning in speech recognition systems. This method focuses on recognizing and distinguishing whole words rather than individual phonemes, which can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;In this context, automating the integration of error units could mean using word-level units directly in the speech recognition system, bypassing phone-based models entirely. By employing words as categories, the system can potentially improve its robustness and accuracy in processing spontaneous speech with variable phoneme appearances.&#10;&#10;Additionally, the researchers mentioned exploring unsupervised learning methods like Grad C's suggestion to use arbitrary windows in time of varying length and cluster those. This approach could help discover intermediate categories that align with transitions or relatively stable points in speech, further aiding in the accurate recognition and processing of larger units such as words or syllables for feedback in the system.">
      <data key="d0">1</data>
    </edge>
    <edge source="Researchers are considering creating intermediate categories, such as words instead of phones (phonemes), to improve word recognition in a speech system because phone-based models can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech. Human listeners can understand munged word models in conversational speech due to cognitive abilities that enable them to deal with missing or distorted phonemes, indicating an inherent flexibility in human spoken language processing. By using words as categories, researchers aim to create a more robust and accurate speech recognition system by focusing on recognizing and distinguishing whole words rather than individual phonemes.&#10;&#10;The idea of discovering intermediate categories like phonemes comes from Grad C's suggestion to use arbitrary windows in time of varying length and cluster those, potentially revealing transitions or relatively stable points in speech. This method aligns with the concept of unsupervised learning, allowing for pattern discovery without relying on predefined human-significant categories such as phonemes.&#10;&#10;While conversational speech may not clearly exhibit these intermediate categories like phones, researchers are exploring this approach to overcome limitations associated with phone-based models and improve overall system performance." target="To create a discriminative system without using significant human categories and avoiding top-down approaches, one could consider the following strategies:&#10;&#10;1. Explore unsupervised learning methods to discover underlying patterns or structures within the data. For example, Grad C's suggestion of using arbitrary windows in time of varying length and clustering them can help identify intermediate categories that are not predefined by human significance.&#10;2. Utilize a bottom-up approach where the system learns from raw data without relying on handcrafted features or predetermined categories. This allows the model to discover its own relevant features and patterns, potentially leading to novel insights about the structure of the data.&#10;3. Investigate the use of context-dependent models that consider sequential information in the data, rather than focusing on individual segments (e.g., phones or words). These models can capture the variability present in speech by taking into account how different contexts influence the acoustic signal.&#10;4. Consider incorporating additional modalities, such as visual or semantic information, to supplement the audio input and provide alternative cues for distinguishing between categories. This can help reduce reliance on human-significant categories while still maintaining a high level of accuracy in the system.&#10;5. Develop hybrid systems that combine unsupervised learning techniques with weakly supervised methods, where some minimal guidance is provided to the model but without explicitly defining the categories. This approach could strike a balance between data-driven discovery and human expertise.">
      <data key="d0">1</data>
    </edge>
    <edge source="The current classification system puts some emphasis on well-matched categories, but it's only marginal with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively. Being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome because the evaluation criteria give more weight to well-matched categories, so performing poorly in this area could negatively affect the overall assessment.&#10;&#10;The importance of this aspect in relation to the overall outcome is not explicitly stated in the transcript, but it's mentioned that there will be debates about it afterwards, indicating that it may have some significance. Additionally, finding a standard answer to what the detectors should focus on within narrow bands and exploring unsupervised learning approaches like Grad C's suggestion of discovering intermediate categories can help improve performance in matching various inputs accurately.&#10;&#10;The use of more suitable categories, such as words instead of phones, could also benefit discriminative learning for speech recognition tasks, as suggested by PhD E. This approach focuses on recognizing and distinguishing whole words instead of individual phonemes, potentially leading to more accurate results." target="The fundamental category that narrow band detectors should look for refers to the specific type of information or signal these detectors are designed to identify within the narrow frequency bands. However, the text does not provide explicit details on what this &quot;category&quot; refers to in the given context. The speakers seem to be contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands.&#10;&#10;The concerns regarding the standard approach to finding this category include the current classification system's marginal emphasis on well-matched categories (with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively). The speakers mention that being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome. They also discuss debates about the importance of this aspect in relation to the overall outcome and suggest that finding a standard answer to what detectors should focus on within narrow bands could help improve performance in matching various inputs accurately.&#10;&#10;The speakers are discussing the use of machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), for tasks such as speech recognition. They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, and they mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, the speakers are discussing the use of various machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM). They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, as indicated by PhD E's question: &quot;they're providing scripts and everything so that basically, you push a button and it does training, and then it does test, and everything?&quot; They also mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models." target=" . Oh , OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: It 's just a HMM , Gaussian mixture model .&#10;Speaker: Grad C&#10;Content: Gaussian mixture HMM .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: Grad C&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah , the SVM thing was an HMM also . It was just a {disfmarker} it {disfmarker} it {disfmarker} it was like a hybrid , like {disfmarker}&#10;Speaker: Grad C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Yeah , this is a g yeah , this i&#10;Speaker: Professor B&#10;Content: what ?&#10;Speaker: PhD D&#10;Content: yeah .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: So , just so that I understand , they 're providing scripts and everything so that basically , uh , you {disfmarker} you push a button and it does training , and then it does test , and everything ? Is">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, the speakers are discussing the use of various machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM). They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, as indicated by PhD E's question: &quot;they're providing scripts and everything so that basically, you push a button and it does training, and then it does test, and everything?&quot; They also mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models." target=" They probably put training {disfmarker} uh , almost certain they put training data there too . Maybe not . So . That 's that . Anybody have anything else ?&#10;Speaker: PhD E&#10;Content: Uh , one {disfmarker} one last question on that . When did they estimate that they would have that system available for download ?&#10;Speaker: PhD D&#10;Content: Um , I guess {disfmarker} I guess one {disfmarker} some preliminary version is already there .&#10;Speaker: PhD E&#10;Content: Oh , so there 's w something you can download to just learn ?&#10;Speaker: PhD D&#10;Content: Yeah , it 's already there . Yeah .&#10;Speaker: PhD E&#10;Content: OK ,&#10;Speaker: PhD D&#10;Content: But they 're actually parallel - y doing some modifications also , I think .&#10;Speaker: PhD E&#10;Content: good .&#10;Speaker: PhD D&#10;Content: So I guess the f final system will be frozen by middle of , like , one more week maybe .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Oh , well that 's">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, the speakers are discussing the use of various machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM). They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, as indicated by PhD E's question: &quot;they're providing scripts and everything so that basically, you push a button and it does training, and then it does test, and everything?&quot; They also mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models." target=" I think ultimately we 'll wind up doing some normalization . I agree .&#10;Speaker: PhD E&#10;Content: So this second stream , will it add latency to the system&#10;Speaker: Professor B&#10;Content: No , it 's in parallel .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ?&#10;Speaker: Grad C&#10;Content: Para&#10;Speaker: Professor B&#10;Content: We 're not talking about computation time here .&#10;Speaker: PhD E&#10;Content: S&#10;Speaker: Professor B&#10;Content: We 're ta I think we 're pretty far out .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: So it 's just in terms of what data it 's depending on . It 's depending on the same data as the other .&#10;Speaker: PhD E&#10;Content: Same data .&#10;Speaker: Professor B&#10;Content: So it 's in parallel .&#10;Speaker: PhD E&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: Grad C&#10;Content: So with this , uh , new stream would you train up a V">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, the speakers are discussing the use of various machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM). They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, as indicated by PhD E's question: &quot;they're providing scripts and everything so that basically, you push a button and it does training, and then it does test, and everything?&quot; They also mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models." target=" so that basically , uh , you {disfmarker} you push a button and it does training , and then it does test , and everything ? Is that {pause} the idea ?&#10;Speaker: PhD D&#10;Content: I {disfmarker} I {disfmarker} I think {disfmarker} yeah , I {disfmarker} I guess something like that . It 's like {vocalsound} {disfmarker} as painless as possible ,&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: is what {disfmarker} Do they provide all the scripts , everything , and then {disfmarker} Just ,&#10;Speaker: PhD E&#10;Content: I see . Hmm . Somehow yo there 's hooks to put your features in and {disfmarker}&#10;Speaker: PhD D&#10;Content: ju Yeah , I th I think .&#10;Speaker: PhD E&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Hmm . Yeah , um . In fact , I mean , if you look into it a little bit , it might be reasonable {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, the speakers are discussing the use of various machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM). They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, as indicated by PhD E's question: &quot;they're providing scripts and everything so that basically, you push a button and it does training, and then it does test, and everything?&quot; They also mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models." target="Yes, there is a possibility to automate the integration of error units like words or syllables into a larger unit for processing and feedback in speech recognition systems. The approach discussed by the researchers involves using words instead of phones as categories for discriminative learning in speech recognition systems. This method focuses on recognizing and distinguishing whole words rather than individual phonemes, which can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;In this context, automating the integration of error units could mean using word-level units directly in the speech recognition system, bypassing phone-based models entirely. By employing words as categories, the system can potentially improve its robustness and accuracy in processing spontaneous speech with variable phoneme appearances.&#10;&#10;Additionally, the researchers mentioned exploring unsupervised learning methods like Grad C's suggestion to use arbitrary windows in time of varying length and cluster those. This approach could help discover intermediate categories that align with transitions or relatively stable points in speech, further aiding in the accurate recognition and processing of larger units such as words or syllables for feedback in the system.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, the speakers are discussing the use of various machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM). They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, as indicated by PhD E's question: &quot;they're providing scripts and everything so that basically, you push a button and it does training, and then it does test, and everything?&quot; They also mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models." target="The fundamental category that narrow band detectors should look for refers to the specific type of information or signal these detectors are designed to identify within the narrow frequency bands. However, the text does not provide explicit details on what this &quot;category&quot; refers to in the given context. The speakers seem to be contemplating whether there is a conventional or typical piece of information that these detectors usually focus on within such narrow bands.&#10;&#10;The concerns regarding the standard approach to finding this category include the current classification system's marginal emphasis on well-matched categories (with a distribution around 40%, 35%, and 25% for well-matched, moderately-matched, and highly-mismatched categories respectively). The speakers mention that being &quot;way off&quot; on the highly-mismatched category would have a significant impact on the outcome. They also discuss debates about the importance of this aspect in relation to the overall outcome and suggest that finding a standard answer to what detectors should focus on within narrow bands could help improve performance in matching various inputs accurately.&#10;&#10;The speakers are discussing the use of machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), for tasks such as speech recognition. They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, and they mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, the speakers are discussing the use of various machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM). They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, as indicated by PhD E's question: &quot;they're providing scripts and everything so that basically, you push a button and it does training, and then it does test, and everything?&quot; They also mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models." target="1. Parallel processing: The conversation does not explicitly mention the specific techniques or neural networks being used in parallel. However, it is stated that a noise removal process would be applied before addressing reverberation, which could be one of the parallel processes.&#10;&#10;2. Orthogonalization: This term generally refers to transforming data to make its dimensions orthogonal (uncorrelated). In the context of signal processing and neural networks, it might involve decorrelating features or weight vectors to improve model performance. The specific method or model for orthogonalization is not mentioned in the conversation.&#10;&#10;3. Frame dropping for estimated silence probabilities: The neural network model used for frame dropping is a neural network also, with its input being somewhere between log mel bands or one of the earlier stages of processing. This neural network is likely responsible for estimating silence probabilities and deciding which frames to drop based on those estimates.&#10;&#10;While the specific techniques and models are not explicitly mentioned, the conversation touches upon parallel processing, orthogonalization, and frame dropping using a neural network for estimated silence probabilities in an audio processing pipeline that involves log mel bands, LDA filtering, downsampling, DCT, on-line normalization, upsampling, delta computation, and possibly other unspecified techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, the speakers are discussing the use of various machine learning models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM). They refer to these models as &quot;HMM&quot; and &quot;the SVM thing&quot;, which was &quot;just a hybrid&quot;. The scripts they refer to likely automate the process of training and testing these models, as indicated by PhD E's question: &quot;they're providing scripts and everything so that basically, you push a button and it does training, and then it does test, and everything?&quot; They also mention normalization and a new stream, which could be related to additional preprocessing or feature engineering for the models." target="Based on the transcript, Professors B and E are discussing the current machine learning projects they are working on and the challenges of coordinating their efforts given their upcoming travel schedules. They mention several models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), and discuss the need to normalize data and create new streams for the models.&#10;&#10;Regarding the specific focus of their attention, Professors B and E mention a software created by &quot;these guys&quot; that is a key part of their work, and they plan to prioritize this software by keeping certain options constant and focusing on experimenting with specific variables. PhD A mentions that they have been working on noise compensation for six weeks and have made progress.&#10;&#10;Therefore, the specific focus of their attention will likely be to keep certain options constant in the machine learning models and prioritize the use and further development of the software created by &quot;these guys,&quot; possibly involving noise compensation work done by PhD A.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, there is a possibility to automate the integration of error units like words or syllables into a larger unit for processing and feedback in speech recognition systems. The approach discussed by the researchers involves using words instead of phones as categories for discriminative learning in speech recognition systems. This method focuses on recognizing and distinguishing whole words rather than individual phonemes, which can be oversimplified and create a &quot;cartoon-ish&quot; representation that doesn't account for the variability in conversational speech.&#10;&#10;In this context, automating the integration of error units could mean using word-level units directly in the speech recognition system, bypassing phone-based models entirely. By employing words as categories, the system can potentially improve its robustness and accuracy in processing spontaneous speech with variable phoneme appearances.&#10;&#10;Additionally, the researchers mentioned exploring unsupervised learning methods like Grad C's suggestion to use arbitrary windows in time of varying length and cluster those. This approach could help discover intermediate categories that align with transitions or relatively stable points in speech, further aiding in the accurate recognition and processing of larger units such as words or syllables for feedback in the system." target="To create a discriminative system without using significant human categories and avoiding top-down approaches, one could consider the following strategies:&#10;&#10;1. Explore unsupervised learning methods to discover underlying patterns or structures within the data. For example, Grad C's suggestion of using arbitrary windows in time of varying length and clustering them can help identify intermediate categories that are not predefined by human significance.&#10;2. Utilize a bottom-up approach where the system learns from raw data without relying on handcrafted features or predetermined categories. This allows the model to discover its own relevant features and patterns, potentially leading to novel insights about the structure of the data.&#10;3. Investigate the use of context-dependent models that consider sequential information in the data, rather than focusing on individual segments (e.g., phones or words). These models can capture the variability present in speech by taking into account how different contexts influence the acoustic signal.&#10;4. Consider incorporating additional modalities, such as visual or semantic information, to supplement the audio input and provide alternative cues for distinguishing between categories. This can help reduce reliance on human-significant categories while still maintaining a high level of accuracy in the system.&#10;5. Develop hybrid systems that combine unsupervised learning techniques with weakly supervised methods, where some minimal guidance is provided to the model but without explicitly defining the categories. This approach could strike a balance between data-driven discovery and human expertise.">
      <data key="d0">1</data>
    </edge>
    <edge source="The brief conversation between PhD E and Professor B seems to be related to a presentation or discussion, where Professor B is asking for permission to go ahead with his point. They also discuss a modification idea brought up by PhD E, which may involve adjusting certain features for better compatibility with a language model. However, the specific topic or context of their conversation is not provided in the transcript." target="Based on the provided transcripts, the system for improving digital processes and handling large vocabulary runs is not explicitly stated as ready. However, PhD E is assigned to take charge of the large vocabulary part and communicate this responsibility to Joe and CC Hari. It would be beneficial to have Professor B use his experience to iron out potential issues, as he has shown concern about potential bias or hurt to the insertion penalty and language model scaling factors when using certain features. His expertise could help address these concerns and improve the system's performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Professors B and E are discussing the current machine learning projects they are working on and the challenges of coordinating their efforts given their upcoming travel schedules. They mention several models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), and discuss the need to normalize data and create new streams for the models.&#10;&#10;Regarding the specific focus of their attention, Professors B and E mention a software created by &quot;these guys&quot; that is a key part of their work, and they plan to prioritize this software by keeping certain options constant and focusing on experimenting with specific variables. PhD A mentions that they have been working on noise compensation for six weeks and have made progress.&#10;&#10;Therefore, the specific focus of their attention will likely be to keep certain options constant in the machine learning models and prioritize the use and further development of the software created by &quot;these guys,&quot; possibly involving noise compensation work done by PhD A." target="k ?&#10;Speaker: Professor B&#10;Content: Yeah . Cuz that was sort of , uh {disfmarker} we {disfmarker} we 'd sort of been working up to that , that {disfmarker} that , uh , he would come here this week and {disfmarker} and we would sort of {disfmarker}&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: Since he 's going out of town like now , and I 'm going out town in a couple weeks , uh , and time is marching , sort of , given all the mu many wonderful things we could be working on , what {disfmarker} what will we actually focus on ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh {disfmarker} and what do we freeze ? And , you know , what do we {disfmarker} ? So , um . I mean , this {pause} software that these guys created was certainly a {disfmarker} a key part . So then there 's something central and there aren't">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Professors B and E are discussing the current machine learning projects they are working on and the challenges of coordinating their efforts given their upcoming travel schedules. They mention several models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), and discuss the need to normalize data and create new streams for the models.&#10;&#10;Regarding the specific focus of their attention, Professors B and E mention a software created by &quot;these guys&quot; that is a key part of their work, and they plan to prioritize this software by keeping certain options constant and focusing on experimenting with specific variables. PhD A mentions that they have been working on noise compensation for six weeks and have made progress.&#10;&#10;Therefore, the specific focus of their attention will likely be to keep certain options constant in the machine learning models and prioritize the use and further development of the software created by &quot;these guys,&quot; possibly involving noise compensation work done by PhD A." target=" {pause} software that these guys created was certainly a {disfmarker} a key part . So then there 's something central and there aren't at least a bunch of different versions going off in {disfmarker} in ways that {pause} differ {pause} trivially . Uh , um , and , um ,&#10;Speaker: PhD E&#10;Content: Yeah . That 's {disfmarker} that 's nice .&#10;Speaker: Professor B&#10;Content: and then within that , I guess the idea was to freeze a certain set of options for now , to run it , uh , a particular way , and decide on what things are gonna be experimented with , as opposed to just experimenting with everything . So keep a certain set of things constant . So , um . Uh , maybe describe roughly what {disfmarker} what we are keeping constant for now , or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Yeah . Well . So we 've been working like six weeks on {disfmarker} on the noise compensation and we end up with something that seems reasonable . Um .&#10;Speaker: PhD E&#10;Content: Are you gonna use {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Professors B and E are discussing the current machine learning projects they are working on and the challenges of coordinating their efforts given their upcoming travel schedules. They mention several models, including Hidden Markov Models (HMM) and Support Vector Machines (SVM), and discuss the need to normalize data and create new streams for the models.&#10;&#10;Regarding the specific focus of their attention, Professors B and E mention a software created by &quot;these guys&quot; that is a key part of their work, and they plan to prioritize this software by keeping certain options constant and focusing on experimenting with specific variables. PhD A mentions that they have been working on noise compensation for six weeks and have made progress.&#10;&#10;Therefore, the specific focus of their attention will likely be to keep certain options constant in the machine learning models and prioritize the use and further development of the software created by &quot;these guys,&quot; possibly involving noise compensation work done by PhD A." target=" .&#10;Speaker: Professor B&#10;Content: Oh . Well you {disfmarker} you may be called upon to help , uh , uh , on account of , uh , all the work in this stuff here has been , uh , with small vocabulary .&#10;Speaker: PhD E&#10;Content: OK . Mm - hmm . So what {disfmarker} how is the , uh , interaction supposed to happen ? Uh , I remember the last time we talked about this , it was sort of up in the air whether they were going to be taking , uh , people 's features and then running them or they were gonna give the system out or {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: PhD E&#10;Content: Oh , so they 're gonna just deliver a system basically .&#10;Speaker: PhD D&#10;Content: Yeah , yeah .&#10;Speaker: Professor B&#10;Content: Do we already have it ?&#10;Speaker: PhD D&#10;Content: Yeah , th I {disfmarker} I guess it 's almost ready .&#10;Speaker: PhD E&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: So">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
