<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." />
    <node id="&#10;Speaker: Professor B&#10;Content: OK . So a week ago {disfmarker} maybe you weren't around when {disfmarker} when {disfmarker} when Hynek and Guenther and I {disfmarker} ?&#10;Speaker: PhD C&#10;Content: Hynek was here .&#10;Speaker: PhD A&#10;Content: Yeah . I didn't .&#10;Speaker: Professor B&#10;Content: Oh , OK . So {disfmarker} Yeah , let 's summarize . Um {disfmarker} And then if I summarize somebody can tell me if I 'm wrong , which will also be possibly helpful . What did I just press here ? I hope this is still working .&#10;Speaker: PhD E&#10;Content: p - p - p&#10;Speaker: Professor B&#10;Content: We , uh {disfmarker} we looked at , {nonvocalsound} uh {disfmarker} anyway we {disfmarker} {vocalsound} after coming back from QualComm we had , you know , very strong feedback and , uh , I think it was {vocalsound} Hynek and Guenter 's and" />
    <node id="Comm we had , you know , very strong feedback and , uh , I think it was {vocalsound} Hynek and Guenter 's and my opinion also that , um , you know , we sort of spread out to look at a number of different ways of doing noise suppression . But given the limited time , uh , it was sort of time to {pause} choose one .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mmm .&#10;Speaker: Professor B&#10;Content: Uh , and so , uh , th the vector Taylor series hadn't really worked out that much . Uh , the subspace stuff , uh , had not been worked with so much . Um , so it sort of came down to spectral subtraction versus Wiener filtering .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Uh , we had a long discussion about how they were the same and how they were d uh , completely different .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh , I mean , fundamentally they 're the same sort of thing but the math is a little different so that there 's" />
    <node id=" maybe testing out the noise {pause} estimation a little bit . I mean , keeping the same method but {disfmarker} but , uh , {vocalsound} seeing if you cou but , um noise estimation could be improved . Those are sort of related issues .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: It probably makes sense to move from there . And then , uh , {vocalsound} later on in the month I think we wanna start including the {pause} neural net at the end . Um . OK . Anything else ?&#10;Speaker: PhD E&#10;Content: The Half Dome was great .&#10;Speaker: Professor B&#10;Content: Good . Yeah . You didn't {disfmarker} didn't fall . That 's good .&#10;Speaker: PhD C&#10;Content: Well , yeah .&#10;Speaker: Professor B&#10;Content: Our e our effort would have been devastated if you guys had {comment} {vocalsound} run into problems .&#10;Speaker: PhD A&#10;Content: So , Hynek is coming back next week , you said ?&#10;Speaker: Professor B&#10;Content: Yeah , that 's" />
    <node id="er} a {disfmarker} a set of small features and continue to iterate and find , uh , a better set .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: OK . Well , short meeting . That 's OK .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: OK . So next week hopefully we 'll {disfmarker} can get Hynek here to {disfmarker} to join us and , uh , uh .&#10;Speaker: PhD A&#10;Content: Should we do digits ?&#10;Speaker: Professor B&#10;Content: Digits , digits . OK , now .&#10;Speaker: PhD A&#10;Content: Go ahead , Morgan . You can start .&#10;Speaker: Professor B&#10;Content: Alright . Let me get my glasses on so I can {pause} see them . OK .&#10;Speaker: PhD A&#10;Content: OK . And we 're off .&#10;Speaker: Professor B&#10;Content: Mm" />
    <node id=" A&#10;Content: So , Hynek is coming back next week , you said ?&#10;Speaker: Professor B&#10;Content: Yeah , that 's the plan .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: I guess the week after he 'll be , uh , going back to Europe , and so we wanna {disfmarker}&#10;Speaker: PhD A&#10;Content: Is he in Europe right now or is he up at {disfmarker} ?&#10;Speaker: Professor B&#10;Content: No , no . He 's {disfmarker} he 's {disfmarker} he 's dropped into the US . Yeah . Yeah .&#10;Speaker: PhD A&#10;Content: Oh . Hmm .&#10;Speaker: Professor B&#10;Content: So . Uh . {vocalsound} So , uh . Uh , the idea was that , uh , we 'd {disfmarker} we 'd sort out where we were going next with this {disfmarker} with this work before he , uh , left on this next trip . Good . {vocalsound} {vocalsound} Uh , Barry , you just got" />
    <node id=" And then we have to be careful with that also {disfmarker} with the neural net&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: because in {comment} the proposal the neural net was also , uh , working on {disfmarker} after frame - dropping .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Oh , that 's a real good point .&#10;Speaker: PhD E&#10;Content: So . Well , we 'll have to be {disfmarker} to do the same kind of correction .&#10;Speaker: Professor B&#10;Content: It might be hard if it 's at the server side . Right ?&#10;Speaker: PhD E&#10;Content: Mmm . Well , we can do the frame - dropping on the server side or we can just be careful at the terminal side to send a couple of more frames before and after , and {disfmarker} So . I think it 's OK .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: You have , um" />
    <node id="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." />
    <node id="Speaker: PhD A&#10;Content: Oh .&#10;Speaker: PhD C&#10;Content: And {disfmarker} &#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: So , should we keep the same {disfmarker} ? I think we might try to keep the same idea of having a neural network , but {vocalsound} training it on more data and adding better features , I think , but {disfmarker} because the current network is just PLP features . Well , it 's trained on noisy {pause} PLP {disfmarker}&#10;Speaker: PhD C&#10;Content: Just the cepstra . Yeah .&#10;Speaker: PhD E&#10;Content: PLP features computed on noisy speech . But {vocalsound} {vocalsound} there is no nothing particularly robust in these features .&#10;Speaker: PhD A&#10;Content: So , I I uh {disfmarker}&#10;Speaker: PhD C&#10;Content: No .&#10;Speaker: PhD E&#10;Content: There 's no RASTA , no {disfmarker}&#10;Speaker: PhD A&#10;Content: So , uh , I" />
    <node id="isfmarker} ?&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Well , depending on its size {disfmarker} Well , one question is , is it on the , um , server side or is it on the terminal side ? Uh , if it 's on the server side , it {disfmarker} you probably don't have to worry too much about size .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So that 's kind of an argument for that . We do still , however , have to consider its latency . So the issue is {disfmarker} is , um , {vocalsound} for instance , could we have a neural net that only looked at the past ?&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: Um , what we 've done in uh {disfmarker} in the past is to use the neural net , uh , to transform , {vocalsound} um , all of the features that we use . So this is done early on . This is essentially , {vocalsound} um ," />
    <node id=" features , {vocalsound} saying it 's currently a {disfmarker} it 's a speech or a nonspeech .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD C&#10;Content: So there is no frame - dropping till the final features , like , including the deltas are computed .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: PhD C&#10;Content: And after the deltas are computed , you just pick up the ones that are marked silence and then drop them .&#10;Speaker: PhD A&#10;Content: Mm - hmm . I see . I see .&#10;Speaker: Professor B&#10;Content: So it would be more or less the same thing with the neural net , I guess , actually .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So . Yeah , that 's what {disfmarker} that 's what {disfmarker} that 's what , uh , this is doing right now .&#10;Speaker: PhD A&#10;Content: I see . OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;" />
    <node id="Speaker: PhD E&#10;Content: It 's just one percent off of the {pause} best proposal .&#10;Speaker: PhD C&#10;Content: Best system .&#10;Speaker: PhD E&#10;Content: It 's between {disfmarker} i we are second actually if we take this system .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Right ?&#10;Speaker: PhD A&#10;Content: Compared to the last evaluation numbers ? Yeah .&#10;Speaker: Professor B&#10;Content: But , uh {disfmarker} w which we sort of were before&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Professor B&#10;Content: but we were considerably far behind . And the thing is , this doesn't have neural net in yet for instance . You know ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: So it {disfmark" />
    <node id="1. The purpose of Stephane's idea was to create one part of the feature vector that is highly discriminative and another part that is not, without processing the latter part through a neural net.&#10;2. This separation was implemented by using PLP (Perceptual Linear Prediction) features computed on noisy speech for input to the neural network, but not utilizing RASTA or similar techniques which could provide more robustness in these features.&#10;3. They discussed different noise suppression methods including vector Taylor series and subspace stuff, however spectral subtraction and Wiener filtering were found to be more viable options. Further testing is planned for these two methods with a focus on improving noise estimation.&#10;4. The group aims to incorporate a neural net into their workflow in the near future, possibly using the latency time available for this purpose." />
    <node id="&#10;Content: Um , after that we still do a mess of other things to {disfmarker} to produce a bunch of features .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: And then those features are not now currently transformed {vocalsound} by the neural net . And then the {disfmarker} the way that we had it in our proposal - two before , we had the neural net transformed features and we had {vocalsound} the untransformed features , which I guess you {disfmarker} you actually did linearly transform with the KLT ,&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . Right .&#10;Speaker: Professor B&#10;Content: but {disfmarker} but {disfmarker} but {disfmarker} uh , to orthogonalize them {disfmarker} but {disfmarker} {vocalsound} but they were not , uh , processed through a neural net . And Stephane 's idea with that , as I recall , was that {vocalsound} you 'd have one part of the feature vector that was very discriminant and another part that wasn" />
    <node id=" Yeah . So that 's the one which Stephane was discussing , like {disfmarker}&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: The smoothing ?&#10;Speaker: PhD C&#10;Content: Yeah . The {disfmarker} you smooth it and then delay the decision by {disfmarker} So .&#10;Speaker: Professor B&#10;Content: Right . OK . So that 's {disfmarker} that 's really not {disfmarker} not bad . So we may in fact {disfmarker} we 'll see what they decide . We may in fact have , {vocalsound} um , the {disfmarker} the , uh , latency time available for {disfmarker} to have a neural net . I mean , sounds like we probably will . So .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: That 'd be good . Cuz I {disfmarker} cuz it certainly always helped us before . So .&#10;Speaker: PhD A&#10;Content: What amount of latency are you thinking about when" />
    <node id="&#10;Content: There 's no RASTA , no {disfmarker}&#10;Speaker: PhD A&#10;Content: So , uh , I {disfmarker} I don't remember what you said {vocalsound} the answer to my , uh , question earlier . Will you {disfmarker} will you train the net on {disfmarker} after you 've done the spectral subtraction or the Wiener filtering ?&#10;Speaker: Professor B&#10;Content: This is a different net .&#10;Speaker: PhD A&#10;Content: Oh .&#10;Speaker: PhD C&#10;Content: So we have a VAD which is like neur that 's a neural net .&#10;Speaker: PhD E&#10;Content: Oh , yeah . Hmm .&#10;Speaker: PhD A&#10;Content: Oh , you 're talking about the VAD net . OK .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: PhD C&#10;Content: So that {disfmarker} that VAD was trained on the noisy features .&#10;Speaker: PhD A" />
    <node id="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." />
    <node id=" far - field microphone . And if we just take only the , um , VAD probabilities computed on the clean signal and apply them on the far - field , uh , test utterances , {vocalsound} then results are much better .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: In some cases it divides the error rate by two .&#10;Speaker: PhD A&#10;Content: Wow .&#10;Speaker: PhD E&#10;Content: So it means that there are stim {comment} still {disfmarker}&#10;Speaker: PhD A&#10;Content: How {disfmarker} how much latency does the , uh {disfmarker} does our VAD add ?&#10;Speaker: PhD E&#10;Content: If {disfmarker} if we can have a good VAD , well , it would be great .&#10;Speaker: PhD A&#10;Content: Is it significant ,&#10;Speaker: PhD E&#10;Content: Uh , right now it 's , um , a neural net with nine frames .&#10;Speaker: PhD A&#10;Content: or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: So it '" />
    <node id=" that with our current VAD we can improve by more than twenty percent .&#10;Speaker: PhD A&#10;Content: On top of the VAD that they provide ?&#10;Speaker: PhD C&#10;Content: No .&#10;Speaker: PhD E&#10;Content: Just using either their VAD or our current VAD .&#10;Speaker: PhD C&#10;Content: Our way .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD E&#10;Content: So , our current VAD is {disfmarker} is more than twenty percent , while their is fourteen .&#10;Speaker: PhD A&#10;Content: Theirs is fourteen ? I see .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Huh .&#10;Speaker: PhD E&#10;Content: So . Yeah . And {pause} another thing that we did also is that we have all this training data for {disfmarker} let 's say , for SpeechDat - Car . We have channel zero which is clean , channel one which is far - field microphone . And if we just take only the , um , VAD probabilities computed on the clean signal and apply them on the far - field ," />
    <node id="Speaker: PhD C&#10;Content: So that {disfmarker} that VAD was trained on the noisy features .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , right now we have , like , uh {disfmarker} we have the cleaned - up features , so we can have a better VAD by training the net on {pause} the cleaned - up speech .&#10;Speaker: PhD A&#10;Content: Mm - hmm . I see . I see .&#10;Speaker: PhD C&#10;Content: Yeah , but we need a VAD for uh noise estimation also . So it 's , like , where do we want to put the VAD ? Uh , it 's like {disfmarker}&#10;Speaker: PhD A&#10;Content: Can you use the same net to do both , or {disfmarker} ?&#10;Speaker: PhD C&#10;Content: For {disfmarker}&#10;Speaker: PhD A&#10;Content: Can you use the same net that you {disfmarker} that I was talking about to do the VAD ?&#10;Speaker: PhD C&#10;Content: M" />
    <node id=" do you have any way of assessing how well or how poorly the noise estimation is currently doing ?&#10;Speaker: PhD E&#10;Content: Mmm . No , we don't .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: We don't have nothing {pause} that {disfmarker}&#10;Speaker: PhD C&#10;Content: Is there {disfmarker} was there any experiment with {disfmarker} ? Well , I {disfmarker} I did {disfmarker} The only experiment where I tried was I used the channel zero VAD for the noise estimation and frame - dropping . So I don't have a {disfmarker} {vocalsound} I don't have a split , like which one helped more .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: So . It {disfmarker} it was the best result I could get .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , that 's the {disfmarker}&#10;Speaker: Professor B&#10;Content: So that" />
    <node id="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" />
    <node id="disfmarker} so if we {disfmarker} if {disfmarker} so which is like if we reduce the delay of VA So , the f the final delay 's now ba is f determined by the delay of the VAD , because the LDA doesn't have any delay . So if we re if we reduce the delay of the VAD , I mean , it 's like effectively reducing the delay .&#10;Speaker: PhD A&#10;Content: How {disfmarker} how much , uh , delay was there on the LDA ?&#10;Speaker: PhD C&#10;Content: So the LDA and the VAD both had a hundred millisecond delay . So and they were in parallel , so which means you pick either one of them {disfmarker}&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: PhD C&#10;Content: the {disfmarker} the biggest , whatever .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , right now the LDA delays are more .&#10;Speaker: Professor B&#10;Content: And" />
    <node id=" makes reference to delay .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So what 's the {disfmarker} ? If you ignore {disfmarker} Um , the VAD is sort of in {disfmarker} in parallel , isn't i isn't it , with {disfmarker} with the {disfmarker} ? I mean , it isn't additive with the {disfmarker} the , uh , LDA and the Wiener filtering , and so forth .&#10;Speaker: PhD C&#10;Content: The LDA ?&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD C&#10;Content: Yeah . So {disfmarker} so what happened right now , we removed the delay of the LDA .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: So we {disfmarker} I mean , if {disfmarker} so if we {disfmarker} if {disfmarker} so which is like if we reduce the delay of VA" />
    <node id=": on the R .&#10;Speaker: PhD E&#10;Content: Yeah . It 's not a median filtering . It 's just {disfmarker} We don't take the median value . We take something {disfmarker} Um , so we have eleven , um , frames .&#10;Speaker: Professor B&#10;Content: Oh , this is for the VAD .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: And {disfmarker} for the VAD , yeah {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , OK .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: and we take th the third .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Grad D&#10;Content: Dar&#10;Speaker: PhD E&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Yeah . Um . So {disfmarker} {comment} Yeah , I was just noticing on this that it makes reference to delay .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So what 's the {" />
    <node id="mm .&#10;Speaker: PhD C&#10;Content: So , right now the LDA delays are more .&#10;Speaker: Professor B&#10;Content: And there {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: Professor B&#10;Content: And there didn't seem to be any , uh , penalty for that ? There didn't seem to be any penalty for making it causal ?&#10;Speaker: PhD C&#10;Content: Pardon ? Oh , no . It actually made it , like , point one percent better or something , actually .&#10;Speaker: Professor B&#10;Content: OK . Well , may as well , then .&#10;Speaker: PhD C&#10;Content: Or something like that&#10;Speaker: Professor B&#10;Content: And he says Wiener filter is {disfmarker} is forty milliseconds delay .&#10;Speaker: PhD C&#10;Content: and {disfmarker}&#10;Speaker: Professor B&#10;Content: So is it {disfmarker} ?&#10;Speaker: PhD C&#10;Content: Yeah . So that 's the one which Stephane was discussing , like {disfmarker}&#10;Speaker: PhD E&#10;Content: Mmm" />
    <node id=" the same net that you {disfmarker} that I was talking about to do the VAD ?&#10;Speaker: PhD C&#10;Content: Mm - hmm . Uh , it actually comes at v at the very end .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So the net {disfmarker} the final net {disfmarker} I mean , which is the feature net {disfmarker} so that actually comes after a chain of , like , LDA plus everything . So it 's , like , it takes a long time to get a decision out of it . And {disfmarker} {vocalsound} and you can actually do it for final frame - dropping , but not for the VA - f noise estimation .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: You see , the idea is that the , um , initial decision to {disfmarker} that {disfmarker} that you 're in silence or speech happens pretty quickly .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD" />
    <node id="1. The disagreement regarding the amount of latency allowed for the neural net concerns the duration that should be used before processing speech data through the network. Some parties are advocating for a shorter length, while others suggest either 130 or 250 milliseconds.&#10;2. The previous standard was 250 milliseconds, but some groups are now lobbying to make it shorter. Meanwhile, another group is arguing for a 130-millisecond latency period.&#10;3. The exact amount of latency that will be allowed has not been determined yet, as the debate between various parties is ongoing. It is crucial to find a balance between the neural network's performance and the constraints on the allowable latency period." />
    <node id=" So there 's the neural net issue . There 's the VAD issue . And , uh , there 's the second stream {pause} thing . And I think those that we {disfmarker} last time we agreed that those are the three things that have to get , uh , focused on .&#10;Speaker: PhD A&#10;Content: What was the issue with the VAD ?&#10;Speaker: Professor B&#10;Content: Well , better {comment} ones are good .&#10;Speaker: PhD A&#10;Content: And so the w the default , uh , boundaries that they provide are {disfmarker} they 're OK , but they 're not all that great ?&#10;Speaker: Professor B&#10;Content: I guess they still allow two hundred milliseconds on either side or some ? Is that what the deal is ?&#10;Speaker: PhD E&#10;Content: Mm - hmm . Uh , so th um , they keep two hundred milliseconds at the beginning and end of speech . And they keep all the {disfmarker}&#10;Speaker: PhD A&#10;Content: Outside the beginnings and end .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh -" />
    <node id="marker} cuz it certainly always helped us before . So .&#10;Speaker: PhD A&#10;Content: What amount of latency are you thinking about when you say that ?&#10;Speaker: Professor B&#10;Content: Uh . Well , they 're {disfmarker} you know , they 're disputing it .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: You know , they 're saying , uh {disfmarker} one group is saying a hundred and thirty milliseconds and another group is saying two hundred and fifty milliseconds . Two hundred and fifty is what it was before actually . So ,&#10;Speaker: PhD A&#10;Content: Oh .&#10;Speaker: Professor B&#10;Content: uh , some people are lobbying {disfmarker} lobbying {comment} to make it shorter .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Um . And , um .&#10;Speaker: PhD A&#10;Content: Were you thinking of the two - fifty or the one - thirty when you said we should {pause} have enough for the neural net ?&#10;Speaker: Professor B&#10;Content: Well , it just {d" />
    <node id=" , as I recall , was that {vocalsound} you 'd have one part of the feature vector that was very discriminant and another part that wasn't ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: uh , which would smooth things a bit for those occasions when , uh , the testing set was quite different than what you 'd trained your discriminant features for . So , um , all of that is {disfmarker} is , uh {disfmarker} still seems like a good idea . The thing is now we know some other constraints . We can't have unlimited amounts of latency . Uh , y you know , that 's still being debated by the {disfmarker} by people in Europe but , {vocalsound} uh , no matter how they end up there , it 's not going to be unlimited amounts ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: so we have to be a little conscious of that . Um . So there 's the neural net issue . There 's the VAD issue . And , uh , there 's the second stream {pause} thing ." />
    <node id="Based on the information provided by PhD E, the default system's performance is reported to be one percent lower than the best proposal. This comparison is made in terms of the system's overall quality or effectiveness; however, a specific metric or context for this assessment has not been detailed in the conversation." />
    <node id="Content: Right .&#10;Speaker: PhD E&#10;Content: Well , if we want to , like , optimize different parameters of {disfmarker}&#10;Speaker: PhD C&#10;Content: Parameters . Yeah .&#10;Speaker: Professor B&#10;Content: Sure .&#10;Speaker: PhD E&#10;Content: Yeah , we can do it later . But , still {disfmarker} so , there will be a piece of software with , {vocalsound} {vocalsound} uh , will give this system , the fifty - three point sixty - six , by default and {disfmarker}&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: How {disfmarker} how is {disfmarker} how good is that ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I {disfmarker} I {disfmarker} I don't have a sense of {disfmarker}&#10;Speaker: PhD E&#10;Content: It 's just one percent off of the {pause} best proposal .&#10;Speaker: PhD C&#10;Content:" />
    <node id="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal." />
    <node id="disfmarker} the , uh , um , {vocalsound} looking at it the other way , you 're gonna be dealing with signals&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: and you 're gonna end up looking at power {disfmarker} uh , noise power that you 're trying to reduce . And so , eh {disfmarker} so there should be a difference {vocalsound} of {disfmarker} you know , conceptually of {disfmarker} of , uh , a factor of two in the exponent .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: But there 're so many different little factors that you adjust in terms of {disfmarker} of , uh , {vocalsound} uh , over - subtraction and {disfmarker} and {disfmarker} and {disfmarker} and {disfmarker} and so forth , um , that {vocalsound} arguably , you 're c and {disfmarker} and {disfmarker} and" />
    <node id="1. Professor B is referring to specific FFT bins when applying a Wiener filter in the context of vocal sound (vocalsound) processing. However, the transcript does not provide exact bin numbers.&#10;2. Adding noise to vocal sound processing can be difficult because it requires finding the right level of noise to add without degrading the quality of the original sound. If the added noise is too loud, it can make the vocals hard to understand; if it's too soft, it might not effectively mask background sounds or hide sensitive information in the audio. Balancing these factors can be challenging and may require careful tuning." />
    <node id=" is {vocalsound} when we apply this procedure on FFT bins , uh , with a Wiener filter .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: And there is no noise addition after {disfmarker} after that .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: So it 's good because {vocalsound} {vocalsound} it 's difficult when we have to add noise to {disfmarker} to {disfmarker} to find the right level .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Are you looking at one in {disfmarker} in particular of these two ?&#10;Speaker: PhD E&#10;Content: Yeah . So the sh it 's the sheet that gives fifty - f three point sixty - six .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Um , {vocalsound} the second sheet is abo uh , about the same . It 's the same , um , idea but it 's working on" />
    <node id=": That 's the best thing .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So , tell me about it .&#10;Speaker: PhD E&#10;Content: So it 's {disfmarker} well , it 's {pause} spectral subtraction or Wiener filtering , um , depending on if we put {disfmarker} if we square the transfer function or not .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: And then with over - estimation of the noise , depending on the , uh {disfmarker} the SNR , with smoothing along time , um , smoothing along frequency .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: It 's very simple , smoothing things .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: And , um , {vocalsound} the best result is {vocalsound} when we apply this procedure on FFT bins , uh , with a Wiener filter .&#10;Speaker: Professor B&#10;Content" />
    <node id=" {vocalsound} the second sheet is abo uh , about the same . It 's the same , um , idea but it 's working on mel bands , {vocalsound} and it 's a spectral subtraction instead of Wiener filter , and there is also a noise addition after , uh , cleaning up the mel bins . Mmm . Well , the results are similar .&#10;Speaker: Professor B&#10;Content: Yeah . I mean , {vocalsound} it 's {disfmarker} {comment} it 's actually , uh , very similar .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: I mean , {vocalsound} if you look at databases , uh , the , uh , one that has the smallest {disfmarker} smaller overall number is actually better on the Finnish and Spanish , uh , but it is , uh , worse on the , uh , Aurora {disfmarker}&#10;Speaker: PhD E&#10;Content: It 's worse on {disfmarker}&#10;Speaker: Professor B&#10;Content: I mean on the , uh , TI - TI - digits ,&#10;Speaker:" />
    <node id=" . But the Guenter 's argument is slightly different . It 's , like , ev even {disfmarker} even if I use a channel zero VAD , I 'm just averaging the {disfmarker} {vocalsound} the s power spectrum . But the Guenter 's argument is , like , if it is a non - stationary {pause} segment , then he doesn't update the noise spectrum . So he 's , like {disfmarker} he tries to capture only the stationary part in it . So the averaging is , like , {vocalsound} different from {pause} updating the noise spectrum only during stationary segments . So , th the Guenter was arguing that , I mean , even if you have a very good VAD , averaging it , like , over the whole thing is not a good idea .&#10;Speaker: Professor B&#10;Content: I see .&#10;Speaker: PhD C&#10;Content: Because you 're averaging the stationary and the non - stationary , and finally you end up getting something which is not really the s because , you {disfmarker} anyway , you can't remove the stationary part fr I mean , non - stationary part from {vocalsound}" />
    <node id=" Mm - hmm .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: So it {disfmarker} so , um , it 's {disfmarker} it it 's not using our full bal bag of tricks , if you will .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh , and it {disfmarker} it is , uh , very close in performance to the best thing that was there before . Uh , but , you know , looking at it another way , maybe more importantly , uh , {vocalsound} we didn't have any explicit noise , uh , handling {disfmarker} stationary {disfmarker} dealing with {disfmarker} e e we didn't explicitly have anything to deal with stationary noise .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And now we do .&#10;Speaker: PhD A&#10;Content: So will the {pause} neural net operate on the output from either the Wiener filtering or the spectral subtraction ? Or will it operate on the" />
    <node id="1. Yes, the idea of having one part of the feature vector that is highly discriminative and another part that is not (without processing the latter part through a neural net) still seems reasonable for vocalsound and VAD in the neural net setting. This is because it can help smooth things out when the testing set is different from what was trained, while also being aware of latency constraints.&#10;2. The exact amount of latency allowed for the neural net is still under debate, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The group aims to find a balance between the neural network's performance and the constraints on the allowable latency period.&#10;3. Using VAD probabilities computed on the clean signal and applying them to far-field test utterances has shown significant improvement, reducing error rates by up to two times in some cases. This suggests that there is still potential for improving the VAD system. The current VAD implementation adds some latency, but further development could potentially reduce this." />
    <node id="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech." />
    <node id="1. The transcript states that using the default boundaries provided by the VAD system results in a fourteen percent improvement. However, by dropping the beginning and end frames of a speech, the group has observed an improvement of more than twenty percent. This is because they can improve the current VAD performance by more than twenty percent, while the other system's VAD only improves by fourteen percent.&#10;2. The method can potentially be further enhanced to exceed a twenty percent improvement. The group has found that using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, in some cases even dividing it by two. This approach takes advantage of the VAD probabilities obtained from the clean signal and applies them to noisy, far-field test utterances, leading to improved performance. The current VAD implementation adds some latency, but further development could potentially reduce this while maintaining or improving the error rate reduction." />
    <node id=" A&#10;Content: Outside the beginnings and end .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: And all the speech pauses , which is {disfmarker} Sometimes on the SpeechDat - Car you have pauses that are more than one or two seconds .&#10;Speaker: PhD A&#10;Content: Wow .&#10;Speaker: PhD E&#10;Content: More than one second for sure . Um .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD E&#10;Content: Yeah . And , yeah , it seems to us that this way of just dropping the beginning and end is not {disfmarker} We cou we can do better , I think ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: because , um , {vocalsound} with this way of dropping the frames they improve {pause} over the baseline by fourteen percent and {vocalsound} Sunil already showed that with our current VAD we can improve by more than twenty percent .&#10;Speaker: PhD A&#10;Content: On top of the VAD that they" />
    <node id="1. When determining whether the size or latency of a neural network is more important, there are a few factors to consider. If the neural network is running on the server side, its size may not be as much of a concern due to the available resources. However, latency remains an issue that needs to be addressed regardless of where the neural network is deployed. The ability to process data using only past information is one potential solution being considered, as it could help reduce latency.&#10;   &#10;2. In the past, the group has used neural networks to transform all features used for processing. This early transformation aims to improve the overall performance of their system. However, they have not yet explored the possibility of using a neural network that only looks at the past, which could further help reduce latency." />
    <node id="1. When choosing an error method for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. On the other hand, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring. This results in a different mathematical treatment and potentially different optimization objectives for the filtering problem.&#10;2. Another key difference is that power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;3. The choice of error method depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand." />
    <node id=" B&#10;Content: And , uh , I mean , fundamentally they 're the same sort of thing but the math is a little different so that there 's a {disfmarker} a {disfmarker} {vocalsound} there 's an exponent difference in the index {disfmarker} you know , what 's the ideal filtering , and depending on how you construct the problem .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: And , uh , I guess it 's sort {disfmarker} you know , after {disfmarker} after that meeting it sort of made more sense to me because {vocalsound} um , if you 're dealing with power spectra then how are you gonna choose your error ? And typically you 'll do {disfmarker} choose something like a variance . And so that means it 'll be something like the square of the power spectra . Whereas when you 're {disfmarker} when you 're doing the {disfmarker} the , uh , um , {vocalsound} looking at it the other way , you 're gonna be dealing with signals&#10;" />
    <node id="The plan for Professor Hynek's visit is to have him join a meeting with the team to discuss and firm up the direction of their project before his subsequent trip to Europe. Professor B wants to finalize the plans before Hynek's departure because they aim to have a clear path for the time he will be gone, specifying what things will be attended to during his absence.&#10;&#10;The group is currently focusing on further testing and improving noise estimation in both spectral subtraction and Wiener filtering methods for noise suppression. They plan to incorporate a neural net into their workflow later in the month." />
    <node id=" that our goal should be by next week , when Hynek comes back , {vocalsound} uh , to {disfmarker} uh , really just to have a firm path , uh , for the {disfmarker} you know , for the time he 's gone , of {disfmarker} of , uh , what things will be attacked . But I would {disfmarker} I would {disfmarker} I would thought think that what we would wanna do is not futz with this stuff for a while because what 'll happen is we 'll change many other things in the system ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: and then we 'll probably wanna come back to this and possibly make some other choices . But , um .&#10;Speaker: PhD A&#10;Content: But just conceptually , where does the neural net go ? Do {disfmarker} do you wanna h run it on the output of the spectrally subtracted {disfmarker} ?&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Well , depending on its size" />
    <node id="1. The process of the VAD (Voice Activity Detection) system involves determining the boundaries between speech and non-speech segments in a signal. In this discussion, they mention using the default boundaries provided by the system but observing that dropping the beginning and end frames of a speech results in an improvement of more than twenty percent, compared to another system's VAD performance improvement of only fourteen percent.&#10;&#10;2. The VAD system has a leeway of two hundred milliseconds at the beginning and end of speech, meaning it considers up to 200ms before and after actual speech as part of the detection. This leeway might contribute to the delay mentioned in the discussion since the VAD system takes additional time into account when detecting speech segments.&#10;&#10;3. The group mentions that reducing the delay of the VAD can effectively reduce the overall delay (transcript statement 3) and, as previously explained, dropping the beginning and end frames results in improved performance. Therefore, optimizing these boundaries is an essential part of managing the VAD's latency or delay.&#10;&#10;4. Additionally, using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, which also contributes to reducing latency or delay while maintaining or improving overall performance." />
    <node id="The use of silencing or stopping speech in the middle of communication is referred to as frame-dropping. Frame-dropping is a technique used in noise reduction systems to improve the quality of noisy speech by discarding certain frames (small segments of audio) deemed to be noise. This technique can be employed at different stages of the noise reduction process, and its use can impact the difference in power between clean and noisy speech.&#10;&#10;In the given transcript, the participants are discussing a specific system that utilizes frame-dropping as part of its noise reduction strategy. They mention that adjusting various factors, such as over-subtraction, Wiener filtering, and spectral subtraction, can help optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;The transcript also indicates a connection between frame-dropping and the use of delta computations in speech processing. Delta computations are used to estimate the first derivative of a signal, which can be helpful for detecting sudden changes or patterns within the signal. By avoiding delta computations early on and implementing frame-dropping as one of the last steps, the system aims to improve noise reduction performance further.&#10;&#10;Additionally, there is a discussion about assessing the effectiveness of noise estimation in the current system. However, it seems that they do not have any established method for evaluating the performance of their noise estimation technique." />
    <node id="er} that you 're in silence or speech happens pretty quickly .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD C&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: Cuz that 's used by some of these other {disfmarker} ?&#10;Speaker: Professor B&#10;Content: And that {disfmarker} Yeah . And that 's sort of fed forward , and {disfmarker} and you say &quot; well , flush everything , it 's not speech anymore &quot; .&#10;Speaker: PhD A&#10;Content: Oh , OK . I see .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: I thought that was only used for doing frame - dropping later on .&#10;Speaker: Professor B&#10;Content: Um , it is used , uh {disfmarker} Yeah , it 's only used f Well , it 's used for frame - dropping . Um , it 's used for end of utterance&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: because , you know , there 's {disfmarker} {voc" />
    <node id=" - dropping .&#10;Speaker: Grad D&#10;Content: S&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: The earlier system was do the frame - dropping and then compute the delta on the {disfmarker}&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: PhD C&#10;Content: So this {disfmarker}&#10;Speaker: PhD A&#10;Content: Which could be a kind of a funny delta . Right ?&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Oh , oh . So that 's fixed in this . Yeah , we talked about that .&#10;Speaker: PhD C&#10;Content: Yeah . So we have no delta . And then {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah . Uh - huh .&#10;Speaker: Professor B&#10;Content: Good .&#10;Speaker: PhD C&#10;Content: So the frame - dropping is the last thing that we do . So , yeah , what we do is we compute the silence probability , convert it to that binary flag ,&#10;Speaker: Professor B&#10;Content: Uh - huh ." />
    <node id=" - dropping problem ? What about this ?&#10;Speaker: PhD C&#10;Content: Uh , y you had something on it . Right ?&#10;Speaker: PhD E&#10;Content: Just the frame - dropping problem . Yeah . But it 's {disfmarker} it 's difficult . Sometime we {disfmarker} we change two {disfmarker} two things together and {disfmarker} But it 's around {pause} maybe {disfmarker} it 's less than one percent .&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: It {disfmarker}&#10;Speaker: Professor B&#10;Content: Well . {vocalsound} But like we 're saying , if there 's four or five things like that then {vocalsound} pretty sho soon you 're talking real improvement .&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . And it {disfmarker} Yeah . And then we have to be careful with that also {disfmarker} with the neural net&#10;Speaker: Professor B&#10;Content: Yeah .&#10;" />
    <node id="Speaker: PhD E&#10;Content: and {disfmarker}&#10;Speaker: Professor B&#10;Content: But , um ,&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: it does seem like , you know , i i i i some compromise between always depending on the first fifteen frames and a a always depending on a {disfmarker} a pause is {disfmarker} is {disfmarker} is a good idea . Uh , maybe you have to weight the estimate from the first - teen {disfmarker} fifteen frames more heavily than {disfmarker} than was done in your first attempt . But {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: but {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah , I guess .&#10;Speaker: Professor B&#10;Content: Yeah . Um . No , I mean {disfmarker} Um , do you have any way of assessing how well or how poorly the noise estimation is currently doing ?&#10;Speaker: PhD E&#10;Content: Mmm . No" />
    <node id="Based on the transcript provided, Professor C did not provide a specific quantification of how much of the improvement in results was due to addressing the frame-dropping issue as compared to other changes made. Professor B asked about this specifically, but the conversation moved on to other topics before any numerical answer was given." />
    <node id=" yeah , what we do is we compute the silence probability , convert it to that binary flag ,&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: PhD C&#10;Content: and then in the end you c up upsample it to {vocalsound} match the final features number of {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Did that help then ?&#10;Speaker: PhD C&#10;Content: It seems to be helping on the well - matched condition . So that 's why this improvement I got from the last result . So . And it actually r reduced a little bit on the high mismatch , so in the final weightage it 's b b better because the well - matched is still weighted more than {disfmarker}&#10;Speaker: Professor B&#10;Content: So , @ @ I mean , you were doing a lot of changes . Did you happen to notice how much , {vocalsound} uh , the change was due to just this frame - dropping problem ? What about this ?&#10;Speaker: PhD C&#10;Content: Uh , y you had something on it . Right ?&#10;Speaker:" />
    <node id=" because , you {disfmarker} anyway , you can't remove the stationary part fr I mean , non - stationary part from {vocalsound} the signal .&#10;Speaker: Professor B&#10;Content: Not using these methods anyway . Yeah .&#10;Speaker: PhD C&#10;Content: So {disfmarker} Yeah . So you just {pause} update only doing {disfmarker} or update only the stationary components . Yeah . So , that 's {disfmarker} so that 's still a slight difference from what Guenter is trying &#10;Speaker: Professor B&#10;Content: Well , yeah . And {disfmarker} and also there 's just the fact that , um , eh , uh , although we 're trying to do very well on this evaluation , um , we actually would like to have something that worked well in general . And , um , relying on having fifteen frames at the front or something is {disfmarker} is pretty {disfmarker}&#10;Speaker: PhD C&#10;Content: Yeah , yeah .&#10;Speaker: Professor B&#10;Content: I mean , you might , you might not .&#10;Speaker: PhD C&#10;Content: Mmm" />
    <node id="1. The transcript does not provide specific information on exactly how much latency the VAD adds to the far-field test utterances when applying VAD probabilities computed on the clean signal. However, it is mentioned that this method significantly decreases the error rate and has potential for further improvement.&#10;   &#10;2. The significance of using a neural net with nine frames for the VAD is that it provides a balance between performance and latency constraints. There seems to be some debate within the group about the ideal length of latency allowed for the neural net, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The current implementation adds some latency, but further development could potentially reduce this while maintaining or improving error rate reduction." />
    <node id="1. The group has observed an improvement of more than twenty percent in speech recognition performance by dropping the beginning and end frames (pauses) of a speech compared to their current VAD system's performance. In contrast, the other system's VAD only showed a fourteen percent improvement with this method.&#10;   &#10;2. A more effective method for improving speech recognition accuracy could be using VAD probabilities computed from clean signals on far-field microphone data. This approach significantly decreases the error rate, in some cases even dividing it by two, and has potential for further enhancement. Additionally, optimizing the neural network used in the VAD system by training it on more data and adding better features could also improve its performance." />
    <node id="1. The amount of noise present in the signal: The more noise there is, relative to the desired speech signal, the greater the difference in power that needs to be addressed. Reducing a large difference in power may require more aggressive noise reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;2. Signal-to-Noise Ratio (SNR): A lower SNR indicates that there is more noise relative to the desired signal, making it harder to separate the two. This can affect the reduction process by requiring more sophisticated noise reduction techniques and a greater balance between noise reduction and signal distortion.&#10;&#10;3. Types of noise: Different types of noise (e.g., white noise, colored noise, non-stationary noise) can have varying effects on the difference in power that needs to be reduced. For example, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;4. Over-subtraction, Wiener filtering, and spectral subtraction: These factors can impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;5. Frame-dropping: This technique discards certain frames (small segments of audio) deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;6. Delta computations: Used for estimating the first derivative of a signal, avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;7. Noise estimation effectiveness: Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;8. Choice of error method: When constructing a filtering problem, the choice between using power spectra or signals directly can significantly affect the reduction process. Power spectra are more sensitive to noise but require additional robustness techniques, while working with signals directly offers more flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem." />
    <node id="1. Mark Frames Before They Reach the Server: Instead of dropping unnecessary frames after they have been transmitted to the server, it is suggested to mark them beforehand. This can be done by analyzing the audio data locally and adding a marker or flag to indicate whether a frame should be dropped or kept.&#10;&#10;2. Use Additional Bits for Differentiation: To better handle speech and non-speech frames on the server side, additional bits can be used in the transmission protocol. These extra bits can serve as flags that differentiate between speech and non-speech frames. This way, the server can process only the necessary frames without having to drop any of them after transmission, thus saving bandwidth and reducing latency.&#10;&#10;3. Improve Neural Network Performance: Training the neural network on more data and incorporating better features and noise reduction techniques during training can help enhance its performance in recognizing speech and differentiating it from non-speech. This would result in fewer unnecessary frames being transmitted, thus reducing the load on the server." />
    <node id=" Professor B&#10;Content: So .&#10;Speaker: PhD A&#10;Content: If the net 's on the server side then it could use all of the {pause} frames .&#10;Speaker: PhD C&#10;Content: Yes , it could be . It 's , like , you mean you just transferred everything and then finally drop the frames after the neural net .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: Right ? Yeah . That 's {disfmarker} that 's one thing which {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: But you could even mark them , before they get to the server .&#10;Speaker: PhD C&#10;Content: Yeah . Right now we are {disfmarker} Uh , ri Right now what {disfmarker} wha what we did is , like , we just mark {disfmarker} we just have this additional bit which goes around the features , {vocalsound} saying it 's currently a {disfmarker} it 's a speech or a nonspeech .&#10;Spe" />
    <node id=" . I think it 's OK .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: You have , um {disfmarker} So when you {disfmarker} Uh , maybe I don't quite understand how this works , but , um , couldn't you just send all of the frames , but mark the ones that are supposed to be dropped ? Cuz you have a bunch more bandwidth . Right ?&#10;Speaker: Professor B&#10;Content: Well , you could . Yeah . I mean , it {disfmarker} it always seemed to us that it would be kind of nice to {disfmarker} in addition to , uh , reducing insertions , actually use up less bandwidth .&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Professor B&#10;Content: But nobody seems to have {vocalsound} cared about that in this {pause} evaluation .&#10;Speaker: PhD A&#10;Content: And that way the net could use {disfmarker}&#10;Speaker: Professor B&#10;Content: So .&#10;Speaker: PhD A&#10;Content: If the net 's on the server side then it could use all of the" />
    <node id="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method. &#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Specifically, they agreed to test and compare the noise estimation techniques of spectral subtraction and Wiener filtering, with the aim of improving the noise estimation while maintaining the same overall method." />
    <node id="1. The transcript does not provide a specific default parameter value for the software system that is one percent off from the best proposal. The closest information provided is that the default system's performance is one percent lower than the best proposal in terms of overall quality or effectiveness, but no actual metric or parameter value is given." />
    <node id=" {disfmarker}&#10;Speaker: Professor B&#10;Content: I mean on the , uh , TI - TI - digits ,&#10;Speaker: PhD E&#10;Content: on the multi - condition in TI - digits . Yeah .&#10;Speaker: Professor B&#10;Content: uh , uh . Um .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So , it probably doesn't matter that much either way . But , um , when you say u uh , unified do you mean , uh , it 's one piece of software now , or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: So now we are , yeah , setting up the software .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Um , it should be ready , uh , very soon . Um , and we&#10;Speaker: PhD A&#10;Content: So what 's {disfmarker} what 's happened ? I think I 've missed something .&#10;Speaker: Professor B&#10;Content: OK . So a week ago {disfmarker} maybe you weren't around when {disfmarker" />
    <node id="Yes, it is possible to use the same neural network for both voice activity detection (VAD) and noise estimation when training on cleaned-up speech features. This approach could simplify the system design by using a single network for multiple tasks. However, care must be taken to ensure that the shared network can effectively learn and differentiate between speech and noise in various conditions. Additionally, it is crucial to optimize the noise reduction techniques, such as over-subtraction, Wiener filtering, and spectral subtraction, to minimize distortion of the clean speech signal while maximizing noise reduction." />
    <node id="1. Conceptual differences in power and noise reduction factors between spectral subtraction and Wiener filtering:&#10;&#10;Spectral Subtraction: This method aims to reduce the noise by creating an estimate of the noise spectrum and then subtracting it from the observed signal's spectrum. The difference is then transformed back into the time domain, resulting in a noise-reduced signal. However, this process may introduce musical noise due to underestimation or overestimation of the noise spectrum.&#10;&#10;Wiener Filtering: Wiener filtering uses statistical properties of both the speech and noise signals to create an optimal filter for noise reduction while minimizing distortion. The filter takes into account the power spectral densities (PSD) of clean speech and noise, as well as their cross-power spectral density (CPSD). By finding the ratio of CPSD to the PSD of noise, the Wiener filter estimates the linear time-invariant system that minimizes the mean square error between the estimated clean signal and the actual clean signal.&#10;&#10;2. Specific &quot;little factors&quot; affecting the differences in practice:&#10;&#10;The amount of noise present in the signal (1): A higher level of noise requires more aggressive reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;Signal-to-Noise Ratio (SNR) (2): Lower SNR values make it harder to separate the desired signal from the noise, leading to a greater balance between noise reduction and signal distortion. More sophisticated techniques may be required for successful separation.&#10;&#10;Types of noise (3): Different noises can affect the difference in power that needs to be reduced. For instance, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;Over-subtraction, Wiener filtering, and spectral subtraction (4): These factors impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;Frame-dropping (5): This technique discards certain frames deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;Delta computations (6): Avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;Noise estimation effectiveness (7): Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;Choice of error method (8): The choice between using power spectra or signals directly in the filtering problem can significantly affect the reduction process, as each method has different sensitivities to noise and flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem." />
    <node id="1. The initial step taken to enhance speech involves applying a technique similar to spectral enhancement or noise reduction on specific FFT (Fast Fourier Transform) bins, which results in creating new FFT's with some noise removed. This process is discussed by Professor B when referring to creating &quot;new FFT's that...have some of the noise removed&quot; before doing further processing to produce various features." />
    <node id="vocalsound} um , all of the features that we use . So this is done early on . This is essentially , {vocalsound} um , um {disfmarker} I guess it 's {disfmarker} it 's more or less like a spee a speech enhancement technique here {disfmarker}&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: right ? {disfmarker} where we 're just kind of creating {vocalsound} new {disfmarker} if not new speech at least new {disfmarker} new FFT 's that {disfmarker} that have {disfmarker} you know , which could be turned into speech {disfmarker} uh , that {disfmarker} that have some of the noise removed .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um , after that we still do a mess of other things to {disfmarker} to produce a bunch of features .&#10;Speaker" />
    <node id="When choosing an error metric for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. In contrast, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring.&#10;&#10;The choice of error metric depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;Power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand." />
    <node id="1. Based on the transcript provided, there is no clear description of a specific method that PhD C used to assess how well the noise estimation was doing. When Professor B asked if there was any experiment or way to assess how well or poorly the noise estimation is currently doing, PhD C mentioned an experiment where they used the channel zero VAD for noise estimation and frame-dropping, but they didn't have a comparison of which part helped more.&#10;2. Furthermore, when Professor B asked if there was any improvement in noise estimation due to addressing the frame-dropping issue specifically, no clear numerical answer was given in the conversation.&#10;3. Therefore, it can be inferred that while PhD C did perform some testing related to noise estimation (using channel zero VAD and frame-dropping), they did not specify a particular method or quantify the results of this assessment." />
    <node id=" this is doing right now .&#10;Speaker: PhD A&#10;Content: I see . OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um . OK . So , uh , what 's , uh {disfmarker} ? That 's {disfmarker} that 's a good set of work that {disfmarker} that , uh {disfmarker}&#10;Speaker: PhD C&#10;Content: Just one more thing . Like , should we do something f more for the noise estimation , because we still {disfmarker} ?&#10;Speaker: Professor B&#10;Content: Yeah . I was wondering about that . That was {disfmarker} I {disfmarker} I had written that down there .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um {disfmarker}&#10;Speaker: PhD E&#10;Content: So , we , uh {disfmarker} actually I did the first experiment . This is {" />
    <node id="Based on the provided transcript, the exact impact of the frame-dropping problem on the improvement seen in the final result cannot be quantified. When Professor B asked how much of the change was due to the frame-dropping problem specifically, PhD C did not provide a clear numerical answer. While they mentioned that addressing the frame-dropping issue was part of the improvements made, there is no specific quantification of its contribution compared to other changes. Additionally, when discussing noise estimation assessment methods, PhD C mentioned an experiment using channel zero VAD for noise estimation and frame-dropping but did not compare which part helped more or provide a clear measure of improvement in noise estimation due to addressing the frame-dropping issue.&#10;&#10;In summary, while the transcript suggests that addressing the frame-dropping problem contributed to the observed improvements, it does not offer specific quantification of its impact or how much of the change can be attributed to this issue." />
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target="&#10;Speaker: Professor B&#10;Content: OK . So a week ago {disfmarker} maybe you weren't around when {disfmarker} when {disfmarker} when Hynek and Guenther and I {disfmarker} ?&#10;Speaker: PhD C&#10;Content: Hynek was here .&#10;Speaker: PhD A&#10;Content: Yeah . I didn't .&#10;Speaker: Professor B&#10;Content: Oh , OK . So {disfmarker} Yeah , let 's summarize . Um {disfmarker} And then if I summarize somebody can tell me if I 'm wrong , which will also be possibly helpful . What did I just press here ? I hope this is still working .&#10;Speaker: PhD E&#10;Content: p - p - p&#10;Speaker: Professor B&#10;Content: We , uh {disfmarker} we looked at , {nonvocalsound} uh {disfmarker} anyway we {disfmarker} {vocalsound} after coming back from QualComm we had , you know , very strong feedback and , uh , I think it was {vocalsound} Hynek and Guenter 's and">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target="Comm we had , you know , very strong feedback and , uh , I think it was {vocalsound} Hynek and Guenter 's and my opinion also that , um , you know , we sort of spread out to look at a number of different ways of doing noise suppression . But given the limited time , uh , it was sort of time to {pause} choose one .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mmm .&#10;Speaker: Professor B&#10;Content: Uh , and so , uh , th the vector Taylor series hadn't really worked out that much . Uh , the subspace stuff , uh , had not been worked with so much . Um , so it sort of came down to spectral subtraction versus Wiener filtering .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Uh , we had a long discussion about how they were the same and how they were d uh , completely different .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh , I mean , fundamentally they 're the same sort of thing but the math is a little different so that there 's">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target=" maybe testing out the noise {pause} estimation a little bit . I mean , keeping the same method but {disfmarker} but , uh , {vocalsound} seeing if you cou but , um noise estimation could be improved . Those are sort of related issues .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: It probably makes sense to move from there . And then , uh , {vocalsound} later on in the month I think we wanna start including the {pause} neural net at the end . Um . OK . Anything else ?&#10;Speaker: PhD E&#10;Content: The Half Dome was great .&#10;Speaker: Professor B&#10;Content: Good . Yeah . You didn't {disfmarker} didn't fall . That 's good .&#10;Speaker: PhD C&#10;Content: Well , yeah .&#10;Speaker: Professor B&#10;Content: Our e our effort would have been devastated if you guys had {comment} {vocalsound} run into problems .&#10;Speaker: PhD A&#10;Content: So , Hynek is coming back next week , you said ?&#10;Speaker: Professor B&#10;Content: Yeah , that 's">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target="er} a {disfmarker} a set of small features and continue to iterate and find , uh , a better set .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: OK . Well , short meeting . That 's OK .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: OK . So next week hopefully we 'll {disfmarker} can get Hynek here to {disfmarker} to join us and , uh , uh .&#10;Speaker: PhD A&#10;Content: Should we do digits ?&#10;Speaker: Professor B&#10;Content: Digits , digits . OK , now .&#10;Speaker: PhD A&#10;Content: Go ahead , Morgan . You can start .&#10;Speaker: Professor B&#10;Content: Alright . Let me get my glasses on so I can {pause} see them . OK .&#10;Speaker: PhD A&#10;Content: OK . And we 're off .&#10;Speaker: Professor B&#10;Content: Mm">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target=" A&#10;Content: So , Hynek is coming back next week , you said ?&#10;Speaker: Professor B&#10;Content: Yeah , that 's the plan .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: I guess the week after he 'll be , uh , going back to Europe , and so we wanna {disfmarker}&#10;Speaker: PhD A&#10;Content: Is he in Europe right now or is he up at {disfmarker} ?&#10;Speaker: Professor B&#10;Content: No , no . He 's {disfmarker} he 's {disfmarker} he 's dropped into the US . Yeah . Yeah .&#10;Speaker: PhD A&#10;Content: Oh . Hmm .&#10;Speaker: Professor B&#10;Content: So . Uh . {vocalsound} So , uh . Uh , the idea was that , uh , we 'd {disfmarker} we 'd sort out where we were going next with this {disfmarker} with this work before he , uh , left on this next trip . Good . {vocalsound} {vocalsound} Uh , Barry , you just got">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target=" And then we have to be careful with that also {disfmarker} with the neural net&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: because in {comment} the proposal the neural net was also , uh , working on {disfmarker} after frame - dropping .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Oh , that 's a real good point .&#10;Speaker: PhD E&#10;Content: So . Well , we 'll have to be {disfmarker} to do the same kind of correction .&#10;Speaker: Professor B&#10;Content: It might be hard if it 's at the server side . Right ?&#10;Speaker: PhD E&#10;Content: Mmm . Well , we can do the frame - dropping on the server side or we can just be careful at the terminal side to send a couple of more frames before and after , and {disfmarker} So . I think it 's OK .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: You have , um">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target="1. The purpose of Stephane's idea was to create one part of the feature vector that is highly discriminative and another part that is not, without processing the latter part through a neural net.&#10;2. This separation was implemented by using PLP (Perceptual Linear Prediction) features computed on noisy speech for input to the neural network, but not utilizing RASTA or similar techniques which could provide more robustness in these features.&#10;3. They discussed different noise suppression methods including vector Taylor series and subspace stuff, however spectral subtraction and Wiener filtering were found to be more viable options. Further testing is planned for these two methods with a focus on improving noise estimation.&#10;4. The group aims to incorporate a neural net into their workflow in the near future, possibly using the latency time available for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target="1. When determining whether the size or latency of a neural network is more important, there are a few factors to consider. If the neural network is running on the server side, its size may not be as much of a concern due to the available resources. However, latency remains an issue that needs to be addressed regardless of where the neural network is deployed. The ability to process data using only past information is one potential solution being considered, as it could help reduce latency.&#10;   &#10;2. In the past, the group has used neural networks to transform all features used for processing. This early transformation aims to improve the overall performance of their system. However, they have not yet explored the possibility of using a neural network that only looks at the past, which could further help reduce latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target="1. When choosing an error method for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. On the other hand, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring. This results in a different mathematical treatment and potentially different optimization objectives for the filtering problem.&#10;2. Another key difference is that power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;3. The choice of error method depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target="The plan for Professor Hynek's visit is to have him join a meeting with the team to discuss and firm up the direction of their project before his subsequent trip to Europe. Professor B wants to finalize the plans before Hynek's departure because they aim to have a clear path for the time he will be gone, specifying what things will be attended to during his absence.&#10;&#10;The group is currently focusing on further testing and improving noise estimation in both spectral subtraction and Wiener filtering methods for noise suppression. They plan to incorporate a neural net into their workflow later in the month.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target="1. The amount of noise present in the signal: The more noise there is, relative to the desired speech signal, the greater the difference in power that needs to be addressed. Reducing a large difference in power may require more aggressive noise reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;2. Signal-to-Noise Ratio (SNR): A lower SNR indicates that there is more noise relative to the desired signal, making it harder to separate the two. This can affect the reduction process by requiring more sophisticated noise reduction techniques and a greater balance between noise reduction and signal distortion.&#10;&#10;3. Types of noise: Different types of noise (e.g., white noise, colored noise, non-stationary noise) can have varying effects on the difference in power that needs to be reduced. For example, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;4. Over-subtraction, Wiener filtering, and spectral subtraction: These factors can impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;5. Frame-dropping: This technique discards certain frames (small segments of audio) deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;6. Delta computations: Used for estimating the first derivative of a signal, avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;7. Noise estimation effectiveness: Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;8. Choice of error method: When constructing a filtering problem, the choice between using power spectra or signals directly can significantly affect the reduction process. Power spectra are more sensitive to noise but require additional robustness techniques, while working with signals directly offers more flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method.&#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Hynek is expected to return the following week, and they aim to have a clear direction for their project before his departure for Europe." target="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method. &#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Specifically, they agreed to test and compare the noise estimation techniques of spectral subtraction and Wiener filtering, with the aim of improving the noise estimation while maintaining the same overall method.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: Professor B&#10;Content: OK . So a week ago {disfmarker} maybe you weren't around when {disfmarker} when {disfmarker} when Hynek and Guenther and I {disfmarker} ?&#10;Speaker: PhD C&#10;Content: Hynek was here .&#10;Speaker: PhD A&#10;Content: Yeah . I didn't .&#10;Speaker: Professor B&#10;Content: Oh , OK . So {disfmarker} Yeah , let 's summarize . Um {disfmarker} And then if I summarize somebody can tell me if I 'm wrong , which will also be possibly helpful . What did I just press here ? I hope this is still working .&#10;Speaker: PhD E&#10;Content: p - p - p&#10;Speaker: Professor B&#10;Content: We , uh {disfmarker} we looked at , {nonvocalsound} uh {disfmarker} anyway we {disfmarker} {vocalsound} after coming back from QualComm we had , you know , very strong feedback and , uh , I think it was {vocalsound} Hynek and Guenter 's and" target="The plan for Professor Hynek's visit is to have him join a meeting with the team to discuss and firm up the direction of their project before his subsequent trip to Europe. Professor B wants to finalize the plans before Hynek's departure because they aim to have a clear path for the time he will be gone, specifying what things will be attended to during his absence.&#10;&#10;The group is currently focusing on further testing and improving noise estimation in both spectral subtraction and Wiener filtering methods for noise suppression. They plan to incorporate a neural net into their workflow later in the month.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Speaker: Professor B&#10;Content: OK . So a week ago {disfmarker} maybe you weren't around when {disfmarker} when {disfmarker} when Hynek and Guenther and I {disfmarker} ?&#10;Speaker: PhD C&#10;Content: Hynek was here .&#10;Speaker: PhD A&#10;Content: Yeah . I didn't .&#10;Speaker: Professor B&#10;Content: Oh , OK . So {disfmarker} Yeah , let 's summarize . Um {disfmarker} And then if I summarize somebody can tell me if I 'm wrong , which will also be possibly helpful . What did I just press here ? I hope this is still working .&#10;Speaker: PhD E&#10;Content: p - p - p&#10;Speaker: Professor B&#10;Content: We , uh {disfmarker} we looked at , {nonvocalsound} uh {disfmarker} anyway we {disfmarker} {vocalsound} after coming back from QualComm we had , you know , very strong feedback and , uh , I think it was {vocalsound} Hynek and Guenter 's and" target="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method. &#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Specifically, they agreed to test and compare the noise estimation techniques of spectral subtraction and Wiener filtering, with the aim of improving the noise estimation while maintaining the same overall method.">
      <data key="d0">1</data>
    </edge>
    <edge source="Comm we had , you know , very strong feedback and , uh , I think it was {vocalsound} Hynek and Guenter 's and my opinion also that , um , you know , we sort of spread out to look at a number of different ways of doing noise suppression . But given the limited time , uh , it was sort of time to {pause} choose one .&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mmm .&#10;Speaker: Professor B&#10;Content: Uh , and so , uh , th the vector Taylor series hadn't really worked out that much . Uh , the subspace stuff , uh , had not been worked with so much . Um , so it sort of came down to spectral subtraction versus Wiener filtering .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Uh , we had a long discussion about how they were the same and how they were d uh , completely different .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh , I mean , fundamentally they 're the same sort of thing but the math is a little different so that there 's" target="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method. &#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Specifically, they agreed to test and compare the noise estimation techniques of spectral subtraction and Wiener filtering, with the aim of improving the noise estimation while maintaining the same overall method.">
      <data key="d0">1</data>
    </edge>
    <edge source=" maybe testing out the noise {pause} estimation a little bit . I mean , keeping the same method but {disfmarker} but , uh , {vocalsound} seeing if you cou but , um noise estimation could be improved . Those are sort of related issues .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: It probably makes sense to move from there . And then , uh , {vocalsound} later on in the month I think we wanna start including the {pause} neural net at the end . Um . OK . Anything else ?&#10;Speaker: PhD E&#10;Content: The Half Dome was great .&#10;Speaker: Professor B&#10;Content: Good . Yeah . You didn't {disfmarker} didn't fall . That 's good .&#10;Speaker: PhD C&#10;Content: Well , yeah .&#10;Speaker: Professor B&#10;Content: Our e our effort would have been devastated if you guys had {comment} {vocalsound} run into problems .&#10;Speaker: PhD A&#10;Content: So , Hynek is coming back next week , you said ?&#10;Speaker: Professor B&#10;Content: Yeah , that 's" target="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training.">
      <data key="d0">1</data>
    </edge>
    <edge source=" maybe testing out the noise {pause} estimation a little bit . I mean , keeping the same method but {disfmarker} but , uh , {vocalsound} seeing if you cou but , um noise estimation could be improved . Those are sort of related issues .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: It probably makes sense to move from there . And then , uh , {vocalsound} later on in the month I think we wanna start including the {pause} neural net at the end . Um . OK . Anything else ?&#10;Speaker: PhD E&#10;Content: The Half Dome was great .&#10;Speaker: Professor B&#10;Content: Good . Yeah . You didn't {disfmarker} didn't fall . That 's good .&#10;Speaker: PhD C&#10;Content: Well , yeah .&#10;Speaker: Professor B&#10;Content: Our e our effort would have been devastated if you guys had {comment} {vocalsound} run into problems .&#10;Speaker: PhD A&#10;Content: So , Hynek is coming back next week , you said ?&#10;Speaker: Professor B&#10;Content: Yeah , that 's" target="The plan for Professor Hynek's visit is to have him join a meeting with the team to discuss and firm up the direction of their project before his subsequent trip to Europe. Professor B wants to finalize the plans before Hynek's departure because they aim to have a clear path for the time he will be gone, specifying what things will be attended to during his absence.&#10;&#10;The group is currently focusing on further testing and improving noise estimation in both spectral subtraction and Wiener filtering methods for noise suppression. They plan to incorporate a neural net into their workflow later in the month.">
      <data key="d0">1</data>
    </edge>
    <edge source=" maybe testing out the noise {pause} estimation a little bit . I mean , keeping the same method but {disfmarker} but , uh , {vocalsound} seeing if you cou but , um noise estimation could be improved . Those are sort of related issues .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: It probably makes sense to move from there . And then , uh , {vocalsound} later on in the month I think we wanna start including the {pause} neural net at the end . Um . OK . Anything else ?&#10;Speaker: PhD E&#10;Content: The Half Dome was great .&#10;Speaker: Professor B&#10;Content: Good . Yeah . You didn't {disfmarker} didn't fall . That 's good .&#10;Speaker: PhD C&#10;Content: Well , yeah .&#10;Speaker: Professor B&#10;Content: Our e our effort would have been devastated if you guys had {comment} {vocalsound} run into problems .&#10;Speaker: PhD A&#10;Content: So , Hynek is coming back next week , you said ?&#10;Speaker: Professor B&#10;Content: Yeah , that 's" target="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method. &#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Specifically, they agreed to test and compare the noise estimation techniques of spectral subtraction and Wiener filtering, with the aim of improving the noise estimation while maintaining the same overall method.">
      <data key="d0">1</data>
    </edge>
    <edge source="er} a {disfmarker} a set of small features and continue to iterate and find , uh , a better set .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Grad D&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: OK . Well , short meeting . That 's OK .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: OK . So next week hopefully we 'll {disfmarker} can get Hynek here to {disfmarker} to join us and , uh , uh .&#10;Speaker: PhD A&#10;Content: Should we do digits ?&#10;Speaker: Professor B&#10;Content: Digits , digits . OK , now .&#10;Speaker: PhD A&#10;Content: Go ahead , Morgan . You can start .&#10;Speaker: Professor B&#10;Content: Alright . Let me get my glasses on so I can {pause} see them . OK .&#10;Speaker: PhD A&#10;Content: OK . And we 're off .&#10;Speaker: Professor B&#10;Content: Mm" target="The plan for Professor Hynek's visit is to have him join a meeting with the team to discuss and firm up the direction of their project before his subsequent trip to Europe. Professor B wants to finalize the plans before Hynek's departure because they aim to have a clear path for the time he will be gone, specifying what things will be attended to during his absence.&#10;&#10;The group is currently focusing on further testing and improving noise estimation in both spectral subtraction and Wiener filtering methods for noise suppression. They plan to incorporate a neural net into their workflow later in the month.">
      <data key="d0">1</data>
    </edge>
    <edge source=" A&#10;Content: So , Hynek is coming back next week , you said ?&#10;Speaker: Professor B&#10;Content: Yeah , that 's the plan .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: I guess the week after he 'll be , uh , going back to Europe , and so we wanna {disfmarker}&#10;Speaker: PhD A&#10;Content: Is he in Europe right now or is he up at {disfmarker} ?&#10;Speaker: Professor B&#10;Content: No , no . He 's {disfmarker} he 's {disfmarker} he 's dropped into the US . Yeah . Yeah .&#10;Speaker: PhD A&#10;Content: Oh . Hmm .&#10;Speaker: Professor B&#10;Content: So . Uh . {vocalsound} So , uh . Uh , the idea was that , uh , we 'd {disfmarker} we 'd sort out where we were going next with this {disfmarker} with this work before he , uh , left on this next trip . Good . {vocalsound} {vocalsound} Uh , Barry , you just got" target="The plan for Professor Hynek's visit is to have him join a meeting with the team to discuss and firm up the direction of their project before his subsequent trip to Europe. Professor B wants to finalize the plans before Hynek's departure because they aim to have a clear path for the time he will be gone, specifying what things will be attended to during his absence.&#10;&#10;The group is currently focusing on further testing and improving noise estimation in both spectral subtraction and Wiener filtering methods for noise suppression. They plan to incorporate a neural net into their workflow later in the month.">
      <data key="d0">1</data>
    </edge>
    <edge source=" And then we have to be careful with that also {disfmarker} with the neural net&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: because in {comment} the proposal the neural net was also , uh , working on {disfmarker} after frame - dropping .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Oh , that 's a real good point .&#10;Speaker: PhD E&#10;Content: So . Well , we 'll have to be {disfmarker} to do the same kind of correction .&#10;Speaker: Professor B&#10;Content: It might be hard if it 's at the server side . Right ?&#10;Speaker: PhD E&#10;Content: Mmm . Well , we can do the frame - dropping on the server side or we can just be careful at the terminal side to send a couple of more frames before and after , and {disfmarker} So . I think it 's OK .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: You have , um" target="1. Mark Frames Before They Reach the Server: Instead of dropping unnecessary frames after they have been transmitted to the server, it is suggested to mark them beforehand. This can be done by analyzing the audio data locally and adding a marker or flag to indicate whether a frame should be dropped or kept.&#10;&#10;2. Use Additional Bits for Differentiation: To better handle speech and non-speech frames on the server side, additional bits can be used in the transmission protocol. These extra bits can serve as flags that differentiate between speech and non-speech frames. This way, the server can process only the necessary frames without having to drop any of them after transmission, thus saving bandwidth and reducing latency.&#10;&#10;3. Improve Neural Network Performance: Training the neural network on more data and incorporating better features and noise reduction techniques during training can help enhance its performance in recognizing speech and differentiating it from non-speech. This would result in fewer unnecessary frames being transmitted, thus reducing the load on the server.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="Speaker: PhD A&#10;Content: Oh .&#10;Speaker: PhD C&#10;Content: And {disfmarker} &#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: So , should we keep the same {disfmarker} ? I think we might try to keep the same idea of having a neural network , but {vocalsound} training it on more data and adding better features , I think , but {disfmarker} because the current network is just PLP features . Well , it 's trained on noisy {pause} PLP {disfmarker}&#10;Speaker: PhD C&#10;Content: Just the cepstra . Yeah .&#10;Speaker: PhD E&#10;Content: PLP features computed on noisy speech . But {vocalsound} {vocalsound} there is no nothing particularly robust in these features .&#10;Speaker: PhD A&#10;Content: So , I I uh {disfmarker}&#10;Speaker: PhD C&#10;Content: No .&#10;Speaker: PhD E&#10;Content: There 's no RASTA , no {disfmarker}&#10;Speaker: PhD A&#10;Content: So , uh , I">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="isfmarker} ?&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Well , depending on its size {disfmarker} Well , one question is , is it on the , um , server side or is it on the terminal side ? Uh , if it 's on the server side , it {disfmarker} you probably don't have to worry too much about size .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So that 's kind of an argument for that . We do still , however , have to consider its latency . So the issue is {disfmarker} is , um , {vocalsound} for instance , could we have a neural net that only looked at the past ?&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: Um , what we 've done in uh {disfmarker} in the past is to use the neural net , uh , to transform , {vocalsound} um , all of the features that we use . So this is done early on . This is essentially , {vocalsound} um ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target=" features , {vocalsound} saying it 's currently a {disfmarker} it 's a speech or a nonspeech .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD C&#10;Content: So there is no frame - dropping till the final features , like , including the deltas are computed .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: PhD C&#10;Content: And after the deltas are computed , you just pick up the ones that are marked silence and then drop them .&#10;Speaker: PhD A&#10;Content: Mm - hmm . I see . I see .&#10;Speaker: Professor B&#10;Content: So it would be more or less the same thing with the neural net , I guess , actually .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So . Yeah , that 's what {disfmarker} that 's what {disfmarker} that 's what , uh , this is doing right now .&#10;Speaker: PhD A&#10;Content: I see . OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="Speaker: PhD E&#10;Content: It 's just one percent off of the {pause} best proposal .&#10;Speaker: PhD C&#10;Content: Best system .&#10;Speaker: PhD E&#10;Content: It 's between {disfmarker} i we are second actually if we take this system .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Right ?&#10;Speaker: PhD A&#10;Content: Compared to the last evaluation numbers ? Yeah .&#10;Speaker: Professor B&#10;Content: But , uh {disfmarker} w which we sort of were before&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Professor B&#10;Content: but we were considerably far behind . And the thing is , this doesn't have neural net in yet for instance . You know ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: So it {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="1. The purpose of Stephane's idea was to create one part of the feature vector that is highly discriminative and another part that is not, without processing the latter part through a neural net.&#10;2. This separation was implemented by using PLP (Perceptual Linear Prediction) features computed on noisy speech for input to the neural network, but not utilizing RASTA or similar techniques which could provide more robustness in these features.&#10;3. They discussed different noise suppression methods including vector Taylor series and subspace stuff, however spectral subtraction and Wiener filtering were found to be more viable options. Further testing is planned for these two methods with a focus on improving noise estimation.&#10;4. The group aims to incorporate a neural net into their workflow in the near future, possibly using the latency time available for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="1. The disagreement regarding the amount of latency allowed for the neural net concerns the duration that should be used before processing speech data through the network. Some parties are advocating for a shorter length, while others suggest either 130 or 250 milliseconds.&#10;2. The previous standard was 250 milliseconds, but some groups are now lobbying to make it shorter. Meanwhile, another group is arguing for a 130-millisecond latency period.&#10;3. The exact amount of latency that will be allowed has not been determined yet, as the debate between various parties is ongoing. It is crucial to find a balance between the neural network's performance and the constraints on the allowable latency period.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="Based on the information provided by PhD E, the default system's performance is reported to be one percent lower than the best proposal. This comparison is made in terms of the system's overall quality or effectiveness; however, a specific metric or context for this assessment has not been detailed in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="1. When determining whether the size or latency of a neural network is more important, there are a few factors to consider. If the neural network is running on the server side, its size may not be as much of a concern due to the available resources. However, latency remains an issue that needs to be addressed regardless of where the neural network is deployed. The ability to process data using only past information is one potential solution being considered, as it could help reduce latency.&#10;   &#10;2. In the past, the group has used neural networks to transform all features used for processing. This early transformation aims to improve the overall performance of their system. However, they have not yet explored the possibility of using a neural network that only looks at the past, which could further help reduce latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="1. The group has observed an improvement of more than twenty percent in speech recognition performance by dropping the beginning and end frames (pauses) of a speech compared to their current VAD system's performance. In contrast, the other system's VAD only showed a fourteen percent improvement with this method.&#10;   &#10;2. A more effective method for improving speech recognition accuracy could be using VAD probabilities computed from clean signals on far-field microphone data. This approach significantly decreases the error rate, in some cases even dividing it by two, and has potential for further enhancement. Additionally, optimizing the neural network used in the VAD system by training it on more data and adding better features could also improve its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="1. The amount of noise present in the signal: The more noise there is, relative to the desired speech signal, the greater the difference in power that needs to be addressed. Reducing a large difference in power may require more aggressive noise reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;2. Signal-to-Noise Ratio (SNR): A lower SNR indicates that there is more noise relative to the desired signal, making it harder to separate the two. This can affect the reduction process by requiring more sophisticated noise reduction techniques and a greater balance between noise reduction and signal distortion.&#10;&#10;3. Types of noise: Different types of noise (e.g., white noise, colored noise, non-stationary noise) can have varying effects on the difference in power that needs to be reduced. For example, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;4. Over-subtraction, Wiener filtering, and spectral subtraction: These factors can impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;5. Frame-dropping: This technique discards certain frames (small segments of audio) deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;6. Delta computations: Used for estimating the first derivative of a signal, avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;7. Noise estimation effectiveness: Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;8. Choice of error method: When constructing a filtering problem, the choice between using power spectra or signals directly can significantly affect the reduction process. Power spectra are more sensitive to noise but require additional robustness techniques, while working with signals directly offers more flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="1. Mark Frames Before They Reach the Server: Instead of dropping unnecessary frames after they have been transmitted to the server, it is suggested to mark them beforehand. This can be done by analyzing the audio data locally and adding a marker or flag to indicate whether a frame should be dropped or kept.&#10;&#10;2. Use Additional Bits for Differentiation: To better handle speech and non-speech frames on the server side, additional bits can be used in the transmission protocol. These extra bits can serve as flags that differentiate between speech and non-speech frames. This way, the server can process only the necessary frames without having to drop any of them after transmission, thus saving bandwidth and reducing latency.&#10;&#10;3. Improve Neural Network Performance: Training the neural network on more data and incorporating better features and noise reduction techniques during training can help enhance its performance in recognizing speech and differentiating it from non-speech. This would result in fewer unnecessary frames being transmitted, thus reducing the load on the server.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="Yes, it is possible to use the same neural network for both voice activity detection (VAD) and noise estimation when training on cleaned-up speech features. This approach could simplify the system design by using a single network for multiple tasks. However, care must be taken to ensure that the shared network can effectively learn and differentiate between speech and noise in various conditions. Additionally, it is crucial to optimize the noise reduction techniques, such as over-subtraction, Wiener filtering, and spectral subtraction, to minimize distortion of the clean speech signal while maximizing noise reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group suggests training the neural network on more data to improve its performance.&#10;2. They also recommend adding better features to the neural network. Currently, the network uses PLP (Perceptual Linear Prediction) features computed on noisy speech, which are not particularly robust. There is no RASTA or other similar techniques being used in these features. The suggestion is to incorporate more robust features and noise reduction techniques during training." target="1. The initial step taken to enhance speech involves applying a technique similar to spectral enhancement or noise reduction on specific FFT (Fast Fourier Transform) bins, which results in creating new FFT's with some noise removed. This process is discussed by Professor B when referring to creating &quot;new FFT's that...have some of the noise removed&quot; before doing further processing to produce various features.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker} ?&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Well , depending on its size {disfmarker} Well , one question is , is it on the , um , server side or is it on the terminal side ? Uh , if it 's on the server side , it {disfmarker} you probably don't have to worry too much about size .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So that 's kind of an argument for that . We do still , however , have to consider its latency . So the issue is {disfmarker} is , um , {vocalsound} for instance , could we have a neural net that only looked at the past ?&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: Um , what we 've done in uh {disfmarker} in the past is to use the neural net , uh , to transform , {vocalsound} um , all of the features that we use . So this is done early on . This is essentially , {vocalsound} um ," target="1. The disagreement regarding the amount of latency allowed for the neural net concerns the duration that should be used before processing speech data through the network. Some parties are advocating for a shorter length, while others suggest either 130 or 250 milliseconds.&#10;2. The previous standard was 250 milliseconds, but some groups are now lobbying to make it shorter. Meanwhile, another group is arguing for a 130-millisecond latency period.&#10;3. The exact amount of latency that will be allowed has not been determined yet, as the debate between various parties is ongoing. It is crucial to find a balance between the neural network's performance and the constraints on the allowable latency period.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker} ?&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Well , depending on its size {disfmarker} Well , one question is , is it on the , um , server side or is it on the terminal side ? Uh , if it 's on the server side , it {disfmarker} you probably don't have to worry too much about size .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So that 's kind of an argument for that . We do still , however , have to consider its latency . So the issue is {disfmarker} is , um , {vocalsound} for instance , could we have a neural net that only looked at the past ?&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: Um , what we 've done in uh {disfmarker} in the past is to use the neural net , uh , to transform , {vocalsound} um , all of the features that we use . So this is done early on . This is essentially , {vocalsound} um ," target="1. Yes, the idea of having one part of the feature vector that is highly discriminative and another part that is not (without processing the latter part through a neural net) still seems reasonable for vocalsound and VAD in the neural net setting. This is because it can help smooth things out when the testing set is different from what was trained, while also being aware of latency constraints.&#10;2. The exact amount of latency allowed for the neural net is still under debate, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The group aims to find a balance between the neural network's performance and the constraints on the allowable latency period.&#10;3. Using VAD probabilities computed on the clean signal and applying them to far-field test utterances has shown significant improvement, reducing error rates by up to two times in some cases. This suggests that there is still potential for improving the VAD system. The current VAD implementation adds some latency, but further development could potentially reduce this.">
      <data key="d0">1</data>
    </edge>
    <edge source="isfmarker} ?&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Well , depending on its size {disfmarker} Well , one question is , is it on the , um , server side or is it on the terminal side ? Uh , if it 's on the server side , it {disfmarker} you probably don't have to worry too much about size .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So that 's kind of an argument for that . We do still , however , have to consider its latency . So the issue is {disfmarker} is , um , {vocalsound} for instance , could we have a neural net that only looked at the past ?&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: Um , what we 've done in uh {disfmarker} in the past is to use the neural net , uh , to transform , {vocalsound} um , all of the features that we use . So this is done early on . This is essentially , {vocalsound} um ," target="1. When determining whether the size or latency of a neural network is more important, there are a few factors to consider. If the neural network is running on the server side, its size may not be as much of a concern due to the available resources. However, latency remains an issue that needs to be addressed regardless of where the neural network is deployed. The ability to process data using only past information is one potential solution being considered, as it could help reduce latency.&#10;   &#10;2. In the past, the group has used neural networks to transform all features used for processing. This early transformation aims to improve the overall performance of their system. However, they have not yet explored the possibility of using a neural network that only looks at the past, which could further help reduce latency.">
      <data key="d0">1</data>
    </edge>
    <edge source=" features , {vocalsound} saying it 's currently a {disfmarker} it 's a speech or a nonspeech .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD C&#10;Content: So there is no frame - dropping till the final features , like , including the deltas are computed .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: PhD C&#10;Content: And after the deltas are computed , you just pick up the ones that are marked silence and then drop them .&#10;Speaker: PhD A&#10;Content: Mm - hmm . I see . I see .&#10;Speaker: Professor B&#10;Content: So it would be more or less the same thing with the neural net , I guess , actually .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So . Yeah , that 's what {disfmarker} that 's what {disfmarker} that 's what , uh , this is doing right now .&#10;Speaker: PhD A&#10;Content: I see . OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;" target="1. The purpose of Stephane's idea was to create one part of the feature vector that is highly discriminative and another part that is not, without processing the latter part through a neural net.&#10;2. This separation was implemented by using PLP (Perceptual Linear Prediction) features computed on noisy speech for input to the neural network, but not utilizing RASTA or similar techniques which could provide more robustness in these features.&#10;3. They discussed different noise suppression methods including vector Taylor series and subspace stuff, however spectral subtraction and Wiener filtering were found to be more viable options. Further testing is planned for these two methods with a focus on improving noise estimation.&#10;4. The group aims to incorporate a neural net into their workflow in the near future, possibly using the latency time available for this purpose.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD E&#10;Content: It 's just one percent off of the {pause} best proposal .&#10;Speaker: PhD C&#10;Content: Best system .&#10;Speaker: PhD E&#10;Content: It 's between {disfmarker} i we are second actually if we take this system .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Right ?&#10;Speaker: PhD A&#10;Content: Compared to the last evaluation numbers ? Yeah .&#10;Speaker: Professor B&#10;Content: But , uh {disfmarker} w which we sort of were before&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Professor B&#10;Content: but we were considerably far behind . And the thing is , this doesn't have neural net in yet for instance . You know ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: So it {disfmark" target="Based on the information provided by PhD E, the default system's performance is reported to be one percent lower than the best proposal. This comparison is made in terms of the system's overall quality or effectiveness; however, a specific metric or context for this assessment has not been detailed in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD E&#10;Content: It 's just one percent off of the {pause} best proposal .&#10;Speaker: PhD C&#10;Content: Best system .&#10;Speaker: PhD E&#10;Content: It 's between {disfmarker} i we are second actually if we take this system .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Right ?&#10;Speaker: PhD A&#10;Content: Compared to the last evaluation numbers ? Yeah .&#10;Speaker: Professor B&#10;Content: But , uh {disfmarker} w which we sort of were before&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Professor B&#10;Content: but we were considerably far behind . And the thing is , this doesn't have neural net in yet for instance . You know ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: So it {disfmark" target="1. The transcript does not provide a specific default parameter value for the software system that is one percent off from the best proposal. The closest information provided is that the default system's performance is one percent lower than the best proposal in terms of overall quality or effectiveness, but no actual metric or parameter value is given.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of Stephane's idea was to create one part of the feature vector that is highly discriminative and another part that is not, without processing the latter part through a neural net.&#10;2. This separation was implemented by using PLP (Perceptual Linear Prediction) features computed on noisy speech for input to the neural network, but not utilizing RASTA or similar techniques which could provide more robustness in these features.&#10;3. They discussed different noise suppression methods including vector Taylor series and subspace stuff, however spectral subtraction and Wiener filtering were found to be more viable options. Further testing is planned for these two methods with a focus on improving noise estimation.&#10;4. The group aims to incorporate a neural net into their workflow in the near future, possibly using the latency time available for this purpose." target="&#10;Content: Um , after that we still do a mess of other things to {disfmarker} to produce a bunch of features .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: And then those features are not now currently transformed {vocalsound} by the neural net . And then the {disfmarker} the way that we had it in our proposal - two before , we had the neural net transformed features and we had {vocalsound} the untransformed features , which I guess you {disfmarker} you actually did linearly transform with the KLT ,&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . Right .&#10;Speaker: Professor B&#10;Content: but {disfmarker} but {disfmarker} but {disfmarker} uh , to orthogonalize them {disfmarker} but {disfmarker} {vocalsound} but they were not , uh , processed through a neural net . And Stephane 's idea with that , as I recall , was that {vocalsound} you 'd have one part of the feature vector that was very discriminant and another part that wasn">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of Stephane's idea was to create one part of the feature vector that is highly discriminative and another part that is not, without processing the latter part through a neural net.&#10;2. This separation was implemented by using PLP (Perceptual Linear Prediction) features computed on noisy speech for input to the neural network, but not utilizing RASTA or similar techniques which could provide more robustness in these features.&#10;3. They discussed different noise suppression methods including vector Taylor series and subspace stuff, however spectral subtraction and Wiener filtering were found to be more viable options. Further testing is planned for these two methods with a focus on improving noise estimation.&#10;4. The group aims to incorporate a neural net into their workflow in the near future, possibly using the latency time available for this purpose." target=" Yeah . So that 's the one which Stephane was discussing , like {disfmarker}&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: The smoothing ?&#10;Speaker: PhD C&#10;Content: Yeah . The {disfmarker} you smooth it and then delay the decision by {disfmarker} So .&#10;Speaker: Professor B&#10;Content: Right . OK . So that 's {disfmarker} that 's really not {disfmarker} not bad . So we may in fact {disfmarker} we 'll see what they decide . We may in fact have , {vocalsound} um , the {disfmarker} the , uh , latency time available for {disfmarker} to have a neural net . I mean , sounds like we probably will . So .&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: That 'd be good . Cuz I {disfmarker} cuz it certainly always helped us before . So .&#10;Speaker: PhD A&#10;Content: What amount of latency are you thinking about when">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of Stephane's idea was to create one part of the feature vector that is highly discriminative and another part that is not, without processing the latter part through a neural net.&#10;2. This separation was implemented by using PLP (Perceptual Linear Prediction) features computed on noisy speech for input to the neural network, but not utilizing RASTA or similar techniques which could provide more robustness in these features.&#10;3. They discussed different noise suppression methods including vector Taylor series and subspace stuff, however spectral subtraction and Wiener filtering were found to be more viable options. Further testing is planned for these two methods with a focus on improving noise estimation.&#10;4. The group aims to incorporate a neural net into their workflow in the near future, possibly using the latency time available for this purpose." target="&#10;Content: There 's no RASTA , no {disfmarker}&#10;Speaker: PhD A&#10;Content: So , uh , I {disfmarker} I don't remember what you said {vocalsound} the answer to my , uh , question earlier . Will you {disfmarker} will you train the net on {disfmarker} after you 've done the spectral subtraction or the Wiener filtering ?&#10;Speaker: Professor B&#10;Content: This is a different net .&#10;Speaker: PhD A&#10;Content: Oh .&#10;Speaker: PhD C&#10;Content: So we have a VAD which is like neur that 's a neural net .&#10;Speaker: PhD E&#10;Content: Oh , yeah . Hmm .&#10;Speaker: PhD A&#10;Content: Oh , you 're talking about the VAD net . OK .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: PhD C&#10;Content: So that {disfmarker} that VAD was trained on the noisy features .&#10;Speaker: PhD A">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of Stephane's idea was to create one part of the feature vector that is highly discriminative and another part that is not, without processing the latter part through a neural net.&#10;2. This separation was implemented by using PLP (Perceptual Linear Prediction) features computed on noisy speech for input to the neural network, but not utilizing RASTA or similar techniques which could provide more robustness in these features.&#10;3. They discussed different noise suppression methods including vector Taylor series and subspace stuff, however spectral subtraction and Wiener filtering were found to be more viable options. Further testing is planned for these two methods with a focus on improving noise estimation.&#10;4. The group aims to incorporate a neural net into their workflow in the near future, possibly using the latency time available for this purpose." target="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of Stephane's idea was to create one part of the feature vector that is highly discriminative and another part that is not, without processing the latter part through a neural net.&#10;2. This separation was implemented by using PLP (Perceptual Linear Prediction) features computed on noisy speech for input to the neural network, but not utilizing RASTA or similar techniques which could provide more robustness in these features.&#10;3. They discussed different noise suppression methods including vector Taylor series and subspace stuff, however spectral subtraction and Wiener filtering were found to be more viable options. Further testing is planned for these two methods with a focus on improving noise estimation.&#10;4. The group aims to incorporate a neural net into their workflow in the near future, possibly using the latency time available for this purpose." target="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of Stephane's idea was to create one part of the feature vector that is highly discriminative and another part that is not, without processing the latter part through a neural net.&#10;2. This separation was implemented by using PLP (Perceptual Linear Prediction) features computed on noisy speech for input to the neural network, but not utilizing RASTA or similar techniques which could provide more robustness in these features.&#10;3. They discussed different noise suppression methods including vector Taylor series and subspace stuff, however spectral subtraction and Wiener filtering were found to be more viable options. Further testing is planned for these two methods with a focus on improving noise estimation.&#10;4. The group aims to incorporate a neural net into their workflow in the near future, possibly using the latency time available for this purpose." target="1. Yes, the idea of having one part of the feature vector that is highly discriminative and another part that is not (without processing the latter part through a neural net) still seems reasonable for vocalsound and VAD in the neural net setting. This is because it can help smooth things out when the testing set is different from what was trained, while also being aware of latency constraints.&#10;2. The exact amount of latency allowed for the neural net is still under debate, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The group aims to find a balance between the neural network's performance and the constraints on the allowable latency period.&#10;3. Using VAD probabilities computed on the clean signal and applying them to far-field test utterances has shown significant improvement, reducing error rates by up to two times in some cases. This suggests that there is still potential for improving the VAD system. The current VAD implementation adds some latency, but further development could potentially reduce this.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of Stephane's idea was to create one part of the feature vector that is highly discriminative and another part that is not, without processing the latter part through a neural net.&#10;2. This separation was implemented by using PLP (Perceptual Linear Prediction) features computed on noisy speech for input to the neural network, but not utilizing RASTA or similar techniques which could provide more robustness in these features.&#10;3. They discussed different noise suppression methods including vector Taylor series and subspace stuff, however spectral subtraction and Wiener filtering were found to be more viable options. Further testing is planned for these two methods with a focus on improving noise estimation.&#10;4. The group aims to incorporate a neural net into their workflow in the near future, possibly using the latency time available for this purpose." target="1. When determining whether the size or latency of a neural network is more important, there are a few factors to consider. If the neural network is running on the server side, its size may not be as much of a concern due to the available resources. However, latency remains an issue that needs to be addressed regardless of where the neural network is deployed. The ability to process data using only past information is one potential solution being considered, as it could help reduce latency.&#10;   &#10;2. In the past, the group has used neural networks to transform all features used for processing. This early transformation aims to improve the overall performance of their system. However, they have not yet explored the possibility of using a neural network that only looks at the past, which could further help reduce latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of Stephane's idea was to create one part of the feature vector that is highly discriminative and another part that is not, without processing the latter part through a neural net.&#10;2. This separation was implemented by using PLP (Perceptual Linear Prediction) features computed on noisy speech for input to the neural network, but not utilizing RASTA or similar techniques which could provide more robustness in these features.&#10;3. They discussed different noise suppression methods including vector Taylor series and subspace stuff, however spectral subtraction and Wiener filtering were found to be more viable options. Further testing is planned for these two methods with a focus on improving noise estimation.&#10;4. The group aims to incorporate a neural net into their workflow in the near future, possibly using the latency time available for this purpose." target="1. When choosing an error method for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. On the other hand, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring. This results in a different mathematical treatment and potentially different optimization objectives for the filtering problem.&#10;2. Another key difference is that power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;3. The choice of error method depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand.">
      <data key="d0">1</data>
    </edge>
    <edge source="&#10;Content: Um , after that we still do a mess of other things to {disfmarker} to produce a bunch of features .&#10;Speaker: PhD A&#10;Content: Right .&#10;Speaker: Professor B&#10;Content: And then those features are not now currently transformed {vocalsound} by the neural net . And then the {disfmarker} the way that we had it in our proposal - two before , we had the neural net transformed features and we had {vocalsound} the untransformed features , which I guess you {disfmarker} you actually did linearly transform with the KLT ,&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . Right .&#10;Speaker: Professor B&#10;Content: but {disfmarker} but {disfmarker} but {disfmarker} uh , to orthogonalize them {disfmarker} but {disfmarker} {vocalsound} but they were not , uh , processed through a neural net . And Stephane 's idea with that , as I recall , was that {vocalsound} you 'd have one part of the feature vector that was very discriminant and another part that wasn" target="1. Yes, the idea of having one part of the feature vector that is highly discriminative and another part that is not (without processing the latter part through a neural net) still seems reasonable for vocalsound and VAD in the neural net setting. This is because it can help smooth things out when the testing set is different from what was trained, while also being aware of latency constraints.&#10;2. The exact amount of latency allowed for the neural net is still under debate, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The group aims to find a balance between the neural network's performance and the constraints on the allowable latency period.&#10;3. Using VAD probabilities computed on the clean signal and applying them to far-field test utterances has shown significant improvement, reducing error rates by up to two times in some cases. This suggests that there is still potential for improving the VAD system. The current VAD implementation adds some latency, but further development could potentially reduce this.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target=" far - field microphone . And if we just take only the , um , VAD probabilities computed on the clean signal and apply them on the far - field , uh , test utterances , {vocalsound} then results are much better .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: In some cases it divides the error rate by two .&#10;Speaker: PhD A&#10;Content: Wow .&#10;Speaker: PhD E&#10;Content: So it means that there are stim {comment} still {disfmarker}&#10;Speaker: PhD A&#10;Content: How {disfmarker} how much latency does the , uh {disfmarker} does our VAD add ?&#10;Speaker: PhD E&#10;Content: If {disfmarker} if we can have a good VAD , well , it would be great .&#10;Speaker: PhD A&#10;Content: Is it significant ,&#10;Speaker: PhD E&#10;Content: Uh , right now it 's , um , a neural net with nine frames .&#10;Speaker: PhD A&#10;Content: or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: So it '">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target=" that with our current VAD we can improve by more than twenty percent .&#10;Speaker: PhD A&#10;Content: On top of the VAD that they provide ?&#10;Speaker: PhD C&#10;Content: No .&#10;Speaker: PhD E&#10;Content: Just using either their VAD or our current VAD .&#10;Speaker: PhD C&#10;Content: Our way .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD E&#10;Content: So , our current VAD is {disfmarker} is more than twenty percent , while their is fourteen .&#10;Speaker: PhD A&#10;Content: Theirs is fourteen ? I see .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Huh .&#10;Speaker: PhD E&#10;Content: So . Yeah . And {pause} another thing that we did also is that we have all this training data for {disfmarker} let 's say , for SpeechDat - Car . We have channel zero which is clean , channel one which is far - field microphone . And if we just take only the , um , VAD probabilities computed on the clean signal and apply them on the far - field ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="Speaker: PhD C&#10;Content: So that {disfmarker} that VAD was trained on the noisy features .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , right now we have , like , uh {disfmarker} we have the cleaned - up features , so we can have a better VAD by training the net on {pause} the cleaned - up speech .&#10;Speaker: PhD A&#10;Content: Mm - hmm . I see . I see .&#10;Speaker: PhD C&#10;Content: Yeah , but we need a VAD for uh noise estimation also . So it 's , like , where do we want to put the VAD ? Uh , it 's like {disfmarker}&#10;Speaker: PhD A&#10;Content: Can you use the same net to do both , or {disfmarker} ?&#10;Speaker: PhD C&#10;Content: For {disfmarker}&#10;Speaker: PhD A&#10;Content: Can you use the same net that you {disfmarker} that I was talking about to do the VAD ?&#10;Speaker: PhD C&#10;Content: M">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target=" do you have any way of assessing how well or how poorly the noise estimation is currently doing ?&#10;Speaker: PhD E&#10;Content: Mmm . No , we don't .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: We don't have nothing {pause} that {disfmarker}&#10;Speaker: PhD C&#10;Content: Is there {disfmarker} was there any experiment with {disfmarker} ? Well , I {disfmarker} I did {disfmarker} The only experiment where I tried was I used the channel zero VAD for the noise estimation and frame - dropping . So I don't have a {disfmarker} {vocalsound} I don't have a split , like which one helped more .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: So . It {disfmarker} it was the best result I could get .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , that 's the {disfmarker}&#10;Speaker: Professor B&#10;Content: So that">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="Based on the information provided by PhD E, the default system's performance is reported to be one percent lower than the best proposal. This comparison is made in terms of the system's overall quality or effectiveness; however, a specific metric or context for this assessment has not been detailed in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="1. The transcript states that using the default boundaries provided by the VAD system results in a fourteen percent improvement. However, by dropping the beginning and end frames of a speech, the group has observed an improvement of more than twenty percent. This is because they can improve the current VAD performance by more than twenty percent, while the other system's VAD only improves by fourteen percent.&#10;2. The method can potentially be further enhanced to exceed a twenty percent improvement. The group has found that using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, in some cases even dividing it by two. This approach takes advantage of the VAD probabilities obtained from the clean signal and applies them to noisy, far-field test utterances, leading to improved performance. The current VAD implementation adds some latency, but further development could potentially reduce this while maintaining or improving the error rate reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="1. When choosing an error method for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. On the other hand, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring. This results in a different mathematical treatment and potentially different optimization objectives for the filtering problem.&#10;2. Another key difference is that power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;3. The choice of error method depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="1. The process of the VAD (Voice Activity Detection) system involves determining the boundaries between speech and non-speech segments in a signal. In this discussion, they mention using the default boundaries provided by the system but observing that dropping the beginning and end frames of a speech results in an improvement of more than twenty percent, compared to another system's VAD performance improvement of only fourteen percent.&#10;&#10;2. The VAD system has a leeway of two hundred milliseconds at the beginning and end of speech, meaning it considers up to 200ms before and after actual speech as part of the detection. This leeway might contribute to the delay mentioned in the discussion since the VAD system takes additional time into account when detecting speech segments.&#10;&#10;3. The group mentions that reducing the delay of the VAD can effectively reduce the overall delay (transcript statement 3) and, as previously explained, dropping the beginning and end frames results in improved performance. Therefore, optimizing these boundaries is an essential part of managing the VAD's latency or delay.&#10;&#10;4. Additionally, using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, which also contributes to reducing latency or delay while maintaining or improving overall performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="1. The transcript does not provide specific information on exactly how much latency the VAD adds to the far-field test utterances when applying VAD probabilities computed on the clean signal. However, it is mentioned that this method significantly decreases the error rate and has potential for further improvement.&#10;   &#10;2. The significance of using a neural net with nine frames for the VAD is that it provides a balance between performance and latency constraints. There seems to be some debate within the group about the ideal length of latency allowed for the neural net, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The current implementation adds some latency, but further development could potentially reduce this while maintaining or improving error rate reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="1. The group has observed an improvement of more than twenty percent in speech recognition performance by dropping the beginning and end frames (pauses) of a speech compared to their current VAD system's performance. In contrast, the other system's VAD only showed a fourteen percent improvement with this method.&#10;   &#10;2. A more effective method for improving speech recognition accuracy could be using VAD probabilities computed from clean signals on far-field microphone data. This approach significantly decreases the error rate, in some cases even dividing it by two, and has potential for further enhancement. Additionally, optimizing the neural network used in the VAD system by training it on more data and adding better features could also improve its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="1. The amount of noise present in the signal: The more noise there is, relative to the desired speech signal, the greater the difference in power that needs to be addressed. Reducing a large difference in power may require more aggressive noise reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;2. Signal-to-Noise Ratio (SNR): A lower SNR indicates that there is more noise relative to the desired signal, making it harder to separate the two. This can affect the reduction process by requiring more sophisticated noise reduction techniques and a greater balance between noise reduction and signal distortion.&#10;&#10;3. Types of noise: Different types of noise (e.g., white noise, colored noise, non-stationary noise) can have varying effects on the difference in power that needs to be reduced. For example, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;4. Over-subtraction, Wiener filtering, and spectral subtraction: These factors can impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;5. Frame-dropping: This technique discards certain frames (small segments of audio) deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;6. Delta computations: Used for estimating the first derivative of a signal, avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;7. Noise estimation effectiveness: Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;8. Choice of error method: When constructing a filtering problem, the choice between using power spectra or signals directly can significantly affect the reduction process. Power spectra are more sensitive to noise but require additional robustness techniques, while working with signals directly offers more flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="1. The transcript does not provide a specific default parameter value for the software system that is one percent off from the best proposal. The closest information provided is that the default system's performance is one percent lower than the best proposal in terms of overall quality or effectiveness, but no actual metric or parameter value is given.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group discusses that their current VAD system has an improvement in performance of more than twenty percent compared to the other system, whose performance is at fourteen percent.&#10;2. Using VAD probabilities computed from clean signals on far-field microphone data can result in a significant decrease in error rate, in some cases even dividing it by two. This method takes advantage of the VAD probabilities obtained from the clean signal and applies them to the noisy, far-field test utterances, leading to improved performance." target="Yes, it is possible to use the same neural network for both voice activity detection (VAD) and noise estimation when training on cleaned-up speech features. This approach could simplify the system design by using a single network for multiple tasks. However, care must be taken to ensure that the shared network can effectively learn and differentiate between speech and noise in various conditions. Additionally, it is crucial to optimize the noise reduction techniques, such as over-subtraction, Wiener filtering, and spectral subtraction, to minimize distortion of the clean speech signal while maximizing noise reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source=" far - field microphone . And if we just take only the , um , VAD probabilities computed on the clean signal and apply them on the far - field , uh , test utterances , {vocalsound} then results are much better .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: In some cases it divides the error rate by two .&#10;Speaker: PhD A&#10;Content: Wow .&#10;Speaker: PhD E&#10;Content: So it means that there are stim {comment} still {disfmarker}&#10;Speaker: PhD A&#10;Content: How {disfmarker} how much latency does the , uh {disfmarker} does our VAD add ?&#10;Speaker: PhD E&#10;Content: If {disfmarker} if we can have a good VAD , well , it would be great .&#10;Speaker: PhD A&#10;Content: Is it significant ,&#10;Speaker: PhD E&#10;Content: Uh , right now it 's , um , a neural net with nine frames .&#10;Speaker: PhD A&#10;Content: or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: So it '" target="1. Yes, the idea of having one part of the feature vector that is highly discriminative and another part that is not (without processing the latter part through a neural net) still seems reasonable for vocalsound and VAD in the neural net setting. This is because it can help smooth things out when the testing set is different from what was trained, while also being aware of latency constraints.&#10;2. The exact amount of latency allowed for the neural net is still under debate, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The group aims to find a balance between the neural network's performance and the constraints on the allowable latency period.&#10;3. Using VAD probabilities computed on the clean signal and applying them to far-field test utterances has shown significant improvement, reducing error rates by up to two times in some cases. This suggests that there is still potential for improving the VAD system. The current VAD implementation adds some latency, but further development could potentially reduce this.">
      <data key="d0">1</data>
    </edge>
    <edge source=" far - field microphone . And if we just take only the , um , VAD probabilities computed on the clean signal and apply them on the far - field , uh , test utterances , {vocalsound} then results are much better .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: In some cases it divides the error rate by two .&#10;Speaker: PhD A&#10;Content: Wow .&#10;Speaker: PhD E&#10;Content: So it means that there are stim {comment} still {disfmarker}&#10;Speaker: PhD A&#10;Content: How {disfmarker} how much latency does the , uh {disfmarker} does our VAD add ?&#10;Speaker: PhD E&#10;Content: If {disfmarker} if we can have a good VAD , well , it would be great .&#10;Speaker: PhD A&#10;Content: Is it significant ,&#10;Speaker: PhD E&#10;Content: Uh , right now it 's , um , a neural net with nine frames .&#10;Speaker: PhD A&#10;Content: or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: So it '" target="1. The transcript does not provide specific information on exactly how much latency the VAD adds to the far-field test utterances when applying VAD probabilities computed on the clean signal. However, it is mentioned that this method significantly decreases the error rate and has potential for further improvement.&#10;   &#10;2. The significance of using a neural net with nine frames for the VAD is that it provides a balance between performance and latency constraints. There seems to be some debate within the group about the ideal length of latency allowed for the neural net, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The current implementation adds some latency, but further development could potentially reduce this while maintaining or improving error rate reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that with our current VAD we can improve by more than twenty percent .&#10;Speaker: PhD A&#10;Content: On top of the VAD that they provide ?&#10;Speaker: PhD C&#10;Content: No .&#10;Speaker: PhD E&#10;Content: Just using either their VAD or our current VAD .&#10;Speaker: PhD C&#10;Content: Our way .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD E&#10;Content: So , our current VAD is {disfmarker} is more than twenty percent , while their is fourteen .&#10;Speaker: PhD A&#10;Content: Theirs is fourteen ? I see .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Huh .&#10;Speaker: PhD E&#10;Content: So . Yeah . And {pause} another thing that we did also is that we have all this training data for {disfmarker} let 's say , for SpeechDat - Car . We have channel zero which is clean , channel one which is far - field microphone . And if we just take only the , um , VAD probabilities computed on the clean signal and apply them on the far - field ," target="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech.">
      <data key="d0">1</data>
    </edge>
    <edge source=" that with our current VAD we can improve by more than twenty percent .&#10;Speaker: PhD A&#10;Content: On top of the VAD that they provide ?&#10;Speaker: PhD C&#10;Content: No .&#10;Speaker: PhD E&#10;Content: Just using either their VAD or our current VAD .&#10;Speaker: PhD C&#10;Content: Our way .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD E&#10;Content: So , our current VAD is {disfmarker} is more than twenty percent , while their is fourteen .&#10;Speaker: PhD A&#10;Content: Theirs is fourteen ? I see .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Huh .&#10;Speaker: PhD E&#10;Content: So . Yeah . And {pause} another thing that we did also is that we have all this training data for {disfmarker} let 's say , for SpeechDat - Car . We have channel zero which is clean , channel one which is far - field microphone . And if we just take only the , um , VAD probabilities computed on the clean signal and apply them on the far - field ," target="1. The transcript states that using the default boundaries provided by the VAD system results in a fourteen percent improvement. However, by dropping the beginning and end frames of a speech, the group has observed an improvement of more than twenty percent. This is because they can improve the current VAD performance by more than twenty percent, while the other system's VAD only improves by fourteen percent.&#10;2. The method can potentially be further enhanced to exceed a twenty percent improvement. The group has found that using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, in some cases even dividing it by two. This approach takes advantage of the VAD probabilities obtained from the clean signal and applies them to noisy, far-field test utterances, leading to improved performance. The current VAD implementation adds some latency, but further development could potentially reduce this while maintaining or improving the error rate reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD C&#10;Content: So that {disfmarker} that VAD was trained on the noisy features .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , right now we have , like , uh {disfmarker} we have the cleaned - up features , so we can have a better VAD by training the net on {pause} the cleaned - up speech .&#10;Speaker: PhD A&#10;Content: Mm - hmm . I see . I see .&#10;Speaker: PhD C&#10;Content: Yeah , but we need a VAD for uh noise estimation also . So it 's , like , where do we want to put the VAD ? Uh , it 's like {disfmarker}&#10;Speaker: PhD A&#10;Content: Can you use the same net to do both , or {disfmarker} ?&#10;Speaker: PhD C&#10;Content: For {disfmarker}&#10;Speaker: PhD A&#10;Content: Can you use the same net that you {disfmarker} that I was talking about to do the VAD ?&#10;Speaker: PhD C&#10;Content: M" target="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD C&#10;Content: So that {disfmarker} that VAD was trained on the noisy features .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , right now we have , like , uh {disfmarker} we have the cleaned - up features , so we can have a better VAD by training the net on {pause} the cleaned - up speech .&#10;Speaker: PhD A&#10;Content: Mm - hmm . I see . I see .&#10;Speaker: PhD C&#10;Content: Yeah , but we need a VAD for uh noise estimation also . So it 's , like , where do we want to put the VAD ? Uh , it 's like {disfmarker}&#10;Speaker: PhD A&#10;Content: Can you use the same net to do both , or {disfmarker} ?&#10;Speaker: PhD C&#10;Content: For {disfmarker}&#10;Speaker: PhD A&#10;Content: Can you use the same net that you {disfmarker} that I was talking about to do the VAD ?&#10;Speaker: PhD C&#10;Content: M" target="Yes, it is possible to use the same neural network for both voice activity detection (VAD) and noise estimation when training on cleaned-up speech features. This approach could simplify the system design by using a single network for multiple tasks. However, care must be taken to ensure that the shared network can effectively learn and differentiate between speech and noise in various conditions. Additionally, it is crucial to optimize the noise reduction techniques, such as over-subtraction, Wiener filtering, and spectral subtraction, to minimize distortion of the clean speech signal while maximizing noise reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD C&#10;Content: So that {disfmarker} that VAD was trained on the noisy features .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , right now we have , like , uh {disfmarker} we have the cleaned - up features , so we can have a better VAD by training the net on {pause} the cleaned - up speech .&#10;Speaker: PhD A&#10;Content: Mm - hmm . I see . I see .&#10;Speaker: PhD C&#10;Content: Yeah , but we need a VAD for uh noise estimation also . So it 's , like , where do we want to put the VAD ? Uh , it 's like {disfmarker}&#10;Speaker: PhD A&#10;Content: Can you use the same net to do both , or {disfmarker} ?&#10;Speaker: PhD C&#10;Content: For {disfmarker}&#10;Speaker: PhD A&#10;Content: Can you use the same net that you {disfmarker} that I was talking about to do the VAD ?&#10;Speaker: PhD C&#10;Content: M" target="1. Based on the transcript provided, there is no clear description of a specific method that PhD C used to assess how well the noise estimation was doing. When Professor B asked if there was any experiment or way to assess how well or poorly the noise estimation is currently doing, PhD C mentioned an experiment where they used the channel zero VAD for noise estimation and frame-dropping, but they didn't have a comparison of which part helped more.&#10;2. Furthermore, when Professor B asked if there was any improvement in noise estimation due to addressing the frame-dropping issue specifically, no clear numerical answer was given in the conversation.&#10;3. Therefore, it can be inferred that while PhD C did perform some testing related to noise estimation (using channel zero VAD and frame-dropping), they did not specify a particular method or quantify the results of this assessment.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do you have any way of assessing how well or how poorly the noise estimation is currently doing ?&#10;Speaker: PhD E&#10;Content: Mmm . No , we don't .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: We don't have nothing {pause} that {disfmarker}&#10;Speaker: PhD C&#10;Content: Is there {disfmarker} was there any experiment with {disfmarker} ? Well , I {disfmarker} I did {disfmarker} The only experiment where I tried was I used the channel zero VAD for the noise estimation and frame - dropping . So I don't have a {disfmarker} {vocalsound} I don't have a split , like which one helped more .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: So . It {disfmarker} it was the best result I could get .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , that 's the {disfmarker}&#10;Speaker: Professor B&#10;Content: So that" target="1. Professor B is referring to specific FFT bins when applying a Wiener filter in the context of vocal sound (vocalsound) processing. However, the transcript does not provide exact bin numbers.&#10;2. Adding noise to vocal sound processing can be difficult because it requires finding the right level of noise to add without degrading the quality of the original sound. If the added noise is too loud, it can make the vocals hard to understand; if it's too soft, it might not effectively mask background sounds or hide sensitive information in the audio. Balancing these factors can be challenging and may require careful tuning.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do you have any way of assessing how well or how poorly the noise estimation is currently doing ?&#10;Speaker: PhD E&#10;Content: Mmm . No , we don't .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: We don't have nothing {pause} that {disfmarker}&#10;Speaker: PhD C&#10;Content: Is there {disfmarker} was there any experiment with {disfmarker} ? Well , I {disfmarker} I did {disfmarker} The only experiment where I tried was I used the channel zero VAD for the noise estimation and frame - dropping . So I don't have a {disfmarker} {vocalsound} I don't have a split , like which one helped more .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: So . It {disfmarker} it was the best result I could get .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , that 's the {disfmarker}&#10;Speaker: Professor B&#10;Content: So that" target="The use of silencing or stopping speech in the middle of communication is referred to as frame-dropping. Frame-dropping is a technique used in noise reduction systems to improve the quality of noisy speech by discarding certain frames (small segments of audio) deemed to be noise. This technique can be employed at different stages of the noise reduction process, and its use can impact the difference in power between clean and noisy speech.&#10;&#10;In the given transcript, the participants are discussing a specific system that utilizes frame-dropping as part of its noise reduction strategy. They mention that adjusting various factors, such as over-subtraction, Wiener filtering, and spectral subtraction, can help optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;The transcript also indicates a connection between frame-dropping and the use of delta computations in speech processing. Delta computations are used to estimate the first derivative of a signal, which can be helpful for detecting sudden changes or patterns within the signal. By avoiding delta computations early on and implementing frame-dropping as one of the last steps, the system aims to improve noise reduction performance further.&#10;&#10;Additionally, there is a discussion about assessing the effectiveness of noise estimation in the current system. However, it seems that they do not have any established method for evaluating the performance of their noise estimation technique.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do you have any way of assessing how well or how poorly the noise estimation is currently doing ?&#10;Speaker: PhD E&#10;Content: Mmm . No , we don't .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: We don't have nothing {pause} that {disfmarker}&#10;Speaker: PhD C&#10;Content: Is there {disfmarker} was there any experiment with {disfmarker} ? Well , I {disfmarker} I did {disfmarker} The only experiment where I tried was I used the channel zero VAD for the noise estimation and frame - dropping . So I don't have a {disfmarker} {vocalsound} I don't have a split , like which one helped more .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: So . It {disfmarker} it was the best result I could get .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , that 's the {disfmarker}&#10;Speaker: Professor B&#10;Content: So that" target="Based on the transcript provided, Professor C did not provide a specific quantification of how much of the improvement in results was due to addressing the frame-dropping issue as compared to other changes made. Professor B asked about this specifically, but the conversation moved on to other topics before any numerical answer was given.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do you have any way of assessing how well or how poorly the noise estimation is currently doing ?&#10;Speaker: PhD E&#10;Content: Mmm . No , we don't .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: We don't have nothing {pause} that {disfmarker}&#10;Speaker: PhD C&#10;Content: Is there {disfmarker} was there any experiment with {disfmarker} ? Well , I {disfmarker} I did {disfmarker} The only experiment where I tried was I used the channel zero VAD for the noise estimation and frame - dropping . So I don't have a {disfmarker} {vocalsound} I don't have a split , like which one helped more .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: So . It {disfmarker} it was the best result I could get .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , that 's the {disfmarker}&#10;Speaker: Professor B&#10;Content: So that" target="1. Based on the transcript provided, there is no clear description of a specific method that PhD C used to assess how well the noise estimation was doing. When Professor B asked if there was any experiment or way to assess how well or poorly the noise estimation is currently doing, PhD C mentioned an experiment where they used the channel zero VAD for noise estimation and frame-dropping, but they didn't have a comparison of which part helped more.&#10;2. Furthermore, when Professor B asked if there was any improvement in noise estimation due to addressing the frame-dropping issue specifically, no clear numerical answer was given in the conversation.&#10;3. Therefore, it can be inferred that while PhD C did perform some testing related to noise estimation (using channel zero VAD and frame-dropping), they did not specify a particular method or quantify the results of this assessment.">
      <data key="d0">1</data>
    </edge>
    <edge source=" do you have any way of assessing how well or how poorly the noise estimation is currently doing ?&#10;Speaker: PhD E&#10;Content: Mmm . No , we don't .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: We don't have nothing {pause} that {disfmarker}&#10;Speaker: PhD C&#10;Content: Is there {disfmarker} was there any experiment with {disfmarker} ? Well , I {disfmarker} I did {disfmarker} The only experiment where I tried was I used the channel zero VAD for the noise estimation and frame - dropping . So I don't have a {disfmarker} {vocalsound} I don't have a split , like which one helped more .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: So . It {disfmarker} it was the best result I could get .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , that 's the {disfmarker}&#10;Speaker: Professor B&#10;Content: So that" target="Based on the provided transcript, the exact impact of the frame-dropping problem on the improvement seen in the final result cannot be quantified. When Professor B asked how much of the change was due to the frame-dropping problem specifically, PhD C did not provide a clear numerical answer. While they mentioned that addressing the frame-dropping issue was part of the improvements made, there is no specific quantification of its contribution compared to other changes. Additionally, when discussing noise estimation assessment methods, PhD C mentioned an experiment using channel zero VAD for noise estimation and frame-dropping but did not compare which part helped more or provide a clear measure of improvement in noise estimation due to addressing the frame-dropping issue.&#10;&#10;In summary, while the transcript suggests that addressing the frame-dropping problem contributed to the observed improvements, it does not offer specific quantification of its impact or how much of the change can be attributed to this issue.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target="disfmarker} so if we {disfmarker} if {disfmarker} so which is like if we reduce the delay of VA So , the f the final delay 's now ba is f determined by the delay of the VAD , because the LDA doesn't have any delay . So if we re if we reduce the delay of the VAD , I mean , it 's like effectively reducing the delay .&#10;Speaker: PhD A&#10;Content: How {disfmarker} how much , uh , delay was there on the LDA ?&#10;Speaker: PhD C&#10;Content: So the LDA and the VAD both had a hundred millisecond delay . So and they were in parallel , so which means you pick either one of them {disfmarker}&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: PhD C&#10;Content: the {disfmarker} the biggest , whatever .&#10;Speaker: PhD A&#10;Content: I see .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So , right now the LDA delays are more .&#10;Speaker: Professor B&#10;Content: And">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target=" makes reference to delay .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So what 's the {disfmarker} ? If you ignore {disfmarker} Um , the VAD is sort of in {disfmarker} in parallel , isn't i isn't it , with {disfmarker} with the {disfmarker} ? I mean , it isn't additive with the {disfmarker} the , uh , LDA and the Wiener filtering , and so forth .&#10;Speaker: PhD C&#10;Content: The LDA ?&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD C&#10;Content: Yeah . So {disfmarker} so what happened right now , we removed the delay of the LDA .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: So we {disfmarker} I mean , if {disfmarker} so if we {disfmarker} if {disfmarker} so which is like if we reduce the delay of VA">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target=": on the R .&#10;Speaker: PhD E&#10;Content: Yeah . It 's not a median filtering . It 's just {disfmarker} We don't take the median value . We take something {disfmarker} Um , so we have eleven , um , frames .&#10;Speaker: Professor B&#10;Content: Oh , this is for the VAD .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: And {disfmarker} for the VAD , yeah {disfmarker}&#10;Speaker: Professor B&#10;Content: Oh , OK .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: and we take th the third .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Grad D&#10;Content: Dar&#10;Speaker: PhD E&#10;Content: Um .&#10;Speaker: Professor B&#10;Content: Yeah . Um . So {disfmarker} {comment} Yeah , I was just noticing on this that it makes reference to delay .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So what 's the {">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target="mm .&#10;Speaker: PhD C&#10;Content: So , right now the LDA delays are more .&#10;Speaker: Professor B&#10;Content: And there {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: Professor B&#10;Content: And there didn't seem to be any , uh , penalty for that ? There didn't seem to be any penalty for making it causal ?&#10;Speaker: PhD C&#10;Content: Pardon ? Oh , no . It actually made it , like , point one percent better or something , actually .&#10;Speaker: Professor B&#10;Content: OK . Well , may as well , then .&#10;Speaker: PhD C&#10;Content: Or something like that&#10;Speaker: Professor B&#10;Content: And he says Wiener filter is {disfmarker} is forty milliseconds delay .&#10;Speaker: PhD C&#10;Content: and {disfmarker}&#10;Speaker: Professor B&#10;Content: So is it {disfmarker} ?&#10;Speaker: PhD C&#10;Content: Yeah . So that 's the one which Stephane was discussing , like {disfmarker}&#10;Speaker: PhD E&#10;Content: Mmm">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target=" the same net that you {disfmarker} that I was talking about to do the VAD ?&#10;Speaker: PhD C&#10;Content: Mm - hmm . Uh , it actually comes at v at the very end .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: So the net {disfmarker} the final net {disfmarker} I mean , which is the feature net {disfmarker} so that actually comes after a chain of , like , LDA plus everything . So it 's , like , it takes a long time to get a decision out of it . And {disfmarker} {vocalsound} and you can actually do it for final frame - dropping , but not for the VA - f noise estimation .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: You see , the idea is that the , um , initial decision to {disfmarker} that {disfmarker} that you 're in silence or speech happens pretty quickly .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target="1. The disagreement regarding the amount of latency allowed for the neural net concerns the duration that should be used before processing speech data through the network. Some parties are advocating for a shorter length, while others suggest either 130 or 250 milliseconds.&#10;2. The previous standard was 250 milliseconds, but some groups are now lobbying to make it shorter. Meanwhile, another group is arguing for a 130-millisecond latency period.&#10;3. The exact amount of latency that will be allowed has not been determined yet, as the debate between various parties is ongoing. It is crucial to find a balance between the neural network's performance and the constraints on the allowable latency period.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target="Based on the information provided by PhD E, the default system's performance is reported to be one percent lower than the best proposal. This comparison is made in terms of the system's overall quality or effectiveness; however, a specific metric or context for this assessment has not been detailed in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target="1. The transcript states that using the default boundaries provided by the VAD system results in a fourteen percent improvement. However, by dropping the beginning and end frames of a speech, the group has observed an improvement of more than twenty percent. This is because they can improve the current VAD performance by more than twenty percent, while the other system's VAD only improves by fourteen percent.&#10;2. The method can potentially be further enhanced to exceed a twenty percent improvement. The group has found that using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, in some cases even dividing it by two. This approach takes advantage of the VAD probabilities obtained from the clean signal and applies them to noisy, far-field test utterances, leading to improved performance. The current VAD implementation adds some latency, but further development could potentially reduce this while maintaining or improving the error rate reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target="1. The process of the VAD (Voice Activity Detection) system involves determining the boundaries between speech and non-speech segments in a signal. In this discussion, they mention using the default boundaries provided by the system but observing that dropping the beginning and end frames of a speech results in an improvement of more than twenty percent, compared to another system's VAD performance improvement of only fourteen percent.&#10;&#10;2. The VAD system has a leeway of two hundred milliseconds at the beginning and end of speech, meaning it considers up to 200ms before and after actual speech as part of the detection. This leeway might contribute to the delay mentioned in the discussion since the VAD system takes additional time into account when detecting speech segments.&#10;&#10;3. The group mentions that reducing the delay of the VAD can effectively reduce the overall delay (transcript statement 3) and, as previously explained, dropping the beginning and end frames results in improved performance. Therefore, optimizing these boundaries is an essential part of managing the VAD's latency or delay.&#10;&#10;4. Additionally, using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, which also contributes to reducing latency or delay while maintaining or improving overall performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target="Based on the transcript provided, Professor C did not provide a specific quantification of how much of the improvement in results was due to addressing the frame-dropping issue as compared to other changes made. Professor B asked about this specifically, but the conversation moved on to other topics before any numerical answer was given.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target="1. The transcript does not provide specific information on exactly how much latency the VAD adds to the far-field test utterances when applying VAD probabilities computed on the clean signal. However, it is mentioned that this method significantly decreases the error rate and has potential for further improvement.&#10;   &#10;2. The significance of using a neural net with nine frames for the VAD is that it provides a balance between performance and latency constraints. There seems to be some debate within the group about the ideal length of latency allowed for the neural net, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The current implementation adds some latency, but further development could potentially reduce this while maintaining or improving error rate reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target="1. The transcript does not provide a specific default parameter value for the software system that is one percent off from the best proposal. The closest information provided is that the default system's performance is one percent lower than the best proposal in terms of overall quality or effectiveness, but no actual metric or parameter value is given.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Both the VAD and LDA had a hundred milliseconds (ms) delay each, as mentioned in the conversation between PhD A and PhD C: &quot;So the LDA and the VAD both had a hundred millisecond delay.&quot;&#10;2. The final delay is now determined by the delay of the VAD instead of the LDA because the LDA delays are currently longer than those of the VAD, as stated by Professor B and PhD C: &quot;So right now, the LDA delays are more&quot; and &quot;And there didn't seem to be any penalty for that? There didn't seem to be any penalty for making it causal.&quot;&#10;3. Additionally, PhD C mentioned that reducing the delay of VAD would effectively reduce the overall delay: &quot;so if we reduce the delay of the VAD, I mean, it's like effectively reducing the delay.&quot;" target="1. Based on the transcript provided, there is no clear description of a specific method that PhD C used to assess how well the noise estimation was doing. When Professor B asked if there was any experiment or way to assess how well or poorly the noise estimation is currently doing, PhD C mentioned an experiment where they used the channel zero VAD for noise estimation and frame-dropping, but they didn't have a comparison of which part helped more.&#10;2. Furthermore, when Professor B asked if there was any improvement in noise estimation due to addressing the frame-dropping issue specifically, no clear numerical answer was given in the conversation.&#10;3. Therefore, it can be inferred that while PhD C did perform some testing related to noise estimation (using channel zero VAD and frame-dropping), they did not specify a particular method or quantify the results of this assessment.">
      <data key="d0">1</data>
    </edge>
    <edge source=" makes reference to delay .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So what 's the {disfmarker} ? If you ignore {disfmarker} Um , the VAD is sort of in {disfmarker} in parallel , isn't i isn't it , with {disfmarker} with the {disfmarker} ? I mean , it isn't additive with the {disfmarker} the , uh , LDA and the Wiener filtering , and so forth .&#10;Speaker: PhD C&#10;Content: The LDA ?&#10;Speaker: Professor B&#10;Content: Right ?&#10;Speaker: PhD C&#10;Content: Yeah . So {disfmarker} so what happened right now , we removed the delay of the LDA .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: So we {disfmarker} I mean , if {disfmarker} so if we {disfmarker} if {disfmarker} so which is like if we reduce the delay of VA" target="1. The process of the VAD (Voice Activity Detection) system involves determining the boundaries between speech and non-speech segments in a signal. In this discussion, they mention using the default boundaries provided by the system but observing that dropping the beginning and end frames of a speech results in an improvement of more than twenty percent, compared to another system's VAD performance improvement of only fourteen percent.&#10;&#10;2. The VAD system has a leeway of two hundred milliseconds at the beginning and end of speech, meaning it considers up to 200ms before and after actual speech as part of the detection. This leeway might contribute to the delay mentioned in the discussion since the VAD system takes additional time into account when detecting speech segments.&#10;&#10;3. The group mentions that reducing the delay of the VAD can effectively reduce the overall delay (transcript statement 3) and, as previously explained, dropping the beginning and end frames results in improved performance. Therefore, optimizing these boundaries is an essential part of managing the VAD's latency or delay.&#10;&#10;4. Additionally, using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, which also contributes to reducing latency or delay while maintaining or improving overall performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The disagreement regarding the amount of latency allowed for the neural net concerns the duration that should be used before processing speech data through the network. Some parties are advocating for a shorter length, while others suggest either 130 or 250 milliseconds.&#10;2. The previous standard was 250 milliseconds, but some groups are now lobbying to make it shorter. Meanwhile, another group is arguing for a 130-millisecond latency period.&#10;3. The exact amount of latency that will be allowed has not been determined yet, as the debate between various parties is ongoing. It is crucial to find a balance between the neural network's performance and the constraints on the allowable latency period." target=" So there 's the neural net issue . There 's the VAD issue . And , uh , there 's the second stream {pause} thing . And I think those that we {disfmarker} last time we agreed that those are the three things that have to get , uh , focused on .&#10;Speaker: PhD A&#10;Content: What was the issue with the VAD ?&#10;Speaker: Professor B&#10;Content: Well , better {comment} ones are good .&#10;Speaker: PhD A&#10;Content: And so the w the default , uh , boundaries that they provide are {disfmarker} they 're OK , but they 're not all that great ?&#10;Speaker: Professor B&#10;Content: I guess they still allow two hundred milliseconds on either side or some ? Is that what the deal is ?&#10;Speaker: PhD E&#10;Content: Mm - hmm . Uh , so th um , they keep two hundred milliseconds at the beginning and end of speech . And they keep all the {disfmarker}&#10;Speaker: PhD A&#10;Content: Outside the beginnings and end .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh -">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The disagreement regarding the amount of latency allowed for the neural net concerns the duration that should be used before processing speech data through the network. Some parties are advocating for a shorter length, while others suggest either 130 or 250 milliseconds.&#10;2. The previous standard was 250 milliseconds, but some groups are now lobbying to make it shorter. Meanwhile, another group is arguing for a 130-millisecond latency period.&#10;3. The exact amount of latency that will be allowed has not been determined yet, as the debate between various parties is ongoing. It is crucial to find a balance between the neural network's performance and the constraints on the allowable latency period." target="marker} cuz it certainly always helped us before . So .&#10;Speaker: PhD A&#10;Content: What amount of latency are you thinking about when you say that ?&#10;Speaker: Professor B&#10;Content: Uh . Well , they 're {disfmarker} you know , they 're disputing it .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: You know , they 're saying , uh {disfmarker} one group is saying a hundred and thirty milliseconds and another group is saying two hundred and fifty milliseconds . Two hundred and fifty is what it was before actually . So ,&#10;Speaker: PhD A&#10;Content: Oh .&#10;Speaker: Professor B&#10;Content: uh , some people are lobbying {disfmarker} lobbying {comment} to make it shorter .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: Um . And , um .&#10;Speaker: PhD A&#10;Content: Were you thinking of the two - fifty or the one - thirty when you said we should {pause} have enough for the neural net ?&#10;Speaker: Professor B&#10;Content: Well , it just {d">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The disagreement regarding the amount of latency allowed for the neural net concerns the duration that should be used before processing speech data through the network. Some parties are advocating for a shorter length, while others suggest either 130 or 250 milliseconds.&#10;2. The previous standard was 250 milliseconds, but some groups are now lobbying to make it shorter. Meanwhile, another group is arguing for a 130-millisecond latency period.&#10;3. The exact amount of latency that will be allowed has not been determined yet, as the debate between various parties is ongoing. It is crucial to find a balance between the neural network's performance and the constraints on the allowable latency period." target=" , as I recall , was that {vocalsound} you 'd have one part of the feature vector that was very discriminant and another part that wasn't ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: uh , which would smooth things a bit for those occasions when , uh , the testing set was quite different than what you 'd trained your discriminant features for . So , um , all of that is {disfmarker} is , uh {disfmarker} still seems like a good idea . The thing is now we know some other constraints . We can't have unlimited amounts of latency . Uh , y you know , that 's still being debated by the {disfmarker} by people in Europe but , {vocalsound} uh , no matter how they end up there , it 's not going to be unlimited amounts ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: so we have to be a little conscious of that . Um . So there 's the neural net issue . There 's the VAD issue . And , uh , there 's the second stream {pause} thing .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The disagreement regarding the amount of latency allowed for the neural net concerns the duration that should be used before processing speech data through the network. Some parties are advocating for a shorter length, while others suggest either 130 or 250 milliseconds.&#10;2. The previous standard was 250 milliseconds, but some groups are now lobbying to make it shorter. Meanwhile, another group is arguing for a 130-millisecond latency period.&#10;3. The exact amount of latency that will be allowed has not been determined yet, as the debate between various parties is ongoing. It is crucial to find a balance between the neural network's performance and the constraints on the allowable latency period." target="Based on the information provided by PhD E, the default system's performance is reported to be one percent lower than the best proposal. This comparison is made in terms of the system's overall quality or effectiveness; however, a specific metric or context for this assessment has not been detailed in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The disagreement regarding the amount of latency allowed for the neural net concerns the duration that should be used before processing speech data through the network. Some parties are advocating for a shorter length, while others suggest either 130 or 250 milliseconds.&#10;2. The previous standard was 250 milliseconds, but some groups are now lobbying to make it shorter. Meanwhile, another group is arguing for a 130-millisecond latency period.&#10;3. The exact amount of latency that will be allowed has not been determined yet, as the debate between various parties is ongoing. It is crucial to find a balance between the neural network's performance and the constraints on the allowable latency period." target="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The disagreement regarding the amount of latency allowed for the neural net concerns the duration that should be used before processing speech data through the network. Some parties are advocating for a shorter length, while others suggest either 130 or 250 milliseconds.&#10;2. The previous standard was 250 milliseconds, but some groups are now lobbying to make it shorter. Meanwhile, another group is arguing for a 130-millisecond latency period.&#10;3. The exact amount of latency that will be allowed has not been determined yet, as the debate between various parties is ongoing. It is crucial to find a balance between the neural network's performance and the constraints on the allowable latency period." target="1. Yes, the idea of having one part of the feature vector that is highly discriminative and another part that is not (without processing the latter part through a neural net) still seems reasonable for vocalsound and VAD in the neural net setting. This is because it can help smooth things out when the testing set is different from what was trained, while also being aware of latency constraints.&#10;2. The exact amount of latency allowed for the neural net is still under debate, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The group aims to find a balance between the neural network's performance and the constraints on the allowable latency period.&#10;3. Using VAD probabilities computed on the clean signal and applying them to far-field test utterances has shown significant improvement, reducing error rates by up to two times in some cases. This suggests that there is still potential for improving the VAD system. The current VAD implementation adds some latency, but further development could potentially reduce this.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The disagreement regarding the amount of latency allowed for the neural net concerns the duration that should be used before processing speech data through the network. Some parties are advocating for a shorter length, while others suggest either 130 or 250 milliseconds.&#10;2. The previous standard was 250 milliseconds, but some groups are now lobbying to make it shorter. Meanwhile, another group is arguing for a 130-millisecond latency period.&#10;3. The exact amount of latency that will be allowed has not been determined yet, as the debate between various parties is ongoing. It is crucial to find a balance between the neural network's performance and the constraints on the allowable latency period." target="1. When determining whether the size or latency of a neural network is more important, there are a few factors to consider. If the neural network is running on the server side, its size may not be as much of a concern due to the available resources. However, latency remains an issue that needs to be addressed regardless of where the neural network is deployed. The ability to process data using only past information is one potential solution being considered, as it could help reduce latency.&#10;   &#10;2. In the past, the group has used neural networks to transform all features used for processing. This early transformation aims to improve the overall performance of their system. However, they have not yet explored the possibility of using a neural network that only looks at the past, which could further help reduce latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The disagreement regarding the amount of latency allowed for the neural net concerns the duration that should be used before processing speech data through the network. Some parties are advocating for a shorter length, while others suggest either 130 or 250 milliseconds.&#10;2. The previous standard was 250 milliseconds, but some groups are now lobbying to make it shorter. Meanwhile, another group is arguing for a 130-millisecond latency period.&#10;3. The exact amount of latency that will be allowed has not been determined yet, as the debate between various parties is ongoing. It is crucial to find a balance between the neural network's performance and the constraints on the allowable latency period." target="1. Mark Frames Before They Reach the Server: Instead of dropping unnecessary frames after they have been transmitted to the server, it is suggested to mark them beforehand. This can be done by analyzing the audio data locally and adding a marker or flag to indicate whether a frame should be dropped or kept.&#10;&#10;2. Use Additional Bits for Differentiation: To better handle speech and non-speech frames on the server side, additional bits can be used in the transmission protocol. These extra bits can serve as flags that differentiate between speech and non-speech frames. This way, the server can process only the necessary frames without having to drop any of them after transmission, thus saving bandwidth and reducing latency.&#10;&#10;3. Improve Neural Network Performance: Training the neural network on more data and incorporating better features and noise reduction techniques during training can help enhance its performance in recognizing speech and differentiating it from non-speech. This would result in fewer unnecessary frames being transmitted, thus reducing the load on the server.">
      <data key="d0">1</data>
    </edge>
    <edge source=" So there 's the neural net issue . There 's the VAD issue . And , uh , there 's the second stream {pause} thing . And I think those that we {disfmarker} last time we agreed that those are the three things that have to get , uh , focused on .&#10;Speaker: PhD A&#10;Content: What was the issue with the VAD ?&#10;Speaker: Professor B&#10;Content: Well , better {comment} ones are good .&#10;Speaker: PhD A&#10;Content: And so the w the default , uh , boundaries that they provide are {disfmarker} they 're OK , but they 're not all that great ?&#10;Speaker: Professor B&#10;Content: I guess they still allow two hundred milliseconds on either side or some ? Is that what the deal is ?&#10;Speaker: PhD E&#10;Content: Mm - hmm . Uh , so th um , they keep two hundred milliseconds at the beginning and end of speech . And they keep all the {disfmarker}&#10;Speaker: PhD A&#10;Content: Outside the beginnings and end .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh -" target="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , as I recall , was that {vocalsound} you 'd have one part of the feature vector that was very discriminant and another part that wasn't ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: uh , which would smooth things a bit for those occasions when , uh , the testing set was quite different than what you 'd trained your discriminant features for . So , um , all of that is {disfmarker} is , uh {disfmarker} still seems like a good idea . The thing is now we know some other constraints . We can't have unlimited amounts of latency . Uh , y you know , that 's still being debated by the {disfmarker} by people in Europe but , {vocalsound} uh , no matter how they end up there , it 's not going to be unlimited amounts ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: so we have to be a little conscious of that . Um . So there 's the neural net issue . There 's the VAD issue . And , uh , there 's the second stream {pause} thing ." target="1. Yes, the idea of having one part of the feature vector that is highly discriminative and another part that is not (without processing the latter part through a neural net) still seems reasonable for vocalsound and VAD in the neural net setting. This is because it can help smooth things out when the testing set is different from what was trained, while also being aware of latency constraints.&#10;2. The exact amount of latency allowed for the neural net is still under debate, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The group aims to find a balance between the neural network's performance and the constraints on the allowable latency period.&#10;3. Using VAD probabilities computed on the clean signal and applying them to far-field test utterances has shown significant improvement, reducing error rates by up to two times in some cases. This suggests that there is still potential for improving the VAD system. The current VAD implementation adds some latency, but further development could potentially reduce this.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the information provided by PhD E, the default system's performance is reported to be one percent lower than the best proposal. This comparison is made in terms of the system's overall quality or effectiveness; however, a specific metric or context for this assessment has not been detailed in the conversation." target="Content: Right .&#10;Speaker: PhD E&#10;Content: Well , if we want to , like , optimize different parameters of {disfmarker}&#10;Speaker: PhD C&#10;Content: Parameters . Yeah .&#10;Speaker: Professor B&#10;Content: Sure .&#10;Speaker: PhD E&#10;Content: Yeah , we can do it later . But , still {disfmarker} so , there will be a piece of software with , {vocalsound} {vocalsound} uh , will give this system , the fifty - three point sixty - six , by default and {disfmarker}&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: How {disfmarker} how is {disfmarker} how good is that ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I {disfmarker} I {disfmarker} I don't have a sense of {disfmarker}&#10;Speaker: PhD E&#10;Content: It 's just one percent off of the {pause} best proposal .&#10;Speaker: PhD C&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the information provided by PhD E, the default system's performance is reported to be one percent lower than the best proposal. This comparison is made in terms of the system's overall quality or effectiveness; however, a specific metric or context for this assessment has not been detailed in the conversation." target="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method. &#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Specifically, they agreed to test and compare the noise estimation techniques of spectral subtraction and Wiener filtering, with the aim of improving the noise estimation while maintaining the same overall method.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the information provided by PhD E, the default system's performance is reported to be one percent lower than the best proposal. This comparison is made in terms of the system's overall quality or effectiveness; however, a specific metric or context for this assessment has not been detailed in the conversation." target="1. The transcript does not provide a specific default parameter value for the software system that is one percent off from the best proposal. The closest information provided is that the default system's performance is one percent lower than the best proposal in terms of overall quality or effectiveness, but no actual metric or parameter value is given.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Right .&#10;Speaker: PhD E&#10;Content: Well , if we want to , like , optimize different parameters of {disfmarker}&#10;Speaker: PhD C&#10;Content: Parameters . Yeah .&#10;Speaker: Professor B&#10;Content: Sure .&#10;Speaker: PhD E&#10;Content: Yeah , we can do it later . But , still {disfmarker} so , there will be a piece of software with , {vocalsound} {vocalsound} uh , will give this system , the fifty - three point sixty - six , by default and {disfmarker}&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: How {disfmarker} how is {disfmarker} how good is that ?&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I {disfmarker} I {disfmarker} I don't have a sense of {disfmarker}&#10;Speaker: PhD E&#10;Content: It 's just one percent off of the {pause} best proposal .&#10;Speaker: PhD C&#10;Content:" target="1. The transcript does not provide a specific default parameter value for the software system that is one percent off from the best proposal. The closest information provided is that the default system's performance is one percent lower than the best proposal in terms of overall quality or effectiveness, but no actual metric or parameter value is given.">
      <data key="d0">1</data>
    </edge>
    <edge source="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal." target="disfmarker} the , uh , um , {vocalsound} looking at it the other way , you 're gonna be dealing with signals&#10;Speaker: PhD C&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: and you 're gonna end up looking at power {disfmarker} uh , noise power that you 're trying to reduce . And so , eh {disfmarker} so there should be a difference {vocalsound} of {disfmarker} you know , conceptually of {disfmarker} of , uh , a factor of two in the exponent .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: But there 're so many different little factors that you adjust in terms of {disfmarker} of , uh , {vocalsound} uh , over - subtraction and {disfmarker} and {disfmarker} and {disfmarker} and {disfmarker} and so forth , um , that {vocalsound} arguably , you 're c and {disfmarker} and {disfmarker} and">
      <data key="d0">1</data>
    </edge>
    <edge source="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal." target="1. When choosing an error method for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. On the other hand, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring. This results in a different mathematical treatment and potentially different optimization objectives for the filtering problem.&#10;2. Another key difference is that power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;3. The choice of error method depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand.">
      <data key="d0">1</data>
    </edge>
    <edge source="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal." target="The use of silencing or stopping speech in the middle of communication is referred to as frame-dropping. Frame-dropping is a technique used in noise reduction systems to improve the quality of noisy speech by discarding certain frames (small segments of audio) deemed to be noise. This technique can be employed at different stages of the noise reduction process, and its use can impact the difference in power between clean and noisy speech.&#10;&#10;In the given transcript, the participants are discussing a specific system that utilizes frame-dropping as part of its noise reduction strategy. They mention that adjusting various factors, such as over-subtraction, Wiener filtering, and spectral subtraction, can help optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;The transcript also indicates a connection between frame-dropping and the use of delta computations in speech processing. Delta computations are used to estimate the first derivative of a signal, which can be helpful for detecting sudden changes or patterns within the signal. By avoiding delta computations early on and implementing frame-dropping as one of the last steps, the system aims to improve noise reduction performance further.&#10;&#10;Additionally, there is a discussion about assessing the effectiveness of noise estimation in the current system. However, it seems that they do not have any established method for evaluating the performance of their noise estimation technique.">
      <data key="d0">1</data>
    </edge>
    <edge source="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal." target="1. The amount of noise present in the signal: The more noise there is, relative to the desired speech signal, the greater the difference in power that needs to be addressed. Reducing a large difference in power may require more aggressive noise reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;2. Signal-to-Noise Ratio (SNR): A lower SNR indicates that there is more noise relative to the desired signal, making it harder to separate the two. This can affect the reduction process by requiring more sophisticated noise reduction techniques and a greater balance between noise reduction and signal distortion.&#10;&#10;3. Types of noise: Different types of noise (e.g., white noise, colored noise, non-stationary noise) can have varying effects on the difference in power that needs to be reduced. For example, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;4. Over-subtraction, Wiener filtering, and spectral subtraction: These factors can impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;5. Frame-dropping: This technique discards certain frames (small segments of audio) deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;6. Delta computations: Used for estimating the first derivative of a signal, avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;7. Noise estimation effectiveness: Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;8. Choice of error method: When constructing a filtering problem, the choice between using power spectra or signals directly can significantly affect the reduction process. Power spectra are more sensitive to noise but require additional robustness techniques, while working with signals directly offers more flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem.">
      <data key="d0">1</data>
    </edge>
    <edge source="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal." target="Yes, it is possible to use the same neural network for both voice activity detection (VAD) and noise estimation when training on cleaned-up speech features. This approach could simplify the system design by using a single network for multiple tasks. However, care must be taken to ensure that the shared network can effectively learn and differentiate between speech and noise in various conditions. Additionally, it is crucial to optimize the noise reduction techniques, such as over-subtraction, Wiener filtering, and spectral subtraction, to minimize distortion of the clean speech signal while maximizing noise reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal." target="1. Conceptual differences in power and noise reduction factors between spectral subtraction and Wiener filtering:&#10;&#10;Spectral Subtraction: This method aims to reduce the noise by creating an estimate of the noise spectrum and then subtracting it from the observed signal's spectrum. The difference is then transformed back into the time domain, resulting in a noise-reduced signal. However, this process may introduce musical noise due to underestimation or overestimation of the noise spectrum.&#10;&#10;Wiener Filtering: Wiener filtering uses statistical properties of both the speech and noise signals to create an optimal filter for noise reduction while minimizing distortion. The filter takes into account the power spectral densities (PSD) of clean speech and noise, as well as their cross-power spectral density (CPSD). By finding the ratio of CPSD to the PSD of noise, the Wiener filter estimates the linear time-invariant system that minimizes the mean square error between the estimated clean signal and the actual clean signal.&#10;&#10;2. Specific &quot;little factors&quot; affecting the differences in practice:&#10;&#10;The amount of noise present in the signal (1): A higher level of noise requires more aggressive reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;Signal-to-Noise Ratio (SNR) (2): Lower SNR values make it harder to separate the desired signal from the noise, leading to a greater balance between noise reduction and signal distortion. More sophisticated techniques may be required for successful separation.&#10;&#10;Types of noise (3): Different noises can affect the difference in power that needs to be reduced. For instance, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;Over-subtraction, Wiener filtering, and spectral subtraction (4): These factors impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;Frame-dropping (5): This technique discards certain frames deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;Delta computations (6): Avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;Noise estimation effectiveness (7): Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;Choice of error method (8): The choice between using power spectra or signals directly in the filtering problem can significantly affect the reduction process, as each method has different sensitivities to noise and flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem.">
      <data key="d0">1</data>
    </edge>
    <edge source="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal." target="1. The initial step taken to enhance speech involves applying a technique similar to spectral enhancement or noise reduction on specific FFT (Fast Fourier Transform) bins, which results in creating new FFT's with some noise removed. This process is discussed by Professor B when referring to creating &quot;new FFT's that...have some of the noise removed&quot; before doing further processing to produce various features.">
      <data key="d0">1</data>
    </edge>
    <edge source="The conceptual difference in power between clean and noisy speech is a factor of two in the exponent. However, there are many adjustable factors involved in noise reduction techniques such as over-subtraction, Wiener filtering, and spectral subtraction that can impact this difference. These factors can be adjusted to optimize noise reduction while minimizing distortion of the clean speech signal." target="When choosing an error metric for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. In contrast, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring.&#10;&#10;The choice of error metric depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;Power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B is referring to specific FFT bins when applying a Wiener filter in the context of vocal sound (vocalsound) processing. However, the transcript does not provide exact bin numbers.&#10;2. Adding noise to vocal sound processing can be difficult because it requires finding the right level of noise to add without degrading the quality of the original sound. If the added noise is too loud, it can make the vocals hard to understand; if it's too soft, it might not effectively mask background sounds or hide sensitive information in the audio. Balancing these factors can be challenging and may require careful tuning." target=" is {vocalsound} when we apply this procedure on FFT bins , uh , with a Wiener filter .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: And there is no noise addition after {disfmarker} after that .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD E&#10;Content: So it 's good because {vocalsound} {vocalsound} it 's difficult when we have to add noise to {disfmarker} to {disfmarker} to find the right level .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Are you looking at one in {disfmarker} in particular of these two ?&#10;Speaker: PhD E&#10;Content: Yeah . So the sh it 's the sheet that gives fifty - f three point sixty - six .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Um , {vocalsound} the second sheet is abo uh , about the same . It 's the same , um , idea but it 's working on">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B is referring to specific FFT bins when applying a Wiener filter in the context of vocal sound (vocalsound) processing. However, the transcript does not provide exact bin numbers.&#10;2. Adding noise to vocal sound processing can be difficult because it requires finding the right level of noise to add without degrading the quality of the original sound. If the added noise is too loud, it can make the vocals hard to understand; if it's too soft, it might not effectively mask background sounds or hide sensitive information in the audio. Balancing these factors can be challenging and may require careful tuning." target=": That 's the best thing .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: So , tell me about it .&#10;Speaker: PhD E&#10;Content: So it 's {disfmarker} well , it 's {pause} spectral subtraction or Wiener filtering , um , depending on if we put {disfmarker} if we square the transfer function or not .&#10;Speaker: Professor B&#10;Content: Right .&#10;Speaker: PhD E&#10;Content: And then with over - estimation of the noise , depending on the , uh {disfmarker} the SNR , with smoothing along time , um , smoothing along frequency .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: It 's very simple , smoothing things .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: And , um , {vocalsound} the best result is {vocalsound} when we apply this procedure on FFT bins , uh , with a Wiener filter .&#10;Speaker: Professor B&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B is referring to specific FFT bins when applying a Wiener filter in the context of vocal sound (vocalsound) processing. However, the transcript does not provide exact bin numbers.&#10;2. Adding noise to vocal sound processing can be difficult because it requires finding the right level of noise to add without degrading the quality of the original sound. If the added noise is too loud, it can make the vocals hard to understand; if it's too soft, it might not effectively mask background sounds or hide sensitive information in the audio. Balancing these factors can be challenging and may require careful tuning." target=" {vocalsound} the second sheet is abo uh , about the same . It 's the same , um , idea but it 's working on mel bands , {vocalsound} and it 's a spectral subtraction instead of Wiener filter , and there is also a noise addition after , uh , cleaning up the mel bins . Mmm . Well , the results are similar .&#10;Speaker: Professor B&#10;Content: Yeah . I mean , {vocalsound} it 's {disfmarker} {comment} it 's actually , uh , very similar .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: I mean , {vocalsound} if you look at databases , uh , the , uh , one that has the smallest {disfmarker} smaller overall number is actually better on the Finnish and Spanish , uh , but it is , uh , worse on the , uh , Aurora {disfmarker}&#10;Speaker: PhD E&#10;Content: It 's worse on {disfmarker}&#10;Speaker: Professor B&#10;Content: I mean on the , uh , TI - TI - digits ,&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B is referring to specific FFT bins when applying a Wiener filter in the context of vocal sound (vocalsound) processing. However, the transcript does not provide exact bin numbers.&#10;2. Adding noise to vocal sound processing can be difficult because it requires finding the right level of noise to add without degrading the quality of the original sound. If the added noise is too loud, it can make the vocals hard to understand; if it's too soft, it might not effectively mask background sounds or hide sensitive information in the audio. Balancing these factors can be challenging and may require careful tuning." target=" . But the Guenter 's argument is slightly different . It 's , like , ev even {disfmarker} even if I use a channel zero VAD , I 'm just averaging the {disfmarker} {vocalsound} the s power spectrum . But the Guenter 's argument is , like , if it is a non - stationary {pause} segment , then he doesn't update the noise spectrum . So he 's , like {disfmarker} he tries to capture only the stationary part in it . So the averaging is , like , {vocalsound} different from {pause} updating the noise spectrum only during stationary segments . So , th the Guenter was arguing that , I mean , even if you have a very good VAD , averaging it , like , over the whole thing is not a good idea .&#10;Speaker: Professor B&#10;Content: I see .&#10;Speaker: PhD C&#10;Content: Because you 're averaging the stationary and the non - stationary , and finally you end up getting something which is not really the s because , you {disfmarker} anyway , you can't remove the stationary part fr I mean , non - stationary part from {vocalsound}">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B is referring to specific FFT bins when applying a Wiener filter in the context of vocal sound (vocalsound) processing. However, the transcript does not provide exact bin numbers.&#10;2. Adding noise to vocal sound processing can be difficult because it requires finding the right level of noise to add without degrading the quality of the original sound. If the added noise is too loud, it can make the vocals hard to understand; if it's too soft, it might not effectively mask background sounds or hide sensitive information in the audio. Balancing these factors can be challenging and may require careful tuning." target=" Mm - hmm .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Professor B&#10;Content: So it {disfmarker} so , um , it 's {disfmarker} it it 's not using our full bal bag of tricks , if you will .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And , uh , and it {disfmarker} it is , uh , very close in performance to the best thing that was there before . Uh , but , you know , looking at it another way , maybe more importantly , uh , {vocalsound} we didn't have any explicit noise , uh , handling {disfmarker} stationary {disfmarker} dealing with {disfmarker} e e we didn't explicitly have anything to deal with stationary noise .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: And now we do .&#10;Speaker: PhD A&#10;Content: So will the {pause} neural net operate on the output from either the Wiener filtering or the spectral subtraction ? Or will it operate on the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B is referring to specific FFT bins when applying a Wiener filter in the context of vocal sound (vocalsound) processing. However, the transcript does not provide exact bin numbers.&#10;2. Adding noise to vocal sound processing can be difficult because it requires finding the right level of noise to add without degrading the quality of the original sound. If the added noise is too loud, it can make the vocals hard to understand; if it's too soft, it might not effectively mask background sounds or hide sensitive information in the audio. Balancing these factors can be challenging and may require careful tuning." target="1. When choosing an error method for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. On the other hand, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring. This results in a different mathematical treatment and potentially different optimization objectives for the filtering problem.&#10;2. Another key difference is that power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;3. The choice of error method depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B is referring to specific FFT bins when applying a Wiener filter in the context of vocal sound (vocalsound) processing. However, the transcript does not provide exact bin numbers.&#10;2. Adding noise to vocal sound processing can be difficult because it requires finding the right level of noise to add without degrading the quality of the original sound. If the added noise is too loud, it can make the vocals hard to understand; if it's too soft, it might not effectively mask background sounds or hide sensitive information in the audio. Balancing these factors can be challenging and may require careful tuning." target="1. The initial step taken to enhance speech involves applying a technique similar to spectral enhancement or noise reduction on specific FFT (Fast Fourier Transform) bins, which results in creating new FFT's with some noise removed. This process is discussed by Professor B when referring to creating &quot;new FFT's that...have some of the noise removed&quot; before doing further processing to produce various features.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Professor B is referring to specific FFT bins when applying a Wiener filter in the context of vocal sound (vocalsound) processing. However, the transcript does not provide exact bin numbers.&#10;2. Adding noise to vocal sound processing can be difficult because it requires finding the right level of noise to add without degrading the quality of the original sound. If the added noise is too loud, it can make the vocals hard to understand; if it's too soft, it might not effectively mask background sounds or hide sensitive information in the audio. Balancing these factors can be challenging and may require careful tuning." target="When choosing an error metric for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. In contrast, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring.&#10;&#10;The choice of error metric depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;Power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Yes, the idea of having one part of the feature vector that is highly discriminative and another part that is not (without processing the latter part through a neural net) still seems reasonable for vocalsound and VAD in the neural net setting. This is because it can help smooth things out when the testing set is different from what was trained, while also being aware of latency constraints.&#10;2. The exact amount of latency allowed for the neural net is still under debate, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The group aims to find a balance between the neural network's performance and the constraints on the allowable latency period.&#10;3. Using VAD probabilities computed on the clean signal and applying them to far-field test utterances has shown significant improvement, reducing error rates by up to two times in some cases. This suggests that there is still potential for improving the VAD system. The current VAD implementation adds some latency, but further development could potentially reduce this." target="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Yes, the idea of having one part of the feature vector that is highly discriminative and another part that is not (without processing the latter part through a neural net) still seems reasonable for vocalsound and VAD in the neural net setting. This is because it can help smooth things out when the testing set is different from what was trained, while also being aware of latency constraints.&#10;2. The exact amount of latency allowed for the neural net is still under debate, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The group aims to find a balance between the neural network's performance and the constraints on the allowable latency period.&#10;3. Using VAD probabilities computed on the clean signal and applying them to far-field test utterances has shown significant improvement, reducing error rates by up to two times in some cases. This suggests that there is still potential for improving the VAD system. The current VAD implementation adds some latency, but further development could potentially reduce this." target="1. The transcript states that using the default boundaries provided by the VAD system results in a fourteen percent improvement. However, by dropping the beginning and end frames of a speech, the group has observed an improvement of more than twenty percent. This is because they can improve the current VAD performance by more than twenty percent, while the other system's VAD only improves by fourteen percent.&#10;2. The method can potentially be further enhanced to exceed a twenty percent improvement. The group has found that using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, in some cases even dividing it by two. This approach takes advantage of the VAD probabilities obtained from the clean signal and applies them to noisy, far-field test utterances, leading to improved performance. The current VAD implementation adds some latency, but further development could potentially reduce this while maintaining or improving the error rate reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Yes, the idea of having one part of the feature vector that is highly discriminative and another part that is not (without processing the latter part through a neural net) still seems reasonable for vocalsound and VAD in the neural net setting. This is because it can help smooth things out when the testing set is different from what was trained, while also being aware of latency constraints.&#10;2. The exact amount of latency allowed for the neural net is still under debate, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The group aims to find a balance between the neural network's performance and the constraints on the allowable latency period.&#10;3. Using VAD probabilities computed on the clean signal and applying them to far-field test utterances has shown significant improvement, reducing error rates by up to two times in some cases. This suggests that there is still potential for improving the VAD system. The current VAD implementation adds some latency, but further development could potentially reduce this." target="1. When determining whether the size or latency of a neural network is more important, there are a few factors to consider. If the neural network is running on the server side, its size may not be as much of a concern due to the available resources. However, latency remains an issue that needs to be addressed regardless of where the neural network is deployed. The ability to process data using only past information is one potential solution being considered, as it could help reduce latency.&#10;   &#10;2. In the past, the group has used neural networks to transform all features used for processing. This early transformation aims to improve the overall performance of their system. However, they have not yet explored the possibility of using a neural network that only looks at the past, which could further help reduce latency.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Yes, the idea of having one part of the feature vector that is highly discriminative and another part that is not (without processing the latter part through a neural net) still seems reasonable for vocalsound and VAD in the neural net setting. This is because it can help smooth things out when the testing set is different from what was trained, while also being aware of latency constraints.&#10;2. The exact amount of latency allowed for the neural net is still under debate, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The group aims to find a balance between the neural network's performance and the constraints on the allowable latency period.&#10;3. Using VAD probabilities computed on the clean signal and applying them to far-field test utterances has shown significant improvement, reducing error rates by up to two times in some cases. This suggests that there is still potential for improving the VAD system. The current VAD implementation adds some latency, but further development could potentially reduce this." target="1. The process of the VAD (Voice Activity Detection) system involves determining the boundaries between speech and non-speech segments in a signal. In this discussion, they mention using the default boundaries provided by the system but observing that dropping the beginning and end frames of a speech results in an improvement of more than twenty percent, compared to another system's VAD performance improvement of only fourteen percent.&#10;&#10;2. The VAD system has a leeway of two hundred milliseconds at the beginning and end of speech, meaning it considers up to 200ms before and after actual speech as part of the detection. This leeway might contribute to the delay mentioned in the discussion since the VAD system takes additional time into account when detecting speech segments.&#10;&#10;3. The group mentions that reducing the delay of the VAD can effectively reduce the overall delay (transcript statement 3) and, as previously explained, dropping the beginning and end frames results in improved performance. Therefore, optimizing these boundaries is an essential part of managing the VAD's latency or delay.&#10;&#10;4. Additionally, using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, which also contributes to reducing latency or delay while maintaining or improving overall performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Yes, the idea of having one part of the feature vector that is highly discriminative and another part that is not (without processing the latter part through a neural net) still seems reasonable for vocalsound and VAD in the neural net setting. This is because it can help smooth things out when the testing set is different from what was trained, while also being aware of latency constraints.&#10;2. The exact amount of latency allowed for the neural net is still under debate, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The group aims to find a balance between the neural network's performance and the constraints on the allowable latency period.&#10;3. Using VAD probabilities computed on the clean signal and applying them to far-field test utterances has shown significant improvement, reducing error rates by up to two times in some cases. This suggests that there is still potential for improving the VAD system. The current VAD implementation adds some latency, but further development could potentially reduce this." target="1. The transcript does not provide specific information on exactly how much latency the VAD adds to the far-field test utterances when applying VAD probabilities computed on the clean signal. However, it is mentioned that this method significantly decreases the error rate and has potential for further improvement.&#10;   &#10;2. The significance of using a neural net with nine frames for the VAD is that it provides a balance between performance and latency constraints. There seems to be some debate within the group about the ideal length of latency allowed for the neural net, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The current implementation adds some latency, but further development could potentially reduce this while maintaining or improving error rate reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech." target="1. The transcript states that using the default boundaries provided by the VAD system results in a fourteen percent improvement. However, by dropping the beginning and end frames of a speech, the group has observed an improvement of more than twenty percent. This is because they can improve the current VAD performance by more than twenty percent, while the other system's VAD only improves by fourteen percent.&#10;2. The method can potentially be further enhanced to exceed a twenty percent improvement. The group has found that using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, in some cases even dividing it by two. This approach takes advantage of the VAD probabilities obtained from the clean signal and applies them to noisy, far-field test utterances, leading to improved performance. The current VAD implementation adds some latency, but further development could potentially reduce this while maintaining or improving the error rate reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech." target="1. The process of the VAD (Voice Activity Detection) system involves determining the boundaries between speech and non-speech segments in a signal. In this discussion, they mention using the default boundaries provided by the system but observing that dropping the beginning and end frames of a speech results in an improvement of more than twenty percent, compared to another system's VAD performance improvement of only fourteen percent.&#10;&#10;2. The VAD system has a leeway of two hundred milliseconds at the beginning and end of speech, meaning it considers up to 200ms before and after actual speech as part of the detection. This leeway might contribute to the delay mentioned in the discussion since the VAD system takes additional time into account when detecting speech segments.&#10;&#10;3. The group mentions that reducing the delay of the VAD can effectively reduce the overall delay (transcript statement 3) and, as previously explained, dropping the beginning and end frames results in improved performance. Therefore, optimizing these boundaries is an essential part of managing the VAD's latency or delay.&#10;&#10;4. Additionally, using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, which also contributes to reducing latency or delay while maintaining or improving overall performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech." target="1. The group has observed an improvement of more than twenty percent in speech recognition performance by dropping the beginning and end frames (pauses) of a speech compared to their current VAD system's performance. In contrast, the other system's VAD only showed a fourteen percent improvement with this method.&#10;   &#10;2. A more effective method for improving speech recognition accuracy could be using VAD probabilities computed from clean signals on far-field microphone data. This approach significantly decreases the error rate, in some cases even dividing it by two, and has potential for further enhancement. Additionally, optimizing the neural network used in the VAD system by training it on more data and adding better features could also improve its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech." target="1. Mark Frames Before They Reach the Server: Instead of dropping unnecessary frames after they have been transmitted to the server, it is suggested to mark them beforehand. This can be done by analyzing the audio data locally and adding a marker or flag to indicate whether a frame should be dropped or kept.&#10;&#10;2. Use Additional Bits for Differentiation: To better handle speech and non-speech frames on the server side, additional bits can be used in the transmission protocol. These extra bits can serve as flags that differentiate between speech and non-speech frames. This way, the server can process only the necessary frames without having to drop any of them after transmission, thus saving bandwidth and reducing latency.&#10;&#10;3. Improve Neural Network Performance: Training the neural network on more data and incorporating better features and noise reduction techniques during training can help enhance its performance in recognizing speech and differentiating it from non-speech. This would result in fewer unnecessary frames being transmitted, thus reducing the load on the server.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech." target="Yes, it is possible to use the same neural network for both voice activity detection (VAD) and noise estimation when training on cleaned-up speech features. This approach could simplify the system design by using a single network for multiple tasks. However, care must be taken to ensure that the shared network can effectively learn and differentiate between speech and noise in various conditions. Additionally, it is crucial to optimize the noise reduction techniques, such as over-subtraction, Wiener filtering, and spectral subtraction, to minimize distortion of the clean speech signal while maximizing noise reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The specific issue with the VAD (Voice Activity Detection) mentioned is that the default boundaries provided by the system are not optimal. They work well enough but can be improved.&#10;2. The VAD allows a leeway of two hundred milliseconds at the beginning and end of speech. This means that when detecting speech, it considers as part of the speech up to 200ms before and after actual speech." target="1. The initial step taken to enhance speech involves applying a technique similar to spectral enhancement or noise reduction on specific FFT (Fast Fourier Transform) bins, which results in creating new FFT's with some noise removed. This process is discussed by Professor B when referring to creating &quot;new FFT's that...have some of the noise removed&quot; before doing further processing to produce various features.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The transcript states that using the default boundaries provided by the VAD system results in a fourteen percent improvement. However, by dropping the beginning and end frames of a speech, the group has observed an improvement of more than twenty percent. This is because they can improve the current VAD performance by more than twenty percent, while the other system's VAD only improves by fourteen percent.&#10;2. The method can potentially be further enhanced to exceed a twenty percent improvement. The group has found that using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, in some cases even dividing it by two. This approach takes advantage of the VAD probabilities obtained from the clean signal and applies them to noisy, far-field test utterances, leading to improved performance. The current VAD implementation adds some latency, but further development could potentially reduce this while maintaining or improving the error rate reduction." target=" A&#10;Content: Outside the beginnings and end .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD E&#10;Content: And all the speech pauses , which is {disfmarker} Sometimes on the SpeechDat - Car you have pauses that are more than one or two seconds .&#10;Speaker: PhD A&#10;Content: Wow .&#10;Speaker: PhD E&#10;Content: More than one second for sure . Um .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: PhD E&#10;Content: Yeah . And , yeah , it seems to us that this way of just dropping the beginning and end is not {disfmarker} We cou we can do better , I think ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: because , um , {vocalsound} with this way of dropping the frames they improve {pause} over the baseline by fourteen percent and {vocalsound} Sunil already showed that with our current VAD we can improve by more than twenty percent .&#10;Speaker: PhD A&#10;Content: On top of the VAD that they">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The transcript states that using the default boundaries provided by the VAD system results in a fourteen percent improvement. However, by dropping the beginning and end frames of a speech, the group has observed an improvement of more than twenty percent. This is because they can improve the current VAD performance by more than twenty percent, while the other system's VAD only improves by fourteen percent.&#10;2. The method can potentially be further enhanced to exceed a twenty percent improvement. The group has found that using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, in some cases even dividing it by two. This approach takes advantage of the VAD probabilities obtained from the clean signal and applies them to noisy, far-field test utterances, leading to improved performance. The current VAD implementation adds some latency, but further development could potentially reduce this while maintaining or improving the error rate reduction." target="1. The process of the VAD (Voice Activity Detection) system involves determining the boundaries between speech and non-speech segments in a signal. In this discussion, they mention using the default boundaries provided by the system but observing that dropping the beginning and end frames of a speech results in an improvement of more than twenty percent, compared to another system's VAD performance improvement of only fourteen percent.&#10;&#10;2. The VAD system has a leeway of two hundred milliseconds at the beginning and end of speech, meaning it considers up to 200ms before and after actual speech as part of the detection. This leeway might contribute to the delay mentioned in the discussion since the VAD system takes additional time into account when detecting speech segments.&#10;&#10;3. The group mentions that reducing the delay of the VAD can effectively reduce the overall delay (transcript statement 3) and, as previously explained, dropping the beginning and end frames results in improved performance. Therefore, optimizing these boundaries is an essential part of managing the VAD's latency or delay.&#10;&#10;4. Additionally, using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, which also contributes to reducing latency or delay while maintaining or improving overall performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The transcript states that using the default boundaries provided by the VAD system results in a fourteen percent improvement. However, by dropping the beginning and end frames of a speech, the group has observed an improvement of more than twenty percent. This is because they can improve the current VAD performance by more than twenty percent, while the other system's VAD only improves by fourteen percent.&#10;2. The method can potentially be further enhanced to exceed a twenty percent improvement. The group has found that using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, in some cases even dividing it by two. This approach takes advantage of the VAD probabilities obtained from the clean signal and applies them to noisy, far-field test utterances, leading to improved performance. The current VAD implementation adds some latency, but further development could potentially reduce this while maintaining or improving the error rate reduction." target="1. The transcript does not provide specific information on exactly how much latency the VAD adds to the far-field test utterances when applying VAD probabilities computed on the clean signal. However, it is mentioned that this method significantly decreases the error rate and has potential for further improvement.&#10;   &#10;2. The significance of using a neural net with nine frames for the VAD is that it provides a balance between performance and latency constraints. There seems to be some debate within the group about the ideal length of latency allowed for the neural net, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The current implementation adds some latency, but further development could potentially reduce this while maintaining or improving error rate reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The transcript states that using the default boundaries provided by the VAD system results in a fourteen percent improvement. However, by dropping the beginning and end frames of a speech, the group has observed an improvement of more than twenty percent. This is because they can improve the current VAD performance by more than twenty percent, while the other system's VAD only improves by fourteen percent.&#10;2. The method can potentially be further enhanced to exceed a twenty percent improvement. The group has found that using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, in some cases even dividing it by two. This approach takes advantage of the VAD probabilities obtained from the clean signal and applies them to noisy, far-field test utterances, leading to improved performance. The current VAD implementation adds some latency, but further development could potentially reduce this while maintaining or improving the error rate reduction." target="1. The group has observed an improvement of more than twenty percent in speech recognition performance by dropping the beginning and end frames (pauses) of a speech compared to their current VAD system's performance. In contrast, the other system's VAD only showed a fourteen percent improvement with this method.&#10;   &#10;2. A more effective method for improving speech recognition accuracy could be using VAD probabilities computed from clean signals on far-field microphone data. This approach significantly decreases the error rate, in some cases even dividing it by two, and has potential for further enhancement. Additionally, optimizing the neural network used in the VAD system by training it on more data and adding better features could also improve its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When choosing an error method for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. On the other hand, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring. This results in a different mathematical treatment and potentially different optimization objectives for the filtering problem.&#10;2. Another key difference is that power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;3. The choice of error method depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand." target=" B&#10;Content: And , uh , I mean , fundamentally they 're the same sort of thing but the math is a little different so that there 's a {disfmarker} a {disfmarker} {vocalsound} there 's an exponent difference in the index {disfmarker} you know , what 's the ideal filtering , and depending on how you construct the problem .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: And , uh , I guess it 's sort {disfmarker} you know , after {disfmarker} after that meeting it sort of made more sense to me because {vocalsound} um , if you 're dealing with power spectra then how are you gonna choose your error ? And typically you 'll do {disfmarker} choose something like a variance . And so that means it 'll be something like the square of the power spectra . Whereas when you 're {disfmarker} when you 're doing the {disfmarker} the , uh , um , {vocalsound} looking at it the other way , you 're gonna be dealing with signals&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When choosing an error method for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. On the other hand, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring. This results in a different mathematical treatment and potentially different optimization objectives for the filtering problem.&#10;2. Another key difference is that power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;3. The choice of error method depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand." target="1. The amount of noise present in the signal: The more noise there is, relative to the desired speech signal, the greater the difference in power that needs to be addressed. Reducing a large difference in power may require more aggressive noise reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;2. Signal-to-Noise Ratio (SNR): A lower SNR indicates that there is more noise relative to the desired signal, making it harder to separate the two. This can affect the reduction process by requiring more sophisticated noise reduction techniques and a greater balance between noise reduction and signal distortion.&#10;&#10;3. Types of noise: Different types of noise (e.g., white noise, colored noise, non-stationary noise) can have varying effects on the difference in power that needs to be reduced. For example, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;4. Over-subtraction, Wiener filtering, and spectral subtraction: These factors can impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;5. Frame-dropping: This technique discards certain frames (small segments of audio) deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;6. Delta computations: Used for estimating the first derivative of a signal, avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;7. Noise estimation effectiveness: Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;8. Choice of error method: When constructing a filtering problem, the choice between using power spectra or signals directly can significantly affect the reduction process. Power spectra are more sensitive to noise but require additional robustness techniques, while working with signals directly offers more flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When choosing an error method for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. On the other hand, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring. This results in a different mathematical treatment and potentially different optimization objectives for the filtering problem.&#10;2. Another key difference is that power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;3. The choice of error method depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand." target="1. Conceptual differences in power and noise reduction factors between spectral subtraction and Wiener filtering:&#10;&#10;Spectral Subtraction: This method aims to reduce the noise by creating an estimate of the noise spectrum and then subtracting it from the observed signal's spectrum. The difference is then transformed back into the time domain, resulting in a noise-reduced signal. However, this process may introduce musical noise due to underestimation or overestimation of the noise spectrum.&#10;&#10;Wiener Filtering: Wiener filtering uses statistical properties of both the speech and noise signals to create an optimal filter for noise reduction while minimizing distortion. The filter takes into account the power spectral densities (PSD) of clean speech and noise, as well as their cross-power spectral density (CPSD). By finding the ratio of CPSD to the PSD of noise, the Wiener filter estimates the linear time-invariant system that minimizes the mean square error between the estimated clean signal and the actual clean signal.&#10;&#10;2. Specific &quot;little factors&quot; affecting the differences in practice:&#10;&#10;The amount of noise present in the signal (1): A higher level of noise requires more aggressive reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;Signal-to-Noise Ratio (SNR) (2): Lower SNR values make it harder to separate the desired signal from the noise, leading to a greater balance between noise reduction and signal distortion. More sophisticated techniques may be required for successful separation.&#10;&#10;Types of noise (3): Different noises can affect the difference in power that needs to be reduced. For instance, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;Over-subtraction, Wiener filtering, and spectral subtraction (4): These factors impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;Frame-dropping (5): This technique discards certain frames deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;Delta computations (6): Avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;Noise estimation effectiveness (7): Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;Choice of error method (8): The choice between using power spectra or signals directly in the filtering problem can significantly affect the reduction process, as each method has different sensitivities to noise and flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When choosing an error method for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. On the other hand, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring. This results in a different mathematical treatment and potentially different optimization objectives for the filtering problem.&#10;2. Another key difference is that power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;3. The choice of error method depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand." target="When choosing an error metric for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. In contrast, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring.&#10;&#10;The choice of error metric depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;Power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand.">
      <data key="d0">1</data>
    </edge>
    <edge source=" B&#10;Content: And , uh , I mean , fundamentally they 're the same sort of thing but the math is a little different so that there 's a {disfmarker} a {disfmarker} {vocalsound} there 's an exponent difference in the index {disfmarker} you know , what 's the ideal filtering , and depending on how you construct the problem .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Professor B&#10;Content: And , uh , I guess it 's sort {disfmarker} you know , after {disfmarker} after that meeting it sort of made more sense to me because {vocalsound} um , if you 're dealing with power spectra then how are you gonna choose your error ? And typically you 'll do {disfmarker} choose something like a variance . And so that means it 'll be something like the square of the power spectra . Whereas when you 're {disfmarker} when you 're doing the {disfmarker} the , uh , um , {vocalsound} looking at it the other way , you 're gonna be dealing with signals&#10;" target="When choosing an error metric for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. In contrast, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring.&#10;&#10;The choice of error metric depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;Power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand.">
      <data key="d0">1</data>
    </edge>
    <edge source="The plan for Professor Hynek's visit is to have him join a meeting with the team to discuss and firm up the direction of their project before his subsequent trip to Europe. Professor B wants to finalize the plans before Hynek's departure because they aim to have a clear path for the time he will be gone, specifying what things will be attended to during his absence.&#10;&#10;The group is currently focusing on further testing and improving noise estimation in both spectral subtraction and Wiener filtering methods for noise suppression. They plan to incorporate a neural net into their workflow later in the month." target=" that our goal should be by next week , when Hynek comes back , {vocalsound} uh , to {disfmarker} uh , really just to have a firm path , uh , for the {disfmarker} you know , for the time he 's gone , of {disfmarker} of , uh , what things will be attacked . But I would {disfmarker} I would {disfmarker} I would thought think that what we would wanna do is not futz with this stuff for a while because what 'll happen is we 'll change many other things in the system ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: and then we 'll probably wanna come back to this and possibly make some other choices . But , um .&#10;Speaker: PhD A&#10;Content: But just conceptually , where does the neural net go ? Do {disfmarker} do you wanna h run it on the output of the spectrally subtracted {disfmarker} ?&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: Well , depending on its size">
      <data key="d0">1</data>
    </edge>
    <edge source="The plan for Professor Hynek's visit is to have him join a meeting with the team to discuss and firm up the direction of their project before his subsequent trip to Europe. Professor B wants to finalize the plans before Hynek's departure because they aim to have a clear path for the time he will be gone, specifying what things will be attended to during his absence.&#10;&#10;The group is currently focusing on further testing and improving noise estimation in both spectral subtraction and Wiener filtering methods for noise suppression. They plan to incorporate a neural net into their workflow later in the month." target="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method. &#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Specifically, they agreed to test and compare the noise estimation techniques of spectral subtraction and Wiener filtering, with the aim of improving the noise estimation while maintaining the same overall method.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of the VAD (Voice Activity Detection) system involves determining the boundaries between speech and non-speech segments in a signal. In this discussion, they mention using the default boundaries provided by the system but observing that dropping the beginning and end frames of a speech results in an improvement of more than twenty percent, compared to another system's VAD performance improvement of only fourteen percent.&#10;&#10;2. The VAD system has a leeway of two hundred milliseconds at the beginning and end of speech, meaning it considers up to 200ms before and after actual speech as part of the detection. This leeway might contribute to the delay mentioned in the discussion since the VAD system takes additional time into account when detecting speech segments.&#10;&#10;3. The group mentions that reducing the delay of the VAD can effectively reduce the overall delay (transcript statement 3) and, as previously explained, dropping the beginning and end frames results in improved performance. Therefore, optimizing these boundaries is an essential part of managing the VAD's latency or delay.&#10;&#10;4. Additionally, using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, which also contributes to reducing latency or delay while maintaining or improving overall performance." target="1. The transcript does not provide specific information on exactly how much latency the VAD adds to the far-field test utterances when applying VAD probabilities computed on the clean signal. However, it is mentioned that this method significantly decreases the error rate and has potential for further improvement.&#10;   &#10;2. The significance of using a neural net with nine frames for the VAD is that it provides a balance between performance and latency constraints. There seems to be some debate within the group about the ideal length of latency allowed for the neural net, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The current implementation adds some latency, but further development could potentially reduce this while maintaining or improving error rate reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The process of the VAD (Voice Activity Detection) system involves determining the boundaries between speech and non-speech segments in a signal. In this discussion, they mention using the default boundaries provided by the system but observing that dropping the beginning and end frames of a speech results in an improvement of more than twenty percent, compared to another system's VAD performance improvement of only fourteen percent.&#10;&#10;2. The VAD system has a leeway of two hundred milliseconds at the beginning and end of speech, meaning it considers up to 200ms before and after actual speech as part of the detection. This leeway might contribute to the delay mentioned in the discussion since the VAD system takes additional time into account when detecting speech segments.&#10;&#10;3. The group mentions that reducing the delay of the VAD can effectively reduce the overall delay (transcript statement 3) and, as previously explained, dropping the beginning and end frames results in improved performance. Therefore, optimizing these boundaries is an essential part of managing the VAD's latency or delay.&#10;&#10;4. Additionally, using VAD probabilities computed from clean signals on far-field microphone data significantly decreases the error rate, which also contributes to reducing latency or delay while maintaining or improving overall performance." target="1. The group has observed an improvement of more than twenty percent in speech recognition performance by dropping the beginning and end frames (pauses) of a speech compared to their current VAD system's performance. In contrast, the other system's VAD only showed a fourteen percent improvement with this method.&#10;   &#10;2. A more effective method for improving speech recognition accuracy could be using VAD probabilities computed from clean signals on far-field microphone data. This approach significantly decreases the error rate, in some cases even dividing it by two, and has potential for further enhancement. Additionally, optimizing the neural network used in the VAD system by training it on more data and adding better features could also improve its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="The use of silencing or stopping speech in the middle of communication is referred to as frame-dropping. Frame-dropping is a technique used in noise reduction systems to improve the quality of noisy speech by discarding certain frames (small segments of audio) deemed to be noise. This technique can be employed at different stages of the noise reduction process, and its use can impact the difference in power between clean and noisy speech.&#10;&#10;In the given transcript, the participants are discussing a specific system that utilizes frame-dropping as part of its noise reduction strategy. They mention that adjusting various factors, such as over-subtraction, Wiener filtering, and spectral subtraction, can help optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;The transcript also indicates a connection between frame-dropping and the use of delta computations in speech processing. Delta computations are used to estimate the first derivative of a signal, which can be helpful for detecting sudden changes or patterns within the signal. By avoiding delta computations early on and implementing frame-dropping as one of the last steps, the system aims to improve noise reduction performance further.&#10;&#10;Additionally, there is a discussion about assessing the effectiveness of noise estimation in the current system. However, it seems that they do not have any established method for evaluating the performance of their noise estimation technique." target="er} that you 're in silence or speech happens pretty quickly .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: PhD C&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: Cuz that 's used by some of these other {disfmarker} ?&#10;Speaker: Professor B&#10;Content: And that {disfmarker} Yeah . And that 's sort of fed forward , and {disfmarker} and you say &quot; well , flush everything , it 's not speech anymore &quot; .&#10;Speaker: PhD A&#10;Content: Oh , OK . I see .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: I thought that was only used for doing frame - dropping later on .&#10;Speaker: Professor B&#10;Content: Um , it is used , uh {disfmarker} Yeah , it 's only used f Well , it 's used for frame - dropping . Um , it 's used for end of utterance&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: because , you know , there 's {disfmarker} {voc">
      <data key="d0">1</data>
    </edge>
    <edge source="The use of silencing or stopping speech in the middle of communication is referred to as frame-dropping. Frame-dropping is a technique used in noise reduction systems to improve the quality of noisy speech by discarding certain frames (small segments of audio) deemed to be noise. This technique can be employed at different stages of the noise reduction process, and its use can impact the difference in power between clean and noisy speech.&#10;&#10;In the given transcript, the participants are discussing a specific system that utilizes frame-dropping as part of its noise reduction strategy. They mention that adjusting various factors, such as over-subtraction, Wiener filtering, and spectral subtraction, can help optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;The transcript also indicates a connection between frame-dropping and the use of delta computations in speech processing. Delta computations are used to estimate the first derivative of a signal, which can be helpful for detecting sudden changes or patterns within the signal. By avoiding delta computations early on and implementing frame-dropping as one of the last steps, the system aims to improve noise reduction performance further.&#10;&#10;Additionally, there is a discussion about assessing the effectiveness of noise estimation in the current system. However, it seems that they do not have any established method for evaluating the performance of their noise estimation technique." target=" - dropping .&#10;Speaker: Grad D&#10;Content: S&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: The earlier system was do the frame - dropping and then compute the delta on the {disfmarker}&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: PhD C&#10;Content: So this {disfmarker}&#10;Speaker: PhD A&#10;Content: Which could be a kind of a funny delta . Right ?&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor B&#10;Content: Oh , oh . So that 's fixed in this . Yeah , we talked about that .&#10;Speaker: PhD C&#10;Content: Yeah . So we have no delta . And then {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah . Uh - huh .&#10;Speaker: Professor B&#10;Content: Good .&#10;Speaker: PhD C&#10;Content: So the frame - dropping is the last thing that we do . So , yeah , what we do is we compute the silence probability , convert it to that binary flag ,&#10;Speaker: Professor B&#10;Content: Uh - huh .">
      <data key="d0">1</data>
    </edge>
    <edge source="The use of silencing or stopping speech in the middle of communication is referred to as frame-dropping. Frame-dropping is a technique used in noise reduction systems to improve the quality of noisy speech by discarding certain frames (small segments of audio) deemed to be noise. This technique can be employed at different stages of the noise reduction process, and its use can impact the difference in power between clean and noisy speech.&#10;&#10;In the given transcript, the participants are discussing a specific system that utilizes frame-dropping as part of its noise reduction strategy. They mention that adjusting various factors, such as over-subtraction, Wiener filtering, and spectral subtraction, can help optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;The transcript also indicates a connection between frame-dropping and the use of delta computations in speech processing. Delta computations are used to estimate the first derivative of a signal, which can be helpful for detecting sudden changes or patterns within the signal. By avoiding delta computations early on and implementing frame-dropping as one of the last steps, the system aims to improve noise reduction performance further.&#10;&#10;Additionally, there is a discussion about assessing the effectiveness of noise estimation in the current system. However, it seems that they do not have any established method for evaluating the performance of their noise estimation technique." target=" - dropping problem ? What about this ?&#10;Speaker: PhD C&#10;Content: Uh , y you had something on it . Right ?&#10;Speaker: PhD E&#10;Content: Just the frame - dropping problem . Yeah . But it 's {disfmarker} it 's difficult . Sometime we {disfmarker} we change two {disfmarker} two things together and {disfmarker} But it 's around {pause} maybe {disfmarker} it 's less than one percent .&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: It {disfmarker}&#10;Speaker: Professor B&#10;Content: Well . {vocalsound} But like we 're saying , if there 's four or five things like that then {vocalsound} pretty sho soon you 're talking real improvement .&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . And it {disfmarker} Yeah . And then we have to be careful with that also {disfmarker} with the neural net&#10;Speaker: Professor B&#10;Content: Yeah .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The use of silencing or stopping speech in the middle of communication is referred to as frame-dropping. Frame-dropping is a technique used in noise reduction systems to improve the quality of noisy speech by discarding certain frames (small segments of audio) deemed to be noise. This technique can be employed at different stages of the noise reduction process, and its use can impact the difference in power between clean and noisy speech.&#10;&#10;In the given transcript, the participants are discussing a specific system that utilizes frame-dropping as part of its noise reduction strategy. They mention that adjusting various factors, such as over-subtraction, Wiener filtering, and spectral subtraction, can help optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;The transcript also indicates a connection between frame-dropping and the use of delta computations in speech processing. Delta computations are used to estimate the first derivative of a signal, which can be helpful for detecting sudden changes or patterns within the signal. By avoiding delta computations early on and implementing frame-dropping as one of the last steps, the system aims to improve noise reduction performance further.&#10;&#10;Additionally, there is a discussion about assessing the effectiveness of noise estimation in the current system. However, it seems that they do not have any established method for evaluating the performance of their noise estimation technique." target="Speaker: PhD E&#10;Content: and {disfmarker}&#10;Speaker: Professor B&#10;Content: But , um ,&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: it does seem like , you know , i i i i some compromise between always depending on the first fifteen frames and a a always depending on a {disfmarker} a pause is {disfmarker} is {disfmarker} is a good idea . Uh , maybe you have to weight the estimate from the first - teen {disfmarker} fifteen frames more heavily than {disfmarker} than was done in your first attempt . But {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: but {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah , I guess .&#10;Speaker: Professor B&#10;Content: Yeah . Um . No , I mean {disfmarker} Um , do you have any way of assessing how well or how poorly the noise estimation is currently doing ?&#10;Speaker: PhD E&#10;Content: Mmm . No">
      <data key="d0">1</data>
    </edge>
    <edge source="The use of silencing or stopping speech in the middle of communication is referred to as frame-dropping. Frame-dropping is a technique used in noise reduction systems to improve the quality of noisy speech by discarding certain frames (small segments of audio) deemed to be noise. This technique can be employed at different stages of the noise reduction process, and its use can impact the difference in power between clean and noisy speech.&#10;&#10;In the given transcript, the participants are discussing a specific system that utilizes frame-dropping as part of its noise reduction strategy. They mention that adjusting various factors, such as over-subtraction, Wiener filtering, and spectral subtraction, can help optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;The transcript also indicates a connection between frame-dropping and the use of delta computations in speech processing. Delta computations are used to estimate the first derivative of a signal, which can be helpful for detecting sudden changes or patterns within the signal. By avoiding delta computations early on and implementing frame-dropping as one of the last steps, the system aims to improve noise reduction performance further.&#10;&#10;Additionally, there is a discussion about assessing the effectiveness of noise estimation in the current system. However, it seems that they do not have any established method for evaluating the performance of their noise estimation technique." target="1. The amount of noise present in the signal: The more noise there is, relative to the desired speech signal, the greater the difference in power that needs to be addressed. Reducing a large difference in power may require more aggressive noise reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;2. Signal-to-Noise Ratio (SNR): A lower SNR indicates that there is more noise relative to the desired signal, making it harder to separate the two. This can affect the reduction process by requiring more sophisticated noise reduction techniques and a greater balance between noise reduction and signal distortion.&#10;&#10;3. Types of noise: Different types of noise (e.g., white noise, colored noise, non-stationary noise) can have varying effects on the difference in power that needs to be reduced. For example, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;4. Over-subtraction, Wiener filtering, and spectral subtraction: These factors can impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;5. Frame-dropping: This technique discards certain frames (small segments of audio) deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;6. Delta computations: Used for estimating the first derivative of a signal, avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;7. Noise estimation effectiveness: Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;8. Choice of error method: When constructing a filtering problem, the choice between using power spectra or signals directly can significantly affect the reduction process. Power spectra are more sensitive to noise but require additional robustness techniques, while working with signals directly offers more flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem.">
      <data key="d0">1</data>
    </edge>
    <edge source="The use of silencing or stopping speech in the middle of communication is referred to as frame-dropping. Frame-dropping is a technique used in noise reduction systems to improve the quality of noisy speech by discarding certain frames (small segments of audio) deemed to be noise. This technique can be employed at different stages of the noise reduction process, and its use can impact the difference in power between clean and noisy speech.&#10;&#10;In the given transcript, the participants are discussing a specific system that utilizes frame-dropping as part of its noise reduction strategy. They mention that adjusting various factors, such as over-subtraction, Wiener filtering, and spectral subtraction, can help optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;The transcript also indicates a connection between frame-dropping and the use of delta computations in speech processing. Delta computations are used to estimate the first derivative of a signal, which can be helpful for detecting sudden changes or patterns within the signal. By avoiding delta computations early on and implementing frame-dropping as one of the last steps, the system aims to improve noise reduction performance further.&#10;&#10;Additionally, there is a discussion about assessing the effectiveness of noise estimation in the current system. However, it seems that they do not have any established method for evaluating the performance of their noise estimation technique." target="1. Conceptual differences in power and noise reduction factors between spectral subtraction and Wiener filtering:&#10;&#10;Spectral Subtraction: This method aims to reduce the noise by creating an estimate of the noise spectrum and then subtracting it from the observed signal's spectrum. The difference is then transformed back into the time domain, resulting in a noise-reduced signal. However, this process may introduce musical noise due to underestimation or overestimation of the noise spectrum.&#10;&#10;Wiener Filtering: Wiener filtering uses statistical properties of both the speech and noise signals to create an optimal filter for noise reduction while minimizing distortion. The filter takes into account the power spectral densities (PSD) of clean speech and noise, as well as their cross-power spectral density (CPSD). By finding the ratio of CPSD to the PSD of noise, the Wiener filter estimates the linear time-invariant system that minimizes the mean square error between the estimated clean signal and the actual clean signal.&#10;&#10;2. Specific &quot;little factors&quot; affecting the differences in practice:&#10;&#10;The amount of noise present in the signal (1): A higher level of noise requires more aggressive reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;Signal-to-Noise Ratio (SNR) (2): Lower SNR values make it harder to separate the desired signal from the noise, leading to a greater balance between noise reduction and signal distortion. More sophisticated techniques may be required for successful separation.&#10;&#10;Types of noise (3): Different noises can affect the difference in power that needs to be reduced. For instance, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;Over-subtraction, Wiener filtering, and spectral subtraction (4): These factors impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;Frame-dropping (5): This technique discards certain frames deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;Delta computations (6): Avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;Noise estimation effectiveness (7): Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;Choice of error method (8): The choice between using power spectra or signals directly in the filtering problem can significantly affect the reduction process, as each method has different sensitivities to noise and flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem.">
      <data key="d0">1</data>
    </edge>
    <edge source="The use of silencing or stopping speech in the middle of communication is referred to as frame-dropping. Frame-dropping is a technique used in noise reduction systems to improve the quality of noisy speech by discarding certain frames (small segments of audio) deemed to be noise. This technique can be employed at different stages of the noise reduction process, and its use can impact the difference in power between clean and noisy speech.&#10;&#10;In the given transcript, the participants are discussing a specific system that utilizes frame-dropping as part of its noise reduction strategy. They mention that adjusting various factors, such as over-subtraction, Wiener filtering, and spectral subtraction, can help optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;The transcript also indicates a connection between frame-dropping and the use of delta computations in speech processing. Delta computations are used to estimate the first derivative of a signal, which can be helpful for detecting sudden changes or patterns within the signal. By avoiding delta computations early on and implementing frame-dropping as one of the last steps, the system aims to improve noise reduction performance further.&#10;&#10;Additionally, there is a discussion about assessing the effectiveness of noise estimation in the current system. However, it seems that they do not have any established method for evaluating the performance of their noise estimation technique." target="Based on the provided transcript, the exact impact of the frame-dropping problem on the improvement seen in the final result cannot be quantified. When Professor B asked how much of the change was due to the frame-dropping problem specifically, PhD C did not provide a clear numerical answer. While they mentioned that addressing the frame-dropping issue was part of the improvements made, there is no specific quantification of its contribution compared to other changes. Additionally, when discussing noise estimation assessment methods, PhD C mentioned an experiment using channel zero VAD for noise estimation and frame-dropping but did not compare which part helped more or provide a clear measure of improvement in noise estimation due to addressing the frame-dropping issue.&#10;&#10;In summary, while the transcript suggests that addressing the frame-dropping problem contributed to the observed improvements, it does not offer specific quantification of its impact or how much of the change can be attributed to this issue.">
      <data key="d0">1</data>
    </edge>
    <edge source=" - dropping problem ? What about this ?&#10;Speaker: PhD C&#10;Content: Uh , y you had something on it . Right ?&#10;Speaker: PhD E&#10;Content: Just the frame - dropping problem . Yeah . But it 's {disfmarker} it 's difficult . Sometime we {disfmarker} we change two {disfmarker} two things together and {disfmarker} But it 's around {pause} maybe {disfmarker} it 's less than one percent .&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: It {disfmarker}&#10;Speaker: Professor B&#10;Content: Well . {vocalsound} But like we 're saying , if there 's four or five things like that then {vocalsound} pretty sho soon you 're talking real improvement .&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . And it {disfmarker} Yeah . And then we have to be careful with that also {disfmarker} with the neural net&#10;Speaker: Professor B&#10;Content: Yeah .&#10;" target="Based on the transcript provided, Professor C did not provide a specific quantification of how much of the improvement in results was due to addressing the frame-dropping issue as compared to other changes made. Professor B asked about this specifically, but the conversation moved on to other topics before any numerical answer was given.">
      <data key="d0">1</data>
    </edge>
    <edge source=" - dropping problem ? What about this ?&#10;Speaker: PhD C&#10;Content: Uh , y you had something on it . Right ?&#10;Speaker: PhD E&#10;Content: Just the frame - dropping problem . Yeah . But it 's {disfmarker} it 's difficult . Sometime we {disfmarker} we change two {disfmarker} two things together and {disfmarker} But it 's around {pause} maybe {disfmarker} it 's less than one percent .&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: It {disfmarker}&#10;Speaker: Professor B&#10;Content: Well . {vocalsound} But like we 're saying , if there 's four or five things like that then {vocalsound} pretty sho soon you 're talking real improvement .&#10;Speaker: PhD E&#10;Content: Yeah . Yeah . And it {disfmarker} Yeah . And then we have to be careful with that also {disfmarker} with the neural net&#10;Speaker: Professor B&#10;Content: Yeah .&#10;" target="Based on the provided transcript, the exact impact of the frame-dropping problem on the improvement seen in the final result cannot be quantified. When Professor B asked how much of the change was due to the frame-dropping problem specifically, PhD C did not provide a clear numerical answer. While they mentioned that addressing the frame-dropping issue was part of the improvements made, there is no specific quantification of its contribution compared to other changes. Additionally, when discussing noise estimation assessment methods, PhD C mentioned an experiment using channel zero VAD for noise estimation and frame-dropping but did not compare which part helped more or provide a clear measure of improvement in noise estimation due to addressing the frame-dropping issue.&#10;&#10;In summary, while the transcript suggests that addressing the frame-dropping problem contributed to the observed improvements, it does not offer specific quantification of its impact or how much of the change can be attributed to this issue.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD E&#10;Content: and {disfmarker}&#10;Speaker: Professor B&#10;Content: But , um ,&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: it does seem like , you know , i i i i some compromise between always depending on the first fifteen frames and a a always depending on a {disfmarker} a pause is {disfmarker} is {disfmarker} is a good idea . Uh , maybe you have to weight the estimate from the first - teen {disfmarker} fifteen frames more heavily than {disfmarker} than was done in your first attempt . But {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: but {disfmarker}&#10;Speaker: PhD E&#10;Content: Yeah , I guess .&#10;Speaker: Professor B&#10;Content: Yeah . Um . No , I mean {disfmarker} Um , do you have any way of assessing how well or how poorly the noise estimation is currently doing ?&#10;Speaker: PhD E&#10;Content: Mmm . No" target="Based on the transcript provided, Professor C did not provide a specific quantification of how much of the improvement in results was due to addressing the frame-dropping issue as compared to other changes made. Professor B asked about this specifically, but the conversation moved on to other topics before any numerical answer was given.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C did not provide a specific quantification of how much of the improvement in results was due to addressing the frame-dropping issue as compared to other changes made. Professor B asked about this specifically, but the conversation moved on to other topics before any numerical answer was given." target=" yeah , what we do is we compute the silence probability , convert it to that binary flag ,&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: PhD C&#10;Content: and then in the end you c up upsample it to {vocalsound} match the final features number of {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Did that help then ?&#10;Speaker: PhD C&#10;Content: It seems to be helping on the well - matched condition . So that 's why this improvement I got from the last result . So . And it actually r reduced a little bit on the high mismatch , so in the final weightage it 's b b better because the well - matched is still weighted more than {disfmarker}&#10;Speaker: Professor B&#10;Content: So , @ @ I mean , you were doing a lot of changes . Did you happen to notice how much , {vocalsound} uh , the change was due to just this frame - dropping problem ? What about this ?&#10;Speaker: PhD C&#10;Content: Uh , y you had something on it . Right ?&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C did not provide a specific quantification of how much of the improvement in results was due to addressing the frame-dropping issue as compared to other changes made. Professor B asked about this specifically, but the conversation moved on to other topics before any numerical answer was given." target=" because , you {disfmarker} anyway , you can't remove the stationary part fr I mean , non - stationary part from {vocalsound} the signal .&#10;Speaker: Professor B&#10;Content: Not using these methods anyway . Yeah .&#10;Speaker: PhD C&#10;Content: So {disfmarker} Yeah . So you just {pause} update only doing {disfmarker} or update only the stationary components . Yeah . So , that 's {disfmarker} so that 's still a slight difference from what Guenter is trying &#10;Speaker: Professor B&#10;Content: Well , yeah . And {disfmarker} and also there 's just the fact that , um , eh , uh , although we 're trying to do very well on this evaluation , um , we actually would like to have something that worked well in general . And , um , relying on having fifteen frames at the front or something is {disfmarker} is pretty {disfmarker}&#10;Speaker: PhD C&#10;Content: Yeah , yeah .&#10;Speaker: Professor B&#10;Content: I mean , you might , you might not .&#10;Speaker: PhD C&#10;Content: Mmm">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C did not provide a specific quantification of how much of the improvement in results was due to addressing the frame-dropping issue as compared to other changes made. Professor B asked about this specifically, but the conversation moved on to other topics before any numerical answer was given." target="1. Based on the transcript provided, there is no clear description of a specific method that PhD C used to assess how well the noise estimation was doing. When Professor B asked if there was any experiment or way to assess how well or poorly the noise estimation is currently doing, PhD C mentioned an experiment where they used the channel zero VAD for noise estimation and frame-dropping, but they didn't have a comparison of which part helped more.&#10;2. Furthermore, when Professor B asked if there was any improvement in noise estimation due to addressing the frame-dropping issue specifically, no clear numerical answer was given in the conversation.&#10;3. Therefore, it can be inferred that while PhD C did perform some testing related to noise estimation (using channel zero VAD and frame-dropping), they did not specify a particular method or quantify the results of this assessment.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C did not provide a specific quantification of how much of the improvement in results was due to addressing the frame-dropping issue as compared to other changes made. Professor B asked about this specifically, but the conversation moved on to other topics before any numerical answer was given." target="Based on the provided transcript, the exact impact of the frame-dropping problem on the improvement seen in the final result cannot be quantified. When Professor B asked how much of the change was due to the frame-dropping problem specifically, PhD C did not provide a clear numerical answer. While they mentioned that addressing the frame-dropping issue was part of the improvements made, there is no specific quantification of its contribution compared to other changes. Additionally, when discussing noise estimation assessment methods, PhD C mentioned an experiment using channel zero VAD for noise estimation and frame-dropping but did not compare which part helped more or provide a clear measure of improvement in noise estimation due to addressing the frame-dropping issue.&#10;&#10;In summary, while the transcript suggests that addressing the frame-dropping problem contributed to the observed improvements, it does not offer specific quantification of its impact or how much of the change can be attributed to this issue.">
      <data key="d0">1</data>
    </edge>
    <edge source=" yeah , what we do is we compute the silence probability , convert it to that binary flag ,&#10;Speaker: Professor B&#10;Content: Uh - huh .&#10;Speaker: PhD C&#10;Content: and then in the end you c up upsample it to {vocalsound} match the final features number of {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Did that help then ?&#10;Speaker: PhD C&#10;Content: It seems to be helping on the well - matched condition . So that 's why this improvement I got from the last result . So . And it actually r reduced a little bit on the high mismatch , so in the final weightage it 's b b better because the well - matched is still weighted more than {disfmarker}&#10;Speaker: Professor B&#10;Content: So , @ @ I mean , you were doing a lot of changes . Did you happen to notice how much , {vocalsound} uh , the change was due to just this frame - dropping problem ? What about this ?&#10;Speaker: PhD C&#10;Content: Uh , y you had something on it . Right ?&#10;Speaker:" target="Based on the provided transcript, the exact impact of the frame-dropping problem on the improvement seen in the final result cannot be quantified. When Professor B asked how much of the change was due to the frame-dropping problem specifically, PhD C did not provide a clear numerical answer. While they mentioned that addressing the frame-dropping issue was part of the improvements made, there is no specific quantification of its contribution compared to other changes. Additionally, when discussing noise estimation assessment methods, PhD C mentioned an experiment using channel zero VAD for noise estimation and frame-dropping but did not compare which part helped more or provide a clear measure of improvement in noise estimation due to addressing the frame-dropping issue.&#10;&#10;In summary, while the transcript suggests that addressing the frame-dropping problem contributed to the observed improvements, it does not offer specific quantification of its impact or how much of the change can be attributed to this issue.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The transcript does not provide specific information on exactly how much latency the VAD adds to the far-field test utterances when applying VAD probabilities computed on the clean signal. However, it is mentioned that this method significantly decreases the error rate and has potential for further improvement.&#10;   &#10;2. The significance of using a neural net with nine frames for the VAD is that it provides a balance between performance and latency constraints. There seems to be some debate within the group about the ideal length of latency allowed for the neural net, with some parties advocating for a shorter length and others suggesting 130 or 250 milliseconds. The current implementation adds some latency, but further development could potentially reduce this while maintaining or improving error rate reduction." target="1. The group has observed an improvement of more than twenty percent in speech recognition performance by dropping the beginning and end frames (pauses) of a speech compared to their current VAD system's performance. In contrast, the other system's VAD only showed a fourteen percent improvement with this method.&#10;   &#10;2. A more effective method for improving speech recognition accuracy could be using VAD probabilities computed from clean signals on far-field microphone data. This approach significantly decreases the error rate, in some cases even dividing it by two, and has potential for further enhancement. Additionally, optimizing the neural network used in the VAD system by training it on more data and adding better features could also improve its performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group has observed an improvement of more than twenty percent in speech recognition performance by dropping the beginning and end frames (pauses) of a speech compared to their current VAD system's performance. In contrast, the other system's VAD only showed a fourteen percent improvement with this method.&#10;   &#10;2. A more effective method for improving speech recognition accuracy could be using VAD probabilities computed from clean signals on far-field microphone data. This approach significantly decreases the error rate, in some cases even dividing it by two, and has potential for further enhancement. Additionally, optimizing the neural network used in the VAD system by training it on more data and adding better features could also improve its performance." target="Yes, it is possible to use the same neural network for both voice activity detection (VAD) and noise estimation when training on cleaned-up speech features. This approach could simplify the system design by using a single network for multiple tasks. However, care must be taken to ensure that the shared network can effectively learn and differentiate between speech and noise in various conditions. Additionally, it is crucial to optimize the noise reduction techniques, such as over-subtraction, Wiener filtering, and spectral subtraction, to minimize distortion of the clean speech signal while maximizing noise reduction.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The amount of noise present in the signal: The more noise there is, relative to the desired speech signal, the greater the difference in power that needs to be addressed. Reducing a large difference in power may require more aggressive noise reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;2. Signal-to-Noise Ratio (SNR): A lower SNR indicates that there is more noise relative to the desired signal, making it harder to separate the two. This can affect the reduction process by requiring more sophisticated noise reduction techniques and a greater balance between noise reduction and signal distortion.&#10;&#10;3. Types of noise: Different types of noise (e.g., white noise, colored noise, non-stationary noise) can have varying effects on the difference in power that needs to be reduced. For example, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;4. Over-subtraction, Wiener filtering, and spectral subtraction: These factors can impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;5. Frame-dropping: This technique discards certain frames (small segments of audio) deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;6. Delta computations: Used for estimating the first derivative of a signal, avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;7. Noise estimation effectiveness: Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;8. Choice of error method: When constructing a filtering problem, the choice between using power spectra or signals directly can significantly affect the reduction process. Power spectra are more sensitive to noise but require additional robustness techniques, while working with signals directly offers more flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem." target="1. Conceptual differences in power and noise reduction factors between spectral subtraction and Wiener filtering:&#10;&#10;Spectral Subtraction: This method aims to reduce the noise by creating an estimate of the noise spectrum and then subtracting it from the observed signal's spectrum. The difference is then transformed back into the time domain, resulting in a noise-reduced signal. However, this process may introduce musical noise due to underestimation or overestimation of the noise spectrum.&#10;&#10;Wiener Filtering: Wiener filtering uses statistical properties of both the speech and noise signals to create an optimal filter for noise reduction while minimizing distortion. The filter takes into account the power spectral densities (PSD) of clean speech and noise, as well as their cross-power spectral density (CPSD). By finding the ratio of CPSD to the PSD of noise, the Wiener filter estimates the linear time-invariant system that minimizes the mean square error between the estimated clean signal and the actual clean signal.&#10;&#10;2. Specific &quot;little factors&quot; affecting the differences in practice:&#10;&#10;The amount of noise present in the signal (1): A higher level of noise requires more aggressive reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;Signal-to-Noise Ratio (SNR) (2): Lower SNR values make it harder to separate the desired signal from the noise, leading to a greater balance between noise reduction and signal distortion. More sophisticated techniques may be required for successful separation.&#10;&#10;Types of noise (3): Different noises can affect the difference in power that needs to be reduced. For instance, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;Over-subtraction, Wiener filtering, and spectral subtraction (4): These factors impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;Frame-dropping (5): This technique discards certain frames deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;Delta computations (6): Avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;Noise estimation effectiveness (7): Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;Choice of error method (8): The choice between using power spectra or signals directly in the filtering problem can significantly affect the reduction process, as each method has different sensitivities to noise and flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Mark Frames Before They Reach the Server: Instead of dropping unnecessary frames after they have been transmitted to the server, it is suggested to mark them beforehand. This can be done by analyzing the audio data locally and adding a marker or flag to indicate whether a frame should be dropped or kept.&#10;&#10;2. Use Additional Bits for Differentiation: To better handle speech and non-speech frames on the server side, additional bits can be used in the transmission protocol. These extra bits can serve as flags that differentiate between speech and non-speech frames. This way, the server can process only the necessary frames without having to drop any of them after transmission, thus saving bandwidth and reducing latency.&#10;&#10;3. Improve Neural Network Performance: Training the neural network on more data and incorporating better features and noise reduction techniques during training can help enhance its performance in recognizing speech and differentiating it from non-speech. This would result in fewer unnecessary frames being transmitted, thus reducing the load on the server." target=" Professor B&#10;Content: So .&#10;Speaker: PhD A&#10;Content: If the net 's on the server side then it could use all of the {pause} frames .&#10;Speaker: PhD C&#10;Content: Yes , it could be . It 's , like , you mean you just transferred everything and then finally drop the frames after the neural net .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: Right ? Yeah . That 's {disfmarker} that 's one thing which {disfmarker}&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: But you could even mark them , before they get to the server .&#10;Speaker: PhD C&#10;Content: Yeah . Right now we are {disfmarker} Uh , ri Right now what {disfmarker} wha what we did is , like , we just mark {disfmarker} we just have this additional bit which goes around the features , {vocalsound} saying it 's currently a {disfmarker} it 's a speech or a nonspeech .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Mark Frames Before They Reach the Server: Instead of dropping unnecessary frames after they have been transmitted to the server, it is suggested to mark them beforehand. This can be done by analyzing the audio data locally and adding a marker or flag to indicate whether a frame should be dropped or kept.&#10;&#10;2. Use Additional Bits for Differentiation: To better handle speech and non-speech frames on the server side, additional bits can be used in the transmission protocol. These extra bits can serve as flags that differentiate between speech and non-speech frames. This way, the server can process only the necessary frames without having to drop any of them after transmission, thus saving bandwidth and reducing latency.&#10;&#10;3. Improve Neural Network Performance: Training the neural network on more data and incorporating better features and noise reduction techniques during training can help enhance its performance in recognizing speech and differentiating it from non-speech. This would result in fewer unnecessary frames being transmitted, thus reducing the load on the server." target=" . I think it 's OK .&#10;Speaker: Professor B&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: You have , um {disfmarker} So when you {disfmarker} Uh , maybe I don't quite understand how this works , but , um , couldn't you just send all of the frames , but mark the ones that are supposed to be dropped ? Cuz you have a bunch more bandwidth . Right ?&#10;Speaker: Professor B&#10;Content: Well , you could . Yeah . I mean , it {disfmarker} it always seemed to us that it would be kind of nice to {disfmarker} in addition to , uh , reducing insertions , actually use up less bandwidth .&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Professor B&#10;Content: But nobody seems to have {vocalsound} cared about that in this {pause} evaluation .&#10;Speaker: PhD A&#10;Content: And that way the net could use {disfmarker}&#10;Speaker: Professor B&#10;Content: So .&#10;Speaker: PhD A&#10;Content: If the net 's on the server side then it could use all of the">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method. &#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Specifically, they agreed to test and compare the noise estimation techniques of spectral subtraction and Wiener filtering, with the aim of improving the noise estimation while maintaining the same overall method." target="1. Conceptual differences in power and noise reduction factors between spectral subtraction and Wiener filtering:&#10;&#10;Spectral Subtraction: This method aims to reduce the noise by creating an estimate of the noise spectrum and then subtracting it from the observed signal's spectrum. The difference is then transformed back into the time domain, resulting in a noise-reduced signal. However, this process may introduce musical noise due to underestimation or overestimation of the noise spectrum.&#10;&#10;Wiener Filtering: Wiener filtering uses statistical properties of both the speech and noise signals to create an optimal filter for noise reduction while minimizing distortion. The filter takes into account the power spectral densities (PSD) of clean speech and noise, as well as their cross-power spectral density (CPSD). By finding the ratio of CPSD to the PSD of noise, the Wiener filter estimates the linear time-invariant system that minimizes the mean square error between the estimated clean signal and the actual clean signal.&#10;&#10;2. Specific &quot;little factors&quot; affecting the differences in practice:&#10;&#10;The amount of noise present in the signal (1): A higher level of noise requires more aggressive reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;Signal-to-Noise Ratio (SNR) (2): Lower SNR values make it harder to separate the desired signal from the noise, leading to a greater balance between noise reduction and signal distortion. More sophisticated techniques may be required for successful separation.&#10;&#10;Types of noise (3): Different noises can affect the difference in power that needs to be reduced. For instance, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;Over-subtraction, Wiener filtering, and spectral subtraction (4): These factors impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;Frame-dropping (5): This technique discards certain frames deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;Delta computations (6): Avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;Noise estimation effectiveness (7): Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;Choice of error method (8): The choice between using power spectra or signals directly in the filtering problem can significantly affect the reduction process, as each method has different sensitivities to noise and flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method. &#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Specifically, they agreed to test and compare the noise estimation techniques of spectral subtraction and Wiener filtering, with the aim of improving the noise estimation while maintaining the same overall method." target="When choosing an error metric for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. In contrast, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring.&#10;&#10;The choice of error metric depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;Power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After returning from QualComm, Professor B, Hynek, and Guenther discussed different methods for noise suppression, including vector Taylor series and subspace stuff. However, they found that spectral subtraction and Wiener filtering were the most viable options.&#10;2. They had a detailed discussion about how these two methods are similar and different in terms of their mathematics and noise estimation techniques.&#10;3. The group decided to move forward with further testing on spectral subtraction and Wiener filtering, focusing on improving noise estimation while keeping the same method. &#10;4. Later in the month, they plan to incorporate a neural net into their workflow.&#10;5. Specifically, they agreed to test and compare the noise estimation techniques of spectral subtraction and Wiener filtering, with the aim of improving the noise estimation while maintaining the same overall method." target="1. Based on the transcript provided, there is no clear description of a specific method that PhD C used to assess how well the noise estimation was doing. When Professor B asked if there was any experiment or way to assess how well or poorly the noise estimation is currently doing, PhD C mentioned an experiment where they used the channel zero VAD for noise estimation and frame-dropping, but they didn't have a comparison of which part helped more.&#10;2. Furthermore, when Professor B asked if there was any improvement in noise estimation due to addressing the frame-dropping issue specifically, no clear numerical answer was given in the conversation.&#10;3. Therefore, it can be inferred that while PhD C did perform some testing related to noise estimation (using channel zero VAD and frame-dropping), they did not specify a particular method or quantify the results of this assessment.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The transcript does not provide a specific default parameter value for the software system that is one percent off from the best proposal. The closest information provided is that the default system's performance is one percent lower than the best proposal in terms of overall quality or effectiveness, but no actual metric or parameter value is given." target=" {disfmarker}&#10;Speaker: Professor B&#10;Content: I mean on the , uh , TI - TI - digits ,&#10;Speaker: PhD E&#10;Content: on the multi - condition in TI - digits . Yeah .&#10;Speaker: Professor B&#10;Content: uh , uh . Um .&#10;Speaker: PhD E&#10;Content: Mmm .&#10;Speaker: Professor B&#10;Content: So , it probably doesn't matter that much either way . But , um , when you say u uh , unified do you mean , uh , it 's one piece of software now , or {disfmarker} ?&#10;Speaker: PhD E&#10;Content: So now we are , yeah , setting up the software .&#10;Speaker: Professor B&#10;Content: Mm - hmm .&#10;Speaker: PhD E&#10;Content: Um , it should be ready , uh , very soon . Um , and we&#10;Speaker: PhD A&#10;Content: So what 's {disfmarker} what 's happened ? I think I 've missed something .&#10;Speaker: Professor B&#10;Content: OK . So a week ago {disfmarker} maybe you weren't around when {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible to use the same neural network for both voice activity detection (VAD) and noise estimation when training on cleaned-up speech features. This approach could simplify the system design by using a single network for multiple tasks. However, care must be taken to ensure that the shared network can effectively learn and differentiate between speech and noise in various conditions. Additionally, it is crucial to optimize the noise reduction techniques, such as over-subtraction, Wiener filtering, and spectral subtraction, to minimize distortion of the clean speech signal while maximizing noise reduction." target="1. Conceptual differences in power and noise reduction factors between spectral subtraction and Wiener filtering:&#10;&#10;Spectral Subtraction: This method aims to reduce the noise by creating an estimate of the noise spectrum and then subtracting it from the observed signal's spectrum. The difference is then transformed back into the time domain, resulting in a noise-reduced signal. However, this process may introduce musical noise due to underestimation or overestimation of the noise spectrum.&#10;&#10;Wiener Filtering: Wiener filtering uses statistical properties of both the speech and noise signals to create an optimal filter for noise reduction while minimizing distortion. The filter takes into account the power spectral densities (PSD) of clean speech and noise, as well as their cross-power spectral density (CPSD). By finding the ratio of CPSD to the PSD of noise, the Wiener filter estimates the linear time-invariant system that minimizes the mean square error between the estimated clean signal and the actual clean signal.&#10;&#10;2. Specific &quot;little factors&quot; affecting the differences in practice:&#10;&#10;The amount of noise present in the signal (1): A higher level of noise requires more aggressive reduction techniques, which can increase the risk of distorting or negatively impacting the clean speech signal.&#10;&#10;Signal-to-Noise Ratio (SNR) (2): Lower SNR values make it harder to separate the desired signal from the noise, leading to a greater balance between noise reduction and signal distortion. More sophisticated techniques may be required for successful separation.&#10;&#10;Types of noise (3): Different noises can affect the difference in power that needs to be reduced. For instance, non-stationary noise may require more adaptive noise reduction techniques and continuous monitoring of noise characteristics.&#10;&#10;Over-subtraction, Wiener filtering, and spectral subtraction (4): These factors impact the difference in power between clean and noisy speech by adjusting how much noise is removed from the signal. They can be fine-tuned to optimize noise reduction while minimizing distortion of the clean speech signal.&#10;&#10;Frame-dropping (5): This technique discards certain frames deemed to be noise, impacting the difference in power between clean and noisy speech by reducing the overall amount of noise present in the signal. Implementing frame-dropping as one of the last steps can help maintain better noise reduction performance.&#10;&#10;Delta computations (6): Avoiding delta computations early on and implementing frame-dropping later in the process can improve noise reduction performance by better detecting sudden changes or patterns within the signal.&#10;&#10;Noise estimation effectiveness (7): Assessing how well the system estimates background noise affects the reduction process. An accurate noise estimate helps ensure that only the appropriate amount of noise is removed from the signal, minimizing distortion of the clean speech signal. However, if an established method for evaluating the performance of a noise estimation technique is not available, it may be challenging to determine its effectiveness and fine-tune noise reduction techniques accordingly.&#10;&#10;Choice of error method (8): The choice between using power spectra or signals directly in the filtering problem can significantly affect the reduction process, as each method has different sensitivities to noise and flexibility in handling non-stationary noise or transient interference. The appropriate error method depends on specific goals and constraints of the filtering problem.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible to use the same neural network for both voice activity detection (VAD) and noise estimation when training on cleaned-up speech features. This approach could simplify the system design by using a single network for multiple tasks. However, care must be taken to ensure that the shared network can effectively learn and differentiate between speech and noise in various conditions. Additionally, it is crucial to optimize the noise reduction techniques, such as over-subtraction, Wiener filtering, and spectral subtraction, to minimize distortion of the clean speech signal while maximizing noise reduction." target="1. The initial step taken to enhance speech involves applying a technique similar to spectral enhancement or noise reduction on specific FFT (Fast Fourier Transform) bins, which results in creating new FFT's with some noise removed. This process is discussed by Professor B when referring to creating &quot;new FFT's that...have some of the noise removed&quot; before doing further processing to produce various features.">
      <data key="d0">1</data>
    </edge>
    <edge source="Yes, it is possible to use the same neural network for both voice activity detection (VAD) and noise estimation when training on cleaned-up speech features. This approach could simplify the system design by using a single network for multiple tasks. However, care must be taken to ensure that the shared network can effectively learn and differentiate between speech and noise in various conditions. Additionally, it is crucial to optimize the noise reduction techniques, such as over-subtraction, Wiener filtering, and spectral subtraction, to minimize distortion of the clean speech signal while maximizing noise reduction." target="When choosing an error metric for power spectra, a variance-based approach is typically used, which involves squaring the power spectra. This means that the error metric will be based on the difference between the power spectra of the clean and noisy signals. In contrast, when dealing with signals directly, the error method might involve measuring the difference between the time-domain signals or their frequency representations (e.g., Fourier or wavelet coefficients) without squaring.&#10;&#10;The choice of error metric depends on the specific goals and constraints of the filtering problem. For example, if the objective is to minimize signal distortion while suppressing noise, an error metric based on signal fidelity (e.g., mean squared error, structural similarity) might be more appropriate than one based on power spectra. However, in some cases, optimizing the error based on power spectra could lead to better overall performance, especially if the noise is predominantly stationary or if the goal is to preserve specific spectral features of the clean signal.&#10;&#10;Power spectra are often more sensitive to noise and interference, especially in low signal-to-noise ratio (SNR) conditions. Therefore, choosing an error method based on power spectra might require additional techniques for robustness and stability, such as regularization or smoothing. In contrast, working with signals directly might offer more flexibility in handling non-stationary noise or transient interference.&#10;&#10;In summary, when constructing a filtering problem, choosing an error method based on power spectra typically involves squaring the power spectra and optimizing the variance difference, while selecting an error method for signals might involve measuring the difference between time-domain or frequency-domain representations directly. The choice of error method depends on the specific goals, constraints, and noise characteristics of the problem at hand.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The initial step taken to enhance speech involves applying a technique similar to spectral enhancement or noise reduction on specific FFT (Fast Fourier Transform) bins, which results in creating new FFT's with some noise removed. This process is discussed by Professor B when referring to creating &quot;new FFT's that...have some of the noise removed&quot; before doing further processing to produce various features." target="vocalsound} um , all of the features that we use . So this is done early on . This is essentially , {vocalsound} um , um {disfmarker} I guess it 's {disfmarker} it 's more or less like a spee a speech enhancement technique here {disfmarker}&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: right ? {disfmarker} where we 're just kind of creating {vocalsound} new {disfmarker} if not new speech at least new {disfmarker} new FFT 's that {disfmarker} that have {disfmarker} you know , which could be turned into speech {disfmarker} uh , that {disfmarker} that have some of the noise removed .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um , after that we still do a mess of other things to {disfmarker} to produce a bunch of features .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on the transcript provided, there is no clear description of a specific method that PhD C used to assess how well the noise estimation was doing. When Professor B asked if there was any experiment or way to assess how well or poorly the noise estimation is currently doing, PhD C mentioned an experiment where they used the channel zero VAD for noise estimation and frame-dropping, but they didn't have a comparison of which part helped more.&#10;2. Furthermore, when Professor B asked if there was any improvement in noise estimation due to addressing the frame-dropping issue specifically, no clear numerical answer was given in the conversation.&#10;3. Therefore, it can be inferred that while PhD C did perform some testing related to noise estimation (using channel zero VAD and frame-dropping), they did not specify a particular method or quantify the results of this assessment." target=" this is doing right now .&#10;Speaker: PhD A&#10;Content: I see . OK .&#10;Speaker: Professor B&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um . OK . So , uh , what 's , uh {disfmarker} ? That 's {disfmarker} that 's a good set of work that {disfmarker} that , uh {disfmarker}&#10;Speaker: PhD C&#10;Content: Just one more thing . Like , should we do something f more for the noise estimation , because we still {disfmarker} ?&#10;Speaker: Professor B&#10;Content: Yeah . I was wondering about that . That was {disfmarker} I {disfmarker} I had written that down there .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Mm - hmm .&#10;Speaker: Professor B&#10;Content: Um {disfmarker}&#10;Speaker: PhD E&#10;Content: So , we , uh {disfmarker} actually I did the first experiment . This is {">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Based on the transcript provided, there is no clear description of a specific method that PhD C used to assess how well the noise estimation was doing. When Professor B asked if there was any experiment or way to assess how well or poorly the noise estimation is currently doing, PhD C mentioned an experiment where they used the channel zero VAD for noise estimation and frame-dropping, but they didn't have a comparison of which part helped more.&#10;2. Furthermore, when Professor B asked if there was any improvement in noise estimation due to addressing the frame-dropping issue specifically, no clear numerical answer was given in the conversation.&#10;3. Therefore, it can be inferred that while PhD C did perform some testing related to noise estimation (using channel zero VAD and frame-dropping), they did not specify a particular method or quantify the results of this assessment." target="Based on the provided transcript, the exact impact of the frame-dropping problem on the improvement seen in the final result cannot be quantified. When Professor B asked how much of the change was due to the frame-dropping problem specifically, PhD C did not provide a clear numerical answer. While they mentioned that addressing the frame-dropping issue was part of the improvements made, there is no specific quantification of its contribution compared to other changes. Additionally, when discussing noise estimation assessment methods, PhD C mentioned an experiment using channel zero VAD for noise estimation and frame-dropping but did not compare which part helped more or provide a clear measure of improvement in noise estimation due to addressing the frame-dropping issue.&#10;&#10;In summary, while the transcript suggests that addressing the frame-dropping problem contributed to the observed improvements, it does not offer specific quantification of its impact or how much of the change can be attributed to this issue.">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
