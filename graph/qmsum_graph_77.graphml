<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="1. The speakers, specifically Postdoc F and Professor G, consider the group of people they have in mind to be a &quot;terrific pool&quot; for potential employment. However, they acknowledge that the data source is limited and their employment may not be able to continue past the end of the month. Postdoc F mentions telling this to the group and expresses hope that she can keep them employed till then.&#10;2. The limitation of their data source is a concern for the speakers, particularly Professor G, who expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints.&#10;3. Postdoc F and Grad A mention the idea of having this group transcribe meetings multiple times for more detailed analysis, but there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources." />
    <node id="disfmarker} {vocalsound} Well , you know , it might be something we 'd wanna do with some , uh , s small subset {pause} of the whole thing .&#10;Speaker: Grad A&#10;Content: Hmm . Where were they when {pause} we needed them ?&#10;Speaker: Postdoc F&#10;Content: I think {disfmarker}&#10;Speaker: Professor G&#10;Content: We certainly wouldn't wanna do it with everything .&#10;Speaker: Postdoc F&#10;Content: And I 'm also thinking these people are a terrific pool . I mean , if , uh {disfmarker} so I {disfmarker} I told them that , um , we don't know if this will continue past the end of the month&#10;Speaker: Professor G&#10;Content: Uh - huh .&#10;Speaker: Postdoc F&#10;Content: and I also {disfmarker} m I think they know that the data p source is limited and I may not be able to keep them employed till the end of the month even , although I hope to .&#10;Speaker: Professor G&#10;Content: The other thing we could do , actually , uh , is , uh , use them" />
    <node id="aker: PhD D&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: Let 's read some digits .&#10;Speaker: Grad A&#10;Content: OK . uuh&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: And we are {disfmarker}" />
    <node id=" , although I hope to .&#10;Speaker: Professor G&#10;Content: The other thing we could do , actually , uh , is , uh , use them for a more detailed analysis of the overlaps .&#10;Speaker: Postdoc F&#10;Content: And {disfmarker} Oh , that 'd be so super . They would be so {disfmarker} s so terrific .&#10;Speaker: Grad A&#10;Content: I mean , this was something that we were talking about .&#10;Speaker: Professor G&#10;Content: Right ?&#10;Speaker: Grad A&#10;Content: We could get a very detailed overlap if they were willing to transcribe each meeting four or five times . Right ? One for each participant . So they could by hand {disfmarker}&#10;Speaker: Professor G&#10;Content: Well , that 's one way to do it .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: But I 've been saying the other thing is just go through it for the overlaps .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm , that 's right .&#10;Speaker" />
    <node id=" you know , the thing I 'm concerned about is we wanted to do these digits&#10;Speaker: Postdoc F&#10;Content: Oh , yeah .&#10;Speaker: Professor G&#10;Content: and {disfmarker} and I haven't heard , uh , from Jose yet .&#10;Speaker: Postdoc F&#10;Content: Oh , yes .&#10;Speaker: PhD D&#10;Content: OK . What do you want ?&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: So {disfmarker}&#10;Speaker: Grad A&#10;Content: We could skip the digits .&#10;Speaker: Professor G&#10;Content: Uh {disfmarker}&#10;Speaker: Grad A&#10;Content: We don't have to read digits each time .&#10;Speaker: Professor G&#10;Content: Uh {disfmarker} I {disfmarker} I {disfmarker} I think it {disfmarker} you know , another {disfmarker} another bunch of digits . More data is good .&#10;Speaker: Grad A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah . Sure .&#10;Spe" />
    <node id=" we want someone else to do , or whatever .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: But this issue of the contents of the meeting in an outline form . OK .&#10;Speaker: Professor G&#10;Content: Yeah . Meaning really isn't my thing . Um {disfmarker}&#10;Speaker: Grad A&#10;Content: I think it just {disfmarker} whoever is interested can do that . I mean , so if someone wants to use that data {disfmarker}&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: We 're running a little short here .&#10;Speaker: Postdoc F&#10;Content: That 's fine .&#10;Speaker: Professor G&#10;Content: We , uh , uh , cou trying to {disfmarker}&#10;Speaker: Postdoc F&#10;Content: I 'm finished .&#10;Speaker: Professor G&#10;Content: eh , was {disfmarker} p Well , you know , the thing I 'm concerned about is we wanted to do these digits&#10;Speaker: Postdoc F&#10;Content: Oh , yeah .&#10;" />
    <node id=" Hmm .&#10;Speaker: Professor G&#10;Content: And let the , uh , uh , statistical system {pause} determine what 's the right way to look at the data .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: I {disfmarker} I , um , I think it would be interesting to find individual features and put them together . I think that you 'd end up with a better system overall .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: But given the limitation in time {vocalsound} and given the fact that Javier 's system already exists {pause} doing this sort of thing ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: uh , but , uh , its main limitation is that , again , it 's only looking at silences which would {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Professor G&#10;Content: maybe that 's a better place to go .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm" />
    <node id="Based on the transcript, PhD D prepared a presentation that includes the results of a study on different energy levels without the law of length. The exact content of the study is not described in detail, but it involves dividing the average measurement by the variance. The length of the presentation is not explicitly stated, but Professor G asks &quot;how long a [the presentation]?&quot; and PhD D responds, suggesting that it will be relatively quick because the results are already prepared. The discussion about the preparation made by PhD D lasts for a few exchanges, but this does not necessarily indicate the length of the actual presentation." />
    <node id=" . More data is good .&#10;Speaker: Grad A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah . Sure .&#10;Speaker: Professor G&#10;Content: So {disfmarker} so I 'd like to do that . But I think , do you , maybe , eh {disfmarker} ? Did you prepare some whole thing you wanted us just to see ?&#10;Speaker: PhD D&#10;Content: Yeah . It 's {disfmarker} it 's prepared .&#10;Speaker: Professor G&#10;Content: Or what was that ? Yeah .&#10;Speaker: Postdoc F&#10;Content: Oh , k Sorry .&#10;Speaker: Professor G&#10;Content: Uh , how long a {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I {disfmarker} I think it 's {disfmarker} it 's fast , because , uh , I have the results , eh , of the study of different energy without the law length . Eh , um , eh , in the {disfmarker} in the measurement , uh , the average , uh , dividing by the {disfmarker} by the , um , variance" />
    <node id="Content: No , I mean , that there 's no point in going through all of that if that 's the bottom line , really .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: So , I {disfmarker} I think we have to start {disfmarker} Uh , I mean , there there 's two suggestions , really , which is , uh {disfmarker} what we said before is that ,&#10;Speaker: PhD D&#10;Content: Mmm , yeah .&#10;Speaker: Professor G&#10;Content: um , it looks like , at least that you haven't found an obvious way to normalize so that the energy is anything like a reliable , uh , indicator of the overlap .&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Professor G&#10;Content: Um , I {disfmarker} I 'm {disfmarker} I 'm still {pause} a little f think that 's a little funny . These things l @ @ seems like there should be ," />
    <node id="&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: and , uh , there should overall be a , um , smaller proportion of the total energy that is explained by any particular harmonic {pause} sequence in the spectrum .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: So those are all things that should be there .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: So far , um , uh , Jose has {disfmarker} has been {disfmarker} By the way , I was told I should be calling you Pepe , but {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: by your friends , but Anyway ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: um , uh , the {disfmarker} has {disfmarker} has , uh , been exploring , uh , e largely the energy issue and , um ," />
    <node id=" the {disfmarker} has {disfmarker} has , uh , been exploring , uh , e largely the energy issue and , um , as with a lot of things , it is not {disfmarker} uh , like this , it 's not as simple as it sounds .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And then there 's , you know {disfmarker} Is it energy ? Is it log energy ? Is it LPC residual energy ? Is it {disfmarker} is it {disfmarker} {vocalsound} is it , uh , delta of those things ? Uh , what is it no Obviously , just a simple number {disfmarker} {vocalsound} absolute number isn't gonna work . So {vocalsound} it should be with {disfmarker} compared to what ? Should there be a long window for the {vocalsound} normalizing factor and a short window for what you 're looking at ?&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Or , you know , how b short should they be ? So ," />
    <node id=" {disfmarker} in the measurement , uh , the average , uh , dividing by the {disfmarker} by the , um , variance . Um , I {disfmarker} th i&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: the other , uh {disfmarker} the {disfmarker} the last w uh , meeting {disfmarker} eh , I don't know if you remain we have problem to {disfmarker} with the {disfmarker} {vocalsound} with {disfmarker} with the parameter {disfmarker} with the representations of parameter , because the {disfmarker} the valleys and the peaks in the signal , eh , look like , eh , it doesn't follow to the {disfmarker} to the energy in the signal .&#10;Speaker: Professor G&#10;Content: Yes . Right .&#10;Speaker: PhD D&#10;Content: And it was a problem , uh , with the scale .&#10;Speaker: Grad A&#10;Content: With what ?&#10;Speaker: PhD D&#10;Content: Eh , the scale .&#10;Speaker: Postdoc" />
    <node id=" when {disfmarker}&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: So , um , if they laugh between two words , you {disfmarker} you 'd get it in between the two words .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Right .&#10;Speaker: Postdoc F&#10;Content: But if they laugh across three or four words you {disfmarker} you get it after those four words . Does that matter ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Well , the thing that you {disfmarker} is hard to deal with is whe {vocalsound} when they speak while laughing . Um , and that 's , uh {disfmarker} I don't think that we can do very well with that .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: So {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: But , um , that 's not as frequent as just laughing between speaking ,&#10;Speaker:" />
    <node id="1. The speakers discuss the limitation of their current data source in encoding overlaps in a detailed and complete manner. They mention that existing transcribers may not be capturing all the necessary start and end points of overlaps.&#10;2. Postdoc F had a meeting with Dave Gelbart, who provided excellent ideas on modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue.&#10;3. A more detailed analysis of overlaps was suggested by Professor G. There is also a mention of potentially using existing systems like Javier's, as creating new ones may not be feasible given time constraints.&#10;4. While the idea of having the group transcribe meetings multiple times for more detailed analysis was mentioned, there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources.&#10;5. Regarding a thorough-going musical score notation, Professor G expresses support for this idea, as it would allow for multiple channels, a single time-line, and increased clarity and flexibility. Postdoc F also agrees that such a system would be ideal. However, there is no clear consensus on implementing this solution specifically." />
    <node id=" desired in the corpus ultimately .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: So we don't have start and end points {nonvocalsound} at each point where there 's an overlap . We just have the {disfmarker} the {nonvocalsound} overlaps {nonvocalsound} encoded in a simple bin . Well , OK . So {nonvocalsound} @ @ the limits of the {nonvocalsound} over of {disfmarker} of the interface are {vocalsound} such that we were {disfmarker} at this meeting we were entertaining how we might either expand {nonvocalsound} the {disfmarker} the {vocalsound} interface or find other tools which already {pause} do what would be useful . Because what would ultimately be , um , ideal in my {disfmarker} my view and I think {disfmarker} I mean , I had the sense that it was consensus , is that , um , a thorough - going musical score notation would be {nonvocalsound} the best way to go . Because {nonvocalsound}" />
    <node id=" that , um , a thorough - going musical score notation would be {nonvocalsound} the best way to go . Because {nonvocalsound} you can have multiple channels , there 's a single time - line , it 's very clear , flexible , and all those nice things .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: OK . So , um , um , I spoke {disfmarker} I had a meeting with Dave Gelbart on {disfmarker} on {disfmarker} and he had , uh , excellent ideas on how {pause} the interface could be {pause} modified to {disfmarker} to do this kind of representation . But , um , he {disfmarker} in the meantime you were checking into the existence of already , um , existing interfaces which might already have these properties . So , do you wanna say something about that ?&#10;Speaker: PhD C&#10;Content: Yes . Um , I {vocalsound} talked with , uh , Munich guys from {disfmarker} from Ludwi - Ludwig Maximilians University , who do a lot of transcribing and transliter" />
    <node id=" , um {disfmarker} OK , and {disfmarker} and also Da - Dave Gelbart . So there 's this {disfmarker} this problem of {disfmarker} and w and {disfmarker} so we had this meeting . Th - the {nonvocalsound} {disfmarker} also Adam , before the {disfmarker} the {disfmarker} before you went away . Uh we , um {disfmarker} regarding the representation {nonvocalsound} of overlaps , because at present , {nonvocalsound} {vocalsound} um , because {nonvocalsound} of the limitations of {vocalsound} th the interface we 're using , overlaps are , uh , not being {nonvocalsound} encoded by {nonvocalsound} the transcribers in as complete {nonvocalsound} and , uh , detailed a way as it might be , and as might be desired {disfmarker} I think would be desired in the corpus ultimately .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: So" />
    <node id="Speaker: PhD B&#10;Content: But you s&#10;Speaker: Grad A&#10;Content: What our decision was is that {pause} we 'll go ahead with what we have with a not very fine time scale on the overlaps .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Right . Yeah .&#10;Speaker: Grad A&#10;Content: And {disfmarker} and do what we can later {pause} to clean that up if we need to .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Right .&#10;Speaker: Postdoc F&#10;Content: And {disfmarker} and I was just thinking that , um , {vocalsound} if it were possible to bring that in , like , {vocalsound} you know , this week , then {nonvocalsound} when they 're encoding the overlaps {nonvocalsound} it would be nice for them to be able to specify when {disfmarker} you know , the start points and end points of overlaps .&#10;Speaker: Professor G&#10;Content: Uh - huh .&#10;Speaker" />
    <node id=" But , I mean , the other thing is since we 've been spending so much time thinking about overlaps is {disfmarker} is maybe get a much more detailed analysis of the overlaps .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: But anyway , I 'm {disfmarker} I 'm open to c our consideration .&#10;Speaker: Postdoc F&#10;Content: That 'd be great .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: I {disfmarker} I don't wanna say that by fiat .&#10;Speaker: Postdoc F&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: I 'm open to every consideration of {vocalsound} what are some other kinds of detailed analysis that would be most useful .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: And , uh , uh ,&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: I {disfmarker" />
    <node id="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." />
    <node id=": Well , um , I can {pause} give you an update on the {pause} transcription effort .&#10;Speaker: Professor G&#10;Content: Great .&#10;Speaker: Postdoc F&#10;Content: Uh , maybe {nonvocalsound} raise the issue of microphone , uh , um procedures with reference to the {pause} cleanliness of the recordings .&#10;Speaker: Professor G&#10;Content: OK , transcription , uh , microphone issues {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And then maybe {nonvocalsound} ask , th uh , these guys . The {disfmarker} we have great {disfmarker} great , uh , p steps forward in terms of the nonspeech - speech pre - segmenting of the signal .&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Well , we have steps forward .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Well , it 's a {disfmarker} it 's a big improvement .&#10;Speaker: PhD C&#10;Content: I would prefer this .&#10;Speaker: Professor G&#10;Content: Yes" />
    <node id="aker: Grad A&#10;Content: Oh , we should definitely get with them then ,&#10;Speaker: Professor G&#10;Content: So .&#10;Speaker: Grad A&#10;Content: and agree upon a format . Though I don't remember email on that . So was I not in the loop on that ?&#10;Speaker: Professor G&#10;Content: Um . Yeah , I don't think I mailed anybody . I just think I told them to contact Jane {disfmarker} that , uh , if they had a {disfmarker}&#10;Speaker: Grad A&#10;Content: Oh , OK .&#10;Speaker: Postdoc F&#10;Content: That 's right .&#10;Speaker: Professor G&#10;Content: if , uh {disfmarker} that {disfmarker} that , uh , as the point person on it .&#10;Speaker: Grad A&#10;Content: Yeah , I think that 's right .&#10;Speaker: Professor G&#10;Content: But {disfmarker}&#10;Speaker: Grad A&#10;Content: Just , uh {disfmarker}&#10;Speaker: Professor G&#10;Content: So , yeah . Maybe I 'll , uh , ping them a little bit about it to" />
    <node id="Speaker: Grad A&#10;Content: OK . We seem to be recording .&#10;Speaker: Professor G&#10;Content: Alright !&#10;Speaker: Grad A&#10;Content: So , sorry about not {disfmarker}&#10;Speaker: Professor G&#10;Content: We 're not crashing .&#10;Speaker: PhD D&#10;Content: Number four .&#10;Speaker: Grad A&#10;Content: not pre - doing everything . The lunch went a little later than I was expecting , Chuck .&#10;Speaker: PhD E&#10;Content: Hmm ?&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Chuck was telling too many jokes , or something ?&#10;Speaker: Grad A&#10;Content: Yep . Pretty much .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: OK . {vocalsound} Does anybody have an agenda ?&#10;Speaker: Grad A&#10;Content: No .&#10;Speaker: Postdoc F&#10;Content: Well , I 'm {disfmarker} I sent a couple of items . They 're {disfmarker} they 're sort of practical .&#10;Speaker: Professor G&#10;Content: I thought" />
    <node id=" the right people connected in , who had the time .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Postdoc F&#10;Content: Yeah , yeah .&#10;Speaker: Professor G&#10;Content: So , um , eh {disfmarker}&#10;Speaker: Grad A&#10;Content: Is he on the mailing list ? The Meeting Recorder mailing li ?&#10;Speaker: Postdoc F&#10;Content: Oh !&#10;Speaker: Grad A&#10;Content: We should add him .&#10;Speaker: Postdoc F&#10;Content: Yeah . I {disfmarker} I {disfmarker} I don't know for sure .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Did something happen , Morgan , that he got put on this , or was he already on it ,&#10;Speaker: Grad A&#10;Content: Add him .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ?&#10;Speaker: Professor G&#10;Content: No , I , eh , eh , p It {disfmarker} it oc I {disfmarker} h it 's {disfmarker} Yeah , something happened" />
    <node id="} it 's a big improvement .&#10;Speaker: PhD C&#10;Content: I would prefer this .&#10;Speaker: Professor G&#10;Content: Yes . Yeah , well . OK . Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: We talk about the {disfmarker} {vocalsound} the results of&#10;Speaker: Professor G&#10;Content: You have some {disfmarker} Yeah .&#10;Speaker: Grad A&#10;Content: I have a little bit of IRAM stuff&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: use {disfmarker}&#10;Speaker: Grad A&#10;Content: but {pause} I 'm not sure if that 's of general interest or not .&#10;Speaker: Professor G&#10;Content: Uh , bigram ?&#10;Speaker: Grad A&#10;Content: IRAM .&#10;Speaker: PhD D&#10;Content: IRAM .&#10;Speaker: Professor G&#10;Content: IRAM .&#10;Speaker: Grad A&#10;Content: IRAM , bigram ,&#10;Speaker: Professor G&#10;Content: Well , m maybe .&#10;Speaker: PhD D&#10;Content: Bi -" />
    <node id="1. The group is divided in their opinion on whether to skip reading through a set of digits for their work. Professor G expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints, while Postdoc F and Grad A suggest that it might not be necessary to read the digits each time. However, there is no explicit consensus on skipping the digits.&#10;2. The group has concerns about not having heard back from a colleague named Jose regarding this task. This concern is expressed by Professor G, who mentions that they haven't heard from Jose yet in relation to the digits." />
    <node id="1. The postdoc and PhD speaker improved the &quot;" />
    <node id="er} they can use it or {disfmarker} ?&#10;Speaker: Postdoc F&#10;Content: Uh , they {disfmarker} they think it 's a terrific improvement . And , um , it real it just makes a {disfmarker} a world of difference .&#10;Speaker: Professor G&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: And , um , y you also did some something in addition which was , um , for those in which there {nonvocalsound} was , uh , quiet speakers in the mix .&#10;Speaker: PhD C&#10;Content: Yeah . Uh , yeah . That {disfmarker} that was one {disfmarker} one {disfmarker} one thing , uh , why I added more mixtures for {disfmarker} for the speech . So I saw that there were loud {disfmarker} loudly speaking speakers and quietly speaking speakers .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: And so I did two mixtures , one for the loud speakers and one for the quiet speakers .&#10;Speaker: Grad A&#10;Content:" />
    <node id="Content: OK .&#10;Speaker: PhD C&#10;Content: It 's just our {disfmarker} our old Munich , uh , loudness - based spectrum on mel scale twenty {disfmarker} twenty critical bands and then loudness .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: And four additional features , which is energy , loudness , modified loudness , and zero crossing rate . So it 's twenty - four {disfmarker} twenty - four features .&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: And you also provided me with several different versions ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: which I compared .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: And so you change {nonvocalsound} parameters . What {disfmarker} do you wanna say something about the parameters {nonvocalsound} that you change ?&#10;Speaker: PhD C&#10;Content: Yeah" />
    <node id="&#10;Content: And so I did two mixtures , one for the loud speakers and one for the quiet speakers .&#10;Speaker: Grad A&#10;Content: And did you hand - label who was loud and who was quiet , or did you just {disfmarker} ?&#10;Speaker: PhD C&#10;Content: I did that for {disfmarker} for five minutes of one dialogue&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD C&#10;Content: and that was enough to {disfmarker} to train the system .&#10;Speaker: PhD B&#10;Content: W What {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: And so it {disfmarker} it adapts , uh , on {disfmarker} while running . So .&#10;Speaker: PhD B&#10;Content: What kind of , uh , front - end processing did you do ?&#10;Speaker: PhD C&#10;Content: Hopefully .&#10;Speaker: PhD D&#10;Content: OK .&#10;Speaker: PhD C&#10;Content: It 's just our {disfmarker} our old Munich , uh , loudness" />
    <node id=" the transcriber 's {nonvocalsound} perspective , uh , those {nonvocalsound} two functions are separate . And Dan Ellis 's hack handles the , {vocalsound} um , choice {nonvocalsound} {disfmarker} the ability to choose different waveforms {vocalsound} from moment to moment .&#10;Speaker: Grad A&#10;Content: But only to listen to , not to look at .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Um {disfmarker}&#10;Speaker: Grad A&#10;Content: The waveform you 're looking at doesn't change .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: That 's true .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Yeah , but {nonvocalsound} that 's {disfmarker} that 's OK , cuz they 're {disfmarker} they 're , you know , they 're focused on the ear anyway .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Postdoc" />
    <node id="'t find the right connector to go into these things ?&#10;Speaker: Grad A&#10;Content: Yep . When I looked , i they listed one microphone and that 's it&#10;Speaker: PhD E&#10;Content: Huh !&#10;Speaker: Grad A&#10;Content: as having that type of connector . But my guess is that Sony maybe uses a different number for their connector than everyone else does . And {disfmarker} and so {disfmarker}&#10;Speaker: Professor G&#10;Content: Mm - hmm . Well , let 's look at it together&#10;Speaker: Grad A&#10;Content: it seems {disfmarker} it seems really unlikely to me that there 's only one .&#10;Speaker: Professor G&#10;Content: and {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And there 's no adaptor for it ?&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Seems like there 'd be a {disfmarker} OK .&#10;Speaker: Grad A&#10;Content: As I said , who knows ?&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;" />
    <node id="1. The speakers generally agree on the importance of using transcribers to mark audible breaths and laughter in the transcripts, which can then be used to train models for speech and nonspeech. This is based on PhD B's suggestion to explicitly mark these non-speech elements and Postdoc F's confirmation that they are currently being marked in curly brackets as &quot;inhale&quot; or &quot;breath.&quot;&#10;   &#10;2. There is also discussion about the usefulness of transcribing nonspeech elements like breath and laughter for modeling purposes, as mentioned by PhD B when they discuss the SRI recognizer being able to transcribe these elements and model them. Professor G agrees with this approach, suggesting that they should not worry so much about features and instead use general features for both speech and nonspeech modeling.&#10;&#10;In summary, the speakers agree on the significance of using transcribers to mark non-speech elements such as breath and laughter in transcripts, which can then be utilized to train models for speech and nonspeech. They also consider modeling these elements to improve the overall performance of their system." />
    <node id=": Yeah .&#10;Speaker: Professor G&#10;Content: And then there 's mass .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Anyway .&#10;Speaker: Postdoc F&#10;Content: I could say something about {disfmarker} about the {disfmarker} Well , I don't know what you wanna do . Yeah .&#10;Speaker: Professor G&#10;Content: About what ?&#10;Speaker: Postdoc F&#10;Content: About the transcribers or anything or {disfmarker} ? I don't know .&#10;Speaker: Professor G&#10;Content: Well , the other {disfmarker}&#10;Speaker: PhD B&#10;Content: But , uh , just to {disfmarker} to , um {disfmarker}&#10;Speaker: Professor G&#10;Content: why don't we do that ?&#10;Speaker: PhD B&#10;Content: One more remark , uh , concerning the SRI recognizer . Um . It is useful to transcribe and then ultimately train models for things like breath , and also laughter is very , very frequent and important to {disfmarker} {vocalsound" />
    <node id="ribe and then ultimately train models for things like breath , and also laughter is very , very frequent and important to {disfmarker} {vocalsound} to model .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So ,&#10;Speaker: Grad A&#10;Content: So ,&#10;Speaker: PhD B&#10;Content: if you can in your transcripts mark {disfmarker}&#10;Speaker: Grad A&#10;Content: mark them ?&#10;Speaker: PhD B&#10;Content: mark very audible breaths and laughter especially ,&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: um {disfmarker}&#10;Speaker: Postdoc F&#10;Content: They are .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Postdoc F&#10;Content: They 're putting {disfmarker} Eh , so in curly brackets they put &quot; inhale &quot; or &quot; breath &quot; .&#10;Speaker: Grad A&#10;Content: Oh , great .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: It {disfmark" />
    <node id=" for s speech and nonspeech . And it was a system which used only one Gaussian for silence and one Gaussian for speech . And now I added , uh , multi - mixture possibility for {disfmarker} {vocalsound} for speech and nonspeech .&#10;Speaker: Professor G&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD C&#10;Content: And I did some training on {disfmarker} on one dialogue , which was transcribed by {disfmarker} Yeah . We {disfmarker} we did a nons s speech - nonspeech transcription .&#10;Speaker: PhD D&#10;Content: Jose .&#10;Speaker: PhD C&#10;Content: Adam , Dave , and I , we did , for that dialogue and I trained it on that . And I did some pre - segmentations for {disfmarker} for Jane . And I 'm not sure how good they are or what {disfmarker} what the transcribers say . They {disfmarker} they can use it or {disfmarker} ?&#10;Speaker: Postdoc F&#10;Content: Uh , they {disfmarker" />
    <node id=": Yeah .&#10;Speaker: PhD B&#10;Content: But , um , that 's not as frequent as just laughing between speaking ,&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: So are {disfmarker} do you treat breath and laughter as phonetically , or as word models , or what ?&#10;Speaker: PhD B&#10;Content: so {disfmarker}&#10;Speaker: Professor G&#10;Content: Uh is it ?&#10;Speaker: PhD D&#10;Content: Huh . I {disfmarker} I think it 's frequent in {disfmarker} in the meeting .&#10;Speaker: Postdoc F&#10;Content: I think he 's right . Yeah .&#10;Speaker: PhD B&#10;Content: We tried both . Uh , currently , um , we use special words . There was a {disfmarker} there 's actually a word for {disfmarker} uh , it 's not just breathing but all kinds of mouth {disfmarker}&#10;Speaker: Grad A&#10;Content: Mm - hmm . Mouth stuff ?&#10;Speaker: PhD B&#10;Content: uh , mouth {disf" />
    <node id="Content: Yeah .&#10;Speaker: Professor G&#10;Content: which is to say , don't worry so much about the , uh , features .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: That is to say , use , you know , as {disfmarker} as you 're doing with the speech , uh , nonspeech , use some very general features .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And , uh , then , uh , look at it more from the aspect of modeling .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: You know , have a {disfmarker} have a couple Markov models and {disfmarker} and , uh , try to indi try to determine , you know , w when is th when are you in an overlap , when are you not in an overlap .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: And let the , uh , uh , statistical system {pause} determine what 's the right way to" />
    <node id="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript." />
    <node id=" doesn't have {disfmarker} it doesn't have any temporal , uh {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Maybe I 'm misremembering , but I did not think it had a Markov {disfmarker}&#10;Speaker: Professor G&#10;Content: I thought it {disfmarker} Yeah . I gues I guess I don't remember either . Uh . It 's been a while .&#10;Speaker: PhD C&#10;Content: Yeah . Uh , I could have a look at it .&#10;Speaker: PhD D&#10;Content: Javier {disfmarker}&#10;Speaker: Professor G&#10;Content: Uh .&#10;Speaker: PhD C&#10;Content: So .&#10;Speaker: PhD D&#10;Content: You mean Ja - eh , eh , Javier program ?&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: No , Javier di doesn't worked with , uh , a Markov {disfmarker}&#10;Speaker: Grad A&#10;Content: Yeah , I didn't think so .&#10;Speaker: PhD D&#10;Content: He on only train {disfmarker}" />
    <node id="rule , yeah .&#10;Speaker: Professor G&#10;Content: So pitch - related and harmonic - related , I 'm {disfmarker} I 'm {pause} somewhat more hopeful for it .&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Professor G&#10;Content: But it seems like if we just wanna get something to work ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: that , uh , their suggestion of {disfmarker} of {disfmarker} Th - they were suggesting going to Markov models , uh , but in addition there 's an expansion of what Javier did . And one of those things , looking at the statistical component ,&#10;Speaker: PhD D&#10;Content: One .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: even if the features that you give it are maybe not ideal for it , it 's just sort of this general filter bank&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: or {disfmarker} {vocalsound} or" />
    <node id=" think with {disfmarker}&#10;Speaker: PhD C&#10;Content: the {disfmarker} to speech - nonspeech as {disfmarker}&#10;Speaker: Grad A&#10;Content: That 's right . But I think Javier 's {disfmarker}&#10;Speaker: PhD C&#10;Content: it 's only speech or it 's {disfmarker} it 's {disfmarker} it 's nonspeech .&#10;Speaker: PhD D&#10;Content: Ah . Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: I think Javier 's might be able to .&#10;Speaker: PhD C&#10;Content: So .&#10;Speaker: Professor G&#10;Content: N n&#10;Speaker: Grad A&#10;Content: It doesn't have the same Gaus - uh , H M M modeling ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: which is I think a drawback .&#10;Speaker: PhD C&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: But , uh {disfmarker}&#10;Spe" />
    <node id="Based on the transcript, it is still considered important to have a good tool for checking and possibly augmenting the IBM transcripts, even in the absence of the person who previously handled this task. This sentiment is expressed by PhD B, who states &quot;I think having a good tool is worth something no matter what&quot; (lines 10-11). Furthermore, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort, implying that the group has made progress in their efforts to improve the transcription process.&#10;&#10;In terms of volunteering to help with this process, Grad A mentions that Dave Gelbart had volunteered (line 28). However, it is not explicitly stated in the transcript whether Dave Gelbart will be specifically assisting with the task of checking and augmenting the IBM transcripts." />
    <node id=": you know , who just doesn't happen to be here anymore . Someone else pays him . So {disfmarker}&#10;Speaker: PhD B&#10;Content: But about the need for transcription ,&#10;Speaker: Postdoc F&#10;Content: Isn't that great ?&#10;Speaker: PhD B&#10;Content: I mean , don't we {disfmarker} didn't we previously {vocalsound} decide that the {pause} IBM {pause} transcripts would have to be {pause} checked anyway and possibly augmented ?&#10;Speaker: Professor G&#10;Content: So . {vocalsound} Yeah .&#10;Speaker: Postdoc F&#10;Content: Yes . That 's true .&#10;Speaker: PhD B&#10;Content: So , I think having a good tool is worth something no matter what .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Yeah . S OK . That 's {disfmarker} that 's a good point .&#10;Speaker: Grad A&#10;Content: Yeah , and Dave Gelbart did volunteer ,&#10;Speaker: Postdoc F&#10;Content: Good .&#10;Speaker: Grad A&#10;Content: and" />
    <node id="s {disfmarker} it 's , uh , uh {disfmarker}&#10;Speaker: Grad A&#10;Content: Well , I 'll dig through the documentation to DragonDictate and ste s see if they still have the little {pause} form .&#10;Speaker: Professor G&#10;Content: But it does happen .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Right ? I mean , and any {disfmarker}&#10;Speaker: PhD B&#10;Content: It 's interesting , uh , I talked to some IBM guys , uh , last January , I think , I was there . And {disfmarker} so people who were working on the {disfmarker} on their ViaVoice dictation product .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And they said , uh , the breathing is really a {disfmarker} a terrible problem {pause} for them , to {disfmarker} to not recognize breathing as speech .&#10;Speaker: Postdoc F&#10;Content: Wow .&#10;Speaker: PhD B&#10;Content: So , anything to reduce breathing is" />
    <node id=" Postdoc F&#10;Content: Mm - hmm . Well , {vocalsound} one would {disfmarker}&#10;Speaker: PhD D&#10;Content: We hope .&#10;Speaker: Grad A&#10;Content: Yeah . Who knows ?&#10;Speaker: Postdoc F&#10;Content: That 'd be {disfmarker} that 'd be nice . I mean , {vocalsound} I {disfmarker} I {disfmarker} I {disfmarker} I 've {disfmarker}&#10;Speaker: PhD B&#10;Content: So who 's gonna do that ? Who 's gonna do forced alignment ?&#10;Speaker: Grad A&#10;Content: Well , u uh , IBM was going to . Um {disfmarker}&#10;Speaker: PhD B&#10;Content: Oh , OK .&#10;Speaker: PhD D&#10;Content: Oh .&#10;Speaker: Grad A&#10;Content: and I imagine they still plan to but {disfmarker} but , you know , I haven't spoken with them about that recently .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Spe" />
    <node id="Based on the transcript, Professor G believes that it is still important to have people wear microphones properly in a real-life situation, even if the recordings may not be perfect. He suggests that trying to have people wear the microphones properly does not hurt the naturalness of the situation. This is because, in the target applications they are discussing, people will not be wearing head-mounted mikes anyway. The head-mounted mikes are only for research purposes and help ensure cleaner recordings. Professor G also mentions that systems exist to do pretty good selection of who's talking, even when microphones are not worn perfectly. Overall, while imperfect recordings may occur when people wear microphones improperly, the benefits of better sound quality and clearer speech outweigh the drawbacks in a research context." />
    <node id=": Postdoc F&#10;Content: Oh .&#10;Speaker: Grad A&#10;Content: But I felt that in a real situation we were very seldom gonna get people to really do it and maybe it wasn't worth concentrating on . But {disfmarker}&#10;Speaker: Professor G&#10;Content: Well , I think that that 's {disfmarker} that 's a good back - off position . That 's what I was saying {vocalsound} earlier , th that , you know , we are gonna get some {vocalsound} recordings that are imperfect and , hey , that 's life . But I {disfmarker} I think that it {disfmarker} it doesn't hurt , uh , the naturalness of the situation to try to have people {pause} wear the microphones properly , if possible ,&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: because , {vocalsound} um , the natural situation is really what we have with the microphones on the table .&#10;Speaker: Grad A&#10;Content: Oh . That 's true .&#10;Speaker: Professor G&#10;Content: I mean , I think , {" />
    <node id="&#10;Speaker: Grad A&#10;Content: Oh . That 's true .&#10;Speaker: Professor G&#10;Content: I mean , I think , {vocalsound} you know , in the target applications that we 're talking about , people aren't gonna be wearing head - mounted mikes anyway .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: So this is just for u these head - mounted mikes are just for use with research .&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And , uh , it 's gonna make {disfmarker} You know , if {disfmarker} if An - Andreas plays around with language modeling , he 's not gonna be m wanna be messed up by people breathing into the microphone .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Professor G&#10;Content: So it 's {disfmarker} it 's , uh , uh {disfmarker}&#10;Speaker: Grad A&#10;Content: Well , I" />
    <node id=": Professor G&#10;Content: So , by doing that , you know , rather than setting any , uh , absolute threshold , you actually can do pretty good , uh , selection of who {disfmarker} who 's talking .&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: Uh {disfmarker} And those {disfmarker} those systems work very well , by the way , I mean , so people use them in {vocalsound} panel discussions and so forth with sound reinforcement differing in {disfmarker} in sort of ,&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor G&#10;Content: uh {disfmarker} and , uh , those {disfmarker} if {disfmarker} Boy , the guy I knew who built them , built them like twenty {disfmarker} twenty years ago ,&#10;Speaker: Grad A&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: so they 're {disfmarker} {vocalsound} it 's {disfmarker} the {disfmarker} the techniques work pretty well .&#10;" />
    <node id=" a couple of items . They 're {disfmarker} they 're sort of practical .&#10;Speaker: Professor G&#10;Content: I thought {pause} somebody had .&#10;Speaker: Postdoc F&#10;Content: I don't know if you 're {disfmarker}&#10;Speaker: Professor G&#10;Content: Yeah , that 's right .&#10;Speaker: Postdoc F&#10;Content: if {disfmarker} if that 's too practical for what we 're {pause} focused on .&#10;Speaker: Grad A&#10;Content: I mean , we don't want anything too practical .&#10;Speaker: Professor G&#10;Content: Yeah , we only want th useless things .&#10;Speaker: Grad A&#10;Content: Yeah , that would be {disfmarker}&#10;Speaker: Professor G&#10;Content: Yeah . No , why don't we talk about practical things ?&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: Sure .&#10;Speaker: Postdoc F&#10;Content: Well , um , I can {pause} give you an update on the {pause} transcription effort .&#10;Speaker: Professor G&#10;Content: Great" />
    <node id="1. The two types of laughter that the postdoc is referring to are:&#10;&#09;* Laughter that occurs in the middle of a sentence.&#10;&#09;* Laughter that occurs after a sentence has been completed.&#10;2. Distinguishing between these two types of laughter is important because they may have different implications for the conversation or speech analysis. For example, laughter in the middle of a sentence might indicate amusement or interruption, while laughter after a sentence could suggest understanding or agreement. By distinguishing between them, researchers can better understand the nuances of human communication and improve the performance of their systems in recognizing and modeling these non-speech elements." />
    <node id=" great .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: It {disfmarker} they {disfmarker} and then in curly brackets they say &quot; laughter &quot; . Now they 're {disfmarker} {vocalsound} they 're not being {pause} awfully precise , uh , m So they 're two types of laughter that are not being distinguished .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: One is {vocalsound} when sometimes s someone will start laughing when they 're in the middle of a sentence .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: And {disfmarker} and then the other one is when they finish the sentence and then they laugh . So , um , I {disfmarker} I did s I did some double checking to look through {disfmarker} I mean , {vocalsound} you 'd need to have extra  e extra complications , like time tags indicating the beginning and ending of {disfmarker}" />
    <node id=" {vocalsound} you 'd need to have extra  e extra complications , like time tags indicating the beginning and ending of {disfmarker} {vocalsound} of the laughing through the utterance .&#10;Speaker: PhD B&#10;Content: It 's not so {disfmarker} I don't think it 's , um {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And that {disfmarker} and what they 're doing is in both cases just saying &quot; curly brackets laughing &quot; a after the unit .&#10;Speaker: PhD B&#10;Content: As {disfmarker} as long as there is an indication that there was laughter somewhere between {pause} two words {vocalsound} I think that 's sufficient ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Good . Oh !&#10;Speaker: Grad A&#10;Content: Against {disfmarker} they could do forced alignment .&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: because actually the recognition of laughter once you kn um {disfmarker} you know , is pretty good" />
    <node id=" .&#10;Speaker: PhD B&#10;Content: because actually the recognition of laughter once you kn um {disfmarker} you know , is pretty good .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: So as long as you can stick a {disfmarker} you know , a t a tag in there that {disfmarker} that indicates that there was laughter ,&#10;Speaker: Grad A&#10;Content: Oh , I didn't know that .&#10;Speaker: PhD B&#10;Content: that would probably be , uh , sufficient to train models .&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: That would be a really interesting {pause} prosodic feature ,&#10;Speaker: Postdoc F&#10;Content: Then {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: And let me ask y and I gotta ask you one thing about that .&#10;Speaker: Grad A&#10;Content: when {disfmarker}&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: So , um ," />
    <node id="The simple concept being discussed involves using a Gaussian distribution for each decision boundary or potential change point in the data. This idea was suggested as a way to determine overlaps or changes between speech and nonspeech portions in a signal. Instead of focusing on specific features, the group discussed modeling this problem with Gaussians and making decisions based on that. The optimal splitting would then be chosen based on these Gaussian models." />
    <node id=" Grad A&#10;Content: Yeah , I didn't think so .&#10;Speaker: PhD D&#10;Content: He on only train {disfmarker}&#10;Speaker: Professor G&#10;Content: Oh , OK . So he 's just {disfmarker} he just computes a Gaussian over potential {disfmarker}&#10;Speaker: Grad A&#10;Content: Yep .&#10;Speaker: PhD D&#10;Content: Yeah . It was only Gaussian .&#10;Speaker: Professor G&#10;Content: Oh , I see . I see .&#10;Speaker: Grad A&#10;Content: And so I {disfmarker} I think it would work fine for detecting overlap .&#10;Speaker: PhD D&#10;Content: This is the idea .&#10;Speaker: Professor G&#10;Content: And {disfmarker} and {disfmarker}&#10;Speaker: Grad A&#10;Content: It 's just , uh , that i it {disfmarker} he has the two - pass issue that {disfmarker} What he does is , as a first pass he {disfmarker} he {disfmarker} p he does , um , a guess at where the divisions might" />
    <node id=" .&#10;Speaker: PhD C&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: But , uh {disfmarker}&#10;Speaker: Professor G&#10;Content: Well , it 's {disfmarker} sort of has a simple one .&#10;Speaker: PhD D&#10;Content: Mmm , yeah .&#10;Speaker: Grad A&#10;Content: Does it ?&#10;Speaker: Professor G&#10;Content: Right ? It 's {disfmarker} it 's just {disfmarker} it 's just a {disfmarker} isn't it just a Gaussian&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: for each {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Yeah . And then {pause} he ch you choose optimal splitting .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Yeah . Oh , it doesn't have {disfmarker} it doesn't have any temporal , uh {disfmarker} ?&#10;Speaker: Grad A&#10;Content" />
    <node id=" And , um , so one of the things that they were pushing in d in discussing with me is , um , w why are you spending so much time , uh , on the , uh , feature issue , uh , when perhaps if you sort of deal with what you were using before&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor G&#10;Content: and then just broadened it a bit , instead of just ta using silence as putative change point also {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Nnn , yeah .&#10;Speaker: Professor G&#10;Content: So then you 've got {disfmarker} you already have the super - structure with Gaussians and H - you know , simple H M Ms and so forth . And you {disfmarker} you might {disfmarker} So there was a {disfmarker} there was a little bit of a {disfmarker} a {disfmarker} a {disfmarker} a difference of opinion because I {disfmarker} I thought that it was {disfmarker} it 's interesting to look at what features are useful .&#10;" />
    <node id=": OK .&#10;Speaker: Professor G&#10;Content: But , uh , uh {disfmarker} So . Yeah , why don't we do the speech - nonspeech discussion ?&#10;Speaker: Postdoc F&#10;Content: Yeah . Do {disfmarker} I {disfmarker} I hear {disfmarker} you {disfmarker} you didn't {disfmarker}&#10;Speaker: PhD C&#10;Content: Speech - nonspeech ? OK .&#10;Speaker: Postdoc F&#10;Content: Uh - huh . Yeah .&#10;Speaker: PhD C&#10;Content: Um , so , uh , what we basically did so far was using the mixed file to {disfmarker} to detect s speech or nonspeech {pause} portions in that .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: And what I did so far is I just used our old Munich system , which is an HMM - ba based system with Gaussian mixtures for s speech and nonspeech . And it was a system which used only one Gaussian for silence and one Gaussian for speech . And now I added" />
    <node id="Based on the transcript, Professor G seems to have mixed feelings about focusing on pitch-related and harmonic-related features as the best use of limited time for finding a reliable indicator in short text messages. At one point, he expresses some skepticism, mentioning that they haven't found any combination that really gave an indicator so far. He also suggests that it may not be the best thing to focus on given the limited time available.&#10;&#10;However, later in the conversation, Professor G mentions that he is somewhat more hopeful for pitch-related and harmonic-related features as a potential indicator. He also mentions that they thought these features should be reasonable indicators.&#10;&#10;Overall, while Professor G expresses some reservations about focusing solely on pitch-related and harmonic-related features with the limited time available, he does not completely dismiss the idea and leaves open the possibility that it could still yield a reliable indicator." />
    <node id="aker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Or , you know , how b short should they be ? So ,&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: th he 's been playing around with a lot of these different things and {disfmarker} and so far at least has not come up with {vocalsound} any combination that really gave you an indicator .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: So I {disfmarker} I still have a hunch that there 's {disfmarker} it 's in there some place , but it may be {disfmarker} given that you have a limited time here , it {disfmarker} it just may not be the best thing to {disfmarker} {vocalsound} to {disfmarker} to focus on for the remaining of it .&#10;Speaker: PhD D&#10;Content: Yeah . To overrule , yeah .&#10;Speaker: Professor G&#10;Content: So pitch - related and harmonic - related , I 'm {disfmarker} I" />
    <node id="marker} I 'm still {pause} a little f think that 's a little funny . These things l @ @ seems like there should be ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: but {disfmarker} {vocalsound} but you don't want to keep , uh {disfmarker} keep knocking at it if it 's {disfmarker} if you 're not getting any {disfmarker} any result with that . But , I mean , the other things that we talked about is , uh , {vocalsound} pitch - related things and harmonicity - related things ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: so {disfmarker} which we thought also should be some kind of a reasonable indicator . Um {disfmarker} But , uh , a completely different tack on it wou is the one that was suggested , uh , by your colleagues in Spain ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: which is to say , don't worry so much about the , uh , features .&#10;Spe" />
    <node id="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further." />
    <node id=" .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: Can you hand me your digit form ?&#10;Speaker: PhD B&#10;Content: Sorry .&#10;Speaker: Grad A&#10;Content: I just wanna mark that you did not read digits .&#10;Speaker: Professor G&#10;Content: OK . Say hi for me .&#10;Speaker: Postdoc F&#10;Content: Good . You {disfmarker} you did get me to thinking about {disfmarker} I {disfmarker} I 'm not really sure which is more frequent , whether f f laughing {disfmarker} I think it may be an individual thing . Some people are more prone to laughing when they 're speaking .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: I was noticing that with Dan in the one that we , uh {disfmarker} we hand tran hand - segmented ,&#10;Speaker: Professor G&#10;Content: Yeah . I think {disfmarker}&#10;Speaker: Postdoc F&#10;Content: But I can't {disfmarker}&#10;Speaker: PhD D" />
    <node id="1. The speakers do not believe that their recordings represent mankind, as stated by Professor G. They acknowledge that they have a very small sample of recordings and are not claiming to get a representation of mankind in these recordings." />
    <node id=" {disfmarker}&#10;Speaker: Postdoc F&#10;Content: But I can't {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: that {disfmarker} th he has these little chuckles as he talks .&#10;Speaker: Postdoc F&#10;Content: Yeah . OK .&#10;Speaker: Professor G&#10;Content: I 'm sure it 's very individual . And {disfmarker} and {disfmarker} one thing that c that we 're not doing , of course , is we 're not claiming to , uh , get {disfmarker} be getting a representation of mankind in these recordings . We have {vocalsound} this very , very tiny sample of {disfmarker} of {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: Speech researchers ?&#10;Speaker: Professor G&#10;Content: Uh , yeah . And {disfmarker} {vocalsound} Yeah , r right .&#10;" />
    <node id=" ?&#10;Speaker: Professor G&#10;Content: Uh , yeah . And {disfmarker} {vocalsound} Yeah , r right .&#10;Speaker: PhD D&#10;Content: Speech research .&#10;Speaker: Professor G&#10;Content: So , uh , who knows . Uh {disfmarker} Yeah . Why don why don't we just {disfmarker} since we 're on this vein , why don't we just continue with , uh , what you were gonna say about the transcriptions&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: and {disfmarker} ?&#10;Speaker: Postdoc F&#10;Content: Um , um , the {disfmarker} I {disfmarker} I 'm really very for I 'm extremely fortunate with the people who , uh , applied and who are transcribing for us . They {vocalsound} are , um , um , uh really perceptive and very , um {disfmarker} and I 'm not just saying that cuz they might be hearing this .&#10;Speaker: Grad A&#10;Content: Cuz they 're gonna be transcribing it in a few" />
    <node id=" you ?&#10;Speaker: Postdoc F&#10;Content: Not directly . I 'm trying to think if {disfmarker} if I could have gotten it over a list .&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: Postdoc F&#10;Content: I don't {disfmarker} I don't think so .&#10;Speaker: Professor G&#10;Content: OK . Well , holidays may have interrupted things , cuz in {disfmarker} in {disfmarker} in {disfmarker} They {vocalsound} seem to want to {pause} get absolutely clear on standards for {disfmarker} transcription standards and so forth with {disfmarker} with us .&#10;Speaker: Postdoc F&#10;Content: Oh ! This was from before December . Yeah .&#10;Speaker: Professor G&#10;Content: Right . Because they 're {disfmarker} they 're presumably going to start recording next month .&#10;Speaker: Postdoc F&#10;Content: OK . OK .&#10;Speaker: Grad A&#10;Content: Oh , we should definitely get with them then ,&#10;Speaker: Professor G&#10;Content: So .&#10;Speaker:" />
    <node id=" saying that cuz they might be hearing this .&#10;Speaker: Grad A&#10;Content: Cuz they 're gonna be transcribing it in a few days .&#10;Speaker: Postdoc F&#10;Content: No , they 're super . They 're {disfmarker} the they {disfmarker} very quick .&#10;Speaker: PhD E&#10;Content: OK . Turn the mikes off and let 's talk .&#10;Speaker: Postdoc F&#10;Content: Yeah , I know . I am {disfmarker} I 'm serious . They 're just super . So I , um , e you know , I {disfmarker} I brought them in and , um , trained them in pairs because I think people can raise questions {disfmarker}&#10;Speaker: Grad A&#10;Content: That 's a good idea .&#10;Speaker: Postdoc F&#10;Content: you know , i i the they think about different things and they think of different {disfmarker} and um , I trained them to , uh , f on about a minute or two of the one that was already transcribed . This also gives me a sense of {disfmarker} You" />
    <node id="1. The choice for the classifier in the first step was a Gaussian distribution, specifically Javier's Gaussian distribution, as suggested by PhD D: &quot;Using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. For the second step, there is no explicit information about how the clustering process performed. The discussion focuses more on using Gaussians for modeling decision boundaries and potential change points in the data. There is no detailed conversation about the clustering process's results or any specific issues related to it." />
    <node id=" , mmm , if s we suppose that , eh , the {disfmarker} the first step {disfmarker} {vocalsound} I mean , the {disfmarker} the classifier what were the classifier from Javier or classifier from Thilo ? W What happen with the second step ? I {disfmarker} I mean , what {disfmarker} what happen with the , eh {disfmarker} the , uh , clu the , uh {disfmarker} the clu the clustering process ?&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Using the {disfmarker} the Gaussian .&#10;Speaker: Grad A&#10;Content: You mean Javier 's ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: What do you mean ?&#10;Speaker: PhD D&#10;Content: I {disfmarker} I mean , that is {disfmarker} is enough {disfmarker} is enough , eh , to work well , eh , to , eh , separate or to distinguish , eh , between overlapping zone" />
    <node id="1. Challenges in detecting laughter during speech: The main challenge is distinguishing between speaking while laughing and regular speech. This requires dealing with the additional complexity of identifying time tags indicating the beginning and end of laughter throughout the utterance. Another issue mentioned was recognizing laughter when it occurs simultaneously with speech, as the system might not perform well in these cases.&#10;&#10;2. Differences from laughter between words: Laughter between words is generally easier to detect since it can be considered a pause or break in the conversation. When laughter occurs during speech, it may require more sophisticated methods for identification, such as marking time tags and dealing with complications arising from overlapping laughter and speech. &#10;&#10;These challenges highlight the importance of addressing various scenarios when transcribing non-speech elements like breath and laughter in order to improve the overall performance of speech recognition systems." />
    <node id="The individuals in this conversation are aware of the limitations of their recordings and do not believe that they represent mankind. They explicitly acknowledge that their sample is very small, and Professor G states that they are not claiming to get a representation of mankind in these recordings. This demonstrates their understanding of the importance of having a large and diverse sample to make accurate generalizations about human behavior." />
    <node id="1. The use of Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones could be beneficial in improving speech recognition, as it would allow for more precise modeling of decision boundaries between speech and non-speech segments. This is based on PhD D's suggestion in the transcript: &quot;I think that an interesting experiment could be to prove that, if we suppose that the first step involves using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. The BIC (Bayesian Information Criterion) criterion can help in selecting the optimal number of Gaussians for modeling the data, balancing model complexity and fit. Using this method could improve the robustness of the speech recognition system by reducing overfitting or underfitting risks during the Gaussian training process.&#10;&#10;3. Manually distinguishing overlapping zones and speech zones requires careful consideration of time tags and potential complications arising from simultaneous laughter and speech, as mentioned in the transcript. This step might be labor-intensive but could provide valuable insights for improving speech recognition systems' performance, especially when dealing with non-speech elements like breath and laughter.&#10;&#10;4. Distinguishing between different types of laughter (e.g., laughter during a sentence vs. laughter after a sentence) is important for better understanding human communication nuances and improving the system's performance in recognizing and modeling these non-speech elements. This could be an additional step or application of Javier's software, allowing researchers to fine-tune their systems further based on specific needs.&#10;&#10;In conclusion, using Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones with the BIC criterion has the potential to improve speech recognition, especially when dealing with non-speech elements like breath and laughter. However, it requires careful consideration of time tags and possible complications during manual annotation, as well as further research on differentiating between various types of laughter." />
    <node id="Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: or {disfmarker} {vocalsound} or cepstrum or something , um {disfmarker} Eee {vocalsound} it 's in there somewhere probably .&#10;Speaker: PhD D&#10;Content: But , eh , what did you think about the possibility of using the Javier software ? Eh , I mean , the , uh {disfmarker} the , uh {disfmarker} the BIC criterion , the {disfmarker} the {disfmarker} t to train the {disfmarker} the Gaussian , eh , using the {disfmarker} the mark , eh , by hand , eh , eh , to distinguish be mmm , to train overlapping zone and speech zone . I mean , eh , {vocalsound} I {disfmarker} I {disfmarker} I think that an interesting , eh , experiment , eh , could be , th eh , to prove that , mmm , if s we suppose that , eh , the {disfmarker} the first step {disfmarker} {vocalsound" />
    <node id="The issue discussed among the group revolved around the observation that the simple concept of using a Gaussian distribution for decision boundaries or potential change points in the data was not working as expected. Specifically, Professor G and PhD D mentioned that the average, which is calculated by dividing the sum of the data points by the number of data points (variance), did not follow the energy in the signal. This became apparent when observing the valleys and peaks in the signal, as they didn't seem to adhere to the expected distribution based on the signal's energy.&#10;&#10;PhD D also pointed out a problem with the scale, which might be contributing to the difficulties encountered when modeling decision boundaries and potential change points in the data. The term &quot;scale&quot; in this context likely refers to how the Gaussian distributions are adjusted or normalized in relation to the data's range or spread. An incorrect scale can result in poor fitting of Gaussians to the data, leading to suboptimal decisions regarding overlaps or changes between speech and nonspeech portions in a signal.&#10;&#10;In summary, the group discussed potential issues with their Gaussian-based approach for modeling decision boundaries in the data, particularly concerning the average's failure to follow the energy in the signal, inconsistencies with valleys and peaks, and problems related to the scale of the Gaussian distributions." />
    <node id="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation." />
    <node id=" different , uh , classes to {disfmarker} to code , uh , the {disfmarker} the overlap , you will use ?&#10;Speaker: Postdoc F&#10;Content: Um , to code d&#10;Speaker: PhD D&#10;Content: What you {disfmarker} you {disfmarker}&#10;Speaker: Postdoc F&#10;Content: so types of overlap ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Um , so {nonvocalsound} at a meeting that wasn't transcribed , we worked up a {disfmarker} a typology .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: And , um {disfmarker}&#10;Speaker: PhD D&#10;Content: Look like , uh , you t you explaining in the blackboard ? The {disfmarker} ? Yeah ? Yeah .&#10;Speaker: Postdoc F&#10;Content: Yes , exactly . That hasn't changed . So it {nonvocalsound} i the {disfmarker} it 's basically a two - tiered structure where the first one is whether {non" />
    <node id="} uh , this {disfmarker} this is {disfmarker} very possibly a different , uh , topic . But , {nonvocalsound} uh , just let me say {pause} with reference to this idea of , um , {vocalsound} higher - order organization within meetings . So like in a {disfmarker} you know , the topics that are covered during a meeting with reference to the other , uh , uses of the data ,&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: so being able to {pause} find where so - and - so talked about such - and - such , then , um , um {disfmarker} e I mean , I {disfmarker} I {disfmarker} I did sort of a {disfmarker} {vocalsound} a rough {pause} pass {nonvocalsound} on encoding , like , episode - like level things on the , uh , transcribed meeting {disfmarker}&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: already transcribed meeting" />
    <node id="1. Professor G, Postdoc F, and possibly other attendees think that practical items can be useful for their purpose, as evidenced by phrases such as &quot;we don't want anything too practical&quot; and &quot;why don't we talk about practical things?&quot; However, they ultimately decide to focus on &quot;useless things,&quot; likely meaning things that are not directly applicable to their current goal but may still provide some value or insight.&#10;2. Grad A seems to have a more negative view of practicality in this context, stating &quot;I mean, we don't want anything too practical&quot; and expressing annoyance with someone named Chuck for being too practical (presumably by telling too many jokes during lunch).&#10;3. There is no direct mention of Javier's opinion on the practicality of items discussed; however, since he focuses on training models rather than using Markov models in his program, it can be inferred that he may lean towards favoring theoretical or abstract approaches over practical ones." />
    <node id="Professionor G suggests using transcribers more thoroughly to analyze overlaps in a more detailed manner. He proposes that they could go through the recordings specifically for the overlaps. This approach aims to improve the encoding of overlaps by focusing on them directly.&#10;&#10;To connect the right people to help with this concern, Professor G has already taken several steps. He mentioned a meeting with Postdoc F and Dave Gelbart, where they discussed modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue. By engaging relevant experts in the field, Professor G is actively working on finding solutions to address the limitations in encoding overlaps." />
    <node id="The name of the alternative tool to Praat, mentioned by PhD C, is &quot;trans transedit.&quot; However, it seems there might be some confusion around this name, as PhD C mentions later in the conversation that they think that's its name. It is essential to confirm the exact name and investigate its features to determine if it would be suitable for addressing the limitations in encoding overlaps during transcription." />
    <node id="disfmarker} Praat 's multi - platform .&#10;Speaker: Postdoc F&#10;Content: But I 'm just wondering , is {disfmarker} ?&#10;Speaker: PhD C&#10;Content: No . No , Praat {disfmarker} Yeah . Yeah .&#10;Speaker: Postdoc F&#10;Content: Oh ! I see .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Oh , I see . So Praat may not be {disfmarker}&#10;Speaker: PhD C&#10;Content: That 's not Praat . It 's called &quot; trans transedit &quot; {pause} I think .&#10;Speaker: Postdoc F&#10;Content: It 's a different one .&#10;Speaker: PhD C&#10;Content: The {disfmarker} the , uh {disfmarker} the tool from {disfmarker} from Susanne .&#10;Speaker: Postdoc F&#10;Content: I see . Oh , I see . OK . OK . Alright .&#10;Speaker: Professor G&#10;Content: The other thing , uh , to keep in mind , uh {disfmarker} I mean ," />
    <node id="isfmarker} uh , potentially {nonvocalsound} so .&#10;Speaker: Grad A&#10;Content: So .&#10;Speaker: Postdoc F&#10;Content: I also wanted to be sure {disfmarker} I mean , I 've {disfmarker} I 've seen the {disfmarker} this {disfmarker} this is called Praat , PRAAT , {nonvocalsound} which I guess means spee speech in Dutch or something .&#10;Speaker: Grad A&#10;Content: Yep .&#10;Speaker: PhD C&#10;Content: Yeah , but then I 'm not sure {pause} that 's the right thing for us .&#10;Speaker: Postdoc F&#10;Content: But {disfmarker} In terms {nonvocalsound} of it being {nonvocalsound} Windows {nonvocalsound} versus {disfmarker}&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: No , no . Praat isn't {disfmarker} Praat 's multi - platform .&#10;Speaker: Postdoc F&#10;Content: But I 'm just wondering , is {" />
    <node id=": Yeah , and Dave Gelbart did volunteer ,&#10;Speaker: Postdoc F&#10;Content: Good .&#10;Speaker: Grad A&#10;Content: and since he 's not here , I 'll repeat it {disfmarker} to at least modify Transcriber , which , if we don't have something else that works , I think that 's a pretty good way of going .&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: And we discussed on some methods to do it . My approach originally , and I 've already hacked on it a little bit {disfmarker} it was too slow because I was trying to display all the waveforms . But he pointed out that you don't really have to . I think that 's a good point .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: That if you just display the mix waveform and then have a user interface for editing the different channels , that 's perfectly sufficient .&#10;Speaker" />
    <node id="1. Marking audible breaths and laughter in transcripts provides crucial data for training models to recognize and model these non-speech elements, which can improve the overall performance of speech recognition systems. This is important because distinguishing between speaking while laughing and regular speech, as well as recognizing laughter during speech, can be challenging.&#10;&#10;2. There are different types of laughter that may have different implications for conversation or speech analysis, such as laughter in the middle of a sentence indicating amusement or interruption, and laughter after a sentence suggesting understanding or agreement. By explicitly marking these non-speech elements, researchers can better understand the nuances of human communication and enhance their systems' ability to recognize and model them effectively.&#10;&#10;3. Marking audible breaths and laughter in transcripts also helps address various scenarios during speech analysis, ensuring that the models are trained on a diverse range of data points including time tags indicating the beginning and end of laughter throughout the utterance. This leads to better-performing models and more accurate speech recognition systems." />
    <node id="1. After Postdoc F brought up the issue of microphone procedures and cleanliness of recordings, Professor G acknowledged the topic by repeating it. However, the specifics of their discussion on this matter are not detailed in the transcript.&#10;2. The group then moved on to discuss progress made in terms of non-speech and speech pre-segmenting of the signal, indicating that they were addressing the limitation of their current data source in accurately encoding overlaps during transcription." />
    <node id="Based on the information provided, the Postdoc (presumably Postdoc F) suggested scheduling a separate discussion with Professor G to go over and share transcripts and forced alignments. However, the specifics of how this would be done or any further details about the suggestion are not mentioned in the transcript." />
    <node id="1. The speakers discuss the importance of having both the notation and machine representation be the same in order to accurately encode overlaps during transcription. This is brought up by Grad A, and Professor G and Postdoc F agree with this sentiment. They also mention the potential existence of relevant collections at various institutions such as Columbia, UW, and NIST, indicating that they are aware of possible resources related to their topic." />
    <node id="isfmarker}&#10;Speaker: Professor G&#10;Content: So , yeah . Maybe I 'll , uh , ping them a little bit about it to {vocalsound} get that straight .&#10;Speaker: Postdoc F&#10;Content: OK . I 'm keeping the conventions {pause} absolutely {pause} as simple {nonvocalsound} as possible .&#10;Speaker: Professor G&#10;Content: Yeah . So is it {disfmarker} cuz with any luck there 'll actually be a {disfmarker} a {disfmarker} there 'll be collections at Columbia , collections at {disfmarker} at UW {disfmarker} I mean Dan {disfmarker} Dan is very interested in doing some other things ,&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Postdoc F&#10;Content: Yeah . Yeah .&#10;Speaker: Grad A&#10;Content: Well , I think it 's important both for the notation and the machine representation to be the same .&#10;Speaker: Professor G&#10;Content: and collections at NIST . So {disfmarker} Yeah .&#10;Speaker: Grad A&#10;Content: So .&#10;Spe" />
    <node id="1. The transcript does not provide a detailed explanation of the IRAM results that PhD D and Professor G are discussing. However, it appears they are referring to some analysis or research data labeled as &quot;IRAM.&quot; There seems to be some uncertainty about the relevance or general interest of this information, as indicated by PhD D's comment, &quot;I'm not sure if that's of general interest or not.&quot; Professor G also repeats &quot;IRAM&quot; and asks a question about &quot;bigram,&quot; which might be related to their discussion. To fully understand the content of their conversation, more context or information is needed.&#10;2. The group does not seem to raise any specific issues or concerns regarding the IRAM results in this part of the transcript. Instead, the discussion revolves around the classifier choice (Gaussian distribution) and the clustering process for the first step, as well as modeling decision boundaries and change points for the second step. It is possible that the group found the IRAM results interesting but decided to continue discussing other aspects of their project in this particular meeting." />
    <node id="1. The two types of laughter that the postdoc is referring to are: a) laughter that occurs in the middle of a sentence, and b) laughter that occurs after a sentence has been completed. &#10;2. These two types of laughter require extra complications such as time tags for distinguishing between them because it can be challenging to differentiate speaking while laughing from regular speech, especially when laughter occurs simultaneously with speech. By using time tags to indicate the beginning and end of laughter throughout the utterance, researchers can better identify and analyze these non-speech elements in conversations or speeches." />
    <edge source="1. The speakers, specifically Postdoc F and Professor G, consider the group of people they have in mind to be a &quot;terrific pool&quot; for potential employment. However, they acknowledge that the data source is limited and their employment may not be able to continue past the end of the month. Postdoc F mentions telling this to the group and expresses hope that she can keep them employed till then.&#10;2. The limitation of their data source is a concern for the speakers, particularly Professor G, who expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints.&#10;3. Postdoc F and Grad A mention the idea of having this group transcribe meetings multiple times for more detailed analysis, but there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources." target="disfmarker} {vocalsound} Well , you know , it might be something we 'd wanna do with some , uh , s small subset {pause} of the whole thing .&#10;Speaker: Grad A&#10;Content: Hmm . Where were they when {pause} we needed them ?&#10;Speaker: Postdoc F&#10;Content: I think {disfmarker}&#10;Speaker: Professor G&#10;Content: We certainly wouldn't wanna do it with everything .&#10;Speaker: Postdoc F&#10;Content: And I 'm also thinking these people are a terrific pool . I mean , if , uh {disfmarker} so I {disfmarker} I told them that , um , we don't know if this will continue past the end of the month&#10;Speaker: Professor G&#10;Content: Uh - huh .&#10;Speaker: Postdoc F&#10;Content: and I also {disfmarker} m I think they know that the data p source is limited and I may not be able to keep them employed till the end of the month even , although I hope to .&#10;Speaker: Professor G&#10;Content: The other thing we could do , actually , uh , is , uh , use them">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, specifically Postdoc F and Professor G, consider the group of people they have in mind to be a &quot;terrific pool&quot; for potential employment. However, they acknowledge that the data source is limited and their employment may not be able to continue past the end of the month. Postdoc F mentions telling this to the group and expresses hope that she can keep them employed till then.&#10;2. The limitation of their data source is a concern for the speakers, particularly Professor G, who expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints.&#10;3. Postdoc F and Grad A mention the idea of having this group transcribe meetings multiple times for more detailed analysis, but there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources." target="aker: PhD D&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: Let 's read some digits .&#10;Speaker: Grad A&#10;Content: OK . uuh&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: And we are {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, specifically Postdoc F and Professor G, consider the group of people they have in mind to be a &quot;terrific pool&quot; for potential employment. However, they acknowledge that the data source is limited and their employment may not be able to continue past the end of the month. Postdoc F mentions telling this to the group and expresses hope that she can keep them employed till then.&#10;2. The limitation of their data source is a concern for the speakers, particularly Professor G, who expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints.&#10;3. Postdoc F and Grad A mention the idea of having this group transcribe meetings multiple times for more detailed analysis, but there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources." target=" , although I hope to .&#10;Speaker: Professor G&#10;Content: The other thing we could do , actually , uh , is , uh , use them for a more detailed analysis of the overlaps .&#10;Speaker: Postdoc F&#10;Content: And {disfmarker} Oh , that 'd be so super . They would be so {disfmarker} s so terrific .&#10;Speaker: Grad A&#10;Content: I mean , this was something that we were talking about .&#10;Speaker: Professor G&#10;Content: Right ?&#10;Speaker: Grad A&#10;Content: We could get a very detailed overlap if they were willing to transcribe each meeting four or five times . Right ? One for each participant . So they could by hand {disfmarker}&#10;Speaker: Professor G&#10;Content: Well , that 's one way to do it .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: But I 've been saying the other thing is just go through it for the overlaps .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm , that 's right .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, specifically Postdoc F and Professor G, consider the group of people they have in mind to be a &quot;terrific pool&quot; for potential employment. However, they acknowledge that the data source is limited and their employment may not be able to continue past the end of the month. Postdoc F mentions telling this to the group and expresses hope that she can keep them employed till then.&#10;2. The limitation of their data source is a concern for the speakers, particularly Professor G, who expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints.&#10;3. Postdoc F and Grad A mention the idea of having this group transcribe meetings multiple times for more detailed analysis, but there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources." target=" you know , the thing I 'm concerned about is we wanted to do these digits&#10;Speaker: Postdoc F&#10;Content: Oh , yeah .&#10;Speaker: Professor G&#10;Content: and {disfmarker} and I haven't heard , uh , from Jose yet .&#10;Speaker: Postdoc F&#10;Content: Oh , yes .&#10;Speaker: PhD D&#10;Content: OK . What do you want ?&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: So {disfmarker}&#10;Speaker: Grad A&#10;Content: We could skip the digits .&#10;Speaker: Professor G&#10;Content: Uh {disfmarker}&#10;Speaker: Grad A&#10;Content: We don't have to read digits each time .&#10;Speaker: Professor G&#10;Content: Uh {disfmarker} I {disfmarker} I {disfmarker} I think it {disfmarker} you know , another {disfmarker} another bunch of digits . More data is good .&#10;Speaker: Grad A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah . Sure .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, specifically Postdoc F and Professor G, consider the group of people they have in mind to be a &quot;terrific pool&quot; for potential employment. However, they acknowledge that the data source is limited and their employment may not be able to continue past the end of the month. Postdoc F mentions telling this to the group and expresses hope that she can keep them employed till then.&#10;2. The limitation of their data source is a concern for the speakers, particularly Professor G, who expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints.&#10;3. Postdoc F and Grad A mention the idea of having this group transcribe meetings multiple times for more detailed analysis, but there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources." target=" we want someone else to do , or whatever .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: But this issue of the contents of the meeting in an outline form . OK .&#10;Speaker: Professor G&#10;Content: Yeah . Meaning really isn't my thing . Um {disfmarker}&#10;Speaker: Grad A&#10;Content: I think it just {disfmarker} whoever is interested can do that . I mean , so if someone wants to use that data {disfmarker}&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: We 're running a little short here .&#10;Speaker: Postdoc F&#10;Content: That 's fine .&#10;Speaker: Professor G&#10;Content: We , uh , uh , cou trying to {disfmarker}&#10;Speaker: Postdoc F&#10;Content: I 'm finished .&#10;Speaker: Professor G&#10;Content: eh , was {disfmarker} p Well , you know , the thing I 'm concerned about is we wanted to do these digits&#10;Speaker: Postdoc F&#10;Content: Oh , yeah .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, specifically Postdoc F and Professor G, consider the group of people they have in mind to be a &quot;terrific pool&quot; for potential employment. However, they acknowledge that the data source is limited and their employment may not be able to continue past the end of the month. Postdoc F mentions telling this to the group and expresses hope that she can keep them employed till then.&#10;2. The limitation of their data source is a concern for the speakers, particularly Professor G, who expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints.&#10;3. Postdoc F and Grad A mention the idea of having this group transcribe meetings multiple times for more detailed analysis, but there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources." target=" Hmm .&#10;Speaker: Professor G&#10;Content: And let the , uh , uh , statistical system {pause} determine what 's the right way to look at the data .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: I {disfmarker} I , um , I think it would be interesting to find individual features and put them together . I think that you 'd end up with a better system overall .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: But given the limitation in time {vocalsound} and given the fact that Javier 's system already exists {pause} doing this sort of thing ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: uh , but , uh , its main limitation is that , again , it 's only looking at silences which would {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Professor G&#10;Content: maybe that 's a better place to go .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, specifically Postdoc F and Professor G, consider the group of people they have in mind to be a &quot;terrific pool&quot; for potential employment. However, they acknowledge that the data source is limited and their employment may not be able to continue past the end of the month. Postdoc F mentions telling this to the group and expresses hope that she can keep them employed till then.&#10;2. The limitation of their data source is a concern for the speakers, particularly Professor G, who expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints.&#10;3. Postdoc F and Grad A mention the idea of having this group transcribe meetings multiple times for more detailed analysis, but there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources." target="1. The speakers discuss the limitation of their current data source in encoding overlaps in a detailed and complete manner. They mention that existing transcribers may not be capturing all the necessary start and end points of overlaps.&#10;2. Postdoc F had a meeting with Dave Gelbart, who provided excellent ideas on modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue.&#10;3. A more detailed analysis of overlaps was suggested by Professor G. There is also a mention of potentially using existing systems like Javier's, as creating new ones may not be feasible given time constraints.&#10;4. While the idea of having the group transcribe meetings multiple times for more detailed analysis was mentioned, there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources.&#10;5. Regarding a thorough-going musical score notation, Professor G expresses support for this idea, as it would allow for multiple channels, a single time-line, and increased clarity and flexibility. Postdoc F also agrees that such a system would be ideal. However, there is no clear consensus on implementing this solution specifically.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, specifically Postdoc F and Professor G, consider the group of people they have in mind to be a &quot;terrific pool&quot; for potential employment. However, they acknowledge that the data source is limited and their employment may not be able to continue past the end of the month. Postdoc F mentions telling this to the group and expresses hope that she can keep them employed till then.&#10;2. The limitation of their data source is a concern for the speakers, particularly Professor G, who expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints.&#10;3. Postdoc F and Grad A mention the idea of having this group transcribe meetings multiple times for more detailed analysis, but there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources." target="1. The group is divided in their opinion on whether to skip reading through a set of digits for their work. Professor G expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints, while Postdoc F and Grad A suggest that it might not be necessary to read the digits each time. However, there is no explicit consensus on skipping the digits.&#10;2. The group has concerns about not having heard back from a colleague named Jose regarding this task. This concern is expressed by Professor G, who mentions that they haven't heard from Jose yet in relation to the digits.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, specifically Postdoc F and Professor G, consider the group of people they have in mind to be a &quot;terrific pool&quot; for potential employment. However, they acknowledge that the data source is limited and their employment may not be able to continue past the end of the month. Postdoc F mentions telling this to the group and expresses hope that she can keep them employed till then.&#10;2. The limitation of their data source is a concern for the speakers, particularly Professor G, who expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints.&#10;3. Postdoc F and Grad A mention the idea of having this group transcribe meetings multiple times for more detailed analysis, but there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources." target="Based on the information provided, the Postdoc (presumably Postdoc F) suggested scheduling a separate discussion with Professor G to go over and share transcripts and forced alignments. However, the specifics of how this would be done or any further details about the suggestion are not mentioned in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers, specifically Postdoc F and Professor G, consider the group of people they have in mind to be a &quot;terrific pool&quot; for potential employment. However, they acknowledge that the data source is limited and their employment may not be able to continue past the end of the month. Postdoc F mentions telling this to the group and expresses hope that she can keep them employed till then.&#10;2. The limitation of their data source is a concern for the speakers, particularly Professor G, who expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints.&#10;3. Postdoc F and Grad A mention the idea of having this group transcribe meetings multiple times for more detailed analysis, but there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources." target="1. The speakers discuss the importance of having both the notation and machine representation be the same in order to accurately encode overlaps during transcription. This is brought up by Grad A, and Professor G and Postdoc F agree with this sentiment. They also mention the potential existence of relevant collections at various institutions such as Columbia, UW, and NIST, indicating that they are aware of possible resources related to their topic.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD D&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: Let 's read some digits .&#10;Speaker: Grad A&#10;Content: OK . uuh&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: And we are {disfmarker}" target="1. The group is divided in their opinion on whether to skip reading through a set of digits for their work. Professor G expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints, while Postdoc F and Grad A suggest that it might not be necessary to read the digits each time. However, there is no explicit consensus on skipping the digits.&#10;2. The group has concerns about not having heard back from a colleague named Jose regarding this task. This concern is expressed by Professor G, who mentions that they haven't heard from Jose yet in relation to the digits.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , although I hope to .&#10;Speaker: Professor G&#10;Content: The other thing we could do , actually , uh , is , uh , use them for a more detailed analysis of the overlaps .&#10;Speaker: Postdoc F&#10;Content: And {disfmarker} Oh , that 'd be so super . They would be so {disfmarker} s so terrific .&#10;Speaker: Grad A&#10;Content: I mean , this was something that we were talking about .&#10;Speaker: Professor G&#10;Content: Right ?&#10;Speaker: Grad A&#10;Content: We could get a very detailed overlap if they were willing to transcribe each meeting four or five times . Right ? One for each participant . So they could by hand {disfmarker}&#10;Speaker: Professor G&#10;Content: Well , that 's one way to do it .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: But I 've been saying the other thing is just go through it for the overlaps .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm , that 's right .&#10;Speaker" target="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , although I hope to .&#10;Speaker: Professor G&#10;Content: The other thing we could do , actually , uh , is , uh , use them for a more detailed analysis of the overlaps .&#10;Speaker: Postdoc F&#10;Content: And {disfmarker} Oh , that 'd be so super . They would be so {disfmarker} s so terrific .&#10;Speaker: Grad A&#10;Content: I mean , this was something that we were talking about .&#10;Speaker: Professor G&#10;Content: Right ?&#10;Speaker: Grad A&#10;Content: We could get a very detailed overlap if they were willing to transcribe each meeting four or five times . Right ? One for each participant . So they could by hand {disfmarker}&#10;Speaker: Professor G&#10;Content: Well , that 's one way to do it .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: But I 've been saying the other thing is just go through it for the overlaps .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm , that 's right .&#10;Speaker" target="Professionor G suggests using transcribers more thoroughly to analyze overlaps in a more detailed manner. He proposes that they could go through the recordings specifically for the overlaps. This approach aims to improve the encoding of overlaps by focusing on them directly.&#10;&#10;To connect the right people to help with this concern, Professor G has already taken several steps. He mentioned a meeting with Postdoc F and Dave Gelbart, where they discussed modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue. By engaging relevant experts in the field, Professor G is actively working on finding solutions to address the limitations in encoding overlaps.">
      <data key="d0">1</data>
    </edge>
    <edge source=" you know , the thing I 'm concerned about is we wanted to do these digits&#10;Speaker: Postdoc F&#10;Content: Oh , yeah .&#10;Speaker: Professor G&#10;Content: and {disfmarker} and I haven't heard , uh , from Jose yet .&#10;Speaker: Postdoc F&#10;Content: Oh , yes .&#10;Speaker: PhD D&#10;Content: OK . What do you want ?&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: So {disfmarker}&#10;Speaker: Grad A&#10;Content: We could skip the digits .&#10;Speaker: Professor G&#10;Content: Uh {disfmarker}&#10;Speaker: Grad A&#10;Content: We don't have to read digits each time .&#10;Speaker: Professor G&#10;Content: Uh {disfmarker} I {disfmarker} I {disfmarker} I think it {disfmarker} you know , another {disfmarker} another bunch of digits . More data is good .&#10;Speaker: Grad A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah . Sure .&#10;Spe" target="1. The group is divided in their opinion on whether to skip reading through a set of digits for their work. Professor G expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints, while Postdoc F and Grad A suggest that it might not be necessary to read the digits each time. However, there is no explicit consensus on skipping the digits.&#10;2. The group has concerns about not having heard back from a colleague named Jose regarding this task. This concern is expressed by Professor G, who mentions that they haven't heard from Jose yet in relation to the digits.">
      <data key="d0">1</data>
    </edge>
    <edge source=" we want someone else to do , or whatever .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: But this issue of the contents of the meeting in an outline form . OK .&#10;Speaker: Professor G&#10;Content: Yeah . Meaning really isn't my thing . Um {disfmarker}&#10;Speaker: Grad A&#10;Content: I think it just {disfmarker} whoever is interested can do that . I mean , so if someone wants to use that data {disfmarker}&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: We 're running a little short here .&#10;Speaker: Postdoc F&#10;Content: That 's fine .&#10;Speaker: Professor G&#10;Content: We , uh , uh , cou trying to {disfmarker}&#10;Speaker: Postdoc F&#10;Content: I 'm finished .&#10;Speaker: Professor G&#10;Content: eh , was {disfmarker} p Well , you know , the thing I 'm concerned about is we wanted to do these digits&#10;Speaker: Postdoc F&#10;Content: Oh , yeah .&#10;" target="1. The group is divided in their opinion on whether to skip reading through a set of digits for their work. Professor G expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints, while Postdoc F and Grad A suggest that it might not be necessary to read the digits each time. However, there is no explicit consensus on skipping the digits.&#10;2. The group has concerns about not having heard back from a colleague named Jose regarding this task. This concern is expressed by Professor G, who mentions that they haven't heard from Jose yet in relation to the digits.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Hmm .&#10;Speaker: Professor G&#10;Content: And let the , uh , uh , statistical system {pause} determine what 's the right way to look at the data .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: I {disfmarker} I , um , I think it would be interesting to find individual features and put them together . I think that you 'd end up with a better system overall .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: But given the limitation in time {vocalsound} and given the fact that Javier 's system already exists {pause} doing this sort of thing ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: uh , but , uh , its main limitation is that , again , it 's only looking at silences which would {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Professor G&#10;Content: maybe that 's a better place to go .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm" target="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Hmm .&#10;Speaker: Professor G&#10;Content: And let the , uh , uh , statistical system {pause} determine what 's the right way to look at the data .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: I {disfmarker} I , um , I think it would be interesting to find individual features and put them together . I think that you 'd end up with a better system overall .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: But given the limitation in time {vocalsound} and given the fact that Javier 's system already exists {pause} doing this sort of thing ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: uh , but , uh , its main limitation is that , again , it 's only looking at silences which would {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Professor G&#10;Content: maybe that 's a better place to go .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm" target="Based on the transcript, Professor G seems to have mixed feelings about focusing on pitch-related and harmonic-related features as the best use of limited time for finding a reliable indicator in short text messages. At one point, he expresses some skepticism, mentioning that they haven't found any combination that really gave an indicator so far. He also suggests that it may not be the best thing to focus on given the limited time available.&#10;&#10;However, later in the conversation, Professor G mentions that he is somewhat more hopeful for pitch-related and harmonic-related features as a potential indicator. He also mentions that they thought these features should be reasonable indicators.&#10;&#10;Overall, while Professor G expresses some reservations about focusing solely on pitch-related and harmonic-related features with the limited time available, he does not completely dismiss the idea and leaves open the possibility that it could still yield a reliable indicator.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD D prepared a presentation that includes the results of a study on different energy levels without the law of length. The exact content of the study is not described in detail, but it involves dividing the average measurement by the variance. The length of the presentation is not explicitly stated, but Professor G asks &quot;how long a [the presentation]?&quot; and PhD D responds, suggesting that it will be relatively quick because the results are already prepared. The discussion about the preparation made by PhD D lasts for a few exchanges, but this does not necessarily indicate the length of the actual presentation." target=" . More data is good .&#10;Speaker: Grad A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah . Sure .&#10;Speaker: Professor G&#10;Content: So {disfmarker} so I 'd like to do that . But I think , do you , maybe , eh {disfmarker} ? Did you prepare some whole thing you wanted us just to see ?&#10;Speaker: PhD D&#10;Content: Yeah . It 's {disfmarker} it 's prepared .&#10;Speaker: Professor G&#10;Content: Or what was that ? Yeah .&#10;Speaker: Postdoc F&#10;Content: Oh , k Sorry .&#10;Speaker: Professor G&#10;Content: Uh , how long a {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I {disfmarker} I think it 's {disfmarker} it 's fast , because , uh , I have the results , eh , of the study of different energy without the law length . Eh , um , eh , in the {disfmarker} in the measurement , uh , the average , uh , dividing by the {disfmarker} by the , um , variance">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD D prepared a presentation that includes the results of a study on different energy levels without the law of length. The exact content of the study is not described in detail, but it involves dividing the average measurement by the variance. The length of the presentation is not explicitly stated, but Professor G asks &quot;how long a [the presentation]?&quot; and PhD D responds, suggesting that it will be relatively quick because the results are already prepared. The discussion about the preparation made by PhD D lasts for a few exchanges, but this does not necessarily indicate the length of the actual presentation." target="Content: No , I mean , that there 's no point in going through all of that if that 's the bottom line , really .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: So , I {disfmarker} I think we have to start {disfmarker} Uh , I mean , there there 's two suggestions , really , which is , uh {disfmarker} what we said before is that ,&#10;Speaker: PhD D&#10;Content: Mmm , yeah .&#10;Speaker: Professor G&#10;Content: um , it looks like , at least that you haven't found an obvious way to normalize so that the energy is anything like a reliable , uh , indicator of the overlap .&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Professor G&#10;Content: Um , I {disfmarker} I 'm {disfmarker} I 'm still {pause} a little f think that 's a little funny . These things l @ @ seems like there should be ,">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD D prepared a presentation that includes the results of a study on different energy levels without the law of length. The exact content of the study is not described in detail, but it involves dividing the average measurement by the variance. The length of the presentation is not explicitly stated, but Professor G asks &quot;how long a [the presentation]?&quot; and PhD D responds, suggesting that it will be relatively quick because the results are already prepared. The discussion about the preparation made by PhD D lasts for a few exchanges, but this does not necessarily indicate the length of the actual presentation." target="&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: and , uh , there should overall be a , um , smaller proportion of the total energy that is explained by any particular harmonic {pause} sequence in the spectrum .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: So those are all things that should be there .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: So far , um , uh , Jose has {disfmarker} has been {disfmarker} By the way , I was told I should be calling you Pepe , but {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: by your friends , but Anyway ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: um , uh , the {disfmarker} has {disfmarker} has , uh , been exploring , uh , e largely the energy issue and , um ,">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD D prepared a presentation that includes the results of a study on different energy levels without the law of length. The exact content of the study is not described in detail, but it involves dividing the average measurement by the variance. The length of the presentation is not explicitly stated, but Professor G asks &quot;how long a [the presentation]?&quot; and PhD D responds, suggesting that it will be relatively quick because the results are already prepared. The discussion about the preparation made by PhD D lasts for a few exchanges, but this does not necessarily indicate the length of the actual presentation." target=" the {disfmarker} has {disfmarker} has , uh , been exploring , uh , e largely the energy issue and , um , as with a lot of things , it is not {disfmarker} uh , like this , it 's not as simple as it sounds .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And then there 's , you know {disfmarker} Is it energy ? Is it log energy ? Is it LPC residual energy ? Is it {disfmarker} is it {disfmarker} {vocalsound} is it , uh , delta of those things ? Uh , what is it no Obviously , just a simple number {disfmarker} {vocalsound} absolute number isn't gonna work . So {vocalsound} it should be with {disfmarker} compared to what ? Should there be a long window for the {vocalsound} normalizing factor and a short window for what you 're looking at ?&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Or , you know , how b short should they be ? So ,">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD D prepared a presentation that includes the results of a study on different energy levels without the law of length. The exact content of the study is not described in detail, but it involves dividing the average measurement by the variance. The length of the presentation is not explicitly stated, but Professor G asks &quot;how long a [the presentation]?&quot; and PhD D responds, suggesting that it will be relatively quick because the results are already prepared. The discussion about the preparation made by PhD D lasts for a few exchanges, but this does not necessarily indicate the length of the actual presentation." target=" {disfmarker} in the measurement , uh , the average , uh , dividing by the {disfmarker} by the , um , variance . Um , I {disfmarker} th i&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: the other , uh {disfmarker} the {disfmarker} the last w uh , meeting {disfmarker} eh , I don't know if you remain we have problem to {disfmarker} with the {disfmarker} {vocalsound} with {disfmarker} with the parameter {disfmarker} with the representations of parameter , because the {disfmarker} the valleys and the peaks in the signal , eh , look like , eh , it doesn't follow to the {disfmarker} to the energy in the signal .&#10;Speaker: Professor G&#10;Content: Yes . Right .&#10;Speaker: PhD D&#10;Content: And it was a problem , uh , with the scale .&#10;Speaker: Grad A&#10;Content: With what ?&#10;Speaker: PhD D&#10;Content: Eh , the scale .&#10;Speaker: Postdoc">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD D prepared a presentation that includes the results of a study on different energy levels without the law of length. The exact content of the study is not described in detail, but it involves dividing the average measurement by the variance. The length of the presentation is not explicitly stated, but Professor G asks &quot;how long a [the presentation]?&quot; and PhD D responds, suggesting that it will be relatively quick because the results are already prepared. The discussion about the preparation made by PhD D lasts for a few exchanges, but this does not necessarily indicate the length of the actual presentation." target=" when {disfmarker}&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: So , um , if they laugh between two words , you {disfmarker} you 'd get it in between the two words .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Right .&#10;Speaker: Postdoc F&#10;Content: But if they laugh across three or four words you {disfmarker} you get it after those four words . Does that matter ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Well , the thing that you {disfmarker} is hard to deal with is whe {vocalsound} when they speak while laughing . Um , and that 's , uh {disfmarker} I don't think that we can do very well with that .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: So {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: But , um , that 's not as frequent as just laughing between speaking ,&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source=" the {disfmarker} has {disfmarker} has , uh , been exploring , uh , e largely the energy issue and , um , as with a lot of things , it is not {disfmarker} uh , like this , it 's not as simple as it sounds .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And then there 's , you know {disfmarker} Is it energy ? Is it log energy ? Is it LPC residual energy ? Is it {disfmarker} is it {disfmarker} {vocalsound} is it , uh , delta of those things ? Uh , what is it no Obviously , just a simple number {disfmarker} {vocalsound} absolute number isn't gonna work . So {vocalsound} it should be with {disfmarker} compared to what ? Should there be a long window for the {vocalsound} normalizing factor and a short window for what you 're looking at ?&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Or , you know , how b short should they be ? So ," target="Based on the transcript, Professor G seems to have mixed feelings about focusing on pitch-related and harmonic-related features as the best use of limited time for finding a reliable indicator in short text messages. At one point, he expresses some skepticism, mentioning that they haven't found any combination that really gave an indicator so far. He also suggests that it may not be the best thing to focus on given the limited time available.&#10;&#10;However, later in the conversation, Professor G mentions that he is somewhat more hopeful for pitch-related and harmonic-related features as a potential indicator. He also mentions that they thought these features should be reasonable indicators.&#10;&#10;Overall, while Professor G expresses some reservations about focusing solely on pitch-related and harmonic-related features with the limited time available, he does not completely dismiss the idea and leaves open the possibility that it could still yield a reliable indicator.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker} in the measurement , uh , the average , uh , dividing by the {disfmarker} by the , um , variance . Um , I {disfmarker} th i&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: the other , uh {disfmarker} the {disfmarker} the last w uh , meeting {disfmarker} eh , I don't know if you remain we have problem to {disfmarker} with the {disfmarker} {vocalsound} with {disfmarker} with the parameter {disfmarker} with the representations of parameter , because the {disfmarker} the valleys and the peaks in the signal , eh , look like , eh , it doesn't follow to the {disfmarker} to the energy in the signal .&#10;Speaker: Professor G&#10;Content: Yes . Right .&#10;Speaker: PhD D&#10;Content: And it was a problem , uh , with the scale .&#10;Speaker: Grad A&#10;Content: With what ?&#10;Speaker: PhD D&#10;Content: Eh , the scale .&#10;Speaker: Postdoc" target="The issue discussed among the group revolved around the observation that the simple concept of using a Gaussian distribution for decision boundaries or potential change points in the data was not working as expected. Specifically, Professor G and PhD D mentioned that the average, which is calculated by dividing the sum of the data points by the number of data points (variance), did not follow the energy in the signal. This became apparent when observing the valleys and peaks in the signal, as they didn't seem to adhere to the expected distribution based on the signal's energy.&#10;&#10;PhD D also pointed out a problem with the scale, which might be contributing to the difficulties encountered when modeling decision boundaries and potential change points in the data. The term &quot;scale&quot; in this context likely refers to how the Gaussian distributions are adjusted or normalized in relation to the data's range or spread. An incorrect scale can result in poor fitting of Gaussians to the data, leading to suboptimal decisions regarding overlaps or changes between speech and nonspeech portions in a signal.&#10;&#10;In summary, the group discussed potential issues with their Gaussian-based approach for modeling decision boundaries in the data, particularly concerning the average's failure to follow the energy in the signal, inconsistencies with valleys and peaks, and problems related to the scale of the Gaussian distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source=" when {disfmarker}&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: So , um , if they laugh between two words , you {disfmarker} you 'd get it in between the two words .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Right .&#10;Speaker: Postdoc F&#10;Content: But if they laugh across three or four words you {disfmarker} you get it after those four words . Does that matter ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Well , the thing that you {disfmarker} is hard to deal with is whe {vocalsound} when they speak while laughing . Um , and that 's , uh {disfmarker} I don't think that we can do very well with that .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: So {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: But , um , that 's not as frequent as just laughing between speaking ,&#10;Speaker:" target="1. The postdoc and PhD speaker improved the &quot;">
      <data key="d0">1</data>
    </edge>
    <edge source=" when {disfmarker}&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: So , um , if they laugh between two words , you {disfmarker} you 'd get it in between the two words .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Right .&#10;Speaker: Postdoc F&#10;Content: But if they laugh across three or four words you {disfmarker} you get it after those four words . Does that matter ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Well , the thing that you {disfmarker} is hard to deal with is whe {vocalsound} when they speak while laughing . Um , and that 's , uh {disfmarker} I don't think that we can do very well with that .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: So {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: But , um , that 's not as frequent as just laughing between speaking ,&#10;Speaker:" target="1. The two types of laughter that the postdoc is referring to are:&#10;&#09;* Laughter that occurs in the middle of a sentence.&#10;&#09;* Laughter that occurs after a sentence has been completed.&#10;2. Distinguishing between these two types of laughter is important because they may have different implications for the conversation or speech analysis. For example, laughter in the middle of a sentence might indicate amusement or interruption, while laughter after a sentence could suggest understanding or agreement. By distinguishing between them, researchers can better understand the nuances of human communication and improve the performance of their systems in recognizing and modeling these non-speech elements.">
      <data key="d0">1</data>
    </edge>
    <edge source=" when {disfmarker}&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: So , um , if they laugh between two words , you {disfmarker} you 'd get it in between the two words .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Right .&#10;Speaker: Postdoc F&#10;Content: But if they laugh across three or four words you {disfmarker} you get it after those four words . Does that matter ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Well , the thing that you {disfmarker} is hard to deal with is whe {vocalsound} when they speak while laughing . Um , and that 's , uh {disfmarker} I don't think that we can do very well with that .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: So {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: But , um , that 's not as frequent as just laughing between speaking ,&#10;Speaker:" target="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further.">
      <data key="d0">1</data>
    </edge>
    <edge source=" when {disfmarker}&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: So , um , if they laugh between two words , you {disfmarker} you 'd get it in between the two words .&#10;Speaker: PhD B&#10;Content: Mm - hmm . Right .&#10;Speaker: Postdoc F&#10;Content: But if they laugh across three or four words you {disfmarker} you get it after those four words . Does that matter ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Well , the thing that you {disfmarker} is hard to deal with is whe {vocalsound} when they speak while laughing . Um , and that 's , uh {disfmarker} I don't think that we can do very well with that .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD B&#10;Content: So {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: But , um , that 's not as frequent as just laughing between speaking ,&#10;Speaker:" target="1. Challenges in detecting laughter during speech: The main challenge is distinguishing between speaking while laughing and regular speech. This requires dealing with the additional complexity of identifying time tags indicating the beginning and end of laughter throughout the utterance. Another issue mentioned was recognizing laughter when it occurs simultaneously with speech, as the system might not perform well in these cases.&#10;&#10;2. Differences from laughter between words: Laughter between words is generally easier to detect since it can be considered a pause or break in the conversation. When laughter occurs during speech, it may require more sophisticated methods for identification, such as marking time tags and dealing with complications arising from overlapping laughter and speech. &#10;&#10;These challenges highlight the importance of addressing various scenarios when transcribing non-speech elements like breath and laughter in order to improve the overall performance of speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discuss the limitation of their current data source in encoding overlaps in a detailed and complete manner. They mention that existing transcribers may not be capturing all the necessary start and end points of overlaps.&#10;2. Postdoc F had a meeting with Dave Gelbart, who provided excellent ideas on modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue.&#10;3. A more detailed analysis of overlaps was suggested by Professor G. There is also a mention of potentially using existing systems like Javier's, as creating new ones may not be feasible given time constraints.&#10;4. While the idea of having the group transcribe meetings multiple times for more detailed analysis was mentioned, there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources.&#10;5. Regarding a thorough-going musical score notation, Professor G expresses support for this idea, as it would allow for multiple channels, a single time-line, and increased clarity and flexibility. Postdoc F also agrees that such a system would be ideal. However, there is no clear consensus on implementing this solution specifically." target=" desired in the corpus ultimately .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: So we don't have start and end points {nonvocalsound} at each point where there 's an overlap . We just have the {disfmarker} the {nonvocalsound} overlaps {nonvocalsound} encoded in a simple bin . Well , OK . So {nonvocalsound} @ @ the limits of the {nonvocalsound} over of {disfmarker} of the interface are {vocalsound} such that we were {disfmarker} at this meeting we were entertaining how we might either expand {nonvocalsound} the {disfmarker} the {vocalsound} interface or find other tools which already {pause} do what would be useful . Because what would ultimately be , um , ideal in my {disfmarker} my view and I think {disfmarker} I mean , I had the sense that it was consensus , is that , um , a thorough - going musical score notation would be {nonvocalsound} the best way to go . Because {nonvocalsound}">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discuss the limitation of their current data source in encoding overlaps in a detailed and complete manner. They mention that existing transcribers may not be capturing all the necessary start and end points of overlaps.&#10;2. Postdoc F had a meeting with Dave Gelbart, who provided excellent ideas on modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue.&#10;3. A more detailed analysis of overlaps was suggested by Professor G. There is also a mention of potentially using existing systems like Javier's, as creating new ones may not be feasible given time constraints.&#10;4. While the idea of having the group transcribe meetings multiple times for more detailed analysis was mentioned, there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources.&#10;5. Regarding a thorough-going musical score notation, Professor G expresses support for this idea, as it would allow for multiple channels, a single time-line, and increased clarity and flexibility. Postdoc F also agrees that such a system would be ideal. However, there is no clear consensus on implementing this solution specifically." target=" that , um , a thorough - going musical score notation would be {nonvocalsound} the best way to go . Because {nonvocalsound} you can have multiple channels , there 's a single time - line , it 's very clear , flexible , and all those nice things .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: OK . So , um , um , I spoke {disfmarker} I had a meeting with Dave Gelbart on {disfmarker} on {disfmarker} and he had , uh , excellent ideas on how {pause} the interface could be {pause} modified to {disfmarker} to do this kind of representation . But , um , he {disfmarker} in the meantime you were checking into the existence of already , um , existing interfaces which might already have these properties . So , do you wanna say something about that ?&#10;Speaker: PhD C&#10;Content: Yes . Um , I {vocalsound} talked with , uh , Munich guys from {disfmarker} from Ludwi - Ludwig Maximilians University , who do a lot of transcribing and transliter">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discuss the limitation of their current data source in encoding overlaps in a detailed and complete manner. They mention that existing transcribers may not be capturing all the necessary start and end points of overlaps.&#10;2. Postdoc F had a meeting with Dave Gelbart, who provided excellent ideas on modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue.&#10;3. A more detailed analysis of overlaps was suggested by Professor G. There is also a mention of potentially using existing systems like Javier's, as creating new ones may not be feasible given time constraints.&#10;4. While the idea of having the group transcribe meetings multiple times for more detailed analysis was mentioned, there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources.&#10;5. Regarding a thorough-going musical score notation, Professor G expresses support for this idea, as it would allow for multiple channels, a single time-line, and increased clarity and flexibility. Postdoc F also agrees that such a system would be ideal. However, there is no clear consensus on implementing this solution specifically." target=" , um {disfmarker} OK , and {disfmarker} and also Da - Dave Gelbart . So there 's this {disfmarker} this problem of {disfmarker} and w and {disfmarker} so we had this meeting . Th - the {nonvocalsound} {disfmarker} also Adam , before the {disfmarker} the {disfmarker} before you went away . Uh we , um {disfmarker} regarding the representation {nonvocalsound} of overlaps , because at present , {nonvocalsound} {vocalsound} um , because {nonvocalsound} of the limitations of {vocalsound} th the interface we 're using , overlaps are , uh , not being {nonvocalsound} encoded by {nonvocalsound} the transcribers in as complete {nonvocalsound} and , uh , detailed a way as it might be , and as might be desired {disfmarker} I think would be desired in the corpus ultimately .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: So">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discuss the limitation of their current data source in encoding overlaps in a detailed and complete manner. They mention that existing transcribers may not be capturing all the necessary start and end points of overlaps.&#10;2. Postdoc F had a meeting with Dave Gelbart, who provided excellent ideas on modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue.&#10;3. A more detailed analysis of overlaps was suggested by Professor G. There is also a mention of potentially using existing systems like Javier's, as creating new ones may not be feasible given time constraints.&#10;4. While the idea of having the group transcribe meetings multiple times for more detailed analysis was mentioned, there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources.&#10;5. Regarding a thorough-going musical score notation, Professor G expresses support for this idea, as it would allow for multiple channels, a single time-line, and increased clarity and flexibility. Postdoc F also agrees that such a system would be ideal. However, there is no clear consensus on implementing this solution specifically." target="Speaker: PhD B&#10;Content: But you s&#10;Speaker: Grad A&#10;Content: What our decision was is that {pause} we 'll go ahead with what we have with a not very fine time scale on the overlaps .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Right . Yeah .&#10;Speaker: Grad A&#10;Content: And {disfmarker} and do what we can later {pause} to clean that up if we need to .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Right .&#10;Speaker: Postdoc F&#10;Content: And {disfmarker} and I was just thinking that , um , {vocalsound} if it were possible to bring that in , like , {vocalsound} you know , this week , then {nonvocalsound} when they 're encoding the overlaps {nonvocalsound} it would be nice for them to be able to specify when {disfmarker} you know , the start points and end points of overlaps .&#10;Speaker: Professor G&#10;Content: Uh - huh .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discuss the limitation of their current data source in encoding overlaps in a detailed and complete manner. They mention that existing transcribers may not be capturing all the necessary start and end points of overlaps.&#10;2. Postdoc F had a meeting with Dave Gelbart, who provided excellent ideas on modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue.&#10;3. A more detailed analysis of overlaps was suggested by Professor G. There is also a mention of potentially using existing systems like Javier's, as creating new ones may not be feasible given time constraints.&#10;4. While the idea of having the group transcribe meetings multiple times for more detailed analysis was mentioned, there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources.&#10;5. Regarding a thorough-going musical score notation, Professor G expresses support for this idea, as it would allow for multiple channels, a single time-line, and increased clarity and flexibility. Postdoc F also agrees that such a system would be ideal. However, there is no clear consensus on implementing this solution specifically." target=" But , I mean , the other thing is since we 've been spending so much time thinking about overlaps is {disfmarker} is maybe get a much more detailed analysis of the overlaps .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: But anyway , I 'm {disfmarker} I 'm open to c our consideration .&#10;Speaker: Postdoc F&#10;Content: That 'd be great .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: I {disfmarker} I don't wanna say that by fiat .&#10;Speaker: Postdoc F&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: I 'm open to every consideration of {vocalsound} what are some other kinds of detailed analysis that would be most useful .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: And , uh , uh ,&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: I {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discuss the limitation of their current data source in encoding overlaps in a detailed and complete manner. They mention that existing transcribers may not be capturing all the necessary start and end points of overlaps.&#10;2. Postdoc F had a meeting with Dave Gelbart, who provided excellent ideas on modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue.&#10;3. A more detailed analysis of overlaps was suggested by Professor G. There is also a mention of potentially using existing systems like Javier's, as creating new ones may not be feasible given time constraints.&#10;4. While the idea of having the group transcribe meetings multiple times for more detailed analysis was mentioned, there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources.&#10;5. Regarding a thorough-going musical score notation, Professor G expresses support for this idea, as it would allow for multiple channels, a single time-line, and increased clarity and flexibility. Postdoc F also agrees that such a system would be ideal. However, there is no clear consensus on implementing this solution specifically." target="1. The group is divided in their opinion on whether to skip reading through a set of digits for their work. Professor G expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints, while Postdoc F and Grad A suggest that it might not be necessary to read the digits each time. However, there is no explicit consensus on skipping the digits.&#10;2. The group has concerns about not having heard back from a colleague named Jose regarding this task. This concern is expressed by Professor G, who mentions that they haven't heard from Jose yet in relation to the digits.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discuss the limitation of their current data source in encoding overlaps in a detailed and complete manner. They mention that existing transcribers may not be capturing all the necessary start and end points of overlaps.&#10;2. Postdoc F had a meeting with Dave Gelbart, who provided excellent ideas on modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue.&#10;3. A more detailed analysis of overlaps was suggested by Professor G. There is also a mention of potentially using existing systems like Javier's, as creating new ones may not be feasible given time constraints.&#10;4. While the idea of having the group transcribe meetings multiple times for more detailed analysis was mentioned, there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources.&#10;5. Regarding a thorough-going musical score notation, Professor G expresses support for this idea, as it would allow for multiple channels, a single time-line, and increased clarity and flexibility. Postdoc F also agrees that such a system would be ideal. However, there is no clear consensus on implementing this solution specifically." target="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discuss the limitation of their current data source in encoding overlaps in a detailed and complete manner. They mention that existing transcribers may not be capturing all the necessary start and end points of overlaps.&#10;2. Postdoc F had a meeting with Dave Gelbart, who provided excellent ideas on modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue.&#10;3. A more detailed analysis of overlaps was suggested by Professor G. There is also a mention of potentially using existing systems like Javier's, as creating new ones may not be feasible given time constraints.&#10;4. While the idea of having the group transcribe meetings multiple times for more detailed analysis was mentioned, there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources.&#10;5. Regarding a thorough-going musical score notation, Professor G expresses support for this idea, as it would allow for multiple channels, a single time-line, and increased clarity and flexibility. Postdoc F also agrees that such a system would be ideal. However, there is no clear consensus on implementing this solution specifically." target="Professionor G suggests using transcribers more thoroughly to analyze overlaps in a more detailed manner. He proposes that they could go through the recordings specifically for the overlaps. This approach aims to improve the encoding of overlaps by focusing on them directly.&#10;&#10;To connect the right people to help with this concern, Professor G has already taken several steps. He mentioned a meeting with Postdoc F and Dave Gelbart, where they discussed modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue. By engaging relevant experts in the field, Professor G is actively working on finding solutions to address the limitations in encoding overlaps.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discuss the limitation of their current data source in encoding overlaps in a detailed and complete manner. They mention that existing transcribers may not be capturing all the necessary start and end points of overlaps.&#10;2. Postdoc F had a meeting with Dave Gelbart, who provided excellent ideas on modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue.&#10;3. A more detailed analysis of overlaps was suggested by Professor G. There is also a mention of potentially using existing systems like Javier's, as creating new ones may not be feasible given time constraints.&#10;4. While the idea of having the group transcribe meetings multiple times for more detailed analysis was mentioned, there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources.&#10;5. Regarding a thorough-going musical score notation, Professor G expresses support for this idea, as it would allow for multiple channels, a single time-line, and increased clarity and flexibility. Postdoc F also agrees that such a system would be ideal. However, there is no clear consensus on implementing this solution specifically." target="Based on the information provided, the Postdoc (presumably Postdoc F) suggested scheduling a separate discussion with Professor G to go over and share transcripts and forced alignments. However, the specifics of how this would be done or any further details about the suggestion are not mentioned in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discuss the limitation of their current data source in encoding overlaps in a detailed and complete manner. They mention that existing transcribers may not be capturing all the necessary start and end points of overlaps.&#10;2. Postdoc F had a meeting with Dave Gelbart, who provided excellent ideas on modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue.&#10;3. A more detailed analysis of overlaps was suggested by Professor G. There is also a mention of potentially using existing systems like Javier's, as creating new ones may not be feasible given time constraints.&#10;4. While the idea of having the group transcribe meetings multiple times for more detailed analysis was mentioned, there is no explicit discussion about employment beyond the end of the month or the availability of additional data sources.&#10;5. Regarding a thorough-going musical score notation, Professor G expresses support for this idea, as it would allow for multiple channels, a single time-line, and increased clarity and flexibility. Postdoc F also agrees that such a system would be ideal. However, there is no clear consensus on implementing this solution specifically." target="1. The speakers discuss the importance of having both the notation and machine representation be the same in order to accurately encode overlaps during transcription. This is brought up by Grad A, and Professor G and Postdoc F agree with this sentiment. They also mention the potential existence of relevant collections at various institutions such as Columbia, UW, and NIST, indicating that they are aware of possible resources related to their topic.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target=": Well , um , I can {pause} give you an update on the {pause} transcription effort .&#10;Speaker: Professor G&#10;Content: Great .&#10;Speaker: Postdoc F&#10;Content: Uh , maybe {nonvocalsound} raise the issue of microphone , uh , um procedures with reference to the {pause} cleanliness of the recordings .&#10;Speaker: Professor G&#10;Content: OK , transcription , uh , microphone issues {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And then maybe {nonvocalsound} ask , th uh , these guys . The {disfmarker} we have great {disfmarker} great , uh , p steps forward in terms of the nonspeech - speech pre - segmenting of the signal .&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Well , we have steps forward .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Well , it 's a {disfmarker} it 's a big improvement .&#10;Speaker: PhD C&#10;Content: I would prefer this .&#10;Speaker: Professor G&#10;Content: Yes">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="aker: Grad A&#10;Content: Oh , we should definitely get with them then ,&#10;Speaker: Professor G&#10;Content: So .&#10;Speaker: Grad A&#10;Content: and agree upon a format . Though I don't remember email on that . So was I not in the loop on that ?&#10;Speaker: Professor G&#10;Content: Um . Yeah , I don't think I mailed anybody . I just think I told them to contact Jane {disfmarker} that , uh , if they had a {disfmarker}&#10;Speaker: Grad A&#10;Content: Oh , OK .&#10;Speaker: Postdoc F&#10;Content: That 's right .&#10;Speaker: Professor G&#10;Content: if , uh {disfmarker} that {disfmarker} that , uh , as the point person on it .&#10;Speaker: Grad A&#10;Content: Yeah , I think that 's right .&#10;Speaker: Professor G&#10;Content: But {disfmarker}&#10;Speaker: Grad A&#10;Content: Just , uh {disfmarker}&#10;Speaker: Professor G&#10;Content: So , yeah . Maybe I 'll , uh , ping them a little bit about it to">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="Speaker: Grad A&#10;Content: OK . We seem to be recording .&#10;Speaker: Professor G&#10;Content: Alright !&#10;Speaker: Grad A&#10;Content: So , sorry about not {disfmarker}&#10;Speaker: Professor G&#10;Content: We 're not crashing .&#10;Speaker: PhD D&#10;Content: Number four .&#10;Speaker: Grad A&#10;Content: not pre - doing everything . The lunch went a little later than I was expecting , Chuck .&#10;Speaker: PhD E&#10;Content: Hmm ?&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Chuck was telling too many jokes , or something ?&#10;Speaker: Grad A&#10;Content: Yep . Pretty much .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: OK . {vocalsound} Does anybody have an agenda ?&#10;Speaker: Grad A&#10;Content: No .&#10;Speaker: Postdoc F&#10;Content: Well , I 'm {disfmarker} I sent a couple of items . They 're {disfmarker} they 're sort of practical .&#10;Speaker: Professor G&#10;Content: I thought">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target=" the right people connected in , who had the time .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Postdoc F&#10;Content: Yeah , yeah .&#10;Speaker: Professor G&#10;Content: So , um , eh {disfmarker}&#10;Speaker: Grad A&#10;Content: Is he on the mailing list ? The Meeting Recorder mailing li ?&#10;Speaker: Postdoc F&#10;Content: Oh !&#10;Speaker: Grad A&#10;Content: We should add him .&#10;Speaker: Postdoc F&#10;Content: Yeah . I {disfmarker} I {disfmarker} I don't know for sure .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: PhD E&#10;Content: Did something happen , Morgan , that he got put on this , or was he already on it ,&#10;Speaker: Grad A&#10;Content: Add him .&#10;Speaker: PhD E&#10;Content: or {disfmarker} ?&#10;Speaker: Professor G&#10;Content: No , I , eh , eh , p It {disfmarker} it oc I {disfmarker} h it 's {disfmarker} Yeah , something happened">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="} it 's a big improvement .&#10;Speaker: PhD C&#10;Content: I would prefer this .&#10;Speaker: Professor G&#10;Content: Yes . Yeah , well . OK . Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: We talk about the {disfmarker} {vocalsound} the results of&#10;Speaker: Professor G&#10;Content: You have some {disfmarker} Yeah .&#10;Speaker: Grad A&#10;Content: I have a little bit of IRAM stuff&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: use {disfmarker}&#10;Speaker: Grad A&#10;Content: but {pause} I 'm not sure if that 's of general interest or not .&#10;Speaker: Professor G&#10;Content: Uh , bigram ?&#10;Speaker: Grad A&#10;Content: IRAM .&#10;Speaker: PhD D&#10;Content: IRAM .&#10;Speaker: Professor G&#10;Content: IRAM .&#10;Speaker: Grad A&#10;Content: IRAM , bigram ,&#10;Speaker: Professor G&#10;Content: Well , m maybe .&#10;Speaker: PhD D&#10;Content: Bi -">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="1. The group is divided in their opinion on whether to skip reading through a set of digits for their work. Professor G expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints, while Postdoc F and Grad A suggest that it might not be necessary to read the digits each time. However, there is no explicit consensus on skipping the digits.&#10;2. The group has concerns about not having heard back from a colleague named Jose regarding this task. This concern is expressed by Professor G, who mentions that they haven't heard from Jose yet in relation to the digits.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="1. The speakers generally agree on the importance of using transcribers to mark audible breaths and laughter in the transcripts, which can then be used to train models for speech and nonspeech. This is based on PhD B's suggestion to explicitly mark these non-speech elements and Postdoc F's confirmation that they are currently being marked in curly brackets as &quot;inhale&quot; or &quot;breath.&quot;&#10;   &#10;2. There is also discussion about the usefulness of transcribing nonspeech elements like breath and laughter for modeling purposes, as mentioned by PhD B when they discuss the SRI recognizer being able to transcribe these elements and model them. Professor G agrees with this approach, suggesting that they should not worry so much about features and instead use general features for both speech and nonspeech modeling.&#10;&#10;In summary, the speakers agree on the significance of using transcribers to mark non-speech elements such as breath and laughter in transcripts, which can then be utilized to train models for speech and nonspeech. They also consider modeling these elements to improve the overall performance of their system.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="Based on the transcript, it is still considered important to have a good tool for checking and possibly augmenting the IBM transcripts, even in the absence of the person who previously handled this task. This sentiment is expressed by PhD B, who states &quot;I think having a good tool is worth something no matter what&quot; (lines 10-11). Furthermore, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort, implying that the group has made progress in their efforts to improve the transcription process.&#10;&#10;In terms of volunteering to help with this process, Grad A mentions that Dave Gelbart had volunteered (line 28). However, it is not explicitly stated in the transcript whether Dave Gelbart will be specifically assisting with the task of checking and augmenting the IBM transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="1. The speakers do not believe that their recordings represent mankind, as stated by Professor G. They acknowledge that they have a very small sample of recordings and are not claiming to get a representation of mankind in these recordings.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="The individuals in this conversation are aware of the limitations of their recordings and do not believe that they represent mankind. They explicitly acknowledge that their sample is very small, and Professor G states that they are not claiming to get a representation of mankind in these recordings. This demonstrates their understanding of the importance of having a large and diverse sample to make accurate generalizations about human behavior.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="1. Professor G, Postdoc F, and possibly other attendees think that practical items can be useful for their purpose, as evidenced by phrases such as &quot;we don't want anything too practical&quot; and &quot;why don't we talk about practical things?&quot; However, they ultimately decide to focus on &quot;useless things,&quot; likely meaning things that are not directly applicable to their current goal but may still provide some value or insight.&#10;2. Grad A seems to have a more negative view of practicality in this context, stating &quot;I mean, we don't want anything too practical&quot; and expressing annoyance with someone named Chuck for being too practical (presumably by telling too many jokes during lunch).&#10;3. There is no direct mention of Javier's opinion on the practicality of items discussed; however, since he focuses on training models rather than using Markov models in his program, it can be inferred that he may lean towards favoring theoretical or abstract approaches over practical ones.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="Professionor G suggests using transcribers more thoroughly to analyze overlaps in a more detailed manner. He proposes that they could go through the recordings specifically for the overlaps. This approach aims to improve the encoding of overlaps by focusing on them directly.&#10;&#10;To connect the right people to help with this concern, Professor G has already taken several steps. He mentioned a meeting with Postdoc F and Dave Gelbart, where they discussed modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue. By engaging relevant experts in the field, Professor G is actively working on finding solutions to address the limitations in encoding overlaps.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="1. After Postdoc F brought up the issue of microphone procedures and cleanliness of recordings, Professor G acknowledged the topic by repeating it. However, the specifics of their discussion on this matter are not detailed in the transcript.&#10;2. The group then moved on to discuss progress made in terms of non-speech and speech pre-segmenting of the signal, indicating that they were addressing the limitation of their current data source in accurately encoding overlaps during transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Speaker A sent the email to the meeting participants, but it is not specified who exactly was included in the email.&#10;2. Professor G's response regarding the progress mentioned in the email is not explicitly stated in the transcript. However, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort. It can be inferred that the progress mentioned in the email was related to these advancements." target="Based on the information provided, the Postdoc (presumably Postdoc F) suggested scheduling a separate discussion with Professor G to go over and share transcripts and forced alignments. However, the specifics of how this would be done or any further details about the suggestion are not mentioned in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Well , um , I can {pause} give you an update on the {pause} transcription effort .&#10;Speaker: Professor G&#10;Content: Great .&#10;Speaker: Postdoc F&#10;Content: Uh , maybe {nonvocalsound} raise the issue of microphone , uh , um procedures with reference to the {pause} cleanliness of the recordings .&#10;Speaker: Professor G&#10;Content: OK , transcription , uh , microphone issues {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And then maybe {nonvocalsound} ask , th uh , these guys . The {disfmarker} we have great {disfmarker} great , uh , p steps forward in terms of the nonspeech - speech pre - segmenting of the signal .&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Well , we have steps forward .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Well , it 's a {disfmarker} it 's a big improvement .&#10;Speaker: PhD C&#10;Content: I would prefer this .&#10;Speaker: Professor G&#10;Content: Yes" target="Based on the transcript, it is still considered important to have a good tool for checking and possibly augmenting the IBM transcripts, even in the absence of the person who previously handled this task. This sentiment is expressed by PhD B, who states &quot;I think having a good tool is worth something no matter what&quot; (lines 10-11). Furthermore, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort, implying that the group has made progress in their efforts to improve the transcription process.&#10;&#10;In terms of volunteering to help with this process, Grad A mentions that Dave Gelbart had volunteered (line 28). However, it is not explicitly stated in the transcript whether Dave Gelbart will be specifically assisting with the task of checking and augmenting the IBM transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Well , um , I can {pause} give you an update on the {pause} transcription effort .&#10;Speaker: Professor G&#10;Content: Great .&#10;Speaker: Postdoc F&#10;Content: Uh , maybe {nonvocalsound} raise the issue of microphone , uh , um procedures with reference to the {pause} cleanliness of the recordings .&#10;Speaker: Professor G&#10;Content: OK , transcription , uh , microphone issues {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And then maybe {nonvocalsound} ask , th uh , these guys . The {disfmarker} we have great {disfmarker} great , uh , p steps forward in terms of the nonspeech - speech pre - segmenting of the signal .&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Well , we have steps forward .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Well , it 's a {disfmarker} it 's a big improvement .&#10;Speaker: PhD C&#10;Content: I would prefer this .&#10;Speaker: Professor G&#10;Content: Yes" target="Based on the transcript, Professor G believes that it is still important to have people wear microphones properly in a real-life situation, even if the recordings may not be perfect. He suggests that trying to have people wear the microphones properly does not hurt the naturalness of the situation. This is because, in the target applications they are discussing, people will not be wearing head-mounted mikes anyway. The head-mounted mikes are only for research purposes and help ensure cleaner recordings. Professor G also mentions that systems exist to do pretty good selection of who's talking, even when microphones are not worn perfectly. Overall, while imperfect recordings may occur when people wear microphones improperly, the benefits of better sound quality and clearer speech outweigh the drawbacks in a research context.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Well , um , I can {pause} give you an update on the {pause} transcription effort .&#10;Speaker: Professor G&#10;Content: Great .&#10;Speaker: Postdoc F&#10;Content: Uh , maybe {nonvocalsound} raise the issue of microphone , uh , um procedures with reference to the {pause} cleanliness of the recordings .&#10;Speaker: Professor G&#10;Content: OK , transcription , uh , microphone issues {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And then maybe {nonvocalsound} ask , th uh , these guys . The {disfmarker} we have great {disfmarker} great , uh , p steps forward in terms of the nonspeech - speech pre - segmenting of the signal .&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Well , we have steps forward .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Well , it 's a {disfmarker} it 's a big improvement .&#10;Speaker: PhD C&#10;Content: I would prefer this .&#10;Speaker: Professor G&#10;Content: Yes" target="Based on the transcript, Professor G seems to have mixed feelings about focusing on pitch-related and harmonic-related features as the best use of limited time for finding a reliable indicator in short text messages. At one point, he expresses some skepticism, mentioning that they haven't found any combination that really gave an indicator so far. He also suggests that it may not be the best thing to focus on given the limited time available.&#10;&#10;However, later in the conversation, Professor G mentions that he is somewhat more hopeful for pitch-related and harmonic-related features as a potential indicator. He also mentions that they thought these features should be reasonable indicators.&#10;&#10;Overall, while Professor G expresses some reservations about focusing solely on pitch-related and harmonic-related features with the limited time available, he does not completely dismiss the idea and leaves open the possibility that it could still yield a reliable indicator.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Well , um , I can {pause} give you an update on the {pause} transcription effort .&#10;Speaker: Professor G&#10;Content: Great .&#10;Speaker: Postdoc F&#10;Content: Uh , maybe {nonvocalsound} raise the issue of microphone , uh , um procedures with reference to the {pause} cleanliness of the recordings .&#10;Speaker: Professor G&#10;Content: OK , transcription , uh , microphone issues {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And then maybe {nonvocalsound} ask , th uh , these guys . The {disfmarker} we have great {disfmarker} great , uh , p steps forward in terms of the nonspeech - speech pre - segmenting of the signal .&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Well , we have steps forward .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Well , it 's a {disfmarker} it 's a big improvement .&#10;Speaker: PhD C&#10;Content: I would prefer this .&#10;Speaker: Professor G&#10;Content: Yes" target="1. The speakers do not believe that their recordings represent mankind, as stated by Professor G. They acknowledge that they have a very small sample of recordings and are not claiming to get a representation of mankind in these recordings.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Well , um , I can {pause} give you an update on the {pause} transcription effort .&#10;Speaker: Professor G&#10;Content: Great .&#10;Speaker: Postdoc F&#10;Content: Uh , maybe {nonvocalsound} raise the issue of microphone , uh , um procedures with reference to the {pause} cleanliness of the recordings .&#10;Speaker: Professor G&#10;Content: OK , transcription , uh , microphone issues {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And then maybe {nonvocalsound} ask , th uh , these guys . The {disfmarker} we have great {disfmarker} great , uh , p steps forward in terms of the nonspeech - speech pre - segmenting of the signal .&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Well , we have steps forward .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Well , it 's a {disfmarker} it 's a big improvement .&#10;Speaker: PhD C&#10;Content: I would prefer this .&#10;Speaker: Professor G&#10;Content: Yes" target="The individuals in this conversation are aware of the limitations of their recordings and do not believe that they represent mankind. They explicitly acknowledge that their sample is very small, and Professor G states that they are not claiming to get a representation of mankind in these recordings. This demonstrates their understanding of the importance of having a large and diverse sample to make accurate generalizations about human behavior.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Well , um , I can {pause} give you an update on the {pause} transcription effort .&#10;Speaker: Professor G&#10;Content: Great .&#10;Speaker: Postdoc F&#10;Content: Uh , maybe {nonvocalsound} raise the issue of microphone , uh , um procedures with reference to the {pause} cleanliness of the recordings .&#10;Speaker: Professor G&#10;Content: OK , transcription , uh , microphone issues {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And then maybe {nonvocalsound} ask , th uh , these guys . The {disfmarker} we have great {disfmarker} great , uh , p steps forward in terms of the nonspeech - speech pre - segmenting of the signal .&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: Well , we have steps forward .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Well , it 's a {disfmarker} it 's a big improvement .&#10;Speaker: PhD C&#10;Content: I would prefer this .&#10;Speaker: Professor G&#10;Content: Yes" target="1. After Postdoc F brought up the issue of microphone procedures and cleanliness of recordings, Professor G acknowledged the topic by repeating it. However, the specifics of their discussion on this matter are not detailed in the transcript.&#10;2. The group then moved on to discuss progress made in terms of non-speech and speech pre-segmenting of the signal, indicating that they were addressing the limitation of their current data source in accurately encoding overlaps during transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: Grad A&#10;Content: OK . We seem to be recording .&#10;Speaker: Professor G&#10;Content: Alright !&#10;Speaker: Grad A&#10;Content: So , sorry about not {disfmarker}&#10;Speaker: Professor G&#10;Content: We 're not crashing .&#10;Speaker: PhD D&#10;Content: Number four .&#10;Speaker: Grad A&#10;Content: not pre - doing everything . The lunch went a little later than I was expecting , Chuck .&#10;Speaker: PhD E&#10;Content: Hmm ?&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Chuck was telling too many jokes , or something ?&#10;Speaker: Grad A&#10;Content: Yep . Pretty much .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: OK . {vocalsound} Does anybody have an agenda ?&#10;Speaker: Grad A&#10;Content: No .&#10;Speaker: Postdoc F&#10;Content: Well , I 'm {disfmarker} I sent a couple of items . They 're {disfmarker} they 're sort of practical .&#10;Speaker: Professor G&#10;Content: I thought" target="Based on the transcript, Professor G believes that it is still important to have people wear microphones properly in a real-life situation, even if the recordings may not be perfect. He suggests that trying to have people wear the microphones properly does not hurt the naturalness of the situation. This is because, in the target applications they are discussing, people will not be wearing head-mounted mikes anyway. The head-mounted mikes are only for research purposes and help ensure cleaner recordings. Professor G also mentions that systems exist to do pretty good selection of who's talking, even when microphones are not worn perfectly. Overall, while imperfect recordings may occur when people wear microphones improperly, the benefits of better sound quality and clearer speech outweigh the drawbacks in a research context.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: Grad A&#10;Content: OK . We seem to be recording .&#10;Speaker: Professor G&#10;Content: Alright !&#10;Speaker: Grad A&#10;Content: So , sorry about not {disfmarker}&#10;Speaker: Professor G&#10;Content: We 're not crashing .&#10;Speaker: PhD D&#10;Content: Number four .&#10;Speaker: Grad A&#10;Content: not pre - doing everything . The lunch went a little later than I was expecting , Chuck .&#10;Speaker: PhD E&#10;Content: Hmm ?&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Chuck was telling too many jokes , or something ?&#10;Speaker: Grad A&#10;Content: Yep . Pretty much .&#10;Speaker: PhD E&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: OK . {vocalsound} Does anybody have an agenda ?&#10;Speaker: Grad A&#10;Content: No .&#10;Speaker: Postdoc F&#10;Content: Well , I 'm {disfmarker} I sent a couple of items . They 're {disfmarker} they 're sort of practical .&#10;Speaker: Professor G&#10;Content: I thought" target="1. Professor G, Postdoc F, and possibly other attendees think that practical items can be useful for their purpose, as evidenced by phrases such as &quot;we don't want anything too practical&quot; and &quot;why don't we talk about practical things?&quot; However, they ultimately decide to focus on &quot;useless things,&quot; likely meaning things that are not directly applicable to their current goal but may still provide some value or insight.&#10;2. Grad A seems to have a more negative view of practicality in this context, stating &quot;I mean, we don't want anything too practical&quot; and expressing annoyance with someone named Chuck for being too practical (presumably by telling too many jokes during lunch).&#10;3. There is no direct mention of Javier's opinion on the practicality of items discussed; however, since he focuses on training models rather than using Markov models in his program, it can be inferred that he may lean towards favoring theoretical or abstract approaches over practical ones.">
      <data key="d0">1</data>
    </edge>
    <edge source="} it 's a big improvement .&#10;Speaker: PhD C&#10;Content: I would prefer this .&#10;Speaker: Professor G&#10;Content: Yes . Yeah , well . OK . Uh {disfmarker}&#10;Speaker: PhD D&#10;Content: We talk about the {disfmarker} {vocalsound} the results of&#10;Speaker: Professor G&#10;Content: You have some {disfmarker} Yeah .&#10;Speaker: Grad A&#10;Content: I have a little bit of IRAM stuff&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: use {disfmarker}&#10;Speaker: Grad A&#10;Content: but {pause} I 'm not sure if that 's of general interest or not .&#10;Speaker: Professor G&#10;Content: Uh , bigram ?&#10;Speaker: Grad A&#10;Content: IRAM .&#10;Speaker: PhD D&#10;Content: IRAM .&#10;Speaker: Professor G&#10;Content: IRAM .&#10;Speaker: Grad A&#10;Content: IRAM , bigram ,&#10;Speaker: Professor G&#10;Content: Well , m maybe .&#10;Speaker: PhD D&#10;Content: Bi -" target="1. The transcript does not provide a detailed explanation of the IRAM results that PhD D and Professor G are discussing. However, it appears they are referring to some analysis or research data labeled as &quot;IRAM.&quot; There seems to be some uncertainty about the relevance or general interest of this information, as indicated by PhD D's comment, &quot;I'm not sure if that's of general interest or not.&quot; Professor G also repeats &quot;IRAM&quot; and asks a question about &quot;bigram,&quot; which might be related to their discussion. To fully understand the content of their conversation, more context or information is needed.&#10;2. The group does not seem to raise any specific issues or concerns regarding the IRAM results in this part of the transcript. Instead, the discussion revolves around the classifier choice (Gaussian distribution) and the clustering process for the first step, as well as modeling decision boundaries and change points for the second step. It is possible that the group found the IRAM results interesting but decided to continue discussing other aspects of their project in this particular meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The group is divided in their opinion on whether to skip reading through a set of digits for their work. Professor G expresses the need to consider using existing systems like Javier's since creating new ones may not be feasible given time constraints, while Postdoc F and Grad A suggest that it might not be necessary to read the digits each time. However, there is no explicit consensus on skipping the digits.&#10;2. The group has concerns about not having heard back from a colleague named Jose regarding this task. This concern is expressed by Professor G, who mentions that they haven't heard from Jose yet in relation to the digits." target="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The postdoc and PhD speaker improved the &quot;" target="er} they can use it or {disfmarker} ?&#10;Speaker: Postdoc F&#10;Content: Uh , they {disfmarker} they think it 's a terrific improvement . And , um , it real it just makes a {disfmarker} a world of difference .&#10;Speaker: Professor G&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: And , um , y you also did some something in addition which was , um , for those in which there {nonvocalsound} was , uh , quiet speakers in the mix .&#10;Speaker: PhD C&#10;Content: Yeah . Uh , yeah . That {disfmarker} that was one {disfmarker} one {disfmarker} one thing , uh , why I added more mixtures for {disfmarker} for the speech . So I saw that there were loud {disfmarker} loudly speaking speakers and quietly speaking speakers .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: And so I did two mixtures , one for the loud speakers and one for the quiet speakers .&#10;Speaker: Grad A&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The postdoc and PhD speaker improved the &quot;" target="Content: OK .&#10;Speaker: PhD C&#10;Content: It 's just our {disfmarker} our old Munich , uh , loudness - based spectrum on mel scale twenty {disfmarker} twenty critical bands and then loudness .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: And four additional features , which is energy , loudness , modified loudness , and zero crossing rate . So it 's twenty - four {disfmarker} twenty - four features .&#10;Speaker: PhD B&#10;Content: Mmm .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: And you also provided me with several different versions ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: which I compared .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: And so you change {nonvocalsound} parameters . What {disfmarker} do you wanna say something about the parameters {nonvocalsound} that you change ?&#10;Speaker: PhD C&#10;Content: Yeah">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The postdoc and PhD speaker improved the &quot;" target="&#10;Content: And so I did two mixtures , one for the loud speakers and one for the quiet speakers .&#10;Speaker: Grad A&#10;Content: And did you hand - label who was loud and who was quiet , or did you just {disfmarker} ?&#10;Speaker: PhD C&#10;Content: I did that for {disfmarker} for five minutes of one dialogue&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: PhD C&#10;Content: and that was enough to {disfmarker} to train the system .&#10;Speaker: PhD B&#10;Content: W What {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD C&#10;Content: And so it {disfmarker} it adapts , uh , on {disfmarker} while running . So .&#10;Speaker: PhD B&#10;Content: What kind of , uh , front - end processing did you do ?&#10;Speaker: PhD C&#10;Content: Hopefully .&#10;Speaker: PhD D&#10;Content: OK .&#10;Speaker: PhD C&#10;Content: It 's just our {disfmarker} our old Munich , uh , loudness">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The postdoc and PhD speaker improved the &quot;" target=" the transcriber 's {nonvocalsound} perspective , uh , those {nonvocalsound} two functions are separate . And Dan Ellis 's hack handles the , {vocalsound} um , choice {nonvocalsound} {disfmarker} the ability to choose different waveforms {vocalsound} from moment to moment .&#10;Speaker: Grad A&#10;Content: But only to listen to , not to look at .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Um {disfmarker}&#10;Speaker: Grad A&#10;Content: The waveform you 're looking at doesn't change .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: That 's true .&#10;Speaker: Grad A&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Yeah , but {nonvocalsound} that 's {disfmarker} that 's OK , cuz they 're {disfmarker} they 're , you know , they 're focused on the ear anyway .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Postdoc">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The postdoc and PhD speaker improved the &quot;" target="'t find the right connector to go into these things ?&#10;Speaker: Grad A&#10;Content: Yep . When I looked , i they listed one microphone and that 's it&#10;Speaker: PhD E&#10;Content: Huh !&#10;Speaker: Grad A&#10;Content: as having that type of connector . But my guess is that Sony maybe uses a different number for their connector than everyone else does . And {disfmarker} and so {disfmarker}&#10;Speaker: Professor G&#10;Content: Mm - hmm . Well , let 's look at it together&#10;Speaker: Grad A&#10;Content: it seems {disfmarker} it seems really unlikely to me that there 's only one .&#10;Speaker: Professor G&#10;Content: and {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And there 's no adaptor for it ?&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Seems like there 'd be a {disfmarker} OK .&#10;Speaker: Grad A&#10;Content: As I said , who knows ?&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The postdoc and PhD speaker improved the &quot;" target="Based on the transcript, it is still considered important to have a good tool for checking and possibly augmenting the IBM transcripts, even in the absence of the person who previously handled this task. This sentiment is expressed by PhD B, who states &quot;I think having a good tool is worth something no matter what&quot; (lines 10-11). Furthermore, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort, implying that the group has made progress in their efforts to improve the transcription process.&#10;&#10;In terms of volunteering to help with this process, Grad A mentions that Dave Gelbart had volunteered (line 28). However, it is not explicitly stated in the transcript whether Dave Gelbart will be specifically assisting with the task of checking and augmenting the IBM transcripts.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The postdoc and PhD speaker improved the &quot;" target="Based on the information provided, the Postdoc (presumably Postdoc F) suggested scheduling a separate discussion with Professor G to go over and share transcripts and forced alignments. However, the specifics of how this would be done or any further details about the suggestion are not mentioned in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers generally agree on the importance of using transcribers to mark audible breaths and laughter in the transcripts, which can then be used to train models for speech and nonspeech. This is based on PhD B's suggestion to explicitly mark these non-speech elements and Postdoc F's confirmation that they are currently being marked in curly brackets as &quot;inhale&quot; or &quot;breath.&quot;&#10;   &#10;2. There is also discussion about the usefulness of transcribing nonspeech elements like breath and laughter for modeling purposes, as mentioned by PhD B when they discuss the SRI recognizer being able to transcribe these elements and model them. Professor G agrees with this approach, suggesting that they should not worry so much about features and instead use general features for both speech and nonspeech modeling.&#10;&#10;In summary, the speakers agree on the significance of using transcribers to mark non-speech elements such as breath and laughter in transcripts, which can then be utilized to train models for speech and nonspeech. They also consider modeling these elements to improve the overall performance of their system." target=": Yeah .&#10;Speaker: Professor G&#10;Content: And then there 's mass .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Anyway .&#10;Speaker: Postdoc F&#10;Content: I could say something about {disfmarker} about the {disfmarker} Well , I don't know what you wanna do . Yeah .&#10;Speaker: Professor G&#10;Content: About what ?&#10;Speaker: Postdoc F&#10;Content: About the transcribers or anything or {disfmarker} ? I don't know .&#10;Speaker: Professor G&#10;Content: Well , the other {disfmarker}&#10;Speaker: PhD B&#10;Content: But , uh , just to {disfmarker} to , um {disfmarker}&#10;Speaker: Professor G&#10;Content: why don't we do that ?&#10;Speaker: PhD B&#10;Content: One more remark , uh , concerning the SRI recognizer . Um . It is useful to transcribe and then ultimately train models for things like breath , and also laughter is very , very frequent and important to {disfmarker} {vocalsound">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers generally agree on the importance of using transcribers to mark audible breaths and laughter in the transcripts, which can then be used to train models for speech and nonspeech. This is based on PhD B's suggestion to explicitly mark these non-speech elements and Postdoc F's confirmation that they are currently being marked in curly brackets as &quot;inhale&quot; or &quot;breath.&quot;&#10;   &#10;2. There is also discussion about the usefulness of transcribing nonspeech elements like breath and laughter for modeling purposes, as mentioned by PhD B when they discuss the SRI recognizer being able to transcribe these elements and model them. Professor G agrees with this approach, suggesting that they should not worry so much about features and instead use general features for both speech and nonspeech modeling.&#10;&#10;In summary, the speakers agree on the significance of using transcribers to mark non-speech elements such as breath and laughter in transcripts, which can then be utilized to train models for speech and nonspeech. They also consider modeling these elements to improve the overall performance of their system." target="ribe and then ultimately train models for things like breath , and also laughter is very , very frequent and important to {disfmarker} {vocalsound} to model .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So ,&#10;Speaker: Grad A&#10;Content: So ,&#10;Speaker: PhD B&#10;Content: if you can in your transcripts mark {disfmarker}&#10;Speaker: Grad A&#10;Content: mark them ?&#10;Speaker: PhD B&#10;Content: mark very audible breaths and laughter especially ,&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: um {disfmarker}&#10;Speaker: Postdoc F&#10;Content: They are .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Postdoc F&#10;Content: They 're putting {disfmarker} Eh , so in curly brackets they put &quot; inhale &quot; or &quot; breath &quot; .&#10;Speaker: Grad A&#10;Content: Oh , great .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: It {disfmark">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers generally agree on the importance of using transcribers to mark audible breaths and laughter in the transcripts, which can then be used to train models for speech and nonspeech. This is based on PhD B's suggestion to explicitly mark these non-speech elements and Postdoc F's confirmation that they are currently being marked in curly brackets as &quot;inhale&quot; or &quot;breath.&quot;&#10;   &#10;2. There is also discussion about the usefulness of transcribing nonspeech elements like breath and laughter for modeling purposes, as mentioned by PhD B when they discuss the SRI recognizer being able to transcribe these elements and model them. Professor G agrees with this approach, suggesting that they should not worry so much about features and instead use general features for both speech and nonspeech modeling.&#10;&#10;In summary, the speakers agree on the significance of using transcribers to mark non-speech elements such as breath and laughter in transcripts, which can then be utilized to train models for speech and nonspeech. They also consider modeling these elements to improve the overall performance of their system." target=" for s speech and nonspeech . And it was a system which used only one Gaussian for silence and one Gaussian for speech . And now I added , uh , multi - mixture possibility for {disfmarker} {vocalsound} for speech and nonspeech .&#10;Speaker: Professor G&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD C&#10;Content: And I did some training on {disfmarker} on one dialogue , which was transcribed by {disfmarker} Yeah . We {disfmarker} we did a nons s speech - nonspeech transcription .&#10;Speaker: PhD D&#10;Content: Jose .&#10;Speaker: PhD C&#10;Content: Adam , Dave , and I , we did , for that dialogue and I trained it on that . And I did some pre - segmentations for {disfmarker} for Jane . And I 'm not sure how good they are or what {disfmarker} what the transcribers say . They {disfmarker} they can use it or {disfmarker} ?&#10;Speaker: Postdoc F&#10;Content: Uh , they {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers generally agree on the importance of using transcribers to mark audible breaths and laughter in the transcripts, which can then be used to train models for speech and nonspeech. This is based on PhD B's suggestion to explicitly mark these non-speech elements and Postdoc F's confirmation that they are currently being marked in curly brackets as &quot;inhale&quot; or &quot;breath.&quot;&#10;   &#10;2. There is also discussion about the usefulness of transcribing nonspeech elements like breath and laughter for modeling purposes, as mentioned by PhD B when they discuss the SRI recognizer being able to transcribe these elements and model them. Professor G agrees with this approach, suggesting that they should not worry so much about features and instead use general features for both speech and nonspeech modeling.&#10;&#10;In summary, the speakers agree on the significance of using transcribers to mark non-speech elements such as breath and laughter in transcripts, which can then be utilized to train models for speech and nonspeech. They also consider modeling these elements to improve the overall performance of their system." target=": Yeah .&#10;Speaker: PhD B&#10;Content: But , um , that 's not as frequent as just laughing between speaking ,&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: So are {disfmarker} do you treat breath and laughter as phonetically , or as word models , or what ?&#10;Speaker: PhD B&#10;Content: so {disfmarker}&#10;Speaker: Professor G&#10;Content: Uh is it ?&#10;Speaker: PhD D&#10;Content: Huh . I {disfmarker} I think it 's frequent in {disfmarker} in the meeting .&#10;Speaker: Postdoc F&#10;Content: I think he 's right . Yeah .&#10;Speaker: PhD B&#10;Content: We tried both . Uh , currently , um , we use special words . There was a {disfmarker} there 's actually a word for {disfmarker} uh , it 's not just breathing but all kinds of mouth {disfmarker}&#10;Speaker: Grad A&#10;Content: Mm - hmm . Mouth stuff ?&#10;Speaker: PhD B&#10;Content: uh , mouth {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers generally agree on the importance of using transcribers to mark audible breaths and laughter in the transcripts, which can then be used to train models for speech and nonspeech. This is based on PhD B's suggestion to explicitly mark these non-speech elements and Postdoc F's confirmation that they are currently being marked in curly brackets as &quot;inhale&quot; or &quot;breath.&quot;&#10;   &#10;2. There is also discussion about the usefulness of transcribing nonspeech elements like breath and laughter for modeling purposes, as mentioned by PhD B when they discuss the SRI recognizer being able to transcribe these elements and model them. Professor G agrees with this approach, suggesting that they should not worry so much about features and instead use general features for both speech and nonspeech modeling.&#10;&#10;In summary, the speakers agree on the significance of using transcribers to mark non-speech elements such as breath and laughter in transcripts, which can then be utilized to train models for speech and nonspeech. They also consider modeling these elements to improve the overall performance of their system." target="Content: Yeah .&#10;Speaker: Professor G&#10;Content: which is to say , don't worry so much about the , uh , features .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: That is to say , use , you know , as {disfmarker} as you 're doing with the speech , uh , nonspeech , use some very general features .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And , uh , then , uh , look at it more from the aspect of modeling .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: You know , have a {disfmarker} have a couple Markov models and {disfmarker} and , uh , try to indi try to determine , you know , w when is th when are you in an overlap , when are you not in an overlap .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: And let the , uh , uh , statistical system {pause} determine what 's the right way to">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers generally agree on the importance of using transcribers to mark audible breaths and laughter in the transcripts, which can then be used to train models for speech and nonspeech. This is based on PhD B's suggestion to explicitly mark these non-speech elements and Postdoc F's confirmation that they are currently being marked in curly brackets as &quot;inhale&quot; or &quot;breath.&quot;&#10;   &#10;2. There is also discussion about the usefulness of transcribing nonspeech elements like breath and laughter for modeling purposes, as mentioned by PhD B when they discuss the SRI recognizer being able to transcribe these elements and model them. Professor G agrees with this approach, suggesting that they should not worry so much about features and instead use general features for both speech and nonspeech modeling.&#10;&#10;In summary, the speakers agree on the significance of using transcribers to mark non-speech elements such as breath and laughter in transcripts, which can then be utilized to train models for speech and nonspeech. They also consider modeling these elements to improve the overall performance of their system." target="1. The two types of laughter that the postdoc is referring to are:&#10;&#09;* Laughter that occurs in the middle of a sentence.&#10;&#09;* Laughter that occurs after a sentence has been completed.&#10;2. Distinguishing between these two types of laughter is important because they may have different implications for the conversation or speech analysis. For example, laughter in the middle of a sentence might indicate amusement or interruption, while laughter after a sentence could suggest understanding or agreement. By distinguishing between them, researchers can better understand the nuances of human communication and improve the performance of their systems in recognizing and modeling these non-speech elements.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers generally agree on the importance of using transcribers to mark audible breaths and laughter in the transcripts, which can then be used to train models for speech and nonspeech. This is based on PhD B's suggestion to explicitly mark these non-speech elements and Postdoc F's confirmation that they are currently being marked in curly brackets as &quot;inhale&quot; or &quot;breath.&quot;&#10;   &#10;2. There is also discussion about the usefulness of transcribing nonspeech elements like breath and laughter for modeling purposes, as mentioned by PhD B when they discuss the SRI recognizer being able to transcribe these elements and model them. Professor G agrees with this approach, suggesting that they should not worry so much about features and instead use general features for both speech and nonspeech modeling.&#10;&#10;In summary, the speakers agree on the significance of using transcribers to mark non-speech elements such as breath and laughter in transcripts, which can then be utilized to train models for speech and nonspeech. They also consider modeling these elements to improve the overall performance of their system." target="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers generally agree on the importance of using transcribers to mark audible breaths and laughter in the transcripts, which can then be used to train models for speech and nonspeech. This is based on PhD B's suggestion to explicitly mark these non-speech elements and Postdoc F's confirmation that they are currently being marked in curly brackets as &quot;inhale&quot; or &quot;breath.&quot;&#10;   &#10;2. There is also discussion about the usefulness of transcribing nonspeech elements like breath and laughter for modeling purposes, as mentioned by PhD B when they discuss the SRI recognizer being able to transcribe these elements and model them. Professor G agrees with this approach, suggesting that they should not worry so much about features and instead use general features for both speech and nonspeech modeling.&#10;&#10;In summary, the speakers agree on the significance of using transcribers to mark non-speech elements such as breath and laughter in transcripts, which can then be utilized to train models for speech and nonspeech. They also consider modeling these elements to improve the overall performance of their system." target="1. Challenges in detecting laughter during speech: The main challenge is distinguishing between speaking while laughing and regular speech. This requires dealing with the additional complexity of identifying time tags indicating the beginning and end of laughter throughout the utterance. Another issue mentioned was recognizing laughter when it occurs simultaneously with speech, as the system might not perform well in these cases.&#10;&#10;2. Differences from laughter between words: Laughter between words is generally easier to detect since it can be considered a pause or break in the conversation. When laughter occurs during speech, it may require more sophisticated methods for identification, such as marking time tags and dealing with complications arising from overlapping laughter and speech. &#10;&#10;These challenges highlight the importance of addressing various scenarios when transcribing non-speech elements like breath and laughter in order to improve the overall performance of speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers generally agree on the importance of using transcribers to mark audible breaths and laughter in the transcripts, which can then be used to train models for speech and nonspeech. This is based on PhD B's suggestion to explicitly mark these non-speech elements and Postdoc F's confirmation that they are currently being marked in curly brackets as &quot;inhale&quot; or &quot;breath.&quot;&#10;   &#10;2. There is also discussion about the usefulness of transcribing nonspeech elements like breath and laughter for modeling purposes, as mentioned by PhD B when they discuss the SRI recognizer being able to transcribe these elements and model them. Professor G agrees with this approach, suggesting that they should not worry so much about features and instead use general features for both speech and nonspeech modeling.&#10;&#10;In summary, the speakers agree on the significance of using transcribers to mark non-speech elements such as breath and laughter in transcripts, which can then be utilized to train models for speech and nonspeech. They also consider modeling these elements to improve the overall performance of their system." target="Professionor G suggests using transcribers more thoroughly to analyze overlaps in a more detailed manner. He proposes that they could go through the recordings specifically for the overlaps. This approach aims to improve the encoding of overlaps by focusing on them directly.&#10;&#10;To connect the right people to help with this concern, Professor G has already taken several steps. He mentioned a meeting with Postdoc F and Dave Gelbart, where they discussed modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue. By engaging relevant experts in the field, Professor G is actively working on finding solutions to address the limitations in encoding overlaps.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers generally agree on the importance of using transcribers to mark audible breaths and laughter in the transcripts, which can then be used to train models for speech and nonspeech. This is based on PhD B's suggestion to explicitly mark these non-speech elements and Postdoc F's confirmation that they are currently being marked in curly brackets as &quot;inhale&quot; or &quot;breath.&quot;&#10;   &#10;2. There is also discussion about the usefulness of transcribing nonspeech elements like breath and laughter for modeling purposes, as mentioned by PhD B when they discuss the SRI recognizer being able to transcribe these elements and model them. Professor G agrees with this approach, suggesting that they should not worry so much about features and instead use general features for both speech and nonspeech modeling.&#10;&#10;In summary, the speakers agree on the significance of using transcribers to mark non-speech elements such as breath and laughter in transcripts, which can then be utilized to train models for speech and nonspeech. They also consider modeling these elements to improve the overall performance of their system." target="1. Marking audible breaths and laughter in transcripts provides crucial data for training models to recognize and model these non-speech elements, which can improve the overall performance of speech recognition systems. This is important because distinguishing between speaking while laughing and regular speech, as well as recognizing laughter during speech, can be challenging.&#10;&#10;2. There are different types of laughter that may have different implications for conversation or speech analysis, such as laughter in the middle of a sentence indicating amusement or interruption, and laughter after a sentence suggesting understanding or agreement. By explicitly marking these non-speech elements, researchers can better understand the nuances of human communication and enhance their systems' ability to recognize and model them effectively.&#10;&#10;3. Marking audible breaths and laughter in transcripts also helps address various scenarios during speech analysis, ensuring that the models are trained on a diverse range of data points including time tags indicating the beginning and end of laughter throughout the utterance. This leads to better-performing models and more accurate speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="ribe and then ultimately train models for things like breath , and also laughter is very , very frequent and important to {disfmarker} {vocalsound} to model .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: So ,&#10;Speaker: Grad A&#10;Content: So ,&#10;Speaker: PhD B&#10;Content: if you can in your transcripts mark {disfmarker}&#10;Speaker: Grad A&#10;Content: mark them ?&#10;Speaker: PhD B&#10;Content: mark very audible breaths and laughter especially ,&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: PhD B&#10;Content: um {disfmarker}&#10;Speaker: Postdoc F&#10;Content: They are .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: Postdoc F&#10;Content: They 're putting {disfmarker} Eh , so in curly brackets they put &quot; inhale &quot; or &quot; breath &quot; .&#10;Speaker: Grad A&#10;Content: Oh , great .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: It {disfmark" target="1. Marking audible breaths and laughter in transcripts provides crucial data for training models to recognize and model these non-speech elements, which can improve the overall performance of speech recognition systems. This is important because distinguishing between speaking while laughing and regular speech, as well as recognizing laughter during speech, can be challenging.&#10;&#10;2. There are different types of laughter that may have different implications for conversation or speech analysis, such as laughter in the middle of a sentence indicating amusement or interruption, and laughter after a sentence suggesting understanding or agreement. By explicitly marking these non-speech elements, researchers can better understand the nuances of human communication and enhance their systems' ability to recognize and model them effectively.&#10;&#10;3. Marking audible breaths and laughter in transcripts also helps address various scenarios during speech analysis, ensuring that the models are trained on a diverse range of data points including time tags indicating the beginning and end of laughter throughout the utterance. This leads to better-performing models and more accurate speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: PhD B&#10;Content: But , um , that 's not as frequent as just laughing between speaking ,&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: So are {disfmarker} do you treat breath and laughter as phonetically , or as word models , or what ?&#10;Speaker: PhD B&#10;Content: so {disfmarker}&#10;Speaker: Professor G&#10;Content: Uh is it ?&#10;Speaker: PhD D&#10;Content: Huh . I {disfmarker} I think it 's frequent in {disfmarker} in the meeting .&#10;Speaker: Postdoc F&#10;Content: I think he 's right . Yeah .&#10;Speaker: PhD B&#10;Content: We tried both . Uh , currently , um , we use special words . There was a {disfmarker} there 's actually a word for {disfmarker} uh , it 's not just breathing but all kinds of mouth {disfmarker}&#10;Speaker: Grad A&#10;Content: Mm - hmm . Mouth stuff ?&#10;Speaker: PhD B&#10;Content: uh , mouth {disf" target="1. The two types of laughter that the postdoc is referring to are:&#10;&#09;* Laughter that occurs in the middle of a sentence.&#10;&#09;* Laughter that occurs after a sentence has been completed.&#10;2. Distinguishing between these two types of laughter is important because they may have different implications for the conversation or speech analysis. For example, laughter in the middle of a sentence might indicate amusement or interruption, while laughter after a sentence could suggest understanding or agreement. By distinguishing between them, researchers can better understand the nuances of human communication and improve the performance of their systems in recognizing and modeling these non-speech elements.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: PhD B&#10;Content: But , um , that 's not as frequent as just laughing between speaking ,&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: So are {disfmarker} do you treat breath and laughter as phonetically , or as word models , or what ?&#10;Speaker: PhD B&#10;Content: so {disfmarker}&#10;Speaker: Professor G&#10;Content: Uh is it ?&#10;Speaker: PhD D&#10;Content: Huh . I {disfmarker} I think it 's frequent in {disfmarker} in the meeting .&#10;Speaker: Postdoc F&#10;Content: I think he 's right . Yeah .&#10;Speaker: PhD B&#10;Content: We tried both . Uh , currently , um , we use special words . There was a {disfmarker} there 's actually a word for {disfmarker} uh , it 's not just breathing but all kinds of mouth {disfmarker}&#10;Speaker: Grad A&#10;Content: Mm - hmm . Mouth stuff ?&#10;Speaker: PhD B&#10;Content: uh , mouth {disf" target="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: PhD B&#10;Content: But , um , that 's not as frequent as just laughing between speaking ,&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: So are {disfmarker} do you treat breath and laughter as phonetically , or as word models , or what ?&#10;Speaker: PhD B&#10;Content: so {disfmarker}&#10;Speaker: Professor G&#10;Content: Uh is it ?&#10;Speaker: PhD D&#10;Content: Huh . I {disfmarker} I think it 's frequent in {disfmarker} in the meeting .&#10;Speaker: Postdoc F&#10;Content: I think he 's right . Yeah .&#10;Speaker: PhD B&#10;Content: We tried both . Uh , currently , um , we use special words . There was a {disfmarker} there 's actually a word for {disfmarker} uh , it 's not just breathing but all kinds of mouth {disfmarker}&#10;Speaker: Grad A&#10;Content: Mm - hmm . Mouth stuff ?&#10;Speaker: PhD B&#10;Content: uh , mouth {disf" target="1. Challenges in detecting laughter during speech: The main challenge is distinguishing between speaking while laughing and regular speech. This requires dealing with the additional complexity of identifying time tags indicating the beginning and end of laughter throughout the utterance. Another issue mentioned was recognizing laughter when it occurs simultaneously with speech, as the system might not perform well in these cases.&#10;&#10;2. Differences from laughter between words: Laughter between words is generally easier to detect since it can be considered a pause or break in the conversation. When laughter occurs during speech, it may require more sophisticated methods for identification, such as marking time tags and dealing with complications arising from overlapping laughter and speech. &#10;&#10;These challenges highlight the importance of addressing various scenarios when transcribing non-speech elements like breath and laughter in order to improve the overall performance of speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Yeah .&#10;Speaker: PhD B&#10;Content: But , um , that 's not as frequent as just laughing between speaking ,&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: So are {disfmarker} do you treat breath and laughter as phonetically , or as word models , or what ?&#10;Speaker: PhD B&#10;Content: so {disfmarker}&#10;Speaker: Professor G&#10;Content: Uh is it ?&#10;Speaker: PhD D&#10;Content: Huh . I {disfmarker} I think it 's frequent in {disfmarker} in the meeting .&#10;Speaker: Postdoc F&#10;Content: I think he 's right . Yeah .&#10;Speaker: PhD B&#10;Content: We tried both . Uh , currently , um , we use special words . There was a {disfmarker} there 's actually a word for {disfmarker} uh , it 's not just breathing but all kinds of mouth {disfmarker}&#10;Speaker: Grad A&#10;Content: Mm - hmm . Mouth stuff ?&#10;Speaker: PhD B&#10;Content: uh , mouth {disf" target="1. Marking audible breaths and laughter in transcripts provides crucial data for training models to recognize and model these non-speech elements, which can improve the overall performance of speech recognition systems. This is important because distinguishing between speaking while laughing and regular speech, as well as recognizing laughter during speech, can be challenging.&#10;&#10;2. There are different types of laughter that may have different implications for conversation or speech analysis, such as laughter in the middle of a sentence indicating amusement or interruption, and laughter after a sentence suggesting understanding or agreement. By explicitly marking these non-speech elements, researchers can better understand the nuances of human communication and enhance their systems' ability to recognize and model them effectively.&#10;&#10;3. Marking audible breaths and laughter in transcripts also helps address various scenarios during speech analysis, ensuring that the models are trained on a diverse range of data points including time tags indicating the beginning and end of laughter throughout the utterance. This leads to better-performing models and more accurate speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Yeah .&#10;Speaker: Professor G&#10;Content: which is to say , don't worry so much about the , uh , features .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: That is to say , use , you know , as {disfmarker} as you 're doing with the speech , uh , nonspeech , use some very general features .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And , uh , then , uh , look at it more from the aspect of modeling .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: You know , have a {disfmarker} have a couple Markov models and {disfmarker} and , uh , try to indi try to determine , you know , w when is th when are you in an overlap , when are you not in an overlap .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: And let the , uh , uh , statistical system {pause} determine what 's the right way to" target="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="Content: Yeah .&#10;Speaker: Professor G&#10;Content: which is to say , don't worry so much about the , uh , features .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: That is to say , use , you know , as {disfmarker} as you 're doing with the speech , uh , nonspeech , use some very general features .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And , uh , then , uh , look at it more from the aspect of modeling .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: You know , have a {disfmarker} have a couple Markov models and {disfmarker} and , uh , try to indi try to determine , you know , w when is th when are you in an overlap , when are you not in an overlap .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: And let the , uh , uh , statistical system {pause} determine what 's the right way to" target="The simple concept being discussed involves using a Gaussian distribution for each decision boundary or potential change point in the data. This idea was suggested as a way to determine overlaps or changes between speech and nonspeech portions in a signal. Instead of focusing on specific features, the group discussed modeling this problem with Gaussians and making decisions based on that. The optimal splitting would then be chosen based on these Gaussian models.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript." target=" doesn't have {disfmarker} it doesn't have any temporal , uh {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Maybe I 'm misremembering , but I did not think it had a Markov {disfmarker}&#10;Speaker: Professor G&#10;Content: I thought it {disfmarker} Yeah . I gues I guess I don't remember either . Uh . It 's been a while .&#10;Speaker: PhD C&#10;Content: Yeah . Uh , I could have a look at it .&#10;Speaker: PhD D&#10;Content: Javier {disfmarker}&#10;Speaker: Professor G&#10;Content: Uh .&#10;Speaker: PhD C&#10;Content: So .&#10;Speaker: PhD D&#10;Content: You mean Ja - eh , eh , Javier program ?&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: No , Javier di doesn't worked with , uh , a Markov {disfmarker}&#10;Speaker: Grad A&#10;Content: Yeah , I didn't think so .&#10;Speaker: PhD D&#10;Content: He on only train {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript." target="rule , yeah .&#10;Speaker: Professor G&#10;Content: So pitch - related and harmonic - related , I 'm {disfmarker} I 'm {pause} somewhat more hopeful for it .&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Professor G&#10;Content: But it seems like if we just wanna get something to work ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: that , uh , their suggestion of {disfmarker} of {disfmarker} Th - they were suggesting going to Markov models , uh , but in addition there 's an expansion of what Javier did . And one of those things , looking at the statistical component ,&#10;Speaker: PhD D&#10;Content: One .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: even if the features that you give it are maybe not ideal for it , it 's just sort of this general filter bank&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: or {disfmarker} {vocalsound} or">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript." target=" think with {disfmarker}&#10;Speaker: PhD C&#10;Content: the {disfmarker} to speech - nonspeech as {disfmarker}&#10;Speaker: Grad A&#10;Content: That 's right . But I think Javier 's {disfmarker}&#10;Speaker: PhD C&#10;Content: it 's only speech or it 's {disfmarker} it 's {disfmarker} it 's nonspeech .&#10;Speaker: PhD D&#10;Content: Ah . Yeah .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: I think Javier 's might be able to .&#10;Speaker: PhD C&#10;Content: So .&#10;Speaker: Professor G&#10;Content: N n&#10;Speaker: Grad A&#10;Content: It doesn't have the same Gaus - uh , H M M modeling ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: which is I think a drawback .&#10;Speaker: PhD C&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: But , uh {disfmarker}&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript." target="The simple concept being discussed involves using a Gaussian distribution for each decision boundary or potential change point in the data. This idea was suggested as a way to determine overlaps or changes between speech and nonspeech portions in a signal. Instead of focusing on specific features, the group discussed modeling this problem with Gaussians and making decisions based on that. The optimal splitting would then be chosen based on these Gaussian models.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript." target="1. The choice for the classifier in the first step was a Gaussian distribution, specifically Javier's Gaussian distribution, as suggested by PhD D: &quot;Using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. For the second step, there is no explicit information about how the clustering process performed. The discussion focuses more on using Gaussians for modeling decision boundaries and potential change points in the data. There is no detailed conversation about the clustering process's results or any specific issues related to it.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript." target="1. The use of Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones could be beneficial in improving speech recognition, as it would allow for more precise modeling of decision boundaries between speech and non-speech segments. This is based on PhD D's suggestion in the transcript: &quot;I think that an interesting experiment could be to prove that, if we suppose that the first step involves using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. The BIC (Bayesian Information Criterion) criterion can help in selecting the optimal number of Gaussians for modeling the data, balancing model complexity and fit. Using this method could improve the robustness of the speech recognition system by reducing overfitting or underfitting risks during the Gaussian training process.&#10;&#10;3. Manually distinguishing overlapping zones and speech zones requires careful consideration of time tags and potential complications arising from simultaneous laughter and speech, as mentioned in the transcript. This step might be labor-intensive but could provide valuable insights for improving speech recognition systems' performance, especially when dealing with non-speech elements like breath and laughter.&#10;&#10;4. Distinguishing between different types of laughter (e.g., laughter during a sentence vs. laughter after a sentence) is important for better understanding human communication nuances and improving the system's performance in recognizing and modeling these non-speech elements. This could be an additional step or application of Javier's software, allowing researchers to fine-tune their systems further based on specific needs.&#10;&#10;In conclusion, using Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones with the BIC criterion has the potential to improve speech recognition, especially when dealing with non-speech elements like breath and laughter. However, it requires careful consideration of time tags and possible complications during manual annotation, as well as further research on differentiating between various types of laughter.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript." target="The issue discussed among the group revolved around the observation that the simple concept of using a Gaussian distribution for decision boundaries or potential change points in the data was not working as expected. Specifically, Professor G and PhD D mentioned that the average, which is calculated by dividing the sum of the data points by the number of data points (variance), did not follow the energy in the signal. This became apparent when observing the valleys and peaks in the signal, as they didn't seem to adhere to the expected distribution based on the signal's energy.&#10;&#10;PhD D also pointed out a problem with the scale, which might be contributing to the difficulties encountered when modeling decision boundaries and potential change points in the data. The term &quot;scale&quot; in this context likely refers to how the Gaussian distributions are adjusted or normalized in relation to the data's range or spread. An incorrect scale can result in poor fitting of Gaussians to the data, leading to suboptimal decisions regarding overlaps or changes between speech and nonspeech portions in a signal.&#10;&#10;In summary, the group discussed potential issues with their Gaussian-based approach for modeling decision boundaries in the data, particularly concerning the average's failure to follow the energy in the signal, inconsistencies with valleys and peaks, and problems related to the scale of the Gaussian distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript." target="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, there is no explicit statement indicating that Javier worked with a Markov model in his program. The discussion suggests that Javier's program focuses on training models, as stated by PhD D: &quot;No, Javier doesn't work with, uh, a Markov {disfmarker}. He only trains {disfmarker}.&quot; This implies that Javier's program may not involve the use of Markov models but rather focuses on other training methods. However, without further clarification from the speakers, this interpretation remains an inference based on the provided transcript." target="1. Professor G, Postdoc F, and possibly other attendees think that practical items can be useful for their purpose, as evidenced by phrases such as &quot;we don't want anything too practical&quot; and &quot;why don't we talk about practical things?&quot; However, they ultimately decide to focus on &quot;useless things,&quot; likely meaning things that are not directly applicable to their current goal but may still provide some value or insight.&#10;2. Grad A seems to have a more negative view of practicality in this context, stating &quot;I mean, we don't want anything too practical&quot; and expressing annoyance with someone named Chuck for being too practical (presumably by telling too many jokes during lunch).&#10;3. There is no direct mention of Javier's opinion on the practicality of items discussed; however, since he focuses on training models rather than using Markov models in his program, it can be inferred that he may lean towards favoring theoretical or abstract approaches over practical ones.">
      <data key="d0">1</data>
    </edge>
    <edge source="rule , yeah .&#10;Speaker: Professor G&#10;Content: So pitch - related and harmonic - related , I 'm {disfmarker} I 'm {pause} somewhat more hopeful for it .&#10;Speaker: PhD D&#10;Content: Yeah . Yeah .&#10;Speaker: Professor G&#10;Content: But it seems like if we just wanna get something to work ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: that , uh , their suggestion of {disfmarker} of {disfmarker} Th - they were suggesting going to Markov models , uh , but in addition there 's an expansion of what Javier did . And one of those things , looking at the statistical component ,&#10;Speaker: PhD D&#10;Content: One .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: even if the features that you give it are maybe not ideal for it , it 's just sort of this general filter bank&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: or {disfmarker} {vocalsound} or" target="Based on the transcript, Professor G seems to have mixed feelings about focusing on pitch-related and harmonic-related features as the best use of limited time for finding a reliable indicator in short text messages. At one point, he expresses some skepticism, mentioning that they haven't found any combination that really gave an indicator so far. He also suggests that it may not be the best thing to focus on given the limited time available.&#10;&#10;However, later in the conversation, Professor G mentions that he is somewhat more hopeful for pitch-related and harmonic-related features as a potential indicator. He also mentions that they thought these features should be reasonable indicators.&#10;&#10;Overall, while Professor G expresses some reservations about focusing solely on pitch-related and harmonic-related features with the limited time available, he does not completely dismiss the idea and leaves open the possibility that it could still yield a reliable indicator.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it is still considered important to have a good tool for checking and possibly augmenting the IBM transcripts, even in the absence of the person who previously handled this task. This sentiment is expressed by PhD B, who states &quot;I think having a good tool is worth something no matter what&quot; (lines 10-11). Furthermore, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort, implying that the group has made progress in their efforts to improve the transcription process.&#10;&#10;In terms of volunteering to help with this process, Grad A mentions that Dave Gelbart had volunteered (line 28). However, it is not explicitly stated in the transcript whether Dave Gelbart will be specifically assisting with the task of checking and augmenting the IBM transcripts." target=": you know , who just doesn't happen to be here anymore . Someone else pays him . So {disfmarker}&#10;Speaker: PhD B&#10;Content: But about the need for transcription ,&#10;Speaker: Postdoc F&#10;Content: Isn't that great ?&#10;Speaker: PhD B&#10;Content: I mean , don't we {disfmarker} didn't we previously {vocalsound} decide that the {pause} IBM {pause} transcripts would have to be {pause} checked anyway and possibly augmented ?&#10;Speaker: Professor G&#10;Content: So . {vocalsound} Yeah .&#10;Speaker: Postdoc F&#10;Content: Yes . That 's true .&#10;Speaker: PhD B&#10;Content: So , I think having a good tool is worth something no matter what .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Yeah . S OK . That 's {disfmarker} that 's a good point .&#10;Speaker: Grad A&#10;Content: Yeah , and Dave Gelbart did volunteer ,&#10;Speaker: Postdoc F&#10;Content: Good .&#10;Speaker: Grad A&#10;Content: and">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it is still considered important to have a good tool for checking and possibly augmenting the IBM transcripts, even in the absence of the person who previously handled this task. This sentiment is expressed by PhD B, who states &quot;I think having a good tool is worth something no matter what&quot; (lines 10-11). Furthermore, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort, implying that the group has made progress in their efforts to improve the transcription process.&#10;&#10;In terms of volunteering to help with this process, Grad A mentions that Dave Gelbart had volunteered (line 28). However, it is not explicitly stated in the transcript whether Dave Gelbart will be specifically assisting with the task of checking and augmenting the IBM transcripts." target="s {disfmarker} it 's , uh , uh {disfmarker}&#10;Speaker: Grad A&#10;Content: Well , I 'll dig through the documentation to DragonDictate and ste s see if they still have the little {pause} form .&#10;Speaker: Professor G&#10;Content: But it does happen .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Right ? I mean , and any {disfmarker}&#10;Speaker: PhD B&#10;Content: It 's interesting , uh , I talked to some IBM guys , uh , last January , I think , I was there . And {disfmarker} so people who were working on the {disfmarker} on their ViaVoice dictation product .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And they said , uh , the breathing is really a {disfmarker} a terrible problem {pause} for them , to {disfmarker} to not recognize breathing as speech .&#10;Speaker: Postdoc F&#10;Content: Wow .&#10;Speaker: PhD B&#10;Content: So , anything to reduce breathing is">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, it is still considered important to have a good tool for checking and possibly augmenting the IBM transcripts, even in the absence of the person who previously handled this task. This sentiment is expressed by PhD B, who states &quot;I think having a good tool is worth something no matter what&quot; (lines 10-11). Furthermore, there are references to &quot;steps forward&quot; and &quot;great improvements&quot; made in the transcription effort, implying that the group has made progress in their efforts to improve the transcription process.&#10;&#10;In terms of volunteering to help with this process, Grad A mentions that Dave Gelbart had volunteered (line 28). However, it is not explicitly stated in the transcript whether Dave Gelbart will be specifically assisting with the task of checking and augmenting the IBM transcripts." target=" Postdoc F&#10;Content: Mm - hmm . Well , {vocalsound} one would {disfmarker}&#10;Speaker: PhD D&#10;Content: We hope .&#10;Speaker: Grad A&#10;Content: Yeah . Who knows ?&#10;Speaker: Postdoc F&#10;Content: That 'd be {disfmarker} that 'd be nice . I mean , {vocalsound} I {disfmarker} I {disfmarker} I {disfmarker} I 've {disfmarker}&#10;Speaker: PhD B&#10;Content: So who 's gonna do that ? Who 's gonna do forced alignment ?&#10;Speaker: Grad A&#10;Content: Well , u uh , IBM was going to . Um {disfmarker}&#10;Speaker: PhD B&#10;Content: Oh , OK .&#10;Speaker: PhD D&#10;Content: Oh .&#10;Speaker: Grad A&#10;Content: and I imagine they still plan to but {disfmarker} but , you know , I haven't spoken with them about that recently .&#10;Speaker: PhD B&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Professor G believes that it is still important to have people wear microphones properly in a real-life situation, even if the recordings may not be perfect. He suggests that trying to have people wear the microphones properly does not hurt the naturalness of the situation. This is because, in the target applications they are discussing, people will not be wearing head-mounted mikes anyway. The head-mounted mikes are only for research purposes and help ensure cleaner recordings. Professor G also mentions that systems exist to do pretty good selection of who's talking, even when microphones are not worn perfectly. Overall, while imperfect recordings may occur when people wear microphones improperly, the benefits of better sound quality and clearer speech outweigh the drawbacks in a research context." target=": Postdoc F&#10;Content: Oh .&#10;Speaker: Grad A&#10;Content: But I felt that in a real situation we were very seldom gonna get people to really do it and maybe it wasn't worth concentrating on . But {disfmarker}&#10;Speaker: Professor G&#10;Content: Well , I think that that 's {disfmarker} that 's a good back - off position . That 's what I was saying {vocalsound} earlier , th that , you know , we are gonna get some {vocalsound} recordings that are imperfect and , hey , that 's life . But I {disfmarker} I think that it {disfmarker} it doesn't hurt , uh , the naturalness of the situation to try to have people {pause} wear the microphones properly , if possible ,&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: because , {vocalsound} um , the natural situation is really what we have with the microphones on the table .&#10;Speaker: Grad A&#10;Content: Oh . That 's true .&#10;Speaker: Professor G&#10;Content: I mean , I think , {">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Professor G believes that it is still important to have people wear microphones properly in a real-life situation, even if the recordings may not be perfect. He suggests that trying to have people wear the microphones properly does not hurt the naturalness of the situation. This is because, in the target applications they are discussing, people will not be wearing head-mounted mikes anyway. The head-mounted mikes are only for research purposes and help ensure cleaner recordings. Professor G also mentions that systems exist to do pretty good selection of who's talking, even when microphones are not worn perfectly. Overall, while imperfect recordings may occur when people wear microphones improperly, the benefits of better sound quality and clearer speech outweigh the drawbacks in a research context." target="&#10;Speaker: Grad A&#10;Content: Oh . That 's true .&#10;Speaker: Professor G&#10;Content: I mean , I think , {vocalsound} you know , in the target applications that we 're talking about , people aren't gonna be wearing head - mounted mikes anyway .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: So this is just for u these head - mounted mikes are just for use with research .&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: And , uh , it 's gonna make {disfmarker} You know , if {disfmarker} if An - Andreas plays around with language modeling , he 's not gonna be m wanna be messed up by people breathing into the microphone .&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Professor G&#10;Content: So it 's {disfmarker} it 's , uh , uh {disfmarker}&#10;Speaker: Grad A&#10;Content: Well , I">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Professor G believes that it is still important to have people wear microphones properly in a real-life situation, even if the recordings may not be perfect. He suggests that trying to have people wear the microphones properly does not hurt the naturalness of the situation. This is because, in the target applications they are discussing, people will not be wearing head-mounted mikes anyway. The head-mounted mikes are only for research purposes and help ensure cleaner recordings. Professor G also mentions that systems exist to do pretty good selection of who's talking, even when microphones are not worn perfectly. Overall, while imperfect recordings may occur when people wear microphones improperly, the benefits of better sound quality and clearer speech outweigh the drawbacks in a research context." target=": Professor G&#10;Content: So , by doing that , you know , rather than setting any , uh , absolute threshold , you actually can do pretty good , uh , selection of who {disfmarker} who 's talking .&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: Uh {disfmarker} And those {disfmarker} those systems work very well , by the way , I mean , so people use them in {vocalsound} panel discussions and so forth with sound reinforcement differing in {disfmarker} in sort of ,&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor G&#10;Content: uh {disfmarker} and , uh , those {disfmarker} if {disfmarker} Boy , the guy I knew who built them , built them like twenty {disfmarker} twenty years ago ,&#10;Speaker: Grad A&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: so they 're {disfmarker} {vocalsound} it 's {disfmarker} the {disfmarker} the techniques work pretty well .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Professor G believes that it is still important to have people wear microphones properly in a real-life situation, even if the recordings may not be perfect. He suggests that trying to have people wear the microphones properly does not hurt the naturalness of the situation. This is because, in the target applications they are discussing, people will not be wearing head-mounted mikes anyway. The head-mounted mikes are only for research purposes and help ensure cleaner recordings. Professor G also mentions that systems exist to do pretty good selection of who's talking, even when microphones are not worn perfectly. Overall, while imperfect recordings may occur when people wear microphones improperly, the benefits of better sound quality and clearer speech outweigh the drawbacks in a research context." target=" a couple of items . They 're {disfmarker} they 're sort of practical .&#10;Speaker: Professor G&#10;Content: I thought {pause} somebody had .&#10;Speaker: Postdoc F&#10;Content: I don't know if you 're {disfmarker}&#10;Speaker: Professor G&#10;Content: Yeah , that 's right .&#10;Speaker: Postdoc F&#10;Content: if {disfmarker} if that 's too practical for what we 're {pause} focused on .&#10;Speaker: Grad A&#10;Content: I mean , we don't want anything too practical .&#10;Speaker: Professor G&#10;Content: Yeah , we only want th useless things .&#10;Speaker: Grad A&#10;Content: Yeah , that would be {disfmarker}&#10;Speaker: Professor G&#10;Content: Yeah . No , why don't we talk about practical things ?&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: Sure .&#10;Speaker: Postdoc F&#10;Content: Well , um , I can {pause} give you an update on the {pause} transcription effort .&#10;Speaker: Professor G&#10;Content: Great">
      <data key="d0">1</data>
    </edge>
    <edge source=" a couple of items . They 're {disfmarker} they 're sort of practical .&#10;Speaker: Professor G&#10;Content: I thought {pause} somebody had .&#10;Speaker: Postdoc F&#10;Content: I don't know if you 're {disfmarker}&#10;Speaker: Professor G&#10;Content: Yeah , that 's right .&#10;Speaker: Postdoc F&#10;Content: if {disfmarker} if that 's too practical for what we 're {pause} focused on .&#10;Speaker: Grad A&#10;Content: I mean , we don't want anything too practical .&#10;Speaker: Professor G&#10;Content: Yeah , we only want th useless things .&#10;Speaker: Grad A&#10;Content: Yeah , that would be {disfmarker}&#10;Speaker: Professor G&#10;Content: Yeah . No , why don't we talk about practical things ?&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: Sure .&#10;Speaker: Postdoc F&#10;Content: Well , um , I can {pause} give you an update on the {pause} transcription effort .&#10;Speaker: Professor G&#10;Content: Great" target="1. Professor G, Postdoc F, and possibly other attendees think that practical items can be useful for their purpose, as evidenced by phrases such as &quot;we don't want anything too practical&quot; and &quot;why don't we talk about practical things?&quot; However, they ultimately decide to focus on &quot;useless things,&quot; likely meaning things that are not directly applicable to their current goal but may still provide some value or insight.&#10;2. Grad A seems to have a more negative view of practicality in this context, stating &quot;I mean, we don't want anything too practical&quot; and expressing annoyance with someone named Chuck for being too practical (presumably by telling too many jokes during lunch).&#10;3. There is no direct mention of Javier's opinion on the practicality of items discussed; however, since he focuses on training models rather than using Markov models in his program, it can be inferred that he may lean towards favoring theoretical or abstract approaches over practical ones.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The two types of laughter that the postdoc is referring to are:&#10;&#09;* Laughter that occurs in the middle of a sentence.&#10;&#09;* Laughter that occurs after a sentence has been completed.&#10;2. Distinguishing between these two types of laughter is important because they may have different implications for the conversation or speech analysis. For example, laughter in the middle of a sentence might indicate amusement or interruption, while laughter after a sentence could suggest understanding or agreement. By distinguishing between them, researchers can better understand the nuances of human communication and improve the performance of their systems in recognizing and modeling these non-speech elements." target=" great .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: It {disfmarker} they {disfmarker} and then in curly brackets they say &quot; laughter &quot; . Now they 're {disfmarker} {vocalsound} they 're not being {pause} awfully precise , uh , m So they 're two types of laughter that are not being distinguished .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: One is {vocalsound} when sometimes s someone will start laughing when they 're in the middle of a sentence .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: And {disfmarker} and then the other one is when they finish the sentence and then they laugh . So , um , I {disfmarker} I did s I did some double checking to look through {disfmarker} I mean , {vocalsound} you 'd need to have extra  e extra complications , like time tags indicating the beginning and ending of {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The two types of laughter that the postdoc is referring to are:&#10;&#09;* Laughter that occurs in the middle of a sentence.&#10;&#09;* Laughter that occurs after a sentence has been completed.&#10;2. Distinguishing between these two types of laughter is important because they may have different implications for the conversation or speech analysis. For example, laughter in the middle of a sentence might indicate amusement or interruption, while laughter after a sentence could suggest understanding or agreement. By distinguishing between them, researchers can better understand the nuances of human communication and improve the performance of their systems in recognizing and modeling these non-speech elements." target=" {vocalsound} you 'd need to have extra  e extra complications , like time tags indicating the beginning and ending of {disfmarker} {vocalsound} of the laughing through the utterance .&#10;Speaker: PhD B&#10;Content: It 's not so {disfmarker} I don't think it 's , um {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And that {disfmarker} and what they 're doing is in both cases just saying &quot; curly brackets laughing &quot; a after the unit .&#10;Speaker: PhD B&#10;Content: As {disfmarker} as long as there is an indication that there was laughter somewhere between {pause} two words {vocalsound} I think that 's sufficient ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Good . Oh !&#10;Speaker: Grad A&#10;Content: Against {disfmarker} they could do forced alignment .&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: because actually the recognition of laughter once you kn um {disfmarker} you know , is pretty good">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The two types of laughter that the postdoc is referring to are:&#10;&#09;* Laughter that occurs in the middle of a sentence.&#10;&#09;* Laughter that occurs after a sentence has been completed.&#10;2. Distinguishing between these two types of laughter is important because they may have different implications for the conversation or speech analysis. For example, laughter in the middle of a sentence might indicate amusement or interruption, while laughter after a sentence could suggest understanding or agreement. By distinguishing between them, researchers can better understand the nuances of human communication and improve the performance of their systems in recognizing and modeling these non-speech elements." target=" .&#10;Speaker: PhD B&#10;Content: because actually the recognition of laughter once you kn um {disfmarker} you know , is pretty good .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: So as long as you can stick a {disfmarker} you know , a t a tag in there that {disfmarker} that indicates that there was laughter ,&#10;Speaker: Grad A&#10;Content: Oh , I didn't know that .&#10;Speaker: PhD B&#10;Content: that would probably be , uh , sufficient to train models .&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: That would be a really interesting {pause} prosodic feature ,&#10;Speaker: Postdoc F&#10;Content: Then {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: And let me ask y and I gotta ask you one thing about that .&#10;Speaker: Grad A&#10;Content: when {disfmarker}&#10;Speaker: PhD B&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: So , um ,">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The two types of laughter that the postdoc is referring to are:&#10;&#09;* Laughter that occurs in the middle of a sentence.&#10;&#09;* Laughter that occurs after a sentence has been completed.&#10;2. Distinguishing between these two types of laughter is important because they may have different implications for the conversation or speech analysis. For example, laughter in the middle of a sentence might indicate amusement or interruption, while laughter after a sentence could suggest understanding or agreement. By distinguishing between them, researchers can better understand the nuances of human communication and improve the performance of their systems in recognizing and modeling these non-speech elements." target="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The two types of laughter that the postdoc is referring to are:&#10;&#09;* Laughter that occurs in the middle of a sentence.&#10;&#09;* Laughter that occurs after a sentence has been completed.&#10;2. Distinguishing between these two types of laughter is important because they may have different implications for the conversation or speech analysis. For example, laughter in the middle of a sentence might indicate amusement or interruption, while laughter after a sentence could suggest understanding or agreement. By distinguishing between them, researchers can better understand the nuances of human communication and improve the performance of their systems in recognizing and modeling these non-speech elements." target="1. Challenges in detecting laughter during speech: The main challenge is distinguishing between speaking while laughing and regular speech. This requires dealing with the additional complexity of identifying time tags indicating the beginning and end of laughter throughout the utterance. Another issue mentioned was recognizing laughter when it occurs simultaneously with speech, as the system might not perform well in these cases.&#10;&#10;2. Differences from laughter between words: Laughter between words is generally easier to detect since it can be considered a pause or break in the conversation. When laughter occurs during speech, it may require more sophisticated methods for identification, such as marking time tags and dealing with complications arising from overlapping laughter and speech. &#10;&#10;These challenges highlight the importance of addressing various scenarios when transcribing non-speech elements like breath and laughter in order to improve the overall performance of speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The two types of laughter that the postdoc is referring to are:&#10;&#09;* Laughter that occurs in the middle of a sentence.&#10;&#09;* Laughter that occurs after a sentence has been completed.&#10;2. Distinguishing between these two types of laughter is important because they may have different implications for the conversation or speech analysis. For example, laughter in the middle of a sentence might indicate amusement or interruption, while laughter after a sentence could suggest understanding or agreement. By distinguishing between them, researchers can better understand the nuances of human communication and improve the performance of their systems in recognizing and modeling these non-speech elements." target="1. The use of Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones could be beneficial in improving speech recognition, as it would allow for more precise modeling of decision boundaries between speech and non-speech segments. This is based on PhD D's suggestion in the transcript: &quot;I think that an interesting experiment could be to prove that, if we suppose that the first step involves using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. The BIC (Bayesian Information Criterion) criterion can help in selecting the optimal number of Gaussians for modeling the data, balancing model complexity and fit. Using this method could improve the robustness of the speech recognition system by reducing overfitting or underfitting risks during the Gaussian training process.&#10;&#10;3. Manually distinguishing overlapping zones and speech zones requires careful consideration of time tags and potential complications arising from simultaneous laughter and speech, as mentioned in the transcript. This step might be labor-intensive but could provide valuable insights for improving speech recognition systems' performance, especially when dealing with non-speech elements like breath and laughter.&#10;&#10;4. Distinguishing between different types of laughter (e.g., laughter during a sentence vs. laughter after a sentence) is important for better understanding human communication nuances and improving the system's performance in recognizing and modeling these non-speech elements. This could be an additional step or application of Javier's software, allowing researchers to fine-tune their systems further based on specific needs.&#10;&#10;In conclusion, using Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones with the BIC criterion has the potential to improve speech recognition, especially when dealing with non-speech elements like breath and laughter. However, it requires careful consideration of time tags and possible complications during manual annotation, as well as further research on differentiating between various types of laughter.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The two types of laughter that the postdoc is referring to are:&#10;&#09;* Laughter that occurs in the middle of a sentence.&#10;&#09;* Laughter that occurs after a sentence has been completed.&#10;2. Distinguishing between these two types of laughter is important because they may have different implications for the conversation or speech analysis. For example, laughter in the middle of a sentence might indicate amusement or interruption, while laughter after a sentence could suggest understanding or agreement. By distinguishing between them, researchers can better understand the nuances of human communication and improve the performance of their systems in recognizing and modeling these non-speech elements." target="1. Marking audible breaths and laughter in transcripts provides crucial data for training models to recognize and model these non-speech elements, which can improve the overall performance of speech recognition systems. This is important because distinguishing between speaking while laughing and regular speech, as well as recognizing laughter during speech, can be challenging.&#10;&#10;2. There are different types of laughter that may have different implications for conversation or speech analysis, such as laughter in the middle of a sentence indicating amusement or interruption, and laughter after a sentence suggesting understanding or agreement. By explicitly marking these non-speech elements, researchers can better understand the nuances of human communication and enhance their systems' ability to recognize and model them effectively.&#10;&#10;3. Marking audible breaths and laughter in transcripts also helps address various scenarios during speech analysis, ensuring that the models are trained on a diverse range of data points including time tags indicating the beginning and end of laughter throughout the utterance. This leads to better-performing models and more accurate speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The two types of laughter that the postdoc is referring to are:&#10;&#09;* Laughter that occurs in the middle of a sentence.&#10;&#09;* Laughter that occurs after a sentence has been completed.&#10;2. Distinguishing between these two types of laughter is important because they may have different implications for the conversation or speech analysis. For example, laughter in the middle of a sentence might indicate amusement or interruption, while laughter after a sentence could suggest understanding or agreement. By distinguishing between them, researchers can better understand the nuances of human communication and improve the performance of their systems in recognizing and modeling these non-speech elements." target="1. The two types of laughter that the postdoc is referring to are: a) laughter that occurs in the middle of a sentence, and b) laughter that occurs after a sentence has been completed. &#10;2. These two types of laughter require extra complications such as time tags for distinguishing between them because it can be challenging to differentiate speaking while laughing from regular speech, especially when laughter occurs simultaneously with speech. By using time tags to indicate the beginning and end of laughter throughout the utterance, researchers can better identify and analyze these non-speech elements in conversations or speeches.">
      <data key="d0">1</data>
    </edge>
    <edge source=" great .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: It {disfmarker} they {disfmarker} and then in curly brackets they say &quot; laughter &quot; . Now they 're {disfmarker} {vocalsound} they 're not being {pause} awfully precise , uh , m So they 're two types of laughter that are not being distinguished .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: One is {vocalsound} when sometimes s someone will start laughing when they 're in the middle of a sentence .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: And {disfmarker} and then the other one is when they finish the sentence and then they laugh . So , um , I {disfmarker} I did s I did some double checking to look through {disfmarker} I mean , {vocalsound} you 'd need to have extra  e extra complications , like time tags indicating the beginning and ending of {disfmarker}" target="1. The two types of laughter that the postdoc is referring to are: a) laughter that occurs in the middle of a sentence, and b) laughter that occurs after a sentence has been completed. &#10;2. These two types of laughter require extra complications such as time tags for distinguishing between them because it can be challenging to differentiate speaking while laughing from regular speech, especially when laughter occurs simultaneously with speech. By using time tags to indicate the beginning and end of laughter throughout the utterance, researchers can better identify and analyze these non-speech elements in conversations or speeches.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {vocalsound} you 'd need to have extra  e extra complications , like time tags indicating the beginning and ending of {disfmarker} {vocalsound} of the laughing through the utterance .&#10;Speaker: PhD B&#10;Content: It 's not so {disfmarker} I don't think it 's , um {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And that {disfmarker} and what they 're doing is in both cases just saying &quot; curly brackets laughing &quot; a after the unit .&#10;Speaker: PhD B&#10;Content: As {disfmarker} as long as there is an indication that there was laughter somewhere between {pause} two words {vocalsound} I think that 's sufficient ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Good . Oh !&#10;Speaker: Grad A&#10;Content: Against {disfmarker} they could do forced alignment .&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: because actually the recognition of laughter once you kn um {disfmarker} you know , is pretty good" target="1. Challenges in detecting laughter during speech: The main challenge is distinguishing between speaking while laughing and regular speech. This requires dealing with the additional complexity of identifying time tags indicating the beginning and end of laughter throughout the utterance. Another issue mentioned was recognizing laughter when it occurs simultaneously with speech, as the system might not perform well in these cases.&#10;&#10;2. Differences from laughter between words: Laughter between words is generally easier to detect since it can be considered a pause or break in the conversation. When laughter occurs during speech, it may require more sophisticated methods for identification, such as marking time tags and dealing with complications arising from overlapping laughter and speech. &#10;&#10;These challenges highlight the importance of addressing various scenarios when transcribing non-speech elements like breath and laughter in order to improve the overall performance of speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {vocalsound} you 'd need to have extra  e extra complications , like time tags indicating the beginning and ending of {disfmarker} {vocalsound} of the laughing through the utterance .&#10;Speaker: PhD B&#10;Content: It 's not so {disfmarker} I don't think it 's , um {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And that {disfmarker} and what they 're doing is in both cases just saying &quot; curly brackets laughing &quot; a after the unit .&#10;Speaker: PhD B&#10;Content: As {disfmarker} as long as there is an indication that there was laughter somewhere between {pause} two words {vocalsound} I think that 's sufficient ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Good . Oh !&#10;Speaker: Grad A&#10;Content: Against {disfmarker} they could do forced alignment .&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: because actually the recognition of laughter once you kn um {disfmarker} you know , is pretty good" target="1. Marking audible breaths and laughter in transcripts provides crucial data for training models to recognize and model these non-speech elements, which can improve the overall performance of speech recognition systems. This is important because distinguishing between speaking while laughing and regular speech, as well as recognizing laughter during speech, can be challenging.&#10;&#10;2. There are different types of laughter that may have different implications for conversation or speech analysis, such as laughter in the middle of a sentence indicating amusement or interruption, and laughter after a sentence suggesting understanding or agreement. By explicitly marking these non-speech elements, researchers can better understand the nuances of human communication and enhance their systems' ability to recognize and model them effectively.&#10;&#10;3. Marking audible breaths and laughter in transcripts also helps address various scenarios during speech analysis, ensuring that the models are trained on a diverse range of data points including time tags indicating the beginning and end of laughter throughout the utterance. This leads to better-performing models and more accurate speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {vocalsound} you 'd need to have extra  e extra complications , like time tags indicating the beginning and ending of {disfmarker} {vocalsound} of the laughing through the utterance .&#10;Speaker: PhD B&#10;Content: It 's not so {disfmarker} I don't think it 's , um {disfmarker}&#10;Speaker: Postdoc F&#10;Content: And that {disfmarker} and what they 're doing is in both cases just saying &quot; curly brackets laughing &quot; a after the unit .&#10;Speaker: PhD B&#10;Content: As {disfmarker} as long as there is an indication that there was laughter somewhere between {pause} two words {vocalsound} I think that 's sufficient ,&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Good . Oh !&#10;Speaker: Grad A&#10;Content: Against {disfmarker} they could do forced alignment .&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: because actually the recognition of laughter once you kn um {disfmarker} you know , is pretty good" target="1. The two types of laughter that the postdoc is referring to are: a) laughter that occurs in the middle of a sentence, and b) laughter that occurs after a sentence has been completed. &#10;2. These two types of laughter require extra complications such as time tags for distinguishing between them because it can be challenging to differentiate speaking while laughing from regular speech, especially when laughter occurs simultaneously with speech. By using time tags to indicate the beginning and end of laughter throughout the utterance, researchers can better identify and analyze these non-speech elements in conversations or speeches.">
      <data key="d0">1</data>
    </edge>
    <edge source="The simple concept being discussed involves using a Gaussian distribution for each decision boundary or potential change point in the data. This idea was suggested as a way to determine overlaps or changes between speech and nonspeech portions in a signal. Instead of focusing on specific features, the group discussed modeling this problem with Gaussians and making decisions based on that. The optimal splitting would then be chosen based on these Gaussian models." target=" Grad A&#10;Content: Yeah , I didn't think so .&#10;Speaker: PhD D&#10;Content: He on only train {disfmarker}&#10;Speaker: Professor G&#10;Content: Oh , OK . So he 's just {disfmarker} he just computes a Gaussian over potential {disfmarker}&#10;Speaker: Grad A&#10;Content: Yep .&#10;Speaker: PhD D&#10;Content: Yeah . It was only Gaussian .&#10;Speaker: Professor G&#10;Content: Oh , I see . I see .&#10;Speaker: Grad A&#10;Content: And so I {disfmarker} I think it would work fine for detecting overlap .&#10;Speaker: PhD D&#10;Content: This is the idea .&#10;Speaker: Professor G&#10;Content: And {disfmarker} and {disfmarker}&#10;Speaker: Grad A&#10;Content: It 's just , uh , that i it {disfmarker} he has the two - pass issue that {disfmarker} What he does is , as a first pass he {disfmarker} he {disfmarker} p he does , um , a guess at where the divisions might">
      <data key="d0">1</data>
    </edge>
    <edge source="The simple concept being discussed involves using a Gaussian distribution for each decision boundary or potential change point in the data. This idea was suggested as a way to determine overlaps or changes between speech and nonspeech portions in a signal. Instead of focusing on specific features, the group discussed modeling this problem with Gaussians and making decisions based on that. The optimal splitting would then be chosen based on these Gaussian models." target=" .&#10;Speaker: PhD C&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: But , uh {disfmarker}&#10;Speaker: Professor G&#10;Content: Well , it 's {disfmarker} sort of has a simple one .&#10;Speaker: PhD D&#10;Content: Mmm , yeah .&#10;Speaker: Grad A&#10;Content: Does it ?&#10;Speaker: Professor G&#10;Content: Right ? It 's {disfmarker} it 's just {disfmarker} it 's just a {disfmarker} isn't it just a Gaussian&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: for each {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Yeah . And then {pause} he ch you choose optimal splitting .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Yeah . Oh , it doesn't have {disfmarker} it doesn't have any temporal , uh {disfmarker} ?&#10;Speaker: Grad A&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="The simple concept being discussed involves using a Gaussian distribution for each decision boundary or potential change point in the data. This idea was suggested as a way to determine overlaps or changes between speech and nonspeech portions in a signal. Instead of focusing on specific features, the group discussed modeling this problem with Gaussians and making decisions based on that. The optimal splitting would then be chosen based on these Gaussian models." target=" And , um , so one of the things that they were pushing in d in discussing with me is , um , w why are you spending so much time , uh , on the , uh , feature issue , uh , when perhaps if you sort of deal with what you were using before&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: Professor G&#10;Content: and then just broadened it a bit , instead of just ta using silence as putative change point also {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Nnn , yeah .&#10;Speaker: Professor G&#10;Content: So then you 've got {disfmarker} you already have the super - structure with Gaussians and H - you know , simple H M Ms and so forth . And you {disfmarker} you might {disfmarker} So there was a {disfmarker} there was a little bit of a {disfmarker} a {disfmarker} a {disfmarker} a difference of opinion because I {disfmarker} I thought that it was {disfmarker} it 's interesting to look at what features are useful .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The simple concept being discussed involves using a Gaussian distribution for each decision boundary or potential change point in the data. This idea was suggested as a way to determine overlaps or changes between speech and nonspeech portions in a signal. Instead of focusing on specific features, the group discussed modeling this problem with Gaussians and making decisions based on that. The optimal splitting would then be chosen based on these Gaussian models." target=": OK .&#10;Speaker: Professor G&#10;Content: But , uh , uh {disfmarker} So . Yeah , why don't we do the speech - nonspeech discussion ?&#10;Speaker: Postdoc F&#10;Content: Yeah . Do {disfmarker} I {disfmarker} I hear {disfmarker} you {disfmarker} you didn't {disfmarker}&#10;Speaker: PhD C&#10;Content: Speech - nonspeech ? OK .&#10;Speaker: Postdoc F&#10;Content: Uh - huh . Yeah .&#10;Speaker: PhD C&#10;Content: Um , so , uh , what we basically did so far was using the mixed file to {disfmarker} to detect s speech or nonspeech {pause} portions in that .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: And what I did so far is I just used our old Munich system , which is an HMM - ba based system with Gaussian mixtures for s speech and nonspeech . And it was a system which used only one Gaussian for silence and one Gaussian for speech . And now I added">
      <data key="d0">1</data>
    </edge>
    <edge source="The simple concept being discussed involves using a Gaussian distribution for each decision boundary or potential change point in the data. This idea was suggested as a way to determine overlaps or changes between speech and nonspeech portions in a signal. Instead of focusing on specific features, the group discussed modeling this problem with Gaussians and making decisions based on that. The optimal splitting would then be chosen based on these Gaussian models." target="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further.">
      <data key="d0">1</data>
    </edge>
    <edge source="The simple concept being discussed involves using a Gaussian distribution for each decision boundary or potential change point in the data. This idea was suggested as a way to determine overlaps or changes between speech and nonspeech portions in a signal. Instead of focusing on specific features, the group discussed modeling this problem with Gaussians and making decisions based on that. The optimal splitting would then be chosen based on these Gaussian models." target="1. The choice for the classifier in the first step was a Gaussian distribution, specifically Javier's Gaussian distribution, as suggested by PhD D: &quot;Using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. For the second step, there is no explicit information about how the clustering process performed. The discussion focuses more on using Gaussians for modeling decision boundaries and potential change points in the data. There is no detailed conversation about the clustering process's results or any specific issues related to it.">
      <data key="d0">1</data>
    </edge>
    <edge source="The simple concept being discussed involves using a Gaussian distribution for each decision boundary or potential change point in the data. This idea was suggested as a way to determine overlaps or changes between speech and nonspeech portions in a signal. Instead of focusing on specific features, the group discussed modeling this problem with Gaussians and making decisions based on that. The optimal splitting would then be chosen based on these Gaussian models." target="1. The use of Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones could be beneficial in improving speech recognition, as it would allow for more precise modeling of decision boundaries between speech and non-speech segments. This is based on PhD D's suggestion in the transcript: &quot;I think that an interesting experiment could be to prove that, if we suppose that the first step involves using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. The BIC (Bayesian Information Criterion) criterion can help in selecting the optimal number of Gaussians for modeling the data, balancing model complexity and fit. Using this method could improve the robustness of the speech recognition system by reducing overfitting or underfitting risks during the Gaussian training process.&#10;&#10;3. Manually distinguishing overlapping zones and speech zones requires careful consideration of time tags and potential complications arising from simultaneous laughter and speech, as mentioned in the transcript. This step might be labor-intensive but could provide valuable insights for improving speech recognition systems' performance, especially when dealing with non-speech elements like breath and laughter.&#10;&#10;4. Distinguishing between different types of laughter (e.g., laughter during a sentence vs. laughter after a sentence) is important for better understanding human communication nuances and improving the system's performance in recognizing and modeling these non-speech elements. This could be an additional step or application of Javier's software, allowing researchers to fine-tune their systems further based on specific needs.&#10;&#10;In conclusion, using Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones with the BIC criterion has the potential to improve speech recognition, especially when dealing with non-speech elements like breath and laughter. However, it requires careful consideration of time tags and possible complications during manual annotation, as well as further research on differentiating between various types of laughter.">
      <data key="d0">1</data>
    </edge>
    <edge source="The simple concept being discussed involves using a Gaussian distribution for each decision boundary or potential change point in the data. This idea was suggested as a way to determine overlaps or changes between speech and nonspeech portions in a signal. Instead of focusing on specific features, the group discussed modeling this problem with Gaussians and making decisions based on that. The optimal splitting would then be chosen based on these Gaussian models." target="The issue discussed among the group revolved around the observation that the simple concept of using a Gaussian distribution for decision boundaries or potential change points in the data was not working as expected. Specifically, Professor G and PhD D mentioned that the average, which is calculated by dividing the sum of the data points by the number of data points (variance), did not follow the energy in the signal. This became apparent when observing the valleys and peaks in the signal, as they didn't seem to adhere to the expected distribution based on the signal's energy.&#10;&#10;PhD D also pointed out a problem with the scale, which might be contributing to the difficulties encountered when modeling decision boundaries and potential change points in the data. The term &quot;scale&quot; in this context likely refers to how the Gaussian distributions are adjusted or normalized in relation to the data's range or spread. An incorrect scale can result in poor fitting of Gaussians to the data, leading to suboptimal decisions regarding overlaps or changes between speech and nonspeech portions in a signal.&#10;&#10;In summary, the group discussed potential issues with their Gaussian-based approach for modeling decision boundaries in the data, particularly concerning the average's failure to follow the energy in the signal, inconsistencies with valleys and peaks, and problems related to the scale of the Gaussian distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source="The simple concept being discussed involves using a Gaussian distribution for each decision boundary or potential change point in the data. This idea was suggested as a way to determine overlaps or changes between speech and nonspeech portions in a signal. Instead of focusing on specific features, the group discussed modeling this problem with Gaussians and making decisions based on that. The optimal splitting would then be chosen based on these Gaussian models." target="1. After Postdoc F brought up the issue of microphone procedures and cleanliness of recordings, Professor G acknowledged the topic by repeating it. However, the specifics of their discussion on this matter are not detailed in the transcript.&#10;2. The group then moved on to discuss progress made in terms of non-speech and speech pre-segmenting of the signal, indicating that they were addressing the limitation of their current data source in accurately encoding overlaps during transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Grad A&#10;Content: Yeah , I didn't think so .&#10;Speaker: PhD D&#10;Content: He on only train {disfmarker}&#10;Speaker: Professor G&#10;Content: Oh , OK . So he 's just {disfmarker} he just computes a Gaussian over potential {disfmarker}&#10;Speaker: Grad A&#10;Content: Yep .&#10;Speaker: PhD D&#10;Content: Yeah . It was only Gaussian .&#10;Speaker: Professor G&#10;Content: Oh , I see . I see .&#10;Speaker: Grad A&#10;Content: And so I {disfmarker} I think it would work fine for detecting overlap .&#10;Speaker: PhD D&#10;Content: This is the idea .&#10;Speaker: Professor G&#10;Content: And {disfmarker} and {disfmarker}&#10;Speaker: Grad A&#10;Content: It 's just , uh , that i it {disfmarker} he has the two - pass issue that {disfmarker} What he does is , as a first pass he {disfmarker} he {disfmarker} p he does , um , a guess at where the divisions might" target="1. The choice for the classifier in the first step was a Gaussian distribution, specifically Javier's Gaussian distribution, as suggested by PhD D: &quot;Using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. For the second step, there is no explicit information about how the clustering process performed. The discussion focuses more on using Gaussians for modeling decision boundaries and potential change points in the data. There is no detailed conversation about the clustering process's results or any specific issues related to it.">
      <data key="d0">1</data>
    </edge>
    <edge source=" .&#10;Speaker: PhD C&#10;Content: OK .&#10;Speaker: Grad A&#10;Content: But , uh {disfmarker}&#10;Speaker: Professor G&#10;Content: Well , it 's {disfmarker} sort of has a simple one .&#10;Speaker: PhD D&#10;Content: Mmm , yeah .&#10;Speaker: Grad A&#10;Content: Does it ?&#10;Speaker: Professor G&#10;Content: Right ? It 's {disfmarker} it 's just {disfmarker} it 's just a {disfmarker} isn't it just a Gaussian&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: for each {disfmarker} ?&#10;Speaker: Grad A&#10;Content: Yeah . And then {pause} he ch you choose optimal splitting .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Yeah . Oh , it doesn't have {disfmarker} it doesn't have any temporal , uh {disfmarker} ?&#10;Speaker: Grad A&#10;Content" target="1. The choice for the classifier in the first step was a Gaussian distribution, specifically Javier's Gaussian distribution, as suggested by PhD D: &quot;Using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. For the second step, there is no explicit information about how the clustering process performed. The discussion focuses more on using Gaussians for modeling decision boundaries and potential change points in the data. There is no detailed conversation about the clustering process's results or any specific issues related to it.">
      <data key="d0">1</data>
    </edge>
    <edge source=": OK .&#10;Speaker: Professor G&#10;Content: But , uh , uh {disfmarker} So . Yeah , why don't we do the speech - nonspeech discussion ?&#10;Speaker: Postdoc F&#10;Content: Yeah . Do {disfmarker} I {disfmarker} I hear {disfmarker} you {disfmarker} you didn't {disfmarker}&#10;Speaker: PhD C&#10;Content: Speech - nonspeech ? OK .&#10;Speaker: Postdoc F&#10;Content: Uh - huh . Yeah .&#10;Speaker: PhD C&#10;Content: Um , so , uh , what we basically did so far was using the mixed file to {disfmarker} to detect s speech or nonspeech {pause} portions in that .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: PhD C&#10;Content: And what I did so far is I just used our old Munich system , which is an HMM - ba based system with Gaussian mixtures for s speech and nonspeech . And it was a system which used only one Gaussian for silence and one Gaussian for speech . And now I added" target="1. The choice for the classifier in the first step was a Gaussian distribution, specifically Javier's Gaussian distribution, as suggested by PhD D: &quot;Using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. For the second step, there is no explicit information about how the clustering process performed. The discussion focuses more on using Gaussians for modeling decision boundaries and potential change points in the data. There is no detailed conversation about the clustering process's results or any specific issues related to it.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Professor G seems to have mixed feelings about focusing on pitch-related and harmonic-related features as the best use of limited time for finding a reliable indicator in short text messages. At one point, he expresses some skepticism, mentioning that they haven't found any combination that really gave an indicator so far. He also suggests that it may not be the best thing to focus on given the limited time available.&#10;&#10;However, later in the conversation, Professor G mentions that he is somewhat more hopeful for pitch-related and harmonic-related features as a potential indicator. He also mentions that they thought these features should be reasonable indicators.&#10;&#10;Overall, while Professor G expresses some reservations about focusing solely on pitch-related and harmonic-related features with the limited time available, he does not completely dismiss the idea and leaves open the possibility that it could still yield a reliable indicator." target="aker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: Or , you know , how b short should they be ? So ,&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: Professor G&#10;Content: th he 's been playing around with a lot of these different things and {disfmarker} and so far at least has not come up with {vocalsound} any combination that really gave you an indicator .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: So I {disfmarker} I still have a hunch that there 's {disfmarker} it 's in there some place , but it may be {disfmarker} given that you have a limited time here , it {disfmarker} it just may not be the best thing to {disfmarker} {vocalsound} to {disfmarker} to focus on for the remaining of it .&#10;Speaker: PhD D&#10;Content: Yeah . To overrule , yeah .&#10;Speaker: Professor G&#10;Content: So pitch - related and harmonic - related , I 'm {disfmarker} I">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, Professor G seems to have mixed feelings about focusing on pitch-related and harmonic-related features as the best use of limited time for finding a reliable indicator in short text messages. At one point, he expresses some skepticism, mentioning that they haven't found any combination that really gave an indicator so far. He also suggests that it may not be the best thing to focus on given the limited time available.&#10;&#10;However, later in the conversation, Professor G mentions that he is somewhat more hopeful for pitch-related and harmonic-related features as a potential indicator. He also mentions that they thought these features should be reasonable indicators.&#10;&#10;Overall, while Professor G expresses some reservations about focusing solely on pitch-related and harmonic-related features with the limited time available, he does not completely dismiss the idea and leaves open the possibility that it could still yield a reliable indicator." target="marker} I 'm still {pause} a little f think that 's a little funny . These things l @ @ seems like there should be ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: but {disfmarker} {vocalsound} but you don't want to keep , uh {disfmarker} keep knocking at it if it 's {disfmarker} if you 're not getting any {disfmarker} any result with that . But , I mean , the other things that we talked about is , uh , {vocalsound} pitch - related things and harmonicity - related things ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: so {disfmarker} which we thought also should be some kind of a reasonable indicator . Um {disfmarker} But , uh , a completely different tack on it wou is the one that was suggested , uh , by your colleagues in Spain ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: which is to say , don't worry so much about the , uh , features .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further." target=" .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: Can you hand me your digit form ?&#10;Speaker: PhD B&#10;Content: Sorry .&#10;Speaker: Grad A&#10;Content: I just wanna mark that you did not read digits .&#10;Speaker: Professor G&#10;Content: OK . Say hi for me .&#10;Speaker: Postdoc F&#10;Content: Good . You {disfmarker} you did get me to thinking about {disfmarker} I {disfmarker} I 'm not really sure which is more frequent , whether f f laughing {disfmarker} I think it may be an individual thing . Some people are more prone to laughing when they 're speaking .&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: I was noticing that with Dan in the one that we , uh {disfmarker} we hand tran hand - segmented ,&#10;Speaker: Professor G&#10;Content: Yeah . I think {disfmarker}&#10;Speaker: Postdoc F&#10;Content: But I can't {disfmarker}&#10;Speaker: PhD D">
      <data key="d0">1</data>
    </edge>
    <edge source="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further." target="1. Challenges in detecting laughter during speech: The main challenge is distinguishing between speaking while laughing and regular speech. This requires dealing with the additional complexity of identifying time tags indicating the beginning and end of laughter throughout the utterance. Another issue mentioned was recognizing laughter when it occurs simultaneously with speech, as the system might not perform well in these cases.&#10;&#10;2. Differences from laughter between words: Laughter between words is generally easier to detect since it can be considered a pause or break in the conversation. When laughter occurs during speech, it may require more sophisticated methods for identification, such as marking time tags and dealing with complications arising from overlapping laughter and speech. &#10;&#10;These challenges highlight the importance of addressing various scenarios when transcribing non-speech elements like breath and laughter in order to improve the overall performance of speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further." target="The individuals in this conversation are aware of the limitations of their recordings and do not believe that they represent mankind. They explicitly acknowledge that their sample is very small, and Professor G states that they are not claiming to get a representation of mankind in these recordings. This demonstrates their understanding of the importance of having a large and diverse sample to make accurate generalizations about human behavior.">
      <data key="d0">1</data>
    </edge>
    <edge source="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further." target="The issue discussed among the group revolved around the observation that the simple concept of using a Gaussian distribution for decision boundaries or potential change points in the data was not working as expected. Specifically, Professor G and PhD D mentioned that the average, which is calculated by dividing the sum of the data points by the number of data points (variance), did not follow the energy in the signal. This became apparent when observing the valleys and peaks in the signal, as they didn't seem to adhere to the expected distribution based on the signal's energy.&#10;&#10;PhD D also pointed out a problem with the scale, which might be contributing to the difficulties encountered when modeling decision boundaries and potential change points in the data. The term &quot;scale&quot; in this context likely refers to how the Gaussian distributions are adjusted or normalized in relation to the data's range or spread. An incorrect scale can result in poor fitting of Gaussians to the data, leading to suboptimal decisions regarding overlaps or changes between speech and nonspeech portions in a signal.&#10;&#10;In summary, the group discussed potential issues with their Gaussian-based approach for modeling decision boundaries in the data, particularly concerning the average's failure to follow the energy in the signal, inconsistencies with valleys and peaks, and problems related to the scale of the Gaussian distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further." target="1. After Postdoc F brought up the issue of microphone procedures and cleanliness of recordings, Professor G acknowledged the topic by repeating it. However, the specifics of their discussion on this matter are not detailed in the transcript.&#10;2. The group then moved on to discuss progress made in terms of non-speech and speech pre-segmenting of the signal, indicating that they were addressing the limitation of their current data source in accurately encoding overlaps during transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further." target="1. The transcript does not provide a detailed explanation of the IRAM results that PhD D and Professor G are discussing. However, it appears they are referring to some analysis or research data labeled as &quot;IRAM.&quot; There seems to be some uncertainty about the relevance or general interest of this information, as indicated by PhD D's comment, &quot;I'm not sure if that's of general interest or not.&quot; Professor G also repeats &quot;IRAM&quot; and asks a question about &quot;bigram,&quot; which might be related to their discussion. To fully understand the content of their conversation, more context or information is needed.&#10;2. The group does not seem to raise any specific issues or concerns regarding the IRAM results in this part of the transcript. Instead, the discussion revolves around the classifier choice (Gaussian distribution) and the clustering process for the first step, as well as modeling decision boundaries and change points for the second step. It is possible that the group found the IRAM results interesting but decided to continue discussing other aspects of their project in this particular meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="The group is discussing the frequency of people laughing while speaking and whether this tendency varies between individuals. Postdoc F mentions that they are not sure which is more frequent, laughing in the middle of speaking or between speaking, and thinks it might be an individual thing. Professor G agrees, and Grad A shares their observation of a colleague (Dan) who laughs frequently during speech. This suggests that the group acknowledges the variation in individuals' tendency to laugh while speaking and is interested in exploring this topic further." target="1. The two types of laughter that the postdoc is referring to are: a) laughter that occurs in the middle of a sentence, and b) laughter that occurs after a sentence has been completed. &#10;2. These two types of laughter require extra complications such as time tags for distinguishing between them because it can be challenging to differentiate speaking while laughing from regular speech, especially when laughter occurs simultaneously with speech. By using time tags to indicate the beginning and end of laughter throughout the utterance, researchers can better identify and analyze these non-speech elements in conversations or speeches.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not believe that their recordings represent mankind, as stated by Professor G. They acknowledge that they have a very small sample of recordings and are not claiming to get a representation of mankind in these recordings." target=" {disfmarker}&#10;Speaker: Postdoc F&#10;Content: But I can't {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: that {disfmarker} th he has these little chuckles as he talks .&#10;Speaker: Postdoc F&#10;Content: Yeah . OK .&#10;Speaker: Professor G&#10;Content: I 'm sure it 's very individual . And {disfmarker} and {disfmarker} one thing that c that we 're not doing , of course , is we 're not claiming to , uh , get {disfmarker} be getting a representation of mankind in these recordings . We have {vocalsound} this very , very tiny sample of {disfmarker} of {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: Speech researchers ?&#10;Speaker: Professor G&#10;Content: Uh , yeah . And {disfmarker} {vocalsound} Yeah , r right .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not believe that their recordings represent mankind, as stated by Professor G. They acknowledge that they have a very small sample of recordings and are not claiming to get a representation of mankind in these recordings." target=" ?&#10;Speaker: Professor G&#10;Content: Uh , yeah . And {disfmarker} {vocalsound} Yeah , r right .&#10;Speaker: PhD D&#10;Content: Speech research .&#10;Speaker: Professor G&#10;Content: So , uh , who knows . Uh {disfmarker} Yeah . Why don why don't we just {disfmarker} since we 're on this vein , why don't we just continue with , uh , what you were gonna say about the transcriptions&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: and {disfmarker} ?&#10;Speaker: Postdoc F&#10;Content: Um , um , the {disfmarker} I {disfmarker} I 'm really very for I 'm extremely fortunate with the people who , uh , applied and who are transcribing for us . They {vocalsound} are , um , um , uh really perceptive and very , um {disfmarker} and I 'm not just saying that cuz they might be hearing this .&#10;Speaker: Grad A&#10;Content: Cuz they 're gonna be transcribing it in a few">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not believe that their recordings represent mankind, as stated by Professor G. They acknowledge that they have a very small sample of recordings and are not claiming to get a representation of mankind in these recordings." target=" you ?&#10;Speaker: Postdoc F&#10;Content: Not directly . I 'm trying to think if {disfmarker} if I could have gotten it over a list .&#10;Speaker: Professor G&#10;Content: OK .&#10;Speaker: Postdoc F&#10;Content: I don't {disfmarker} I don't think so .&#10;Speaker: Professor G&#10;Content: OK . Well , holidays may have interrupted things , cuz in {disfmarker} in {disfmarker} in {disfmarker} They {vocalsound} seem to want to {pause} get absolutely clear on standards for {disfmarker} transcription standards and so forth with {disfmarker} with us .&#10;Speaker: Postdoc F&#10;Content: Oh ! This was from before December . Yeah .&#10;Speaker: Professor G&#10;Content: Right . Because they 're {disfmarker} they 're presumably going to start recording next month .&#10;Speaker: Postdoc F&#10;Content: OK . OK .&#10;Speaker: Grad A&#10;Content: Oh , we should definitely get with them then ,&#10;Speaker: Professor G&#10;Content: So .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not believe that their recordings represent mankind, as stated by Professor G. They acknowledge that they have a very small sample of recordings and are not claiming to get a representation of mankind in these recordings." target=" saying that cuz they might be hearing this .&#10;Speaker: Grad A&#10;Content: Cuz they 're gonna be transcribing it in a few days .&#10;Speaker: Postdoc F&#10;Content: No , they 're super . They 're {disfmarker} the they {disfmarker} very quick .&#10;Speaker: PhD E&#10;Content: OK . Turn the mikes off and let 's talk .&#10;Speaker: Postdoc F&#10;Content: Yeah , I know . I am {disfmarker} I 'm serious . They 're just super . So I , um , e you know , I {disfmarker} I brought them in and , um , trained them in pairs because I think people can raise questions {disfmarker}&#10;Speaker: Grad A&#10;Content: That 's a good idea .&#10;Speaker: Postdoc F&#10;Content: you know , i i the they think about different things and they think of different {disfmarker} and um , I trained them to , uh , f on about a minute or two of the one that was already transcribed . This also gives me a sense of {disfmarker} You">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not believe that their recordings represent mankind, as stated by Professor G. They acknowledge that they have a very small sample of recordings and are not claiming to get a representation of mankind in these recordings." target="The individuals in this conversation are aware of the limitations of their recordings and do not believe that they represent mankind. They explicitly acknowledge that their sample is very small, and Professor G states that they are not claiming to get a representation of mankind in these recordings. This demonstrates their understanding of the importance of having a large and diverse sample to make accurate generalizations about human behavior.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not believe that their recordings represent mankind, as stated by Professor G. They acknowledge that they have a very small sample of recordings and are not claiming to get a representation of mankind in these recordings." target="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not believe that their recordings represent mankind, as stated by Professor G. They acknowledge that they have a very small sample of recordings and are not claiming to get a representation of mankind in these recordings." target="1. Professor G, Postdoc F, and possibly other attendees think that practical items can be useful for their purpose, as evidenced by phrases such as &quot;we don't want anything too practical&quot; and &quot;why don't we talk about practical things?&quot; However, they ultimately decide to focus on &quot;useless things,&quot; likely meaning things that are not directly applicable to their current goal but may still provide some value or insight.&#10;2. Grad A seems to have a more negative view of practicality in this context, stating &quot;I mean, we don't want anything too practical&quot; and expressing annoyance with someone named Chuck for being too practical (presumably by telling too many jokes during lunch).&#10;3. There is no direct mention of Javier's opinion on the practicality of items discussed; however, since he focuses on training models rather than using Markov models in his program, it can be inferred that he may lean towards favoring theoretical or abstract approaches over practical ones.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not believe that their recordings represent mankind, as stated by Professor G. They acknowledge that they have a very small sample of recordings and are not claiming to get a representation of mankind in these recordings." target="1. After Postdoc F brought up the issue of microphone procedures and cleanliness of recordings, Professor G acknowledged the topic by repeating it. However, the specifics of their discussion on this matter are not detailed in the transcript.&#10;2. The group then moved on to discuss progress made in terms of non-speech and speech pre-segmenting of the signal, indicating that they were addressing the limitation of their current data source in accurately encoding overlaps during transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers do not believe that their recordings represent mankind, as stated by Professor G. They acknowledge that they have a very small sample of recordings and are not claiming to get a representation of mankind in these recordings." target="1. The speakers discuss the importance of having both the notation and machine representation be the same in order to accurately encode overlaps during transcription. This is brought up by Grad A, and Professor G and Postdoc F agree with this sentiment. They also mention the potential existence of relevant collections at various institutions such as Columbia, UW, and NIST, indicating that they are aware of possible resources related to their topic.">
      <data key="d0">1</data>
    </edge>
    <edge source=" {disfmarker}&#10;Speaker: Postdoc F&#10;Content: But I can't {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: that {disfmarker} th he has these little chuckles as he talks .&#10;Speaker: Postdoc F&#10;Content: Yeah . OK .&#10;Speaker: Professor G&#10;Content: I 'm sure it 's very individual . And {disfmarker} and {disfmarker} one thing that c that we 're not doing , of course , is we 're not claiming to , uh , get {disfmarker} be getting a representation of mankind in these recordings . We have {vocalsound} this very , very tiny sample of {disfmarker} of {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: Speech researchers ?&#10;Speaker: Professor G&#10;Content: Uh , yeah . And {disfmarker} {vocalsound} Yeah , r right .&#10;" target="The individuals in this conversation are aware of the limitations of their recordings and do not believe that they represent mankind. They explicitly acknowledge that their sample is very small, and Professor G states that they are not claiming to get a representation of mankind in these recordings. This demonstrates their understanding of the importance of having a large and diverse sample to make accurate generalizations about human behavior.">
      <data key="d0">1</data>
    </edge>
    <edge source=" ?&#10;Speaker: Professor G&#10;Content: Uh , yeah . And {disfmarker} {vocalsound} Yeah , r right .&#10;Speaker: PhD D&#10;Content: Speech research .&#10;Speaker: Professor G&#10;Content: So , uh , who knows . Uh {disfmarker} Yeah . Why don why don't we just {disfmarker} since we 're on this vein , why don't we just continue with , uh , what you were gonna say about the transcriptions&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: and {disfmarker} ?&#10;Speaker: Postdoc F&#10;Content: Um , um , the {disfmarker} I {disfmarker} I 'm really very for I 'm extremely fortunate with the people who , uh , applied and who are transcribing for us . They {vocalsound} are , um , um , uh really perceptive and very , um {disfmarker} and I 'm not just saying that cuz they might be hearing this .&#10;Speaker: Grad A&#10;Content: Cuz they 're gonna be transcribing it in a few" target="The individuals in this conversation are aware of the limitations of their recordings and do not believe that they represent mankind. They explicitly acknowledge that their sample is very small, and Professor G states that they are not claiming to get a representation of mankind in these recordings. This demonstrates their understanding of the importance of having a large and diverse sample to make accurate generalizations about human behavior.">
      <data key="d0">1</data>
    </edge>
    <edge source=" ?&#10;Speaker: Professor G&#10;Content: Uh , yeah . And {disfmarker} {vocalsound} Yeah , r right .&#10;Speaker: PhD D&#10;Content: Speech research .&#10;Speaker: Professor G&#10;Content: So , uh , who knows . Uh {disfmarker} Yeah . Why don why don't we just {disfmarker} since we 're on this vein , why don't we just continue with , uh , what you were gonna say about the transcriptions&#10;Speaker: Postdoc F&#10;Content: OK .&#10;Speaker: Professor G&#10;Content: and {disfmarker} ?&#10;Speaker: Postdoc F&#10;Content: Um , um , the {disfmarker} I {disfmarker} I 'm really very for I 'm extremely fortunate with the people who , uh , applied and who are transcribing for us . They {vocalsound} are , um , um , uh really perceptive and very , um {disfmarker} and I 'm not just saying that cuz they might be hearing this .&#10;Speaker: Grad A&#10;Content: Cuz they 're gonna be transcribing it in a few" target="The name of the alternative tool to Praat, mentioned by PhD C, is &quot;trans transedit.&quot; However, it seems there might be some confusion around this name, as PhD C mentions later in the conversation that they think that's its name. It is essential to confirm the exact name and investigate its features to determine if it would be suitable for addressing the limitations in encoding overlaps during transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The choice for the classifier in the first step was a Gaussian distribution, specifically Javier's Gaussian distribution, as suggested by PhD D: &quot;Using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. For the second step, there is no explicit information about how the clustering process performed. The discussion focuses more on using Gaussians for modeling decision boundaries and potential change points in the data. There is no detailed conversation about the clustering process's results or any specific issues related to it." target=" , mmm , if s we suppose that , eh , the {disfmarker} the first step {disfmarker} {vocalsound} I mean , the {disfmarker} the classifier what were the classifier from Javier or classifier from Thilo ? W What happen with the second step ? I {disfmarker} I mean , what {disfmarker} what happen with the , eh {disfmarker} the , uh , clu the , uh {disfmarker} the clu the clustering process ?&#10;Speaker: Grad A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Using the {disfmarker} the Gaussian .&#10;Speaker: Grad A&#10;Content: You mean Javier 's ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: What do you mean ?&#10;Speaker: PhD D&#10;Content: I {disfmarker} I mean , that is {disfmarker} is enough {disfmarker} is enough , eh , to work well , eh , to , eh , separate or to distinguish , eh , between overlapping zone">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The choice for the classifier in the first step was a Gaussian distribution, specifically Javier's Gaussian distribution, as suggested by PhD D: &quot;Using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. For the second step, there is no explicit information about how the clustering process performed. The discussion focuses more on using Gaussians for modeling decision boundaries and potential change points in the data. There is no detailed conversation about the clustering process's results or any specific issues related to it." target="1. The use of Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones could be beneficial in improving speech recognition, as it would allow for more precise modeling of decision boundaries between speech and non-speech segments. This is based on PhD D's suggestion in the transcript: &quot;I think that an interesting experiment could be to prove that, if we suppose that the first step involves using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. The BIC (Bayesian Information Criterion) criterion can help in selecting the optimal number of Gaussians for modeling the data, balancing model complexity and fit. Using this method could improve the robustness of the speech recognition system by reducing overfitting or underfitting risks during the Gaussian training process.&#10;&#10;3. Manually distinguishing overlapping zones and speech zones requires careful consideration of time tags and potential complications arising from simultaneous laughter and speech, as mentioned in the transcript. This step might be labor-intensive but could provide valuable insights for improving speech recognition systems' performance, especially when dealing with non-speech elements like breath and laughter.&#10;&#10;4. Distinguishing between different types of laughter (e.g., laughter during a sentence vs. laughter after a sentence) is important for better understanding human communication nuances and improving the system's performance in recognizing and modeling these non-speech elements. This could be an additional step or application of Javier's software, allowing researchers to fine-tune their systems further based on specific needs.&#10;&#10;In conclusion, using Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones with the BIC criterion has the potential to improve speech recognition, especially when dealing with non-speech elements like breath and laughter. However, it requires careful consideration of time tags and possible complications during manual annotation, as well as further research on differentiating between various types of laughter.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The choice for the classifier in the first step was a Gaussian distribution, specifically Javier's Gaussian distribution, as suggested by PhD D: &quot;Using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. For the second step, there is no explicit information about how the clustering process performed. The discussion focuses more on using Gaussians for modeling decision boundaries and potential change points in the data. There is no detailed conversation about the clustering process's results or any specific issues related to it." target="The issue discussed among the group revolved around the observation that the simple concept of using a Gaussian distribution for decision boundaries or potential change points in the data was not working as expected. Specifically, Professor G and PhD D mentioned that the average, which is calculated by dividing the sum of the data points by the number of data points (variance), did not follow the energy in the signal. This became apparent when observing the valleys and peaks in the signal, as they didn't seem to adhere to the expected distribution based on the signal's energy.&#10;&#10;PhD D also pointed out a problem with the scale, which might be contributing to the difficulties encountered when modeling decision boundaries and potential change points in the data. The term &quot;scale&quot; in this context likely refers to how the Gaussian distributions are adjusted or normalized in relation to the data's range or spread. An incorrect scale can result in poor fitting of Gaussians to the data, leading to suboptimal decisions regarding overlaps or changes between speech and nonspeech portions in a signal.&#10;&#10;In summary, the group discussed potential issues with their Gaussian-based approach for modeling decision boundaries in the data, particularly concerning the average's failure to follow the energy in the signal, inconsistencies with valleys and peaks, and problems related to the scale of the Gaussian distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The choice for the classifier in the first step was a Gaussian distribution, specifically Javier's Gaussian distribution, as suggested by PhD D: &quot;Using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. For the second step, there is no explicit information about how the clustering process performed. The discussion focuses more on using Gaussians for modeling decision boundaries and potential change points in the data. There is no detailed conversation about the clustering process's results or any specific issues related to it." target="1. The transcript does not provide a detailed explanation of the IRAM results that PhD D and Professor G are discussing. However, it appears they are referring to some analysis or research data labeled as &quot;IRAM.&quot; There seems to be some uncertainty about the relevance or general interest of this information, as indicated by PhD D's comment, &quot;I'm not sure if that's of general interest or not.&quot; Professor G also repeats &quot;IRAM&quot; and asks a question about &quot;bigram,&quot; which might be related to their discussion. To fully understand the content of their conversation, more context or information is needed.&#10;2. The group does not seem to raise any specific issues or concerns regarding the IRAM results in this part of the transcript. Instead, the discussion revolves around the classifier choice (Gaussian distribution) and the clustering process for the first step, as well as modeling decision boundaries and change points for the second step. It is possible that the group found the IRAM results interesting but decided to continue discussing other aspects of their project in this particular meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Challenges in detecting laughter during speech: The main challenge is distinguishing between speaking while laughing and regular speech. This requires dealing with the additional complexity of identifying time tags indicating the beginning and end of laughter throughout the utterance. Another issue mentioned was recognizing laughter when it occurs simultaneously with speech, as the system might not perform well in these cases.&#10;&#10;2. Differences from laughter between words: Laughter between words is generally easier to detect since it can be considered a pause or break in the conversation. When laughter occurs during speech, it may require more sophisticated methods for identification, such as marking time tags and dealing with complications arising from overlapping laughter and speech. &#10;&#10;These challenges highlight the importance of addressing various scenarios when transcribing non-speech elements like breath and laughter in order to improve the overall performance of speech recognition systems." target="1. The use of Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones could be beneficial in improving speech recognition, as it would allow for more precise modeling of decision boundaries between speech and non-speech segments. This is based on PhD D's suggestion in the transcript: &quot;I think that an interesting experiment could be to prove that, if we suppose that the first step involves using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. The BIC (Bayesian Information Criterion) criterion can help in selecting the optimal number of Gaussians for modeling the data, balancing model complexity and fit. Using this method could improve the robustness of the speech recognition system by reducing overfitting or underfitting risks during the Gaussian training process.&#10;&#10;3. Manually distinguishing overlapping zones and speech zones requires careful consideration of time tags and potential complications arising from simultaneous laughter and speech, as mentioned in the transcript. This step might be labor-intensive but could provide valuable insights for improving speech recognition systems' performance, especially when dealing with non-speech elements like breath and laughter.&#10;&#10;4. Distinguishing between different types of laughter (e.g., laughter during a sentence vs. laughter after a sentence) is important for better understanding human communication nuances and improving the system's performance in recognizing and modeling these non-speech elements. This could be an additional step or application of Javier's software, allowing researchers to fine-tune their systems further based on specific needs.&#10;&#10;In conclusion, using Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones with the BIC criterion has the potential to improve speech recognition, especially when dealing with non-speech elements like breath and laughter. However, it requires careful consideration of time tags and possible complications during manual annotation, as well as further research on differentiating between various types of laughter.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Challenges in detecting laughter during speech: The main challenge is distinguishing between speaking while laughing and regular speech. This requires dealing with the additional complexity of identifying time tags indicating the beginning and end of laughter throughout the utterance. Another issue mentioned was recognizing laughter when it occurs simultaneously with speech, as the system might not perform well in these cases.&#10;&#10;2. Differences from laughter between words: Laughter between words is generally easier to detect since it can be considered a pause or break in the conversation. When laughter occurs during speech, it may require more sophisticated methods for identification, such as marking time tags and dealing with complications arising from overlapping laughter and speech. &#10;&#10;These challenges highlight the importance of addressing various scenarios when transcribing non-speech elements like breath and laughter in order to improve the overall performance of speech recognition systems." target="1. Marking audible breaths and laughter in transcripts provides crucial data for training models to recognize and model these non-speech elements, which can improve the overall performance of speech recognition systems. This is important because distinguishing between speaking while laughing and regular speech, as well as recognizing laughter during speech, can be challenging.&#10;&#10;2. There are different types of laughter that may have different implications for conversation or speech analysis, such as laughter in the middle of a sentence indicating amusement or interruption, and laughter after a sentence suggesting understanding or agreement. By explicitly marking these non-speech elements, researchers can better understand the nuances of human communication and enhance their systems' ability to recognize and model them effectively.&#10;&#10;3. Marking audible breaths and laughter in transcripts also helps address various scenarios during speech analysis, ensuring that the models are trained on a diverse range of data points including time tags indicating the beginning and end of laughter throughout the utterance. This leads to better-performing models and more accurate speech recognition systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Challenges in detecting laughter during speech: The main challenge is distinguishing between speaking while laughing and regular speech. This requires dealing with the additional complexity of identifying time tags indicating the beginning and end of laughter throughout the utterance. Another issue mentioned was recognizing laughter when it occurs simultaneously with speech, as the system might not perform well in these cases.&#10;&#10;2. Differences from laughter between words: Laughter between words is generally easier to detect since it can be considered a pause or break in the conversation. When laughter occurs during speech, it may require more sophisticated methods for identification, such as marking time tags and dealing with complications arising from overlapping laughter and speech. &#10;&#10;These challenges highlight the importance of addressing various scenarios when transcribing non-speech elements like breath and laughter in order to improve the overall performance of speech recognition systems." target="1. The two types of laughter that the postdoc is referring to are: a) laughter that occurs in the middle of a sentence, and b) laughter that occurs after a sentence has been completed. &#10;2. These two types of laughter require extra complications such as time tags for distinguishing between them because it can be challenging to differentiate speaking while laughing from regular speech, especially when laughter occurs simultaneously with speech. By using time tags to indicate the beginning and end of laughter throughout the utterance, researchers can better identify and analyze these non-speech elements in conversations or speeches.">
      <data key="d0">1</data>
    </edge>
    <edge source="The individuals in this conversation are aware of the limitations of their recordings and do not believe that they represent mankind. They explicitly acknowledge that their sample is very small, and Professor G states that they are not claiming to get a representation of mankind in these recordings. This demonstrates their understanding of the importance of having a large and diverse sample to make accurate generalizations about human behavior." target="The issue discussed among the group revolved around the observation that the simple concept of using a Gaussian distribution for decision boundaries or potential change points in the data was not working as expected. Specifically, Professor G and PhD D mentioned that the average, which is calculated by dividing the sum of the data points by the number of data points (variance), did not follow the energy in the signal. This became apparent when observing the valleys and peaks in the signal, as they didn't seem to adhere to the expected distribution based on the signal's energy.&#10;&#10;PhD D also pointed out a problem with the scale, which might be contributing to the difficulties encountered when modeling decision boundaries and potential change points in the data. The term &quot;scale&quot; in this context likely refers to how the Gaussian distributions are adjusted or normalized in relation to the data's range or spread. An incorrect scale can result in poor fitting of Gaussians to the data, leading to suboptimal decisions regarding overlaps or changes between speech and nonspeech portions in a signal.&#10;&#10;In summary, the group discussed potential issues with their Gaussian-based approach for modeling decision boundaries in the data, particularly concerning the average's failure to follow the energy in the signal, inconsistencies with valleys and peaks, and problems related to the scale of the Gaussian distributions.">
      <data key="d0">1</data>
    </edge>
    <edge source="The individuals in this conversation are aware of the limitations of their recordings and do not believe that they represent mankind. They explicitly acknowledge that their sample is very small, and Professor G states that they are not claiming to get a representation of mankind in these recordings. This demonstrates their understanding of the importance of having a large and diverse sample to make accurate generalizations about human behavior." target="Professionor G suggests using transcribers more thoroughly to analyze overlaps in a more detailed manner. He proposes that they could go through the recordings specifically for the overlaps. This approach aims to improve the encoding of overlaps by focusing on them directly.&#10;&#10;To connect the right people to help with this concern, Professor G has already taken several steps. He mentioned a meeting with Postdoc F and Dave Gelbart, where they discussed modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue. By engaging relevant experts in the field, Professor G is actively working on finding solutions to address the limitations in encoding overlaps.">
      <data key="d0">1</data>
    </edge>
    <edge source="The individuals in this conversation are aware of the limitations of their recordings and do not believe that they represent mankind. They explicitly acknowledge that their sample is very small, and Professor G states that they are not claiming to get a representation of mankind in these recordings. This demonstrates their understanding of the importance of having a large and diverse sample to make accurate generalizations about human behavior." target="1. The transcript does not provide a detailed explanation of the IRAM results that PhD D and Professor G are discussing. However, it appears they are referring to some analysis or research data labeled as &quot;IRAM.&quot; There seems to be some uncertainty about the relevance or general interest of this information, as indicated by PhD D's comment, &quot;I'm not sure if that's of general interest or not.&quot; Professor G also repeats &quot;IRAM&quot; and asks a question about &quot;bigram,&quot; which might be related to their discussion. To fully understand the content of their conversation, more context or information is needed.&#10;2. The group does not seem to raise any specific issues or concerns regarding the IRAM results in this part of the transcript. Instead, the discussion revolves around the classifier choice (Gaussian distribution) and the clustering process for the first step, as well as modeling decision boundaries and change points for the second step. It is possible that the group found the IRAM results interesting but decided to continue discussing other aspects of their project in this particular meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The use of Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones could be beneficial in improving speech recognition, as it would allow for more precise modeling of decision boundaries between speech and non-speech segments. This is based on PhD D's suggestion in the transcript: &quot;I think that an interesting experiment could be to prove that, if we suppose that the first step involves using the Gaussian... I mean, that is enough to work well, to separate or to distinguish between overlapping zones.&quot;&#10;&#10;2. The BIC (Bayesian Information Criterion) criterion can help in selecting the optimal number of Gaussians for modeling the data, balancing model complexity and fit. Using this method could improve the robustness of the speech recognition system by reducing overfitting or underfitting risks during the Gaussian training process.&#10;&#10;3. Manually distinguishing overlapping zones and speech zones requires careful consideration of time tags and potential complications arising from simultaneous laughter and speech, as mentioned in the transcript. This step might be labor-intensive but could provide valuable insights for improving speech recognition systems' performance, especially when dealing with non-speech elements like breath and laughter.&#10;&#10;4. Distinguishing between different types of laughter (e.g., laughter during a sentence vs. laughter after a sentence) is important for better understanding human communication nuances and improving the system's performance in recognizing and modeling these non-speech elements. This could be an additional step or application of Javier's software, allowing researchers to fine-tune their systems further based on specific needs.&#10;&#10;In conclusion, using Javier's software for training a Gaussian model by manually distinguishing overlapping zones and speech zones with the BIC criterion has the potential to improve speech recognition, especially when dealing with non-speech elements like breath and laughter. However, it requires careful consideration of time tags and possible complications during manual annotation, as well as further research on differentiating between various types of laughter." target="Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Professor G&#10;Content: or {disfmarker} {vocalsound} or cepstrum or something , um {disfmarker} Eee {vocalsound} it 's in there somewhere probably .&#10;Speaker: PhD D&#10;Content: But , eh , what did you think about the possibility of using the Javier software ? Eh , I mean , the , uh {disfmarker} the , uh {disfmarker} the BIC criterion , the {disfmarker} the {disfmarker} t to train the {disfmarker} the Gaussian , eh , using the {disfmarker} the mark , eh , by hand , eh , eh , to distinguish be mmm , to train overlapping zone and speech zone . I mean , eh , {vocalsound} I {disfmarker} I {disfmarker} I think that an interesting , eh , experiment , eh , could be , th eh , to prove that , mmm , if s we suppose that , eh , the {disfmarker} the first step {disfmarker} {vocalsound">
      <data key="d0">1</data>
    </edge>
    <edge source="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation." target=" different , uh , classes to {disfmarker} to code , uh , the {disfmarker} the overlap , you will use ?&#10;Speaker: Postdoc F&#10;Content: Um , to code d&#10;Speaker: PhD D&#10;Content: What you {disfmarker} you {disfmarker}&#10;Speaker: Postdoc F&#10;Content: so types of overlap ?&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Um , so {nonvocalsound} at a meeting that wasn't transcribed , we worked up a {disfmarker} a typology .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: And , um {disfmarker}&#10;Speaker: PhD D&#10;Content: Look like , uh , you t you explaining in the blackboard ? The {disfmarker} ? Yeah ? Yeah .&#10;Speaker: Postdoc F&#10;Content: Yes , exactly . That hasn't changed . So it {nonvocalsound} i the {disfmarker} it 's basically a two - tiered structure where the first one is whether {non">
      <data key="d0">1</data>
    </edge>
    <edge source="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation." target="} uh , this {disfmarker} this is {disfmarker} very possibly a different , uh , topic . But , {nonvocalsound} uh , just let me say {pause} with reference to this idea of , um , {vocalsound} higher - order organization within meetings . So like in a {disfmarker} you know , the topics that are covered during a meeting with reference to the other , uh , uses of the data ,&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: so being able to {pause} find where so - and - so talked about such - and - such , then , um , um {disfmarker} e I mean , I {disfmarker} I {disfmarker} I did sort of a {disfmarker} {vocalsound} a rough {pause} pass {nonvocalsound} on encoding , like , episode - like level things on the , uh , transcribed meeting {disfmarker}&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Postdoc F&#10;Content: already transcribed meeting">
      <data key="d0">1</data>
    </edge>
    <edge source="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation." target="1. Professor G, Postdoc F, and possibly other attendees think that practical items can be useful for their purpose, as evidenced by phrases such as &quot;we don't want anything too practical&quot; and &quot;why don't we talk about practical things?&quot; However, they ultimately decide to focus on &quot;useless things,&quot; likely meaning things that are not directly applicable to their current goal but may still provide some value or insight.&#10;2. Grad A seems to have a more negative view of practicality in this context, stating &quot;I mean, we don't want anything too practical&quot; and expressing annoyance with someone named Chuck for being too practical (presumably by telling too many jokes during lunch).&#10;3. There is no direct mention of Javier's opinion on the practicality of items discussed; however, since he focuses on training models rather than using Markov models in his program, it can be inferred that he may lean towards favoring theoretical or abstract approaches over practical ones.">
      <data key="d0">1</data>
    </edge>
    <edge source="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation." target="Professionor G suggests using transcribers more thoroughly to analyze overlaps in a more detailed manner. He proposes that they could go through the recordings specifically for the overlaps. This approach aims to improve the encoding of overlaps by focusing on them directly.&#10;&#10;To connect the right people to help with this concern, Professor G has already taken several steps. He mentioned a meeting with Postdoc F and Dave Gelbart, where they discussed modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue. By engaging relevant experts in the field, Professor G is actively working on finding solutions to address the limitations in encoding overlaps.">
      <data key="d0">1</data>
    </edge>
    <edge source="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation." target="The name of the alternative tool to Praat, mentioned by PhD C, is &quot;trans transedit.&quot; However, it seems there might be some confusion around this name, as PhD C mentions later in the conversation that they think that's its name. It is essential to confirm the exact name and investigate its features to determine if it would be suitable for addressing the limitations in encoding overlaps during transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation." target="1. After Postdoc F brought up the issue of microphone procedures and cleanliness of recordings, Professor G acknowledged the topic by repeating it. However, the specifics of their discussion on this matter are not detailed in the transcript.&#10;2. The group then moved on to discuss progress made in terms of non-speech and speech pre-segmenting of the signal, indicating that they were addressing the limitation of their current data source in accurately encoding overlaps during transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation." target="Based on the information provided, the Postdoc (presumably Postdoc F) suggested scheduling a separate discussion with Professor G to go over and share transcripts and forced alignments. However, the specifics of how this would be done or any further details about the suggestion are not mentioned in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="The purpose of the meeting was to discuss the limitation of their current data source in accurately encoding overlaps during transcription. The speakers wanted to address the issue that existing transcribers may not be capturing all necessary start and end points of these overlaps. Attendees likely included Professor G, Postdoc F, and PhD C, as they are mentioned contributing to the discussion. It is unclear whether other individuals were present or participated in the conversation." target="1. The speakers discuss the importance of having both the notation and machine representation be the same in order to accurately encode overlaps during transcription. This is brought up by Grad A, and Professor G and Postdoc F agree with this sentiment. They also mention the potential existence of relevant collections at various institutions such as Columbia, UW, and NIST, indicating that they are aware of possible resources related to their topic.">
      <data key="d0">1</data>
    </edge>
    <edge source="Professionor G suggests using transcribers more thoroughly to analyze overlaps in a more detailed manner. He proposes that they could go through the recordings specifically for the overlaps. This approach aims to improve the encoding of overlaps by focusing on them directly.&#10;&#10;To connect the right people to help with this concern, Professor G has already taken several steps. He mentioned a meeting with Postdoc F and Dave Gelbart, where they discussed modifying the interface for better representation of overlaps. Additionally, PhD C spoke with researchers from Ludwig Maximilians University and Dave Gelbart about this issue. By engaging relevant experts in the field, Professor G is actively working on finding solutions to address the limitations in encoding overlaps." target="The name of the alternative tool to Praat, mentioned by PhD C, is &quot;trans transedit.&quot; However, it seems there might be some confusion around this name, as PhD C mentions later in the conversation that they think that's its name. It is essential to confirm the exact name and investigate its features to determine if it would be suitable for addressing the limitations in encoding overlaps during transcription.">
      <data key="d0">1</data>
    </edge>
    <edge source="The name of the alternative tool to Praat, mentioned by PhD C, is &quot;trans transedit.&quot; However, it seems there might be some confusion around this name, as PhD C mentions later in the conversation that they think that's its name. It is essential to confirm the exact name and investigate its features to determine if it would be suitable for addressing the limitations in encoding overlaps during transcription." target="disfmarker} Praat 's multi - platform .&#10;Speaker: Postdoc F&#10;Content: But I 'm just wondering , is {disfmarker} ?&#10;Speaker: PhD C&#10;Content: No . No , Praat {disfmarker} Yeah . Yeah .&#10;Speaker: Postdoc F&#10;Content: Oh ! I see .&#10;Speaker: PhD C&#10;Content: Yeah .&#10;Speaker: Postdoc F&#10;Content: Oh , I see . So Praat may not be {disfmarker}&#10;Speaker: PhD C&#10;Content: That 's not Praat . It 's called &quot; trans transedit &quot; {pause} I think .&#10;Speaker: Postdoc F&#10;Content: It 's a different one .&#10;Speaker: PhD C&#10;Content: The {disfmarker} the , uh {disfmarker} the tool from {disfmarker} from Susanne .&#10;Speaker: Postdoc F&#10;Content: I see . Oh , I see . OK . OK . Alright .&#10;Speaker: Professor G&#10;Content: The other thing , uh , to keep in mind , uh {disfmarker} I mean ,">
      <data key="d0">1</data>
    </edge>
    <edge source="The name of the alternative tool to Praat, mentioned by PhD C, is &quot;trans transedit.&quot; However, it seems there might be some confusion around this name, as PhD C mentions later in the conversation that they think that's its name. It is essential to confirm the exact name and investigate its features to determine if it would be suitable for addressing the limitations in encoding overlaps during transcription." target="isfmarker} uh , potentially {nonvocalsound} so .&#10;Speaker: Grad A&#10;Content: So .&#10;Speaker: Postdoc F&#10;Content: I also wanted to be sure {disfmarker} I mean , I 've {disfmarker} I 've seen the {disfmarker} this {disfmarker} this is called Praat , PRAAT , {nonvocalsound} which I guess means spee speech in Dutch or something .&#10;Speaker: Grad A&#10;Content: Yep .&#10;Speaker: PhD C&#10;Content: Yeah , but then I 'm not sure {pause} that 's the right thing for us .&#10;Speaker: Postdoc F&#10;Content: But {disfmarker} In terms {nonvocalsound} of it being {nonvocalsound} Windows {nonvocalsound} versus {disfmarker}&#10;Speaker: Professor G&#10;Content: Yeah .&#10;Speaker: Grad A&#10;Content: No , no . Praat isn't {disfmarker} Praat 's multi - platform .&#10;Speaker: Postdoc F&#10;Content: But I 'm just wondering , is {">
      <data key="d0">1</data>
    </edge>
    <edge source="The name of the alternative tool to Praat, mentioned by PhD C, is &quot;trans transedit.&quot; However, it seems there might be some confusion around this name, as PhD C mentions later in the conversation that they think that's its name. It is essential to confirm the exact name and investigate its features to determine if it would be suitable for addressing the limitations in encoding overlaps during transcription." target=": Yeah , and Dave Gelbart did volunteer ,&#10;Speaker: Postdoc F&#10;Content: Good .&#10;Speaker: Grad A&#10;Content: and since he 's not here , I 'll repeat it {disfmarker} to at least modify Transcriber , which , if we don't have something else that works , I think that 's a pretty good way of going .&#10;Speaker: PhD C&#10;Content: Mmm .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: And we discussed on some methods to do it . My approach originally , and I 've already hacked on it a little bit {disfmarker} it was too slow because I was trying to display all the waveforms . But he pointed out that you don't really have to . I think that 's a good point .&#10;Speaker: Postdoc F&#10;Content: Mm - hmm .&#10;Speaker: Professor G&#10;Content: Mm - hmm .&#10;Speaker: Grad A&#10;Content: That if you just display the mix waveform and then have a user interface for editing the different channels , that 's perfectly sufficient .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Marking audible breaths and laughter in transcripts provides crucial data for training models to recognize and model these non-speech elements, which can improve the overall performance of speech recognition systems. This is important because distinguishing between speaking while laughing and regular speech, as well as recognizing laughter during speech, can be challenging.&#10;&#10;2. There are different types of laughter that may have different implications for conversation or speech analysis, such as laughter in the middle of a sentence indicating amusement or interruption, and laughter after a sentence suggesting understanding or agreement. By explicitly marking these non-speech elements, researchers can better understand the nuances of human communication and enhance their systems' ability to recognize and model them effectively.&#10;&#10;3. Marking audible breaths and laughter in transcripts also helps address various scenarios during speech analysis, ensuring that the models are trained on a diverse range of data points including time tags indicating the beginning and end of laughter throughout the utterance. This leads to better-performing models and more accurate speech recognition systems." target="1. The two types of laughter that the postdoc is referring to are: a) laughter that occurs in the middle of a sentence, and b) laughter that occurs after a sentence has been completed. &#10;2. These two types of laughter require extra complications such as time tags for distinguishing between them because it can be challenging to differentiate speaking while laughing from regular speech, especially when laughter occurs simultaneously with speech. By using time tags to indicate the beginning and end of laughter throughout the utterance, researchers can better identify and analyze these non-speech elements in conversations or speeches.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After Postdoc F brought up the issue of microphone procedures and cleanliness of recordings, Professor G acknowledged the topic by repeating it. However, the specifics of their discussion on this matter are not detailed in the transcript.&#10;2. The group then moved on to discuss progress made in terms of non-speech and speech pre-segmenting of the signal, indicating that they were addressing the limitation of their current data source in accurately encoding overlaps during transcription." target="Based on the information provided, the Postdoc (presumably Postdoc F) suggested scheduling a separate discussion with Professor G to go over and share transcripts and forced alignments. However, the specifics of how this would be done or any further details about the suggestion are not mentioned in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After Postdoc F brought up the issue of microphone procedures and cleanliness of recordings, Professor G acknowledged the topic by repeating it. However, the specifics of their discussion on this matter are not detailed in the transcript.&#10;2. The group then moved on to discuss progress made in terms of non-speech and speech pre-segmenting of the signal, indicating that they were addressing the limitation of their current data source in accurately encoding overlaps during transcription." target="1. The speakers discuss the importance of having both the notation and machine representation be the same in order to accurately encode overlaps during transcription. This is brought up by Grad A, and Professor G and Postdoc F agree with this sentiment. They also mention the potential existence of relevant collections at various institutions such as Columbia, UW, and NIST, indicating that they are aware of possible resources related to their topic.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. After Postdoc F brought up the issue of microphone procedures and cleanliness of recordings, Professor G acknowledged the topic by repeating it. However, the specifics of their discussion on this matter are not detailed in the transcript.&#10;2. The group then moved on to discuss progress made in terms of non-speech and speech pre-segmenting of the signal, indicating that they were addressing the limitation of their current data source in accurately encoding overlaps during transcription." target="1. The transcript does not provide a detailed explanation of the IRAM results that PhD D and Professor G are discussing. However, it appears they are referring to some analysis or research data labeled as &quot;IRAM.&quot; There seems to be some uncertainty about the relevance or general interest of this information, as indicated by PhD D's comment, &quot;I'm not sure if that's of general interest or not.&quot; Professor G also repeats &quot;IRAM&quot; and asks a question about &quot;bigram,&quot; which might be related to their discussion. To fully understand the content of their conversation, more context or information is needed.&#10;2. The group does not seem to raise any specific issues or concerns regarding the IRAM results in this part of the transcript. Instead, the discussion revolves around the classifier choice (Gaussian distribution) and the clustering process for the first step, as well as modeling decision boundaries and change points for the second step. It is possible that the group found the IRAM results interesting but decided to continue discussing other aspects of their project in this particular meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the information provided, the Postdoc (presumably Postdoc F) suggested scheduling a separate discussion with Professor G to go over and share transcripts and forced alignments. However, the specifics of how this would be done or any further details about the suggestion are not mentioned in the transcript." target="1. The transcript does not provide a detailed explanation of the IRAM results that PhD D and Professor G are discussing. However, it appears they are referring to some analysis or research data labeled as &quot;IRAM.&quot; There seems to be some uncertainty about the relevance or general interest of this information, as indicated by PhD D's comment, &quot;I'm not sure if that's of general interest or not.&quot; Professor G also repeats &quot;IRAM&quot; and asks a question about &quot;bigram,&quot; which might be related to their discussion. To fully understand the content of their conversation, more context or information is needed.&#10;2. The group does not seem to raise any specific issues or concerns regarding the IRAM results in this part of the transcript. Instead, the discussion revolves around the classifier choice (Gaussian distribution) and the clustering process for the first step, as well as modeling decision boundaries and change points for the second step. It is possible that the group found the IRAM results interesting but decided to continue discussing other aspects of their project in this particular meeting.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The speakers discuss the importance of having both the notation and machine representation be the same in order to accurately encode overlaps during transcription. This is brought up by Grad A, and Professor G and Postdoc F agree with this sentiment. They also mention the potential existence of relevant collections at various institutions such as Columbia, UW, and NIST, indicating that they are aware of possible resources related to their topic." target="isfmarker}&#10;Speaker: Professor G&#10;Content: So , yeah . Maybe I 'll , uh , ping them a little bit about it to {vocalsound} get that straight .&#10;Speaker: Postdoc F&#10;Content: OK . I 'm keeping the conventions {pause} absolutely {pause} as simple {nonvocalsound} as possible .&#10;Speaker: Professor G&#10;Content: Yeah . So is it {disfmarker} cuz with any luck there 'll actually be a {disfmarker} a {disfmarker} there 'll be collections at Columbia , collections at {disfmarker} at UW {disfmarker} I mean Dan {disfmarker} Dan is very interested in doing some other things ,&#10;Speaker: Grad A&#10;Content: Right .&#10;Speaker: Postdoc F&#10;Content: Yeah . Yeah .&#10;Speaker: Grad A&#10;Content: Well , I think it 's important both for the notation and the machine representation to be the same .&#10;Speaker: Professor G&#10;Content: and collections at NIST . So {disfmarker} Yeah .&#10;Speaker: Grad A&#10;Content: So .&#10;Spe">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
