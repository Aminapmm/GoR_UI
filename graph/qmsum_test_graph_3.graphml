<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="weight" attr.type="long" />
  <graph edgedefault="undirected">
    <node id="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" />
    <node id=": Unvoiced . Well .&#10;Speaker: PhD B&#10;Content: and to {disfmarker} to concat this new feature . But the result are n with the neural network I have more or less the same result .&#10;Speaker: PhD A&#10;Content: As using just the cepstrum ,&#10;Speaker: PhD B&#10;Content: Result .&#10;Speaker: PhD A&#10;Content: or {disfmarker} ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Yeah . It 's neve e e sometime it 's worse , sometime it 's a little bit better , but not significantly .&#10;Speaker: PhD A&#10;Content: Uh , is it with TI - digits , or with {disfmarker} ?&#10;Speaker: PhD B&#10;Content: And {disfmarker} No , I work with eh , Italian and Spanish basically .&#10;Speaker: PhD A&#10;Content: OK . OK .&#10;Speaker: PhD B&#10;Content: And if I don't y use the neural network , and use directly the feature the results are worse .&#10;Speaker:" />
    <node id="m sorry ?&#10;Speaker: PhD A&#10;Content: Yeah , we actually added an additional binary feature to the cepstrum , just the baseline .&#10;Speaker: PhD D&#10;Content: Yeah ?&#10;Speaker: PhD B&#10;Content: You did some experiment .&#10;Speaker: PhD A&#10;Content: Yeah , yeah . Well , in {disfmarker} in the case of TI - digits it didn't actually give us anything , because there wasn't any f anything to discriminate between speech ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and it was very short . But Italian was like very {disfmarker} it was a huge improvement on Italian .&#10;Speaker: PhD D&#10;Content: Hmm . Well {disfmarker} Mm - hmm . But anyway the question is even more , is within speech , can we get some features ? Are we drop dropping information that can might be useful within speech ,&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: I mean . To {disfmarker} maybe to distinguish between voice sound and unvoiced sounds ?&#10;Speaker: PhD A&#10;Content" />
    <node id=": PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: What I meant was you had something like cepstra or something , right ?&#10;Speaker: PhD A&#10;Content: Yeah , yeah , yeah , yeah .&#10;Speaker: Professor C&#10;Content: And so one difference is that , I guess you were taking spectra .&#10;Speaker: PhD A&#10;Content: The speech .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah . But I guess it 's the s exactly the same thing because on the heads uh , handset they just applied this Wiener filter and then compute cepstral features ,&#10;Speaker: PhD A&#10;Content: Yeah , the cepstral f The difference is like {disfmarker} There may be a slight difference in the way {disfmarker}&#10;Speaker: PhD D&#10;Content: right ? or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: because they use exactly the baseline system for converting the cepstrum once you have the speech . I mean , if we are using our own code for th I mean that {disfmarker} that could" />
    <node id=": cumulants , yeah .&#10;Speaker: PhD A&#10;Content: maybe higher - order cumulants&#10;Speaker: Professor C&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: and {disfmarker} Yeah . It was {disfmarker} it was {disfmarker}&#10;Speaker: Professor C&#10;Content: Or m e&#10;Speaker: PhD A&#10;Content: Yeah . I mean , he was showing up uh some {disfmarker} something on noisy speech ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: some improvement on the noisy speech .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Some small vocabulary tasks .&#10;Speaker: Professor C&#10;Content: Uh .&#10;Speaker: PhD A&#10;Content: So it was on PLP derived cepstral coefficients .&#10;Speaker: Professor C&#10;Content: Yeah , but again {disfmarker} You could argue that th that 's exactly what the neural network does .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: So" />
    <node id="aker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: What {disfmarker} one {disfmarker} one um p one thing is like what {disfmarker} before we started using this VAD in this Aurora , the {disfmarker} th what we did was like , I {disfmarker} I guess most of you know about this , adding this additional speech - silence bit to the cepstrum and training the HMM on that .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: That is just a binary feature and that seems to be {vocalsound} improving a lot on the SpeechDat - Car where there is a lot of noise but not much on the TI - digits . So , a adding an additional feature to distin to discriminate between speech and nonspeech was helping . That 's it .&#10;Speaker: PhD D&#10;Content: Wait {disfmarker} I {disfmarker} I 'm sorry ?&#10;Speaker: PhD A&#10;Content: Yeah , we actually added an additional binary feature to the cepstrum , just the baseline ." />
    <node id=" order to get rid of the effects of noise .&#10;Speaker: PhD A&#10;Content: So .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah . Uh , yeah . So there is this . And maybe we {disfmarker} well we find some people so that {vocalsound} uh , agree to maybe work with us , and they have implementation of VTS techniques so it 's um , Vector Taylor Series that are used to mmm , {vocalsound} uh f to model the transformation between clean cepstra and noisy cepstra . So . Well , if you take the standard model of channel plus noise , uh , it 's {disfmarker} it 's a nonlinear eh uh , transformation in the cepstral domain .&#10;Speaker: Professor C&#10;Content: Mm - hmm . Yes .&#10;Speaker: PhD D&#10;Content: And uh , there is a way to approximate this using uh , first - order or second - order Taylor Series and it can be used for {vocalsound} uh , getting rid of the noise and the channel effect .&#10;Speaker: Professor C&#10;Content: Who is doing this" />
    <node id="Based on the transcript provided, Professor C mentioned that they abandoned the use of lapel microphones because they were not too hot or too cold, but the distance from the mouth resulted in more background noise. They also caused issues with getting rid of noise and channel effect. Instead, they are using wireless headset microphones, which can be closer to the speaker's mouth and reduce background noise." />
    <node id="Speaker: Professor C&#10;Content: Uh , is it the twenty - fourth ?&#10;Speaker: PhD F&#10;Content: now we 're on .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh Chuck , is the mike type wireless {disfmarker}&#10;Speaker: PhD F&#10;Content: Yes .&#10;Speaker: PhD A&#10;Content: wireless headset ? OK .&#10;Speaker: PhD F&#10;Content: Yes .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: For you it is .&#10;Speaker: Professor C&#10;Content: Yeah . We uh {disfmarker} we abandoned the lapel because they sort of were not too {disfmarker} not too hot , not too cold , they were {disfmarker} you know , they were {vocalsound} uh , far enough away that you got more background noise , uh , and uh {disfmarker} and so forth&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Professor C&#10;Content: but they weren't so close that they got quite the {disfmarker} you" />
    <node id=" used for {vocalsound} uh , getting rid of the noise and the channel effect .&#10;Speaker: Professor C&#10;Content: Who is doing this ?&#10;Speaker: PhD D&#10;Content: Uh w working in the cepstral domain ? So there is one guy in Grenada ,&#10;Speaker: PhD B&#10;Content: Yeah , in Grenada one of my friend .&#10;Speaker: PhD D&#10;Content: and another in {pause} uh , Lucent that I met at ICASSP .&#10;Speaker: Professor C&#10;Content: Who 's the guy in Grenada ?&#10;Speaker: PhD D&#10;Content: uh ,&#10;Speaker: PhD B&#10;Content: Uh , Jose Carlos Segura .&#10;Speaker: Professor C&#10;Content: I don't know him .&#10;Speaker: PhD A&#10;Content: This VTS has been proposed by CMU ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Is it {disfmarker} is it the CMU ? Yeah , yeah , OK .&#10;Speaker: PhD B&#10;Content: Yeah , yeah , yeah . Originally the idea was from CMU .&#10;Spe" />
    <node id="aker: Professor C&#10;Content: There 's uh , some kinds of junk that you get with these things that you don't get with the lapel uh , little mouth clicks and breaths and so forth are worse with these than with the lapel , but given the choice we {disfmarker} there seemed to be very strong opinions for uh , getting rid of lapels .&#10;Speaker: PhD A&#10;Content: The mike number is {disfmarker}&#10;Speaker: Professor C&#10;Content: So ,&#10;Speaker: PhD F&#10;Content: Uh , your mike number 's written on the back of that unit there .&#10;Speaker: PhD A&#10;Content: Oh yeah . One .&#10;Speaker: PhD F&#10;Content: And then the channel number 's usually one less than that .&#10;Speaker: PhD A&#10;Content: Oh , OK . OK .&#10;Speaker: PhD F&#10;Content: It - it 's one less than what 's written on the back of your {disfmarker}&#10;Speaker: PhD A&#10;Content: OK . OK .&#10;Speaker: PhD F&#10;Content: yeah . So you should be zero , actually .&#10;Speaker: PhD A" />
    <node id="aker: Professor C&#10;Content: Cool . Um , so maybe uh , just briefly , you could remind us about the related experiments . Cuz you did some stuff that you talked about last week , I guess ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Um , where you were also combining something {disfmarker} both of you I guess were both combining something from the uh , French Telecom system with {vocalsound} the u uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: I {disfmarker} I don't know whether it was system one or system two , or {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Mm - hmm . It was system one . So&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: we {disfmarker} The main thing that we did is just to take the spectral subtraction from the France Telecom , which provide us some speech samples that are uh , with noise removed .&#10;Speaker: Professor C&#10;Content: So I let me {d" />
    <node id=" Professor C&#10;Content: Hmm .&#10;Speaker: PhD F&#10;Content: Um .&#10;Speaker: Professor C&#10;Content: OK . We can ask him sometime .&#10;Speaker: PhD F&#10;Content: But {disfmarker} Yeah . I don't know whether it monitors the keyboard or actually looks at the console TTY , so maybe if you echoed something to the you know , dev {disfmarker} dev console or something .&#10;Speaker: Professor C&#10;Content: You probably wouldn't ordinarily , though . Yeah . Right ? You probably wouldn't ordinarily .&#10;Speaker: PhD F&#10;Content: Hmm ?&#10;Speaker: Professor C&#10;Content: I mean you sort of {disfmarker} you 're at home and you 're trying to log in , and it takes forever to even log you in , and you probably go , &quot; screw this &quot; ,&#10;Speaker: PhD F&#10;Content: Yeah , yeah .&#10;Speaker: Professor C&#10;Content: and {disfmarker} {vocalsound} You know .&#10;Speaker: PhD F&#10;Content: Yeah . Yeah , so , um ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD" />
    <node id=" Professor C&#10;Content: I remember I {disfmarker} I forget whether it was when the Rutgers or {disfmarker} or Hopkins workshop , I remember one of the workshops I was at there were {disfmarker} everybody was real excited cuz they got twenty - five machines and there was some kind of P - make like thing that sit sent things out .&#10;Speaker: PhD F&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: So all twenty - five people were sending things to all twenty - five machines&#10;Speaker: PhD F&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Professor C&#10;Content: and {vocalsound} and things were a lot less efficient than if you 'd just use your own machine .&#10;Speaker: PhD F&#10;Content: Yeah . Yep . Yeah , exactly . Yeah , you have to be a little bit careful .&#10;Speaker: Professor C&#10;Content: as I recall , but . Yeah .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: PhD F&#10;Content: Um , but uh , you can also {disfmarker} If you have" />
    <node id="1. Wiener filter and cepstral features are both techniques used in speech processing to reduce noise and improve signal quality. A Wiener filter is a type of filter used to minimize the mean square error between the desired signal and the filtered signal, while cepstral features are a representation of the spectral envelope of a speech signal.&#10;2. In the given transcript, PhD D mentions that in the handset used for the experiment, a Wiener filter is applied first and then cepstral features are computed. This suggests that the Wiener filter is used as a pre-processing step to reduce noise before extracting cepstral features.&#10;3. PhD A mentions that the signal subspace approach, which has an in-built Wiener filtering, works well for noisy signals but not very well for poor SNR conditions and colored noise. This suggests that Wiener filtering can help improve the quality of noisy speech signals, but its effectiveness may depend on the type and level of noise present in the signal.&#10;4. The difference between using a Wiener filter and cepstral features in the baseline system for converting the cepstrum is not explicitly stated in the given transcript. However, it can be inferred that the baseline system uses cepstral features as input to the HMM (Hidden Markov Model) for speech recognition. The addition of a Wiener filter as a pre-processing step can help improve the quality of the cepstral features and thus potentially improve the performance of the speech recognition system.&#10;5. Overall, the use of Wiener filters and cepstral features in speech processing can help reduce noise and improve signal quality, which can lead to better performance in speech recognition systems. However, their effectiveness may depend on the type and level of noise present in the signal." />
    <node id="aker: PhD A&#10;Content: The signal subspace ? The {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: The signal subspace approach has actually an in - built Wiener filtering in it .&#10;Speaker: Professor C&#10;Content: Oh , OK .&#10;Speaker: PhD A&#10;Content: Yeah . It is like a KL transform followed by a Wiener filter . Is the signal is {disfmarker} is a signal substrate .&#10;Speaker: Professor C&#10;Content: Oh , oh , OK so the difference is the KL .&#10;Speaker: PhD A&#10;Content: So , the {disfmarker} the different {disfmarker} the c the {disfmarker} the advantage of combining two things is mainly coming from the signal subspace approach doesn't work very well if the SNR is very bad . It 's {disfmarker} it works very poorly with the poor SNR conditions , and in colored noise .&#10;Speaker: Professor C&#10;Content: I see . So essentially you could do simple spectral subtraction , followed by a KL transform , followed by a&#10;Speaker: PhD A&#10;Content" />
    <node id="Content: I see . So essentially you could do simple spectral subtraction , followed by a KL transform , followed by a&#10;Speaker: PhD A&#10;Content: Wiener filtering . It 's a {disfmarker} it 's a cascade of two s&#10;Speaker: Professor C&#10;Content: Wiener filter . Yeah , in general , you don't {disfmarker} that 's right you don't wanna othorg orthogonalize if the things are noisy . Actually . Um , that was something that uh , Herve and I were talking about with um , the multi - band stuff , that if you 're converting things to from uh , bands , groups of bands into cepstral coef you know , local sort of local cepstral coefficients that it 's not that great to do it if it 's noisy .&#10;Speaker: PhD A&#10;Content: Mm - hmm . OK . Yeah . So .&#10;Speaker: Professor C&#10;Content: Uh , so .&#10;Speaker: PhD A&#10;Content: So that {disfmarker} that 's one reason maybe we could combine s some {disfmarker} something to improve SNR a little bit" />
    <node id="1. To submit jobs more efficiently, it is suggested to use a script that calls the &quot;run command&quot; on each sub-job. However, you should be careful not to saturate the network by running too many jobs at once. A safe number would be around ten jobs running simultaneously.&#10;2. There isn't a specific command to distribute a job to the fastest available machine while duplicating the user's environment. However, using the &quot;run command&quot; will help you find the fastest available machine to run your job. To ensure your job runs on a machine with similar characteristics as your local machine, use attributes like &quot;Linux&quot; or specify an individual machine by its attribute (the name of itself) in the attribute lines when submitting the job.&#10;3. For specific tasks like running an HTK training job that takes a long time, you can utilize the &quot;run command&quot; to find another machine and run your job there. This way, you don't have to write a P-make script or wait for your typical machine to become available." />
    <node id=" which your job won't be evicted from .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: But , the machines that don't have that attribute , if a job gets fired up on that , which could be somebody 's desktop machine , and {disfmarker} and they were at lunch ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: they come back from lunch and they start typing on the console , then your machine will get evicted {disfmarker} your job {comment} will get evicted from their machine and be restarted on another machine . Automatically . So {disfmarker} which can cause you to lose time , right ? If you had a two hour job , and it got halfway through and then somebody came back to their machine and it got evicted . So . If you don't want your job to run on a machine where it could be evicted , then you give it the minus {disfmarker} the attribute , you know , &quot; no evict &quot; , and it 'll pick a machine that it can't be evicted from . So .&#10;" />
    <node id="Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad G&#10;Content: So , um , let 's say I have like , a thousand little {disfmarker} little jobs to do ?&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad G&#10;Content: Um , how do I do it with &quot; run command &quot; ? I mean do {disfmarker}&#10;Speaker: PhD F&#10;Content: You could write a script uh , which called run command on each sub - job&#10;Speaker: Grad G&#10;Content: Uh - huh . A thousand times ?&#10;Speaker: PhD F&#10;Content: right ? But you probably wanna be careful with that&#10;Speaker: Grad G&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: because um , you don't wanna saturate the network . Uh , so , um , you know , you should {disfmarker} you should probably not run more than , say ten jobs yourself at any one time , uh , just because then it would keep other people {disfmarker}&#10;Speaker: Grad G&#10;Content: Oh , too much file transfer and stuff ." />
    <node id=" example if you only want your thing to run under Linux , you can give it the Linux attribute , and then it will find the fastest available Linux machine and run it on that . So . You can control where your jobs go , to a certain extent , all the way down to an individual machine . Each machine has an attribute which is the name of itself . So you can give that as an attribute and it 'll only run on that . If there 's already a job running , on some machine that you 're trying to select , your job will get queued up , and then when that resource , that machine becomes available , your job will get exported there . So , there 's a lot of nice features to it and it kinda helps to balance the load of the machines and uh , right now Andreas and I have been the main ones using it and we 're {disfmarker} Uh . The SRI recognizer has all this P - make customs stuff built into it .&#10;Speaker: Professor C&#10;Content: So as I understand , you know , he 's using all the machines and you 're using all the machines ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: Professor C&#10;Content: is" />
    <node id=" at once . And then it 'll make sure that it never goes above that .&#10;Speaker: Grad G&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: So ,&#10;Speaker: Grad G&#10;Content: Right . OK .&#10;Speaker: PhD F&#10;Content: I can get some documentation .&#10;Speaker: PhD D&#10;Content: So it {disfmarker} it 's {disfmarker} it 's not systematically queued . I mean all the jobs are running . If you launch twenty jobs , they are all running . Alright .&#10;Speaker: PhD F&#10;Content: It depends . If you {disfmarker} &quot; Run command &quot; , that I mentioned before , is {disfmarker} doesn't know about other things that you might be running .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: So , it would be possible to run a hundred run jobs at once ,&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: and they wouldn't know about each other . But if you use P - make , then , it knows about all the jobs that it has" />
    <node id=" the machines and you 're using all the machines ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: Professor C&#10;Content: is the rough division of {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah . Exactly . Yeah , you know , I {disfmarker} I sort of got started {comment} using the recognizer just recently and uh , uh I fired off a training job , and then I fired off a recognition job and I get this email about midnight from Andreas saying , &quot; uh , are you running two {vocalsound} trainings simultaneously s my m my jobs are not getting run . &quot; So I had to back off a little bit . But , soon as we get some more machines then uh {disfmarker} then we 'll have more compute available . So , um , that 's just a quick update about what we 've got . So .&#10;Speaker: Grad G&#10;Content: Um , I have {disfmarker} I have a question about the uh , parallelization ?&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad G&#10;Content: So , um , let 's say I" />
    <node id=" , uh if you didn't wanna write a P - make script and you just had a , uh {disfmarker} an HTK training job that you know is gonna take uh , six hours to run , and somebody 's using , uh , the machine you typically use , you can say &quot; run command &quot; and your HTK thing and it 'll find another machine , the fastest currently available machine and {disfmarker} and run your job there .&#10;Speaker: Professor C&#10;Content: Now , does it have the same sort of behavior as P - make , which is that , you know , if you run something on somebody 's machine and they come in and hit a key then it {disfmarker}&#10;Speaker: PhD F&#10;Content: Yes . Yeah , there are um {disfmarker} Right . So some of the machines at the institute , um , have this attribute called &quot; no evict &quot; . And if you specify that , in {disfmarker} in one of your attribute lines , then it 'll go to a machine which your job won't be evicted from .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;" />
    <node id="Based on the transcript, PhD A mentions that Guenter Hirsch is in charge of the project that is similar to TI-digits. They are potentially generating HTK scripts, but it is not clear whether they are converging on HTK or using some other speech recognition toolkit from Mississippi State. PhD A mentions &quot;Mississippi State maybe&quot; in relation to this question." />
    <node id="Speaker: Professor C&#10;Content: Sort of a {disfmarker} sort of like what they did with TI - digits , and ?&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Professor C&#10;Content: Yeah , OK .&#10;Speaker: PhD A&#10;Content: I m I guess Guenter Hirsch is in charge of that . Guenter Hirsch and TI .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Maybe Roger {disfmarker} r Roger , maybe in charge of .&#10;Speaker: Professor C&#10;Content: And then they 're {disfmarker} they 're uh , uh , generating HTK scripts to {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah . Yeah , I don't know . There are {disfmarker} they have {disfmarker} there is no {disfmarker} I don't know if they are converging on HTK or are using some Mississippi State ,&#10;Speaker: Professor C&#10;Content: Mis - Mississippi State maybe ,&#10;Speaker: PhD A&#10;Content: yeah . I 'm not sure about that ." />
    <node id=" um , primary detectors for these acoustic events , and using the outputs of these robust detectors to do speech recognition . Um , and , um , these {disfmarker} these primary detectors , um , will be , uh , inspired by , you know , multi - band techniques , um , doing things , um , similar to Larry Saul 's work on , uh , graphical models to {disfmarker} to detect these {disfmarker} these , uh , acoustic events . And , um , so I {disfmarker} I been {disfmarker} I been thinking about that and some of the issues that I 've been running into are , um , exactly what {disfmarker} what kind of acoustic events I need , what {disfmarker} um , what acoustic events will provide a {disfmarker} a good enough coverage to {disfmarker} in order to do the later recognition steps . And , also , um , once I decide a set of acoustic events , um , h how do I {disfmarker} how do I get labels ? Training data for {disfmarker} for these acoustic events . And , then later on down the line , I can" />
    <node id=" small vocabulary tasks . But we {disfmarker} we going to address this Wall Street Journal in our next stage , which is also going to be a noisy task so s very few people have reported something on using some continuous speech at all . So , there are some {disfmarker} I mean , I was looking at some literature on speech enhancement applied to large vocabulary tasks and spectral subtraction doesn't seems to be the thing to do for large vocabulary tasks . And it 's {disfmarker} Always people have shown improvement with Wiener filtering and maybe subspace approach over spectral subtraction everywhere . But if we {disfmarker} if we have to use simple spectral subtraction , we may have to do some optimization {pause} to make it work @ @ .&#10;Speaker: Professor C&#10;Content: So they 're making {disfmarker} there {disfmarker} Somebody 's generating Wall Street Journal with additive {disfmarker} artificially added noise or something ?&#10;Speaker: PhD A&#10;Content: Yeah , yeah .&#10;Speaker: Professor C&#10;Content: Sort of a {disfmarker} sort of like what they did with TI - digits , and ?&#10;Spe" />
    <node id=" of these things . And that 's what most o again , most of our work has been done with that , with {disfmarker} with uh , connected digits .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Professor C&#10;Content: Um , but uh , we have recognition now with some of the continuous speech , large vocabulary continuous speech , using Switchboard {disfmarker} uh , Switchboard recognizer ,&#10;Speaker: PhD A&#10;Content: Yeah . OK .&#10;Speaker: Professor C&#10;Content: uh , no training , {vocalsound} from this , just {disfmarker} just plain using the Switchboard .&#10;Speaker: PhD A&#10;Content: Oh . You just take the Switchboard trained {disfmarker} ? Yeah ,&#10;Speaker: Professor C&#10;Content: That 's {disfmarker} that 's what we 're doing ,&#10;Speaker: PhD A&#10;Content: yeah .&#10;Speaker: Professor C&#10;Content: yeah . Now there are some adaptation though ,&#10;Speaker: PhD A&#10;Content: OK . Yeah . That 's cool .&#10;Speaker: Professor C&#10;Content: that {" />
    <node id="Based on the transcript, when PhD A added online normalization to the system with a different time constant, they found that it did not lead to an improvement over not using online normalization. They mentioned that they had to change the value of the update factor to see any improvements, but even then, it was not better than without online normalization. This suggests that the change in time constant and addition of online normalization did not result in performance improvements in this case." />
    <node id=" improves by just supplying the information of the {disfmarker} I mean the w speech and nonspeech . And uh , I found that the baseline itself improves by twenty - two percent by just giving the wuh .&#10;Speaker: Professor C&#10;Content: Uh , can you back up a second , I {disfmarker} I {disfmarker} I missed something , uh , I guess my mind wandered . Ad - ad When you added the on - line normalization and so forth , uh , uh things got better again ?&#10;Speaker: PhD A&#10;Content: Yeah . No .&#10;Speaker: Professor C&#10;Content: or is it ?&#10;Speaker: PhD A&#10;Content: No . No , things didn't get better with the same time constant that we used .&#10;Speaker: Professor C&#10;Content: Did it not ? No , no . With a different time constant .&#10;Speaker: PhD A&#10;Content: With the different time constant I found that {disfmarker} I mean , I didn't get an improvement over not using on - line normalization ,&#10;Speaker: Professor C&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: because I {disf" />
    <node id=" using on - line normalization ,&#10;Speaker: Professor C&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: because I {disfmarker} I found that I would have change the value of the update factor .&#10;Speaker: Professor C&#10;Content: No you didn't , OK .&#10;Speaker: PhD A&#10;Content: But I didn't play it with play {disfmarker} play quite a bit to make it better than .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So , it 's still not {disfmarker}&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: I mean , the on - line normalization didn't give me any improvement .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: And uh , so ,&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: oh yeah So I just stopped there with the uh , speech enhancement . The {disfmarker} the other thing what I tried was the {disfmarker} adding the uh , endpoint information to the baseline" />
    <node id=" , on - line normalization also on top of that . And that {disfmarker} there {disfmarker} there also I n I found that I have to make some changes to their time constant that I used because th it has a {disfmarker} a mean and variance update time constant and {disfmarker} which is not suitable for the enhanced speech , and whatever we try it on with proposal - one . But um , I didn't {disfmarker} I didn't play with that time constant a lot , I just t g I just found that I have to reduce the value {disfmarker} I mean , I have to increase the time constant , or reduce the value of the update value . That 's all I found So I have to . Uh , Yeah . And uh , uh , the other {disfmarker} other thing what I tried was , I just um , uh , took the baseline and then ran it with the endpoint inf uh th information , just the Aurora baseline , to see that how much the baseline itself improves by just supplying the information of the {disfmarker} I mean the w speech and nonspeech . And uh , I found that the baseline" />
    <node id=" constants of the on - line normalization .&#10;Speaker: PhD A&#10;Content: Yeah , yeah . Yeah .&#10;Speaker: PhD D&#10;Content: Because if we keep the value that was submitted uh , it doesn't help at all . You can remove on - line normalization , or put it , it doesn't change anything . Uh , uh , as long as you have the spectral subtraction . But , you can still find some kind of optimum somewhere , and we don't know where exactly&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: but , uh .&#10;Speaker: PhD A&#10;Content: Yeah , I assume .&#10;Speaker: Professor C&#10;Content: So it sounds like you should look at some tables of results or something&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and see where i where the {disfmarker} {vocalsound} where they were different and what we can learn from it .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm ." />
    <node id="marker} well , with our modification to reduce the delay of the {disfmarker} the LDA filters ,&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: and&#10;Speaker: PhD B&#10;Content: And the filter {disfmarker}&#10;Speaker: PhD D&#10;Content: Well there are slight modifications , but it was the full proposal - one . In your case , if you tried just putting LDA , then maybe on - line normalization {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Only LDA . Yeah . Af - I {disfmarker} after that I added on - line normalization , yeah .&#10;Speaker: PhD D&#10;Content: Mm - hmm . So we just tried directly to {disfmarker} to just , keep the system as it was and , um , when we plug the spectral subtraction it improves uh , signif significantly . Um , but , what seems clear also is that we have to retune the time constants of the on - line normalization .&#10;Speaker: PhD A&#10;Content: Yeah , yeah . Yeah .&#10;Speaker: PhD D&#10;Content" />
    <node id="1. Distance from the mouth: The lapel microphones were placed too far from the speaker's mouth, which resulted in more background noise and made it difficult to isolate the speaker's voice.&#10;2. Channel effect and noise issues: The lapel microphones also caused problems with getting rid of noise and channel effect, making them less suitable for clear audio recordings compared to wireless headset microphones." />
    <node id=" France Telecom , which provide us some speech samples that are uh , with noise removed .&#10;Speaker: Professor C&#10;Content: So I let me {disfmarker} let me just stop you there . So then , one distinction is that uh , you were taking the actual France Telecom features and then applying something to {disfmarker}&#10;Speaker: PhD A&#10;Content: Uh , no there is a slight different . Uh I mean , which are extracted at the handset because they had another back - end blind equalization {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah . But that 's what I mean .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: But u u Sorry ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: yeah , I 'm not being {disfmarker} I 'm not being clear .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: What I meant was you had something like cepstra or something , right" />
    <node id="When deciding between using FFT bins or mel energies for processing a proposal, the following factors should be considered:&#10;&#10;1. Purpose of processing: If the goal is to reconstruct speech, then using FFT bins might be a better option as it can provide more accurate information about the original signal. On the other hand, if the purpose is not to reconstruct speech but rather for speech recognition, there may not be much difference between using FFT bins or mel energies, since a linear weighting will be applied after processing in either case.&#10;2. Computational cost: Mel energies might be less computationally expensive than FFT bins, as they involve applying a mel filter bank to the power spectrum of the signal, which can be faster and more efficient than calculating FFT bins directly.&#10;3. Feature extraction: Using FFT bins or mel energies can lead to different features being extracted from the signal, which might have different implications for downstream tasks such as speech recognition or noise reduction. Careful consideration should be given to the specific requirements of the task at hand and the characteristics of the input signal when choosing between these two options.&#10;&#10;Therefore, whether reconstructing speech is a relevant consideration in this choice depends on the specific purpose of processing. If the goal is to preserve as much information about the original speech signal as possible, then using FFT bins might be more appropriate. However, if the focus is on recognizing or analyzing speech signals rather than reconstructing them, then mel energies could be a viable and potentially more efficient alternative." />
    <node id=" ?&#10;Speaker: PhD D&#10;Content: Um ,&#10;Speaker: PhD F&#10;Content: Does it matter , or {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I d I don't know . Well , it 's both {disfmarker} both uh , cases can i&#10;Speaker: PhD F&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: Yeah . So - some of the proposal , uh , we 're doing this on the bin {disfmarker} on the FFT bins ,&#10;Speaker: PhD F&#10;Content: Hmm .&#10;Speaker: PhD D&#10;Content: others on the um , mel energies . You can do both , but I cannot tell you what 's {disfmarker} which one might be better or {disfmarker} I {disfmarker}&#10;Speaker: PhD F&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: I guess if you want to reconstruct the speech , it may be a good idea to do it on FFT bins .&#10;Speaker: PhD D&#10;Content: I don't know . Yeah , but&#10;Speaker: PhD F&#10;Content: Mmm" />
    <node id=" FFT bins .&#10;Speaker: PhD D&#10;Content: I don't know . Yeah , but&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: But for speech recognition , it may not . I mean it may not be very different if you do it on mel warped or whether you do it on FFT . So you 're going to do a linear weighting anyway after that .&#10;Speaker: PhD F&#10;Content: I see .&#10;Speaker: PhD A&#10;Content: Well {disfmarker} Yeah ?&#10;Speaker: PhD F&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: So , it may not be really a big different .&#10;Speaker: PhD D&#10;Content: Well , it gives something different , but I don't know what are the , pros and cons of both .&#10;Speaker: PhD A&#10;Content: It I Uh - huh .&#10;Speaker: Professor C&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: So&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: The other thing is like when you 're putting in a speech enhancement technique , uh ," />
    <node id=" , but , you 'll do throw something away . And so the question is , uh , can we figure out if there 's something we 've thrown away that we shouldn't have . And um . So , when they were looking at the difference between the filter bank and the FFT that was going into the filter bank , I was thinking &quot; oh , OK , so they 're picking on something they 're looking on it to figure out noise , or voice {disfmarker} voiced property whatever . &quot; So that {disfmarker} that 's interesting . Maybe that helps to drive the {disfmarker} the thought process of coming up with the features . But for me sort of the interesting thing was , &quot; well , but is there just something in that difference which is useful ? &quot; So another way of doing it , maybe , would be just to take the FFT uh , power spectrum , and feed it into a neural network ,&#10;Speaker: PhD B&#10;Content: To know {disfmarker}&#10;Speaker: Professor C&#10;Content: and then use it , you know , in combination , or alone , or {disfmarker} or whatever&#10;Speaker: PhD F&#10;Content: Wi" />
    <node id="mm . Mmm .&#10;Speaker: Professor C&#10;Content: But I {disfmarker} I {disfmarker} I have to tell you , I can't remember the conference , but , uh , I think it 's about ten years ago , I remember going to one of the speech conferences and {disfmarker} and uh , I saw within very short distance of one another a couple different posters that showed about the wonders of some auditory inspired front - end or something , and a couple posters away it was somebody who compared one to uh , just putting in the FFT and the FFT did slightly better . So I mean the {disfmarker} i i It 's true there 's lots of variability ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: but again we have these wonderful statistical mechanisms for quantifying that a that variability , and you know , doing something reasonable with it .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So , um , uh , It - it 's same , you know , argument that 's gone both ways about uh , you know" />
    <node id="aker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Um ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Do y wanna say anything about {disfmarker} You {disfmarker} you actually been {disfmarker} Uh , last week you were doing this stuff with Pierre , you were {disfmarker} you were mentioning . Is that {disfmarker} that something worth talking about , or {disfmarker} ?&#10;Speaker: Grad E&#10;Content: Um , it 's {disfmarker} Well , um , it {disfmarker} I don't think it directly relates . Um , well , so , I was helping a speech researcher named Pierre Divenyi and he 's int He wanted to um , look at um , how people respond to formant changes , I think . Um . So he {disfmarker} he created a lot of synthetic audio files of vowel - to - vowel transitions , and then he wanted a psycho - acoustic um , spectrum . And he wanted to look at um , how the energy is moving {pause} over time in that spectrum and compare" />
    <node id="1. The slight decrease in performance for the well-matched (MM) category and significant improvement for HM is due to the addition of a new feature or technique, but it's not statistically significant for MM and only slightly worse for well-matched. However, the improvement for HM is significant.&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM.&#10;3. The discussion about the well-matched condition being unusual implies that this specific test set (well-matched) might respond differently to changes in features or techniques compared to other test sets, such as HM. This could explain why some methods show significant improvement for HM but not for MM.&#10;4. There is a general acknowledgement in the conversation that there's a lot of variability in performance when comparing different features and techniques, and that statistical mechanisms should be used to quantify this variability and make reasonable decisions based on it. This highlights the importance of using appropriate statistical tests to determine the significance of changes in performance when evaluating new features or techniques." />
    <node id=" slightly worse for well - matched .&#10;Speaker: PhD B&#10;Content: But&#10;Speaker: PhD D&#10;Content: Um , but this is not significant . But , the problem is that it 's not significant , but if you put this in the , mmm , uh , spreadsheet , it 's still worse . Even with very minor {disfmarker} uh , even if it 's only slightly worse for well - matched .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: And significantly better for HM . Uh , but , well . I don't think it 's importa important because when they will change their metric , uh , uh , mainly because of uh , when you p you plug the um , frame dropping in the baseline system , it will improve a lot HM , and MM ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: so , um , I guess what will happen {disfmarker} I don't know what will happen . But , the different contribution , I think , for the different test set will be more even .&#10;Speaker: PhD A&#10;Content: Because the {disf" />
    <node id=" , the different contribution , I think , for the different test set will be more even .&#10;Speaker: PhD A&#10;Content: Because the {disfmarker} your improvement on HM and MM will also go down significantly in the spreadsheet so . But the {pause} the well - matched may still {disfmarker}&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I mean the well - matched may be the one which is least affected by adding the endpoint information .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: Yeah . So the {disfmarker} the MM {disfmarker}&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: MM and HM are going to be v hugely affected by it . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah , so um , yeah .&#10;Speaker: PhD A&#10;Content: Yeah . But they d the {disfmarker} everything I mean is like , but there that 's how they reduce {disfmarker} why they reduce the qualification to twenty - five percent" />
    <node id="er} everything I mean is like , but there that 's how they reduce {disfmarker} why they reduce the qualification to twenty - five percent or some {disfmarker} something on .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: But are they changing the weighting ?&#10;Speaker: PhD A&#10;Content: Uh , no , I guess they are going ahead with the same weighting .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Yeah . So there 's nothing on {disfmarker}&#10;Speaker: Professor C&#10;Content: I don't understand that .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: I guess I {disfmarker} I haven't been part of the discussion , so , um , it seems to me that the well - matched condition is gonna be unusual ,&#10;Speaker: PhD A&#10;Content: Usual .&#10;Speaker: Professor C&#10;Content: in this case . Unusual .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Professor C&#10;Content:" />
    <node id="Based on the transcript, PhD A is not staying in the area permanently because they are currently residing at OGI and will be returning to that location after leaving the meeting. The exact date of their return is not specified in the transcript." />
    <node id=": Uh , no&#10;Speaker: Professor C&#10;Content: Is that out of the way ?&#10;Speaker: PhD A&#10;Content: not yet . Uh , yesterday I called up a lady who ha who will have a vacant room from May thirtieth and she said she 's interviewing two more people . So . And she would get back to me on Monday . So that 's {disfmarker} that 's only thing I have and Diane has a few more houses . She 's going to take some pictures and send me after I go back . So it 's {disfmarker} that 's {disfmarker}&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Oh . So you 're not down here permanently yet ?&#10;Speaker: PhD A&#10;Content: No . I 'm going back to OGI today .&#10;Speaker: PhD F&#10;Content: Ah ! Oh , OK .&#10;Speaker: Grad G&#10;Content: Oh .&#10;Speaker: Professor C&#10;Content: OK . And then , you 're coming back uh {disfmarker}&#10;Speaker: PhD A&#10;Content: Uh , i I mean ," />
    <node id=": R - nineteen ?&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: R - nineteen .&#10;Speaker: PhD F&#10;Content: Yeah . We 're {disfmarker} This is session R - nineteen .&#10;Speaker: Professor C&#10;Content: If you say so . O K . Do we have anything like an agenda ? What 's going on ? Um . I guess um . So . One thing {disfmarker}&#10;Speaker: PhD F&#10;Content: Sunil 's here for the summer ?&#10;Speaker: Professor C&#10;Content: Sunil 's here for the summer , right . Um , so , one thing is to talk about a kick off meeting maybe uh , and then just uh , I guess uh , progress reports individually , and then uh , plans for where we go between now and then , pretty much . Um .&#10;Speaker: PhD F&#10;Content: I could say a few words about um , some of the uh , compute stuff that 's happening around here , so that people in the group know .&#10;Speaker: Professor C&#10;Content: Mm - hmm . OK . Why don't you start with that ? That" />
    <node id=" uh , looking a little bit into uh , TRAPS um , and doing {disfmarker} doing TRAPS on {disfmarker} on these e events too , just , um , seeing {disfmarker} seeing if that 's possible . Uh , and um , other than that , uh , I was kicked out of I - house for living there for four years .&#10;Speaker: Professor C&#10;Content: Oh no . So you live in a cardboard box in the street now&#10;Speaker: Grad G&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: or , no ?&#10;Speaker: Grad G&#10;Content: Uh , well , s s som something like that .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad G&#10;Content: In Albany , yeah . Yeah . And uh . Yep . That 's it .&#10;Speaker: Professor C&#10;Content: Suni - i d ' you v did uh {disfmarker} did you find a place ?&#10;Speaker: PhD A&#10;Content: Uh , no&#10;Speaker: Professor C&#10;Content: Is that out of the way ?&#10;Speaker: PhD A&#10;Content: not yet ." />
    <node id=" Yeah . So , in that case , I 'm going to be here on thirty - first definitely .&#10;Speaker: Professor C&#10;Content: until you {disfmarker} OK .&#10;Speaker: Grad E&#10;Content: You know , if you 're in a desperate situation and you need a place to stay , you could stay with me for a while . I 've got a spare bedroom right now .&#10;Speaker: PhD A&#10;Content: Oh . OK . Thanks . That sure is nice of you . So , it may be he needs more than me .&#10;Speaker: Grad G&#10;Content: Oh r oh . Oh no , no . My {disfmarker} my cardboard box is actually a nice spacious two bedroom apartment .&#10;Speaker: Professor C&#10;Content: So a two bedroom cardboard box . Th - that 's great .&#10;Speaker: PhD A&#10;Content: Yeah . Yeah . {vocalsound} Yeah .&#10;Speaker: Professor C&#10;Content: Thanks Dave .&#10;Speaker: Grad G&#10;Content: yeah&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Um ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;" />
    <node id="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." />
    <node id=": PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: you know ?&#10;Speaker: PhD A&#10;Content: So I 've been thinking about combining the Wiener filtering with signal subspace ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I mean just to see all {disfmarker} some {disfmarker} some such permutation combination to see whether it really helps or not .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm . Mm - hmm . Yeah . Yeah .&#10;Speaker: Professor C&#10;Content: How is it {disfmarker} I {disfmarker} I guess I 'm ignorant about this , how does {disfmarker} I mean , since Wiener filter also assumes that you 're {disfmarker} that you 're adding together the two signals , how is {disfmarker} how is that differ from signal subspace ?&#10;Speaker: PhD A&#10;Content: The signal subspace ? The {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Spe" />
    <node id=" that {disfmarker} that 's one reason maybe we could combine s some {disfmarker} something to improve SNR a little bit , first stage ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and then do a something in the second stage which could take it further .&#10;Speaker: PhD D&#10;Content: What was your point about {disfmarker} about colored noise there ?&#10;Speaker: PhD A&#10;Content: Oh , the colored noise uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: the colored noise {disfmarker} the {disfmarker} the v the signal subspace approach has {disfmarker} I mean , it {disfmarker} it actually depends on inverting the matrices . So it {disfmarker} it {disfmarker} ac the covariance matrix of the noise . So if {disfmarker} if it is not positive definite ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I mean it has a {" />
    <node id=" definite ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I mean it has a {disfmarker} it 's {disfmarker} It doesn't behave very well if it is not positive definite ak It works very well with white noise because we know for sure that it has a positive definite .&#10;Speaker: Professor C&#10;Content: So you should do spectral subtraction and then add noise .&#10;Speaker: PhD A&#10;Content: So the way they get around is like they do an inverse filtering , first of the colo colored noise&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and then make the noise white ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and then finally when you reconstruct the speech back , you do this filtering again .&#10;Speaker: PhD D&#10;Content: Yeah , right .&#10;Speaker: Professor C&#10;Content: I was only half kidding . I mean if you {disfmarker} sort of {vocalsound} you do the s spectral subtraction , that also gets rid {disfmarker}&#10;" />
    <node id="1. Useful information in the difference between a filter bank and FFT: There might be useful information in the difference between using a filter bank and FFT for feature extraction. The discussion reveals that researchers have been looking at this difference to analyze noise or voiced property, which could potentially help drive the thought process of coming up with features.&#10;&#10;2. Feeding FFT power spectrum into a neural network: Based on PhD B's statement, feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features. However, in their research comparing new features using a neural network versus traditional cepstrum or other features in Italian and Spanish datasets, they found no statistically significant difference in performance.&#10;&#10;In conclusion, there could be useful information in the difference between using a filter bank and FFT for feature extraction, and feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features, although it may not yield significantly different results compared to traditional methods." />
    <node id=" 's just a baseline uh , which would show us &quot; well , what are we really getting out of the filters &quot; , or maybe i i probably not by itself , but in combination , uh ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: you know , maybe there 's something to be gained from it , and let the {disfmarker} But , you know , y you 've only worked with us for a short time , maybe in a year or two you w you will actually come up with the right set of things to extract from this information . But , maybe the neural net and the H M Ms could figure it out quicker than you .&#10;Speaker: PhD B&#10;Content: Maybe .&#10;Speaker: Professor C&#10;Content: So .&#10;Speaker: PhD B&#10;Content: Yeah ,&#10;Speaker: Professor C&#10;Content: It 's just a thought .&#10;Speaker: PhD B&#10;Content: I can {disfmarker} I will try to do that .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: What {disfmarker} one {disfmarker}" />
    <node id="Based on the discussion, an eighth-order PC (autocorrelation) polynomial with real coefficients will have complex roots that come in conjugate pairs. Each pair corresponds to a positive and negative frequency. In the context of synthetic speech, this would result in three or four peak positions, which can be useful for signal processing techniques such as spectral subtraction, KL transform, or inverse filtering. Additionally, higher-order cumulants or neural networks could further improve the performance on noisy speech." />
    <node id="PC polynomial 's gonna have real coefficients . So I think that means that every root that is not a real root {comment} is gonna be a c complex pair ,&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: um , of a complex value and its conjugate . Um . So for each {disfmarker} And if you look at that on the unit circle , um , one of these {disfmarker} one of the members of the pair will be a positive frequency , one will be a negative frequency , I think . So I just {disfmarker} So , um , f for the {disfmarker} I 'm using an eighth - order polynomial and I 'll get three or four of these pairs&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: which give me s which gives me three or four peak positions .&#10;Speaker: Professor C&#10;Content: This is from synthetic speech , or {disfmarker} ?&#10;Speaker: Grad E&#10;Content: It 's {disfmarker" />
    <node id=" PhD D&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: uh {disfmarker} Formant tracking with it can be a little tricky cuz you get these funny {vocalsound} values in {disfmarker} in real speech ,&#10;Speaker: PhD F&#10;Content: So you just {disfmarker} You typically just get a few roots ?&#10;Speaker: Professor C&#10;Content: but .&#10;Speaker: PhD F&#10;Content: You know , two or three ,&#10;Speaker: Professor C&#10;Content: Well you get these complex pairs .&#10;Speaker: PhD F&#10;Content: something like that ?&#10;Speaker: Professor C&#10;Content: And it depends on the order that you 're doing , but .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: Right . So , um , if {disfmarker} @ @ {comment} Every root that 's {disfmarker} Since it 's a real signal , the LPC polynomial 's gonna have real coefficients . So I think that means that every root that is not a real root {comment} is gonna be a" />
    <node id="1. When using the new feature with a neural network compared to just the cepstrum or other features in Italian and Spanish datasets, there is no significant difference in performance. The new feature sometimes results in slightly better performance, and sometimes it's slightly worse, but not statistically significant (PhD B).&#10;2. Without using the neural network, adding an additional binary feature to the cepstrum worsens the results. This additional feature helps discriminate between speech and non-speech frames, which improves performance when using a neural network (PhD A).&#10;3. There is potential useful information in comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network. However, these methods may not yield significantly different results compared to traditional methods (implied from the discussion)." />
    <node id="1. The purpose of implementing a baseline using a combination of data-driven and non-data-driven methods is to serve as a reference point for evaluating the performance of filters in Linear Discriminant Analysis (LDA). By comparing the results obtained from the filters with those of the baseline, researchers can better understand the contribution of the filters and assess their effectiveness.&#10;2. The potential outcome of implementing this baseline could include valuable insights into the performance of the LDA filters and their impact on the overall system. Researchers might discover whether the data-driven filters contribute significantly to the system's performance or if they introduce too much variability, making it difficult to achieve consistent results in training and testing stages.&#10;3. This process may also lead to the development of better methods for combining data-driven and non-data-driven techniques, potentially improving the robustness and generalizability of LDA models.&#10;4. Furthermore, evaluating filters using a baseline could provide a deeper understanding of how different test sets (such as well-matched, mismatched, or highly mismatched) respond to various features and techniques. This knowledge can be used to tailor specific methods for particular test sets, further enhancing the system's performance.&#10;5. Implementing a baseline in this context emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques." />
    <node id=" C&#10;Content: So , um , uh , It - it 's same , you know , argument that 's gone both ways about uh , you know , we have these data driven filters , in LDA , and on the other hand , if it 's data driven it means it 's driven by things that have lots of variability , and that are necessarily {disfmarker} not necessarily gonna be the same in training and test , so , in some ways it 's good to have data driven things , and in some ways it 's bad to have data driven things . So ,&#10;Speaker: PhD A&#10;Content: Yeah , d&#10;Speaker: Professor C&#10;Content: part of what we 're discovering , is ways to combine things that are data driven than are not .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , so anyway , it 's just a thought , that {disfmarker} that if we {disfmarker} if we had that {disfmarker} maybe it 's just a baseline uh , which would show us &quot; well , what are we really getting out of the filters &quot; , or maybe i i probably not by" />
    <node id=" in {disfmarker} I just take the cepstral coefficients coming from their system and then plug in LDA on top of that . But the LDA filter that I used was different from what we submitted in the proposal .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: What I did was {vocalsound} I took the LDA filter 's design using clean speech , uh , mainly because the speech is already cleaned up after the enhancement so , instead of using this , uh , narrow {disfmarker} narrow band LDA filter that we submitted uh , I got new filters . So that seems to be giving {disfmarker} uh , improving over their uh , system . Slightly . But , not very significantly . And uh , that was uh , showing any improvement over {disfmarker} final {disfmarker} by plugging in an LDA . And uh , so then after {disfmarker} after that I {disfmarker} I added uh , on - line normalization also on top of that . And that {disfmarker} there {disfmarker} there also I n I" />
    <node id=" from the post uh Aurora submission maybe .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh , yeah , after the submission the {disfmarker} what I 've been working on mainly was to take {disfmarker} take other s submissions and then over their system , what they submitted , because we didn't have any speech enhancement system in {disfmarker} in ours . So {disfmarker} So I tried uh , And u First I tried just LDA . And then I found that uh , I mean , if {disfmarker} if I combine it with LDA , it gives @ @ improvement over theirs . Uh {disfmarker}&#10;Speaker: PhD F&#10;Content: Are y are you saying LDA ?&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: PhD F&#10;Content: LDA . OK .&#10;Speaker: PhD A&#10;Content: So , just {disfmarker} just the LDA filters . I just plug in {disfmarker} I just take the cepstral coefficients coming from their system and then plug in LDA on top of that . But" />
    <node id=" they were different and what we can learn from it .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD A&#10;Content: without any change . OK .&#10;Speaker: PhD B&#10;Content: But it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Well ,&#10;Speaker: PhD B&#10;Content: It 's the new .&#10;Speaker: PhD D&#10;Content: with {disfmarker} with {disfmarker} with changes ,&#10;Speaker: PhD A&#10;Content: with&#10;Speaker: PhD B&#10;Content: The new .&#10;Speaker: PhD D&#10;Content: because we change it the system to have {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh yeah , I mean the {disfmarker} the new LDA filters .&#10;Speaker: PhD B&#10;Content: The new .&#10;Speaker: PhD A&#10;Content: I mean {disfmarker} OK .&#10;Speaker: PhD D&#10;Content: Yeah . LDA filters . There are other things that we finally were shown to improve also like , the sixty - four" />
    <node id="1. Write a script that calls the &quot;run command&quot; on each sub-job, but be careful not to run too many jobs simultaneously.&#10;2. A safe number of jobs to run at once is around ten, as running more could lead to network saturation and negatively affect other users' experiences.&#10;3. Implement logic in your script to ensure that only ten jobs are running at any given time, and fire off another job when one finishes. This approach allows you to manage many small tasks while being mindful of network resources and following best practices for job management." />
    <node id=" just because then it would keep other people {disfmarker}&#10;Speaker: Grad G&#10;Content: Oh , too much file transfer and stuff .&#10;Speaker: PhD F&#10;Content: Well it 's not that so much as that , you know , e with {disfmarker} if everybody ran fifty jobs at once then it would just bring everything to a halt and , you know , people 's jobs would get delayed , so it 's sort of a sharing thing . Um ,&#10;Speaker: Grad G&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: so you should try to limit it to somet sometim some number around ten jobs at a time . Um . So if you had a script for example that had a thousand things it needed to run , um , you 'd somehow need to put some logic in there if you were gonna use &quot; run command &quot; , uh , to only have ten of those going at a time . And uh , then , when one of those finished you 'd fire off another one . Um ,&#10;Speaker: Professor C&#10;Content: I remember I {disfmarker} I forget whether it was when the Rutgers or {disfmarker} or Hopkins workshop" />
    <node id=" F&#10;Content: and they wouldn't know about each other . But if you use P - make , then , it knows about all the jobs that it has to run&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: and it can control , uh , how many it runs simultaneously .&#10;Speaker: Professor C&#10;Content: So &quot; run command &quot; doesn't use P - make , or {disfmarker} ?&#10;Speaker: PhD F&#10;Content: It uses &quot; export &quot; underlyingly . But , if you {disfmarker} i It 's meant to be run one job at a time ? So you could fire off a thousand of those , and it doesn't know {disfmarker} any one of those doesn't know about the other ones that are running .&#10;Speaker: Professor C&#10;Content: So why would one use that rather than P - make ?&#10;Speaker: PhD F&#10;Content: Well , if you have , um {disfmarker} Like , for example , uh if you didn't wanna write a P - make script and you just had a , uh {disfmarker} an HTK training job that" />
    <node id="1. According to the transcript, Professor C mentioned the possibility of using the SRI recognizer for testing on large vocabulary data with a distant microphone. However, it is not clear if this has been attempted already.&#10;2. The potential challenges in using the SRI recognizer for testing on large vocabulary data with a distant microphone include the following:&#10;* Fear of trying it out, as mentioned by Professor C (&quot;Yeah, cuz everybody's scared.&quot;)&#10;* Possible overloading of the CPU or hardware due to the demands of processing large vocabulary data from a distant microphone, as also mentioned by Professor C (&quot;You'll see a little smoke coming up from the CPU or something trying to do it.&quot;).&#10;* The need for robust detectors of acoustic events, which can provide good enough coverage and training data for later recognition steps. This is based on Grad G's description of their proposed PhD work on using acoustic events for speech recognition (&quot;Building robust primary detectors for these acoustic events and using the outputs of these robust detectors to do speech recognition.&quot;).&#10;* The challenges in determining what kind of acoustic events are needed, as well as obtaining labels or training data for these acoustic events, as mentioned by Grad G." />
    <node id=" adaptation though ,&#10;Speaker: PhD A&#10;Content: OK . Yeah . That 's cool .&#10;Speaker: Professor C&#10;Content: that {disfmarker} that uh , Andreas has been playing with ,&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: but we 're hop uh , actually uh , Dave and I were just talking earlier today about maybe at some point not that distant future , trying some of the techniques that we 've talked about on , uh , some of the large vocabulary data . Um , I mean , I guess no one had done {disfmarker} yet done test one on the distant mike using uh , the SRI recognizer and , uh ,&#10;Speaker: PhD F&#10;Content: I don't {disfmarker} not that I know of .&#10;Speaker: Professor C&#10;Content: Yeah , cuz everybody 's scared .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: You 'll see a little smoke coming up from the {disfmarker} the CPU or something {vocalsound} trying to {disfmarker} trying to do it ,&#10;" />
    <node id="isfmarker} hasn't heard about uh , what I 've been doing .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad G&#10;Content: Um , so , um , I told you I was {disfmarker} I was {disfmarker} I was getting prepared to take this qualifier exam . So basically that 's just , um , trying to propose um , uh , your next your {disfmarker} your following years of {disfmarker} of your PHD work , trying {disfmarker} trying to find a project to {disfmarker} to define and {disfmarker} and to work on . So , I 've been , uh , looking into , um , doing something about r uh , speech recognition using acoustic events . So , um , the idea is you have all these {disfmarker} these different events , for example voicing , nasality , R - coloring , you know burst or noise , uh , frication , that kinda stuff , um , building robust um , primary detectors for these acoustic events , and using the outputs of these robust detectors to do speech recognition . Um , and , um , these {disf" />
    <node id="1. The &quot;run command&quot; in the P-made and Customs network is a command that allows users to execute jobs on the fastest available machine in the network. It helps distribute tasks efficiently across the network by finding the best machine based on specified attributes like memory, speed, and operating system.&#10;&#10;2. Users can use the &quot;run command&quot; to run their jobs on a specific type of machine, such as one running Linux, or even on an individual machine by specifying its attribute (the name of itself). This ensures that the job is run on a machine with similar characteristics as the user's local machine.&#10;&#10;3. The &quot;run command&quot; does not use P-make and can be used to run jobs one at a time, without knowing about other jobs running simultaneously. It's particularly useful when users don't want to write a P-make script or for specific tasks like running an HTK training job that takes a long time. This way, users can utilize the network's resources more efficiently and manage many small tasks while being mindful of network resources and following best practices for job management.&#10;&#10;4. As mentioned in the transcript, using the &quot;run command&quot; can be an alternative to P-make, especially for users who don't want to write a P-make script or when dealing with individual long-running tasks like HTK training jobs." />
    <node id="} and run it there and it 'll duplicate your environment . So you can try this as a simple test with uh , the L S command . So you can say &quot; run dash command L S &quot; , and , um , it 'll actually export that {vocalsound} LS command to some machine in the institute , and um , do an LS on your current directory . So , substitute LS for whatever command you want to run , and um {disfmarker} And that 's a simple way to get started using {disfmarker} using this . And , so , soon , when we get all the new machines up , {vocalsound} um , e then we 'll have lots more compute to use . Now th one of the nice things is that uh , each machine that 's part of the P - make and Customs network has attributes associated with it . Uh , attributes like how much memory the machine has , what its speed is , what its operating system , and when you use something like &quot; run command &quot; , you can specify those attributes for your program . For example if you only want your thing to run under Linux , you can give it the Linux attribute , and then it will find the fastest available Linux machine and run" />
    <node id=" - make and Customs . You don't actually have to write P - make scripts and things like that . The simplest thing {disfmarker} And I can send an email around or , maybe I should do an FAQ on the web site about it or something . Um ,&#10;Speaker: Professor C&#10;Content: How about an email that points to the FAQ ,&#10;Speaker: PhD F&#10;Content: there 's a c&#10;Speaker: Professor C&#10;Content: you know what I 'm saying ?&#10;Speaker: PhD F&#10;Content: Yeah , yeah .&#10;Speaker: Professor C&#10;Content: so that you can {disfmarker} Yeah .&#10;Speaker: PhD F&#10;Content: Uh , there 's a command , uh , that you can use called &quot; run command &quot; . &quot; Run dash command &quot; , &quot; run hyphen command &quot; . And , if you say that and then some job that you want to execute , uh , it will find the fastest currently available machine , and export your job to that machine , and uh {disfmarker} and run it there and it 'll duplicate your environment . So you can try this as a simple test with uh , the L S command . So you" />
    <node id="1. Kick-off meeting: The group plans to discuss having a kick-off meeting. No details about the meeting are provided in the transcript.&#10;2. Progress reports: It is mentioned that there will be individual progress reports during the session. However, no specific reports are discussed in the transcript.&#10;3. Plans for the summer: Professor C mentions that Sunil is present for the summer and they plan to talk about a kick-off meeting and individual plans between then and the end of the summer.&#10;4. Update on compute stuff happening around the group: PhD F shares an update about new machines ordered, a compute farm setup, and using P-make and Customs for running jobs on the machines. They also mention that they need to be careful with parallelization to ensure efficiency." />
    <node id=" people in the group know .&#10;Speaker: Professor C&#10;Content: Mm - hmm . OK . Why don't you start with that ? That 's sort of {disfmarker}&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Yeah ?&#10;Speaker: PhD F&#10;Content: We um {disfmarker} So we just put in an order for about twelve new machines , uh , to use as sort of a compute farm . And um , uh , we ordered uh , SUN - Blade - one - hundreds , and um , I 'm not sure exactly how long it 'll take for those to come in , but , uh , in addition , we 're running {disfmarker} So the plan for using these is , uh , we 're running P - make and Customs here and Andreas has sort of gotten that all uh , fixed up and up to speed . And he 's got a number of little utilities that make it very easy to um , {vocalsound} run things using P - make and Customs . You don't actually have to write P - make scripts and things like that . The simplest thing {disfmarker} And I" />
    <node id="&#10;Content: OK . OK .&#10;Speaker: PhD F&#10;Content: yeah . So you should be zero , actually .&#10;Speaker: PhD A&#10;Content: Hello ? Yeah .&#10;Speaker: PhD F&#10;Content: For your uh , channel number .&#10;Speaker: PhD A&#10;Content: Yep , yep .&#10;Speaker: Professor C&#10;Content: And you should do a lot of talking so we get a lot more of your pronunciations . no , they don't {disfmarker} don't have a {disfmarker} have any Indian pronunciations .&#10;Speaker: PhD F&#10;Content: So what we usually do is um , we typically will have our meetings&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: and then at the end of the meetings we 'll read the digits . Everybody goes around and reads the digits on the {disfmarker} the bottom of their forms .&#10;Speaker: Professor C&#10;Content: Session R&#10;Speaker: PhD D&#10;Content: R - nineteen ?&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: R - nineteen .&#10;Speaker" />
    <node id="1. Extending the coefficient of the mel filter bank: By extending the coefficient of the mel filter bank, an approximation of the signal's spectrum can be obtained. This method is useful for analyzing signals and extracting features, which can then be used for various applications such as speech processing, noise reduction, or pattern recognition.&#10;2. Finding another feature to discriminate between voice and unvoice sounds: The speakers suggest comparing the variance of a new feature to discriminate between voice (speech) and unvoice (non-speech) sounds. They propose that if the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This idea stems from the assumption that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances.&#10;3. The motivation behind this approach is to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems." />
    <node id=": PhD B&#10;Content: You can extend the coefficient of the mel filter bank and obtain an approximation of the spectrum of the signal .&#10;Speaker: PhD A&#10;Content: Mmm . OK .&#10;Speaker: PhD B&#10;Content: I do the difference {disfmarker}&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: I found a difference at the variance of this different&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: because , suppose we {disfmarker} we think that if the variance is high , maybe you have n uh , noise .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And if the variance is small , maybe you have uh , speech .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: To {disfmarker} to To {disfmarker} The idea is to found another feature for discriminate between voice sound and unvoice sound .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: And we try to use this new feature {" />
    <node id="PhD B is explaining to PhD A that they can extend the coefficients of the mel filter bank to obtain an approximation of the signal's spectrum. This method allows for further analysis of signals and the extraction of features, which can be useful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it." />
    <node id=" PhD B&#10;Content: that , like , the auto - correlation , the R - zero and R - one over R - zero&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD B&#10;Content: and another estimation of the var the variance of the difference for {disfmarker} of the spec si uh , spectrum of the signal and {disfmarker} and the spectrum of time after filt mel filter bank .&#10;Speaker: PhD A&#10;Content: I 'm so sorry . I didn't get it .&#10;Speaker: PhD B&#10;Content: Nuh . Well . Anyway . The {disfmarker} First you have the sp the spectrum of the signal ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: and you have the {disfmarker} on the other side you have the output of the mel filter bank .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: You can extend the coefficient of the mel filter bank and obtain an approximation of the spectrum of the signal .&#10;Speaker: PhD" />
    <node id="The addition of a binary speech-silence feature to the cepstrum and training the HMM on it resulted in significant improvement for the Italian dataset, but there was no noticeable effect for the TI-digits dataset. This is because the TI-digits dataset did not have any substantial distinction between speech and non-speech frames to benefit from this additional feature. In contrast, the Italian dataset showed a considerable improvement due to the presence of more noise in the recordings. The binary speech-silence feature helped discriminate between speech and non-speech frames, which improved overall performance when using a neural network. However, without using a neural network, adding the binary feature worsened the results for both datasets, as it did not have any help in discriminating between speech and non-speech frames." />
    <node id="1. Similarities: Both a filter bank and the proposed targets for a Wi-Fi system involve processing signals by dividing them into different frequency bands or &quot;bins.&quot; This allows for analysis of each band's characteristics, enabling better understanding of the signal as a whole. In the context of speech processing, this can help identify phonetic features or distinguish between voiced and unvoiced sounds (as discussed in the transcript).&#10;2. Differences: The primary difference lies in the purpose and implementation of each method. A filter bank is designed to analyze signals and extract features by dividing them into specific frequency bands based on predefined criteria, such as mel-frequency cepstral coefficients (MFCCs) or linear predictive coding (LPC). On the other hand, the proposed targets for a Wi-Fi system might involve using various techniques like channel bonding, beamforming, or spatial reuse to improve network performance.&#10;3. Phones: In the transcript, phones refer to individual speech sounds in phonetics. Both filter banks and the proposed Wi-Fi targets can be used to analyze or enhance phone signals, but the methods are different. A filter bank helps extract features from phone signals for better speech processing, while Wi-Fi system targets aim to improve communication between devices (which may involve phone calls) by optimizing network performance.&#10;&#10;In summary, both a filter bank and the proposed Wi-Fi system targets involve processing signals in frequency bands, but their purposes and implementation methods differ significantly. The use of a filter bank for speech processing can provide insights into phonetic features, while Wi-Fi system targets aim to improve communication by optimizing network performance." />
    <node id=" then use it , you know , in combination , or alone , or {disfmarker} or whatever&#10;Speaker: PhD F&#10;Content: Wi - with what targets ?&#10;Speaker: PhD A&#10;Content: Voiced , unvoiced is like {disfmarker}&#10;Speaker: Professor C&#10;Content: Uh , no .&#10;Speaker: PhD A&#10;Content: Oh . Or anything .&#10;Speaker: Professor C&#10;Content: No the {disfmarker} just the same {disfmarker} same way we 're using {disfmarker} I mean , the same way that we 're using the filter bank .&#10;Speaker: PhD F&#10;Content: Phones .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: Professor C&#10;Content: Exact way {disfmarker} the same way we 're using the filter bank .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: I mean , the filter bank is good for all the reasons that we say it 's good . But it 's different . And , you know , maybe if it 's used in" />
    <node id="1. Extending the coefficient of the mel filter bank is used to obtain an approximation of the signal's spectrum, which allows for further analysis of signals and extraction of features helpful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.&#10;   &#10;2. The idea behind finding another feature to discriminate between voice and unvoiced sounds is based on comparing the variance of this new feature. The assumption is that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances. If the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This approach aims to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.&#10;&#10;These ideas are further discussed in the transcript provided, where they test the new feature on Italian and Spanish datasets with mixed results when compared to using only cepstrum or other features. They also explore the potential benefits of comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network." />
    <node id="1. The parameters that PhD B referred to when mentioning trying to include another new parameter to the traditional parameter in their voicing research are the cepstrum coefficient, auto-correlation, R-zero, and R-one over R-zero. These parameters are likely being used to extend the coefficients of the mel filter bank and approximate the signal's spectrum for further analysis and feature extraction in speech processing or pattern recognition applications." />
    <node id="&#10;Speaker: PhD B&#10;Content: I don't have good result . Are {pause} similar or a little bit worse .&#10;Speaker: PhD A&#10;Content: With what {disfmarker} what other new p new parameter ?&#10;Speaker: Grad G&#10;Content: You 're talking about your voicing ?&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So maybe {disfmarker} You probably need to back up a bit&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: seeing as how Sunil ,&#10;Speaker: PhD B&#10;Content: I tried to include another new parameter to the traditional parameter ,&#10;Speaker: Professor C&#10;Content: yeah .&#10;Speaker: PhD B&#10;Content: the coe the cepstrum coefficient ,&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: that , like , the auto - correlation , the R - zero and R - one over R - zero&#10;Speaker: PhD A" />
    <node id=" ? Yeah , yeah , OK .&#10;Speaker: PhD B&#10;Content: Yeah , yeah , yeah . Originally the idea was from CMU .&#10;Speaker: PhD A&#10;Content: From C .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: Well , it 's again a different thing {vocalsound} {vocalsound} that could be tried . Um ,&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: Mmm , yeah .&#10;Speaker: Professor C&#10;Content: Yeah , so at any rate , you 're looking general , uh , standing back from it , looking at ways to combine one form or another of uh , noise removal , uh , with {disfmarker} with these other things we have ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: uh , looks like a worthy thing to {disfmarker} to do here .&#10;Speaker: PhD D&#10;Content: Uh , yeah . But , yeah . But for sure" />
    <node id="1. Complementary insights: By combining different methods like KLT (Karhunen-Love Transform) and probabilities with the current approach, one can gain complementary insights into the variability of the power spectrum. KLT is a technique used for dimensionality reduction and decorrelation, while probability theory can help in modeling the randomness and uncertainty present in the signal. By combining these methods, it might be possible to capture different aspects of the variability in the power spectrum, leading to a more comprehensive understanding.&#10;2. Improved robustness: Combining multiple methods can lead to increased robustness in the analysis of the power spectrum. Each method brings its strengths and weaknesses, and blending them together may help compensate for individual shortcomings. For instance, KLT is sensitive to the choice of basis functions, while probabilistic methods might struggle with model misspecification. By combining these approaches, one can potentially achieve a more reliable analysis of the power spectrum variability.&#10;3. Neural network-driven selection: As suggested by PhD D, allowing a neural network to determine which features or methods are useful in capturing the missing elements in the analysis of the power spectrum could lead to improved performance. By combining different methods and letting the neural network choose the most relevant ones, it might be possible to capture more nuanced information about the variability in the power spectrum.&#10;4. Enhanced feature extraction: Combining various methods for feature extraction, such as comparing filter bank and FFT, can lead to better identification of speech or noise signals in the power spectrum. By using a combination of techniques, researchers might be able to extract more meaningful features that help improve voice/unvoice discrimination, thereby enhancing the robustness and accuracy of various signal processing systems.&#10;&#10;In summary, combining different methods such as KLT and probabilities with the current approach can provide complementary insights, improved robustness, neural network-driven selection, and enhanced feature extraction for addressing missing elements in the analysis of variability in the power spectrum." />
    <node id=" is good for all the reasons that we say it 's good . But it 's different . And , you know , maybe if it 's used in combination , it will get at something that we 're missing . And maybe , you know , using , orth you know , KLT , or uh , um , adding probabilities , I mean , all th all the different ways that we 've been playing with , that we would let the {disfmarker} essentially let the neural network determine what is it that 's useful , that we 're missing here .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Yeah , but there is so much variability in the power spectrum .&#10;Speaker: Professor C&#10;Content: Well , that 's probably why y i it would be unlikely to work as well by itself , but it might help in combination .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mmm .&#10;Speaker: Professor C&#10;Content: But I {disfmarker} I {disfmarker} I have to" />
    <node id="To help the person track the peaks in the PLP (Perceptual Linear Prediction) LPC (Linear Predictive Coding) spectra and compare them to listener tests over time, the following technique was used:&#10;&#10;1. The roots of the LPC polynomial were found using the PLP LPC coefficients, as suggested by Stephane. This allowed for tracking the peaks in the PLP LPC spectra.&#10;2. A psycho-acoustic spectrum was provided to compare the energy distribution over time with the listener tests.&#10;3. The peaks in the PLP LPC spectra were tracked using the roots of the LPC polynomial, enabling a comparison of how they change over time.&#10;&#10;This technique enabled a detailed analysis of how the spectral peaks behave and evolve, making it possible to compare them with listener test results." />
    <node id=" wanted a psycho - acoustic um , spectrum . And he wanted to look at um , how the energy is moving {pause} over time in that spectrum and compare that to the {disfmarker} to the listener tests . And , um . So , I gave him a PLP spectrum . And {disfmarker} to um {disfmarker} he {disfmarker} he t wanted to track the peaks so he could look at how they 're moving . So I took the um , PLP LPC coefficients and um , I found the roots . This was something that Stephane suggested . I found the roots of the um , LPC polynomial to , um , track the peaks in the , um , PLP LPC spectra .&#10;Speaker: PhD A&#10;Content: well there is aligned spectral pairs , is like the {disfmarker} the {disfmarker} Is that the aligned s&#10;Speaker: Professor C&#10;Content: It 's a r root LPC , uh , of some sort .&#10;Speaker: PhD A&#10;Content: Oh , no .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content" />
    <node id=" PhD F&#10;Content: If this is synthetic speech can't you just get the formants directly ? I mean h how is the speech created ?&#10;Speaker: Grad E&#10;Content: It was created from a synthesizer , and um {disfmarker}&#10;Speaker: PhD F&#10;Content: Wasn't a formant synthesizer was it ?&#10;Speaker: Professor C&#10;Content: I bet it {disfmarker} it might have {disfmarker} may have been&#10;Speaker: Grad E&#10;Content: I {disfmarker} d d this {disfmarker}&#10;Speaker: Professor C&#10;Content: but maybe he didn't have control over it or something ?&#10;Speaker: Grad E&#10;Content: In {disfmarker} in fact w we {disfmarker} we could get , um , formant frequencies out of the synthesizer , as well . And , um , w one thing that the , um , LPC approach will hopefully give me in addition , um , is that I {disfmarker} I might be able to find the b the bandwidths of these humps as well . Um , Stephane suggested looking at each complex pair as" />
    <node id="1. Evaluation conditions for the Broadcast News study on noisy speech with a large vocabulary included various focus conditions that were noisy. However, spectral subtraction was not applied in this study. Instead, multi-stream methods were used.&#10;2. The speakers did not provide specific details about the exact techniques and methods employed during the Broadcast News evaluation. They only mentioned using multi-stream approaches as an alternative to spectral subtraction for noisy large vocabulary conditions." />
    <node id="aker: Professor C&#10;Content: Mis - Mississippi State maybe ,&#10;Speaker: PhD A&#10;Content: yeah . I 'm not sure about that .&#10;Speaker: Professor C&#10;Content: yeah . Yeah , so that 'll be a little {disfmarker} little task in itself .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Um , well we 've {disfmarker} Yeah , it 's true for the additive noise , y artificially added noise we 've always used small vocabulary too . But for n there 's been noisy speech this larv large vocabulary that we 've worked with in Broadcast News . So we we did the Broadcast News evaluation&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: and some of the focus conditions were noisy and {disfmarker} and {disfmarker}&#10;Speaker: PhD A&#10;Content: It had additive n&#10;Speaker: Professor C&#10;Content: But we {disfmarker} but we didn't do spectral subtraction . We were doing our funny stuff , right ? We were doing multi multi uh , multi - stream and" />
    <node id="1. PhD A has been working on a signal subspace approach for speech enhancement, specifically an algorithm that decomposes the noisy signal into signal and noise subspaces to estimate clean speech. They have been testing this on Matlab and are trying to optimize the performance by adjusting the CPU usage.&#10;2. PhD A also tried to implement online normalization in their algorithm but found that it did not improve the results.&#10;3. In addition, PhD A combined endpoint information with the baseline system as another related experiment.&#10;4. However, there is no explicit mention of positive results from these experiments in the transcript. Therefore, it is unclear if they have found any positive results to port it to C and update it in the shared repository." />
    <node id="&#10;Content: So , I 've been actually running some s So far I 've been trying it only on Matlab . I have to {disfmarker} to {disfmarker} to test whether it works first or not&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and then I 'll p port it to C and I 'll update it with the repository once I find it it giving any some positive result . So , yeah .&#10;Speaker: Professor C&#10;Content: S So you s you So you said one thing I want to jump on for a second . So {disfmarker} so now you 're {disfmarker} you 're getting tuned into the repository thing that he has here&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and {disfmarker} so we we 'll have a {vocalsound} single place where the stuff is .&#10;Speaker: PhD A&#10;Content: Yep . Yeah .&#10;Speaker: Professor C&#10;Content: Cool . Um , so maybe uh , just briefly , you could remind us about the related experiments . Cuz you did some" />
    <node id="marker} good update .&#10;Speaker: PhD A&#10;Content: Ye Yeah , and I {disfmarker} I came back and I started working on uh , some other speech enhancement algorithm . I mean , so {disfmarker} I {disfmarker} from the submission what I found that people have tried spectral subtraction and Wiener filtering . These are the main uh , approaches where people have tried ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: so just to {disfmarker} just to fill the space with some f few more speech enhancement algorithms to see whether it improves a lot , I {disfmarker} I 've been working on this uh , signal subspace approach for speech enhancement where you take the noisy signal and then decomposing the signal s and the noise subspace and then try to estimate the clean speech from the signal plus noise subspace . And {disfmarker}&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So , I 've been actually running some s So far I 've been trying it only on Matlab . I have to {disf" />
    <node id=" up from the {disfmarker} the CPU or something {vocalsound} trying to {disfmarker} trying to do it ,&#10;Speaker: PhD F&#10;Content: That 's right&#10;Speaker: Professor C&#10;Content: but uh , yeah . But , you 're right that {disfmarker} that {disfmarker} that 's a real good point , that uh , we {disfmarker} we don't know yeah , uh , I mean , what if any of these ta I guess that 's why they 're pushing that in the uh {disfmarker} in the evaluation .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , But um , Good . OK . Anything else going on ? at you guys ' end ,&#10;Speaker: PhD B&#10;Content: I don't have good result , with the {disfmarker} inc including the new parameters ,&#10;Speaker: Professor C&#10;Content: or {disfmarker} ?&#10;Speaker: PhD B&#10;Content: I don't have good result . Are {pause} similar or a little bit worse .&#10;Speaker: PhD" />
    <node id="1. The slight decrease in performance for the well-matched (MM) category and the significant improvement for HM are due to the addition of a new feature or technique. However, the improvement for HM is statistically significant, while the decrease in MM is not. This could be because the well-matched test set responds differently to changes in features or techniques compared to other test sets (such as HM).&#10;&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM. As a result, the contribution of different test sets may change when a new metric is used.&#10;&#10;The implementation of a baseline using data-driven and non-data-driven methods in Linear Discriminant Analysis (LDA) can provide valuable insights into the performance of LDA filters and their impact on the overall system. It also emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques." />
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target=": Unvoiced . Well .&#10;Speaker: PhD B&#10;Content: and to {disfmarker} to concat this new feature . But the result are n with the neural network I have more or less the same result .&#10;Speaker: PhD A&#10;Content: As using just the cepstrum ,&#10;Speaker: PhD B&#10;Content: Result .&#10;Speaker: PhD A&#10;Content: or {disfmarker} ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Yeah . It 's neve e e sometime it 's worse , sometime it 's a little bit better , but not significantly .&#10;Speaker: PhD A&#10;Content: Uh , is it with TI - digits , or with {disfmarker} ?&#10;Speaker: PhD B&#10;Content: And {disfmarker} No , I work with eh , Italian and Spanish basically .&#10;Speaker: PhD A&#10;Content: OK . OK .&#10;Speaker: PhD B&#10;Content: And if I don't y use the neural network , and use directly the feature the results are worse .&#10;Speaker:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="m sorry ?&#10;Speaker: PhD A&#10;Content: Yeah , we actually added an additional binary feature to the cepstrum , just the baseline .&#10;Speaker: PhD D&#10;Content: Yeah ?&#10;Speaker: PhD B&#10;Content: You did some experiment .&#10;Speaker: PhD A&#10;Content: Yeah , yeah . Well , in {disfmarker} in the case of TI - digits it didn't actually give us anything , because there wasn't any f anything to discriminate between speech ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and it was very short . But Italian was like very {disfmarker} it was a huge improvement on Italian .&#10;Speaker: PhD D&#10;Content: Hmm . Well {disfmarker} Mm - hmm . But anyway the question is even more , is within speech , can we get some features ? Are we drop dropping information that can might be useful within speech ,&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: I mean . To {disfmarker} maybe to distinguish between voice sound and unvoiced sounds ?&#10;Speaker: PhD A&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target=": PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: What I meant was you had something like cepstra or something , right ?&#10;Speaker: PhD A&#10;Content: Yeah , yeah , yeah , yeah .&#10;Speaker: Professor C&#10;Content: And so one difference is that , I guess you were taking spectra .&#10;Speaker: PhD A&#10;Content: The speech .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah . But I guess it 's the s exactly the same thing because on the heads uh , handset they just applied this Wiener filter and then compute cepstral features ,&#10;Speaker: PhD A&#10;Content: Yeah , the cepstral f The difference is like {disfmarker} There may be a slight difference in the way {disfmarker}&#10;Speaker: PhD D&#10;Content: right ? or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: because they use exactly the baseline system for converting the cepstrum once you have the speech . I mean , if we are using our own code for th I mean that {disfmarker} that could">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target=": cumulants , yeah .&#10;Speaker: PhD A&#10;Content: maybe higher - order cumulants&#10;Speaker: Professor C&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: and {disfmarker} Yeah . It was {disfmarker} it was {disfmarker}&#10;Speaker: Professor C&#10;Content: Or m e&#10;Speaker: PhD A&#10;Content: Yeah . I mean , he was showing up uh some {disfmarker} something on noisy speech ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: some improvement on the noisy speech .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Some small vocabulary tasks .&#10;Speaker: Professor C&#10;Content: Uh .&#10;Speaker: PhD A&#10;Content: So it was on PLP derived cepstral coefficients .&#10;Speaker: Professor C&#10;Content: Yeah , but again {disfmarker} You could argue that th that 's exactly what the neural network does .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: So">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="aker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: What {disfmarker} one {disfmarker} one um p one thing is like what {disfmarker} before we started using this VAD in this Aurora , the {disfmarker} th what we did was like , I {disfmarker} I guess most of you know about this , adding this additional speech - silence bit to the cepstrum and training the HMM on that .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: That is just a binary feature and that seems to be {vocalsound} improving a lot on the SpeechDat - Car where there is a lot of noise but not much on the TI - digits . So , a adding an additional feature to distin to discriminate between speech and nonspeech was helping . That 's it .&#10;Speaker: PhD D&#10;Content: Wait {disfmarker} I {disfmarker} I 'm sorry ?&#10;Speaker: PhD A&#10;Content: Yeah , we actually added an additional binary feature to the cepstrum , just the baseline .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target=" order to get rid of the effects of noise .&#10;Speaker: PhD A&#10;Content: So .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah . Uh , yeah . So there is this . And maybe we {disfmarker} well we find some people so that {vocalsound} uh , agree to maybe work with us , and they have implementation of VTS techniques so it 's um , Vector Taylor Series that are used to mmm , {vocalsound} uh f to model the transformation between clean cepstra and noisy cepstra . So . Well , if you take the standard model of channel plus noise , uh , it 's {disfmarker} it 's a nonlinear eh uh , transformation in the cepstral domain .&#10;Speaker: Professor C&#10;Content: Mm - hmm . Yes .&#10;Speaker: PhD D&#10;Content: And uh , there is a way to approximate this using uh , first - order or second - order Taylor Series and it can be used for {vocalsound} uh , getting rid of the noise and the channel effect .&#10;Speaker: Professor C&#10;Content: Who is doing this">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="1. Wiener filter and cepstral features are both techniques used in speech processing to reduce noise and improve signal quality. A Wiener filter is a type of filter used to minimize the mean square error between the desired signal and the filtered signal, while cepstral features are a representation of the spectral envelope of a speech signal.&#10;2. In the given transcript, PhD D mentions that in the handset used for the experiment, a Wiener filter is applied first and then cepstral features are computed. This suggests that the Wiener filter is used as a pre-processing step to reduce noise before extracting cepstral features.&#10;3. PhD A mentions that the signal subspace approach, which has an in-built Wiener filtering, works well for noisy signals but not very well for poor SNR conditions and colored noise. This suggests that Wiener filtering can help improve the quality of noisy speech signals, but its effectiveness may depend on the type and level of noise present in the signal.&#10;4. The difference between using a Wiener filter and cepstral features in the baseline system for converting the cepstrum is not explicitly stated in the given transcript. However, it can be inferred that the baseline system uses cepstral features as input to the HMM (Hidden Markov Model) for speech recognition. The addition of a Wiener filter as a pre-processing step can help improve the quality of the cepstral features and thus potentially improve the performance of the speech recognition system.&#10;5. Overall, the use of Wiener filters and cepstral features in speech processing can help reduce noise and improve signal quality, which can lead to better performance in speech recognition systems. However, their effectiveness may depend on the type and level of noise present in the signal.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="Based on the transcript, when PhD A added online normalization to the system with a different time constant, they found that it did not lead to an improvement over not using online normalization. They mentioned that they had to change the value of the update factor to see any improvements, but even then, it was not better than without online normalization. This suggests that the change in time constant and addition of online normalization did not result in performance improvements in this case.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="1. The slight decrease in performance for the well-matched (MM) category and significant improvement for HM is due to the addition of a new feature or technique, but it's not statistically significant for MM and only slightly worse for well-matched. However, the improvement for HM is significant.&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM.&#10;3. The discussion about the well-matched condition being unusual implies that this specific test set (well-matched) might respond differently to changes in features or techniques compared to other test sets, such as HM. This could explain why some methods show significant improvement for HM but not for MM.&#10;4. There is a general acknowledgement in the conversation that there's a lot of variability in performance when comparing different features and techniques, and that statistical mechanisms should be used to quantify this variability and make reasonable decisions based on it. This highlights the importance of using appropriate statistical tests to determine the significance of changes in performance when evaluating new features or techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="1. Useful information in the difference between a filter bank and FFT: There might be useful information in the difference between using a filter bank and FFT for feature extraction. The discussion reveals that researchers have been looking at this difference to analyze noise or voiced property, which could potentially help drive the thought process of coming up with features.&#10;&#10;2. Feeding FFT power spectrum into a neural network: Based on PhD B's statement, feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features. However, in their research comparing new features using a neural network versus traditional cepstrum or other features in Italian and Spanish datasets, they found no statistically significant difference in performance.&#10;&#10;In conclusion, there could be useful information in the difference between using a filter bank and FFT for feature extraction, and feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features, although it may not yield significantly different results compared to traditional methods.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="1. When using the new feature with a neural network compared to just the cepstrum or other features in Italian and Spanish datasets, there is no significant difference in performance. The new feature sometimes results in slightly better performance, and sometimes it's slightly worse, but not statistically significant (PhD B).&#10;2. Without using the neural network, adding an additional binary feature to the cepstrum worsens the results. This additional feature helps discriminate between speech and non-speech frames, which improves performance when using a neural network (PhD A).&#10;3. There is potential useful information in comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network. However, these methods may not yield significantly different results compared to traditional methods (implied from the discussion).">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="1. According to the transcript, Professor C mentioned the possibility of using the SRI recognizer for testing on large vocabulary data with a distant microphone. However, it is not clear if this has been attempted already.&#10;2. The potential challenges in using the SRI recognizer for testing on large vocabulary data with a distant microphone include the following:&#10;* Fear of trying it out, as mentioned by Professor C (&quot;Yeah, cuz everybody's scared.&quot;)&#10;* Possible overloading of the CPU or hardware due to the demands of processing large vocabulary data from a distant microphone, as also mentioned by Professor C (&quot;You'll see a little smoke coming up from the CPU or something trying to do it.&quot;).&#10;* The need for robust detectors of acoustic events, which can provide good enough coverage and training data for later recognition steps. This is based on Grad G's description of their proposed PhD work on using acoustic events for speech recognition (&quot;Building robust primary detectors for these acoustic events and using the outputs of these robust detectors to do speech recognition.&quot;).&#10;* The challenges in determining what kind of acoustic events are needed, as well as obtaining labels or training data for these acoustic events, as mentioned by Grad G.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="1. Extending the coefficient of the mel filter bank: By extending the coefficient of the mel filter bank, an approximation of the signal's spectrum can be obtained. This method is useful for analyzing signals and extracting features, which can then be used for various applications such as speech processing, noise reduction, or pattern recognition.&#10;2. Finding another feature to discriminate between voice and unvoice sounds: The speakers suggest comparing the variance of a new feature to discriminate between voice (speech) and unvoice (non-speech) sounds. They propose that if the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This idea stems from the assumption that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances.&#10;3. The motivation behind this approach is to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="The addition of a binary speech-silence feature to the cepstrum and training the HMM on it resulted in significant improvement for the Italian dataset, but there was no noticeable effect for the TI-digits dataset. This is because the TI-digits dataset did not have any substantial distinction between speech and non-speech frames to benefit from this additional feature. In contrast, the Italian dataset showed a considerable improvement due to the presence of more noise in the recordings. The binary speech-silence feature helped discriminate between speech and non-speech frames, which improved overall performance when using a neural network. However, without using a neural network, adding the binary feature worsened the results for both datasets, as it did not have any help in discriminating between speech and non-speech frames.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="To help the person track the peaks in the PLP (Perceptual Linear Prediction) LPC (Linear Predictive Coding) spectra and compare them to listener tests over time, the following technique was used:&#10;&#10;1. The roots of the LPC polynomial were found using the PLP LPC coefficients, as suggested by Stephane. This allowed for tracking the peaks in the PLP LPC spectra.&#10;2. A psycho-acoustic spectrum was provided to compare the energy distribution over time with the listener tests.&#10;3. The peaks in the PLP LPC spectra were tracked using the roots of the LPC polynomial, enabling a comparison of how they change over time.&#10;&#10;This technique enabled a detailed analysis of how the spectral peaks behave and evolve, making it possible to compare them with listener test results.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The difference in performance when using the new feature with a neural network compared to using just the cepstrum or other features in Italian and Spanish datasets is not significant. There is sometimes a slight improvement, and sometimes it's slightly worse, but not enough to be considered statistically significant. (PhD B: &quot;Yeah, it's never significantly better or worse.&quot;)&#10;2. The results are worse without using the neural network when adding an additional binary feature to the cepstrum. This is because the neural network helps in discriminating between speech and non-speech frames, which improves the performance. (PhD A: &quot;What we did was... adding this additional speech-silence bit to the cepstrum and training the HMM on that. That seems to be improving a lot... So, adding an additional feature to discriminate between speech and non-speech was helping.&quot;)" target="1. The slight decrease in performance for the well-matched (MM) category and the significant improvement for HM are due to the addition of a new feature or technique. However, the improvement for HM is statistically significant, while the decrease in MM is not. This could be because the well-matched test set responds differently to changes in features or techniques compared to other test sets (such as HM).&#10;&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM. As a result, the contribution of different test sets may change when a new metric is used.&#10;&#10;The implementation of a baseline using data-driven and non-data-driven methods in Linear Discriminant Analysis (LDA) can provide valuable insights into the performance of LDA filters and their impact on the overall system. It also emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source=": Unvoiced . Well .&#10;Speaker: PhD B&#10;Content: and to {disfmarker} to concat this new feature . But the result are n with the neural network I have more or less the same result .&#10;Speaker: PhD A&#10;Content: As using just the cepstrum ,&#10;Speaker: PhD B&#10;Content: Result .&#10;Speaker: PhD A&#10;Content: or {disfmarker} ?&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: Yeah . It 's neve e e sometime it 's worse , sometime it 's a little bit better , but not significantly .&#10;Speaker: PhD A&#10;Content: Uh , is it with TI - digits , or with {disfmarker} ?&#10;Speaker: PhD B&#10;Content: And {disfmarker} No , I work with eh , Italian and Spanish basically .&#10;Speaker: PhD A&#10;Content: OK . OK .&#10;Speaker: PhD B&#10;Content: And if I don't y use the neural network , and use directly the feature the results are worse .&#10;Speaker:" target="1. When using the new feature with a neural network compared to just the cepstrum or other features in Italian and Spanish datasets, there is no significant difference in performance. The new feature sometimes results in slightly better performance, and sometimes it's slightly worse, but not statistically significant (PhD B).&#10;2. Without using the neural network, adding an additional binary feature to the cepstrum worsens the results. This additional feature helps discriminate between speech and non-speech frames, which improves performance when using a neural network (PhD A).&#10;3. There is potential useful information in comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network. However, these methods may not yield significantly different results compared to traditional methods (implied from the discussion).">
      <data key="d0">1</data>
    </edge>
    <edge source="m sorry ?&#10;Speaker: PhD A&#10;Content: Yeah , we actually added an additional binary feature to the cepstrum , just the baseline .&#10;Speaker: PhD D&#10;Content: Yeah ?&#10;Speaker: PhD B&#10;Content: You did some experiment .&#10;Speaker: PhD A&#10;Content: Yeah , yeah . Well , in {disfmarker} in the case of TI - digits it didn't actually give us anything , because there wasn't any f anything to discriminate between speech ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and it was very short . But Italian was like very {disfmarker} it was a huge improvement on Italian .&#10;Speaker: PhD D&#10;Content: Hmm . Well {disfmarker} Mm - hmm . But anyway the question is even more , is within speech , can we get some features ? Are we drop dropping information that can might be useful within speech ,&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: I mean . To {disfmarker} maybe to distinguish between voice sound and unvoiced sounds ?&#10;Speaker: PhD A&#10;Content" target="Based on the transcript, PhD A mentions that Guenter Hirsch is in charge of the project that is similar to TI-digits. They are potentially generating HTK scripts, but it is not clear whether they are converging on HTK or using some other speech recognition toolkit from Mississippi State. PhD A mentions &quot;Mississippi State maybe&quot; in relation to this question.">
      <data key="d0">1</data>
    </edge>
    <edge source="m sorry ?&#10;Speaker: PhD A&#10;Content: Yeah , we actually added an additional binary feature to the cepstrum , just the baseline .&#10;Speaker: PhD D&#10;Content: Yeah ?&#10;Speaker: PhD B&#10;Content: You did some experiment .&#10;Speaker: PhD A&#10;Content: Yeah , yeah . Well , in {disfmarker} in the case of TI - digits it didn't actually give us anything , because there wasn't any f anything to discriminate between speech ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and it was very short . But Italian was like very {disfmarker} it was a huge improvement on Italian .&#10;Speaker: PhD D&#10;Content: Hmm . Well {disfmarker} Mm - hmm . But anyway the question is even more , is within speech , can we get some features ? Are we drop dropping information that can might be useful within speech ,&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: I mean . To {disfmarker} maybe to distinguish between voice sound and unvoiced sounds ?&#10;Speaker: PhD A&#10;Content" target="1. When using the new feature with a neural network compared to just the cepstrum or other features in Italian and Spanish datasets, there is no significant difference in performance. The new feature sometimes results in slightly better performance, and sometimes it's slightly worse, but not statistically significant (PhD B).&#10;2. Without using the neural network, adding an additional binary feature to the cepstrum worsens the results. This additional feature helps discriminate between speech and non-speech frames, which improves performance when using a neural network (PhD A).&#10;3. There is potential useful information in comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network. However, these methods may not yield significantly different results compared to traditional methods (implied from the discussion).">
      <data key="d0">1</data>
    </edge>
    <edge source="m sorry ?&#10;Speaker: PhD A&#10;Content: Yeah , we actually added an additional binary feature to the cepstrum , just the baseline .&#10;Speaker: PhD D&#10;Content: Yeah ?&#10;Speaker: PhD B&#10;Content: You did some experiment .&#10;Speaker: PhD A&#10;Content: Yeah , yeah . Well , in {disfmarker} in the case of TI - digits it didn't actually give us anything , because there wasn't any f anything to discriminate between speech ,&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and it was very short . But Italian was like very {disfmarker} it was a huge improvement on Italian .&#10;Speaker: PhD D&#10;Content: Hmm . Well {disfmarker} Mm - hmm . But anyway the question is even more , is within speech , can we get some features ? Are we drop dropping information that can might be useful within speech ,&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: I mean . To {disfmarker} maybe to distinguish between voice sound and unvoiced sounds ?&#10;Speaker: PhD A&#10;Content" target="The addition of a binary speech-silence feature to the cepstrum and training the HMM on it resulted in significant improvement for the Italian dataset, but there was no noticeable effect for the TI-digits dataset. This is because the TI-digits dataset did not have any substantial distinction between speech and non-speech frames to benefit from this additional feature. In contrast, the Italian dataset showed a considerable improvement due to the presence of more noise in the recordings. The binary speech-silence feature helped discriminate between speech and non-speech frames, which improved overall performance when using a neural network. However, without using a neural network, adding the binary feature worsened the results for both datasets, as it did not have any help in discriminating between speech and non-speech frames.">
      <data key="d0">1</data>
    </edge>
    <edge source=": PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: What I meant was you had something like cepstra or something , right ?&#10;Speaker: PhD A&#10;Content: Yeah , yeah , yeah , yeah .&#10;Speaker: Professor C&#10;Content: And so one difference is that , I guess you were taking spectra .&#10;Speaker: PhD A&#10;Content: The speech .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah . But I guess it 's the s exactly the same thing because on the heads uh , handset they just applied this Wiener filter and then compute cepstral features ,&#10;Speaker: PhD A&#10;Content: Yeah , the cepstral f The difference is like {disfmarker} There may be a slight difference in the way {disfmarker}&#10;Speaker: PhD D&#10;Content: right ? or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: because they use exactly the baseline system for converting the cepstrum once you have the speech . I mean , if we are using our own code for th I mean that {disfmarker} that could" target="1. Wiener filter and cepstral features are both techniques used in speech processing to reduce noise and improve signal quality. A Wiener filter is a type of filter used to minimize the mean square error between the desired signal and the filtered signal, while cepstral features are a representation of the spectral envelope of a speech signal.&#10;2. In the given transcript, PhD D mentions that in the handset used for the experiment, a Wiener filter is applied first and then cepstral features are computed. This suggests that the Wiener filter is used as a pre-processing step to reduce noise before extracting cepstral features.&#10;3. PhD A mentions that the signal subspace approach, which has an in-built Wiener filtering, works well for noisy signals but not very well for poor SNR conditions and colored noise. This suggests that Wiener filtering can help improve the quality of noisy speech signals, but its effectiveness may depend on the type and level of noise present in the signal.&#10;4. The difference between using a Wiener filter and cepstral features in the baseline system for converting the cepstrum is not explicitly stated in the given transcript. However, it can be inferred that the baseline system uses cepstral features as input to the HMM (Hidden Markov Model) for speech recognition. The addition of a Wiener filter as a pre-processing step can help improve the quality of the cepstral features and thus potentially improve the performance of the speech recognition system.&#10;5. Overall, the use of Wiener filters and cepstral features in speech processing can help reduce noise and improve signal quality, which can lead to better performance in speech recognition systems. However, their effectiveness may depend on the type and level of noise present in the signal.">
      <data key="d0">1</data>
    </edge>
    <edge source=": PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: What I meant was you had something like cepstra or something , right ?&#10;Speaker: PhD A&#10;Content: Yeah , yeah , yeah , yeah .&#10;Speaker: Professor C&#10;Content: And so one difference is that , I guess you were taking spectra .&#10;Speaker: PhD A&#10;Content: The speech .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah . But I guess it 's the s exactly the same thing because on the heads uh , handset they just applied this Wiener filter and then compute cepstral features ,&#10;Speaker: PhD A&#10;Content: Yeah , the cepstral f The difference is like {disfmarker} There may be a slight difference in the way {disfmarker}&#10;Speaker: PhD D&#10;Content: right ? or {disfmarker} ?&#10;Speaker: PhD A&#10;Content: because they use exactly the baseline system for converting the cepstrum once you have the speech . I mean , if we are using our own code for th I mean that {disfmarker} that could" target="1. When using the new feature with a neural network compared to just the cepstrum or other features in Italian and Spanish datasets, there is no significant difference in performance. The new feature sometimes results in slightly better performance, and sometimes it's slightly worse, but not statistically significant (PhD B).&#10;2. Without using the neural network, adding an additional binary feature to the cepstrum worsens the results. This additional feature helps discriminate between speech and non-speech frames, which improves performance when using a neural network (PhD A).&#10;3. There is potential useful information in comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network. However, these methods may not yield significantly different results compared to traditional methods (implied from the discussion).">
      <data key="d0">1</data>
    </edge>
    <edge source=": cumulants , yeah .&#10;Speaker: PhD A&#10;Content: maybe higher - order cumulants&#10;Speaker: Professor C&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: and {disfmarker} Yeah . It was {disfmarker} it was {disfmarker}&#10;Speaker: Professor C&#10;Content: Or m e&#10;Speaker: PhD A&#10;Content: Yeah . I mean , he was showing up uh some {disfmarker} something on noisy speech ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: some improvement on the noisy speech .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Some small vocabulary tasks .&#10;Speaker: Professor C&#10;Content: Uh .&#10;Speaker: PhD A&#10;Content: So it was on PLP derived cepstral coefficients .&#10;Speaker: Professor C&#10;Content: Yeah , but again {disfmarker} You could argue that th that 's exactly what the neural network does .&#10;Speaker: PhD A&#10;Content: Mmm .&#10;Speaker: Professor C&#10;Content: So" target="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: What {disfmarker} one {disfmarker} one um p one thing is like what {disfmarker} before we started using this VAD in this Aurora , the {disfmarker} th what we did was like , I {disfmarker} I guess most of you know about this , adding this additional speech - silence bit to the cepstrum and training the HMM on that .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: That is just a binary feature and that seems to be {vocalsound} improving a lot on the SpeechDat - Car where there is a lot of noise but not much on the TI - digits . So , a adding an additional feature to distin to discriminate between speech and nonspeech was helping . That 's it .&#10;Speaker: PhD D&#10;Content: Wait {disfmarker} I {disfmarker} I 'm sorry ?&#10;Speaker: PhD A&#10;Content: Yeah , we actually added an additional binary feature to the cepstrum , just the baseline ." target="1. Wiener filter and cepstral features are both techniques used in speech processing to reduce noise and improve signal quality. A Wiener filter is a type of filter used to minimize the mean square error between the desired signal and the filtered signal, while cepstral features are a representation of the spectral envelope of a speech signal.&#10;2. In the given transcript, PhD D mentions that in the handset used for the experiment, a Wiener filter is applied first and then cepstral features are computed. This suggests that the Wiener filter is used as a pre-processing step to reduce noise before extracting cepstral features.&#10;3. PhD A mentions that the signal subspace approach, which has an in-built Wiener filtering, works well for noisy signals but not very well for poor SNR conditions and colored noise. This suggests that Wiener filtering can help improve the quality of noisy speech signals, but its effectiveness may depend on the type and level of noise present in the signal.&#10;4. The difference between using a Wiener filter and cepstral features in the baseline system for converting the cepstrum is not explicitly stated in the given transcript. However, it can be inferred that the baseline system uses cepstral features as input to the HMM (Hidden Markov Model) for speech recognition. The addition of a Wiener filter as a pre-processing step can help improve the quality of the cepstral features and thus potentially improve the performance of the speech recognition system.&#10;5. Overall, the use of Wiener filters and cepstral features in speech processing can help reduce noise and improve signal quality, which can lead to better performance in speech recognition systems. However, their effectiveness may depend on the type and level of noise present in the signal.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: What {disfmarker} one {disfmarker} one um p one thing is like what {disfmarker} before we started using this VAD in this Aurora , the {disfmarker} th what we did was like , I {disfmarker} I guess most of you know about this , adding this additional speech - silence bit to the cepstrum and training the HMM on that .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: That is just a binary feature and that seems to be {vocalsound} improving a lot on the SpeechDat - Car where there is a lot of noise but not much on the TI - digits . So , a adding an additional feature to distin to discriminate between speech and nonspeech was helping . That 's it .&#10;Speaker: PhD D&#10;Content: Wait {disfmarker} I {disfmarker} I 'm sorry ?&#10;Speaker: PhD A&#10;Content: Yeah , we actually added an additional binary feature to the cepstrum , just the baseline ." target="The addition of a binary speech-silence feature to the cepstrum and training the HMM on it resulted in significant improvement for the Italian dataset, but there was no noticeable effect for the TI-digits dataset. This is because the TI-digits dataset did not have any substantial distinction between speech and non-speech frames to benefit from this additional feature. In contrast, the Italian dataset showed a considerable improvement due to the presence of more noise in the recordings. The binary speech-silence feature helped discriminate between speech and non-speech frames, which improved overall performance when using a neural network. However, without using a neural network, adding the binary feature worsened the results for both datasets, as it did not have any help in discriminating between speech and non-speech frames.">
      <data key="d0">1</data>
    </edge>
    <edge source=" order to get rid of the effects of noise .&#10;Speaker: PhD A&#10;Content: So .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: Yeah . Uh , yeah . So there is this . And maybe we {disfmarker} well we find some people so that {vocalsound} uh , agree to maybe work with us , and they have implementation of VTS techniques so it 's um , Vector Taylor Series that are used to mmm , {vocalsound} uh f to model the transformation between clean cepstra and noisy cepstra . So . Well , if you take the standard model of channel plus noise , uh , it 's {disfmarker} it 's a nonlinear eh uh , transformation in the cepstral domain .&#10;Speaker: Professor C&#10;Content: Mm - hmm . Yes .&#10;Speaker: PhD D&#10;Content: And uh , there is a way to approximate this using uh , first - order or second - order Taylor Series and it can be used for {vocalsound} uh , getting rid of the noise and the channel effect .&#10;Speaker: Professor C&#10;Content: Who is doing this" target="1. Wiener filter and cepstral features are both techniques used in speech processing to reduce noise and improve signal quality. A Wiener filter is a type of filter used to minimize the mean square error between the desired signal and the filtered signal, while cepstral features are a representation of the spectral envelope of a speech signal.&#10;2. In the given transcript, PhD D mentions that in the handset used for the experiment, a Wiener filter is applied first and then cepstral features are computed. This suggests that the Wiener filter is used as a pre-processing step to reduce noise before extracting cepstral features.&#10;3. PhD A mentions that the signal subspace approach, which has an in-built Wiener filtering, works well for noisy signals but not very well for poor SNR conditions and colored noise. This suggests that Wiener filtering can help improve the quality of noisy speech signals, but its effectiveness may depend on the type and level of noise present in the signal.&#10;4. The difference between using a Wiener filter and cepstral features in the baseline system for converting the cepstrum is not explicitly stated in the given transcript. However, it can be inferred that the baseline system uses cepstral features as input to the HMM (Hidden Markov Model) for speech recognition. The addition of a Wiener filter as a pre-processing step can help improve the quality of the cepstral features and thus potentially improve the performance of the speech recognition system.&#10;5. Overall, the use of Wiener filters and cepstral features in speech processing can help reduce noise and improve signal quality, which can lead to better performance in speech recognition systems. However, their effectiveness may depend on the type and level of noise present in the signal.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C mentioned that they abandoned the use of lapel microphones because they were not too hot or too cold, but the distance from the mouth resulted in more background noise. They also caused issues with getting rid of noise and channel effect. Instead, they are using wireless headset microphones, which can be closer to the speaker's mouth and reduce background noise." target="Speaker: Professor C&#10;Content: Uh , is it the twenty - fourth ?&#10;Speaker: PhD F&#10;Content: now we 're on .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh Chuck , is the mike type wireless {disfmarker}&#10;Speaker: PhD F&#10;Content: Yes .&#10;Speaker: PhD A&#10;Content: wireless headset ? OK .&#10;Speaker: PhD F&#10;Content: Yes .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: For you it is .&#10;Speaker: Professor C&#10;Content: Yeah . We uh {disfmarker} we abandoned the lapel because they sort of were not too {disfmarker} not too hot , not too cold , they were {disfmarker} you know , they were {vocalsound} uh , far enough away that you got more background noise , uh , and uh {disfmarker} and so forth&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Professor C&#10;Content: but they weren't so close that they got quite the {disfmarker} you">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C mentioned that they abandoned the use of lapel microphones because they were not too hot or too cold, but the distance from the mouth resulted in more background noise. They also caused issues with getting rid of noise and channel effect. Instead, they are using wireless headset microphones, which can be closer to the speaker's mouth and reduce background noise." target=" used for {vocalsound} uh , getting rid of the noise and the channel effect .&#10;Speaker: Professor C&#10;Content: Who is doing this ?&#10;Speaker: PhD D&#10;Content: Uh w working in the cepstral domain ? So there is one guy in Grenada ,&#10;Speaker: PhD B&#10;Content: Yeah , in Grenada one of my friend .&#10;Speaker: PhD D&#10;Content: and another in {pause} uh , Lucent that I met at ICASSP .&#10;Speaker: Professor C&#10;Content: Who 's the guy in Grenada ?&#10;Speaker: PhD D&#10;Content: uh ,&#10;Speaker: PhD B&#10;Content: Uh , Jose Carlos Segura .&#10;Speaker: Professor C&#10;Content: I don't know him .&#10;Speaker: PhD A&#10;Content: This VTS has been proposed by CMU ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: Is it {disfmarker} is it the CMU ? Yeah , yeah , OK .&#10;Speaker: PhD B&#10;Content: Yeah , yeah , yeah . Originally the idea was from CMU .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C mentioned that they abandoned the use of lapel microphones because they were not too hot or too cold, but the distance from the mouth resulted in more background noise. They also caused issues with getting rid of noise and channel effect. Instead, they are using wireless headset microphones, which can be closer to the speaker's mouth and reduce background noise." target="aker: Professor C&#10;Content: There 's uh , some kinds of junk that you get with these things that you don't get with the lapel uh , little mouth clicks and breaths and so forth are worse with these than with the lapel , but given the choice we {disfmarker} there seemed to be very strong opinions for uh , getting rid of lapels .&#10;Speaker: PhD A&#10;Content: The mike number is {disfmarker}&#10;Speaker: Professor C&#10;Content: So ,&#10;Speaker: PhD F&#10;Content: Uh , your mike number 's written on the back of that unit there .&#10;Speaker: PhD A&#10;Content: Oh yeah . One .&#10;Speaker: PhD F&#10;Content: And then the channel number 's usually one less than that .&#10;Speaker: PhD A&#10;Content: Oh , OK . OK .&#10;Speaker: PhD F&#10;Content: It - it 's one less than what 's written on the back of your {disfmarker}&#10;Speaker: PhD A&#10;Content: OK . OK .&#10;Speaker: PhD F&#10;Content: yeah . So you should be zero , actually .&#10;Speaker: PhD A">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C mentioned that they abandoned the use of lapel microphones because they were not too hot or too cold, but the distance from the mouth resulted in more background noise. They also caused issues with getting rid of noise and channel effect. Instead, they are using wireless headset microphones, which can be closer to the speaker's mouth and reduce background noise." target="aker: Professor C&#10;Content: Cool . Um , so maybe uh , just briefly , you could remind us about the related experiments . Cuz you did some stuff that you talked about last week , I guess ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Um , where you were also combining something {disfmarker} both of you I guess were both combining something from the uh , French Telecom system with {vocalsound} the u uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: I {disfmarker} I don't know whether it was system one or system two , or {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Mm - hmm . It was system one . So&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: we {disfmarker} The main thing that we did is just to take the spectral subtraction from the France Telecom , which provide us some speech samples that are uh , with noise removed .&#10;Speaker: Professor C&#10;Content: So I let me {d">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C mentioned that they abandoned the use of lapel microphones because they were not too hot or too cold, but the distance from the mouth resulted in more background noise. They also caused issues with getting rid of noise and channel effect. Instead, they are using wireless headset microphones, which can be closer to the speaker's mouth and reduce background noise." target=" Professor C&#10;Content: Hmm .&#10;Speaker: PhD F&#10;Content: Um .&#10;Speaker: Professor C&#10;Content: OK . We can ask him sometime .&#10;Speaker: PhD F&#10;Content: But {disfmarker} Yeah . I don't know whether it monitors the keyboard or actually looks at the console TTY , so maybe if you echoed something to the you know , dev {disfmarker} dev console or something .&#10;Speaker: Professor C&#10;Content: You probably wouldn't ordinarily , though . Yeah . Right ? You probably wouldn't ordinarily .&#10;Speaker: PhD F&#10;Content: Hmm ?&#10;Speaker: Professor C&#10;Content: I mean you sort of {disfmarker} you 're at home and you 're trying to log in , and it takes forever to even log you in , and you probably go , &quot; screw this &quot; ,&#10;Speaker: PhD F&#10;Content: Yeah , yeah .&#10;Speaker: Professor C&#10;Content: and {disfmarker} {vocalsound} You know .&#10;Speaker: PhD F&#10;Content: Yeah . Yeah , so , um ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C mentioned that they abandoned the use of lapel microphones because they were not too hot or too cold, but the distance from the mouth resulted in more background noise. They also caused issues with getting rid of noise and channel effect. Instead, they are using wireless headset microphones, which can be closer to the speaker's mouth and reduce background noise." target=" Professor C&#10;Content: I remember I {disfmarker} I forget whether it was when the Rutgers or {disfmarker} or Hopkins workshop , I remember one of the workshops I was at there were {disfmarker} everybody was real excited cuz they got twenty - five machines and there was some kind of P - make like thing that sit sent things out .&#10;Speaker: PhD F&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: So all twenty - five people were sending things to all twenty - five machines&#10;Speaker: PhD F&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Professor C&#10;Content: and {vocalsound} and things were a lot less efficient than if you 'd just use your own machine .&#10;Speaker: PhD F&#10;Content: Yeah . Yep . Yeah , exactly . Yeah , you have to be a little bit careful .&#10;Speaker: Professor C&#10;Content: as I recall , but . Yeah .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: PhD F&#10;Content: Um , but uh , you can also {disfmarker} If you have">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C mentioned that they abandoned the use of lapel microphones because they were not too hot or too cold, but the distance from the mouth resulted in more background noise. They also caused issues with getting rid of noise and channel effect. Instead, they are using wireless headset microphones, which can be closer to the speaker's mouth and reduce background noise." target="1. Distance from the mouth: The lapel microphones were placed too far from the speaker's mouth, which resulted in more background noise and made it difficult to isolate the speaker's voice.&#10;2. Channel effect and noise issues: The lapel microphones also caused problems with getting rid of noise and channel effect, making them less suitable for clear audio recordings compared to wireless headset microphones.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C mentioned that they abandoned the use of lapel microphones because they were not too hot or too cold, but the distance from the mouth resulted in more background noise. They also caused issues with getting rid of noise and channel effect. Instead, they are using wireless headset microphones, which can be closer to the speaker's mouth and reduce background noise." target="When deciding between using FFT bins or mel energies for processing a proposal, the following factors should be considered:&#10;&#10;1. Purpose of processing: If the goal is to reconstruct speech, then using FFT bins might be a better option as it can provide more accurate information about the original signal. On the other hand, if the purpose is not to reconstruct speech but rather for speech recognition, there may not be much difference between using FFT bins or mel energies, since a linear weighting will be applied after processing in either case.&#10;2. Computational cost: Mel energies might be less computationally expensive than FFT bins, as they involve applying a mel filter bank to the power spectrum of the signal, which can be faster and more efficient than calculating FFT bins directly.&#10;3. Feature extraction: Using FFT bins or mel energies can lead to different features being extracted from the signal, which might have different implications for downstream tasks such as speech recognition or noise reduction. Careful consideration should be given to the specific requirements of the task at hand and the characteristics of the input signal when choosing between these two options.&#10;&#10;Therefore, whether reconstructing speech is a relevant consideration in this choice depends on the specific purpose of processing. If the goal is to preserve as much information about the original speech signal as possible, then using FFT bins might be more appropriate. However, if the focus is on recognizing or analyzing speech signals rather than reconstructing them, then mel energies could be a viable and potentially more efficient alternative.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C mentioned that they abandoned the use of lapel microphones because they were not too hot or too cold, but the distance from the mouth resulted in more background noise. They also caused issues with getting rid of noise and channel effect. Instead, they are using wireless headset microphones, which can be closer to the speaker's mouth and reduce background noise." target="Based on the discussion, an eighth-order PC (autocorrelation) polynomial with real coefficients will have complex roots that come in conjugate pairs. Each pair corresponds to a positive and negative frequency. In the context of synthetic speech, this would result in three or four peak positions, which can be useful for signal processing techniques such as spectral subtraction, KL transform, or inverse filtering. Additionally, higher-order cumulants or neural networks could further improve the performance on noisy speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript provided, Professor C mentioned that they abandoned the use of lapel microphones because they were not too hot or too cold, but the distance from the mouth resulted in more background noise. They also caused issues with getting rid of noise and channel effect. Instead, they are using wireless headset microphones, which can be closer to the speaker's mouth and reduce background noise." target="1. The parameters that PhD B referred to when mentioning trying to include another new parameter to the traditional parameter in their voicing research are the cepstrum coefficient, auto-correlation, R-zero, and R-one over R-zero. These parameters are likely being used to extend the coefficients of the mel filter bank and approximate the signal's spectrum for further analysis and feature extraction in speech processing or pattern recognition applications.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: Professor C&#10;Content: Uh , is it the twenty - fourth ?&#10;Speaker: PhD F&#10;Content: now we 're on .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh Chuck , is the mike type wireless {disfmarker}&#10;Speaker: PhD F&#10;Content: Yes .&#10;Speaker: PhD A&#10;Content: wireless headset ? OK .&#10;Speaker: PhD F&#10;Content: Yes .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: For you it is .&#10;Speaker: Professor C&#10;Content: Yeah . We uh {disfmarker} we abandoned the lapel because they sort of were not too {disfmarker} not too hot , not too cold , they were {disfmarker} you know , they were {vocalsound} uh , far enough away that you got more background noise , uh , and uh {disfmarker} and so forth&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Professor C&#10;Content: but they weren't so close that they got quite the {disfmarker} you" target="1. Distance from the mouth: The lapel microphones were placed too far from the speaker's mouth, which resulted in more background noise and made it difficult to isolate the speaker's voice.&#10;2. Channel effect and noise issues: The lapel microphones also caused problems with getting rid of noise and channel effect, making them less suitable for clear audio recordings compared to wireless headset microphones.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: Professor C&#10;Content: There 's uh , some kinds of junk that you get with these things that you don't get with the lapel uh , little mouth clicks and breaths and so forth are worse with these than with the lapel , but given the choice we {disfmarker} there seemed to be very strong opinions for uh , getting rid of lapels .&#10;Speaker: PhD A&#10;Content: The mike number is {disfmarker}&#10;Speaker: Professor C&#10;Content: So ,&#10;Speaker: PhD F&#10;Content: Uh , your mike number 's written on the back of that unit there .&#10;Speaker: PhD A&#10;Content: Oh yeah . One .&#10;Speaker: PhD F&#10;Content: And then the channel number 's usually one less than that .&#10;Speaker: PhD A&#10;Content: Oh , OK . OK .&#10;Speaker: PhD F&#10;Content: It - it 's one less than what 's written on the back of your {disfmarker}&#10;Speaker: PhD A&#10;Content: OK . OK .&#10;Speaker: PhD F&#10;Content: yeah . So you should be zero , actually .&#10;Speaker: PhD A" target="1. Distance from the mouth: The lapel microphones were placed too far from the speaker's mouth, which resulted in more background noise and made it difficult to isolate the speaker's voice.&#10;2. Channel effect and noise issues: The lapel microphones also caused problems with getting rid of noise and channel effect, making them less suitable for clear audio recordings compared to wireless headset microphones.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: Professor C&#10;Content: Cool . Um , so maybe uh , just briefly , you could remind us about the related experiments . Cuz you did some stuff that you talked about last week , I guess ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Um , where you were also combining something {disfmarker} both of you I guess were both combining something from the uh , French Telecom system with {vocalsound} the u uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: I {disfmarker} I don't know whether it was system one or system two , or {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Mm - hmm . It was system one . So&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: we {disfmarker} The main thing that we did is just to take the spectral subtraction from the France Telecom , which provide us some speech samples that are uh , with noise removed .&#10;Speaker: Professor C&#10;Content: So I let me {d" target="1. Distance from the mouth: The lapel microphones were placed too far from the speaker's mouth, which resulted in more background noise and made it difficult to isolate the speaker's voice.&#10;2. Channel effect and noise issues: The lapel microphones also caused problems with getting rid of noise and channel effect, making them less suitable for clear audio recordings compared to wireless headset microphones.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: Professor C&#10;Content: Cool . Um , so maybe uh , just briefly , you could remind us about the related experiments . Cuz you did some stuff that you talked about last week , I guess ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Um , where you were also combining something {disfmarker} both of you I guess were both combining something from the uh , French Telecom system with {vocalsound} the u uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: I {disfmarker} I don't know whether it was system one or system two , or {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Mm - hmm . It was system one . So&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: we {disfmarker} The main thing that we did is just to take the spectral subtraction from the France Telecom , which provide us some speech samples that are uh , with noise removed .&#10;Speaker: Professor C&#10;Content: So I let me {d" target="To help the person track the peaks in the PLP (Perceptual Linear Prediction) LPC (Linear Predictive Coding) spectra and compare them to listener tests over time, the following technique was used:&#10;&#10;1. The roots of the LPC polynomial were found using the PLP LPC coefficients, as suggested by Stephane. This allowed for tracking the peaks in the PLP LPC spectra.&#10;2. A psycho-acoustic spectrum was provided to compare the energy distribution over time with the listener tests.&#10;3. The peaks in the PLP LPC spectra were tracked using the roots of the LPC polynomial, enabling a comparison of how they change over time.&#10;&#10;This technique enabled a detailed analysis of how the spectral peaks behave and evolve, making it possible to compare them with listener test results.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: Professor C&#10;Content: Cool . Um , so maybe uh , just briefly , you could remind us about the related experiments . Cuz you did some stuff that you talked about last week , I guess ?&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: Um , where you were also combining something {disfmarker} both of you I guess were both combining something from the uh , French Telecom system with {vocalsound} the u uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: Professor C&#10;Content: I {disfmarker} I don't know whether it was system one or system two , or {disfmarker} ?&#10;Speaker: PhD D&#10;Content: Mm - hmm . It was system one . So&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD D&#10;Content: we {disfmarker} The main thing that we did is just to take the spectral subtraction from the France Telecom , which provide us some speech samples that are uh , with noise removed .&#10;Speaker: Professor C&#10;Content: So I let me {d" target="1. PhD A has been working on a signal subspace approach for speech enhancement, specifically an algorithm that decomposes the noisy signal into signal and noise subspaces to estimate clean speech. They have been testing this on Matlab and are trying to optimize the performance by adjusting the CPU usage.&#10;2. PhD A also tried to implement online normalization in their algorithm but found that it did not improve the results.&#10;3. In addition, PhD A combined endpoint information with the baseline system as another related experiment.&#10;4. However, there is no explicit mention of positive results from these experiments in the transcript. Therefore, it is unclear if they have found any positive results to port it to C and update it in the shared repository.">
      <data key="d0">1</data>
    </edge>
    <edge source=" Professor C&#10;Content: I remember I {disfmarker} I forget whether it was when the Rutgers or {disfmarker} or Hopkins workshop , I remember one of the workshops I was at there were {disfmarker} everybody was real excited cuz they got twenty - five machines and there was some kind of P - make like thing that sit sent things out .&#10;Speaker: PhD F&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: Professor C&#10;Content: So all twenty - five people were sending things to all twenty - five machines&#10;Speaker: PhD F&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Professor C&#10;Content: and {vocalsound} and things were a lot less efficient than if you 'd just use your own machine .&#10;Speaker: PhD F&#10;Content: Yeah . Yep . Yeah , exactly . Yeah , you have to be a little bit careful .&#10;Speaker: Professor C&#10;Content: as I recall , but . Yeah .&#10;Speaker: PhD D&#10;Content: Hmm .&#10;Speaker: PhD F&#10;Content: Um , but uh , you can also {disfmarker} If you have" target="1. Kick-off meeting: The group plans to discuss having a kick-off meeting. No details about the meeting are provided in the transcript.&#10;2. Progress reports: It is mentioned that there will be individual progress reports during the session. However, no specific reports are discussed in the transcript.&#10;3. Plans for the summer: Professor C mentions that Sunil is present for the summer and they plan to talk about a kick-off meeting and individual plans between then and the end of the summer.&#10;4. Update on compute stuff happening around the group: PhD F shares an update about new machines ordered, a compute farm setup, and using P-make and Customs for running jobs on the machines. They also mention that they need to be careful with parallelization to ensure efficiency.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Wiener filter and cepstral features are both techniques used in speech processing to reduce noise and improve signal quality. A Wiener filter is a type of filter used to minimize the mean square error between the desired signal and the filtered signal, while cepstral features are a representation of the spectral envelope of a speech signal.&#10;2. In the given transcript, PhD D mentions that in the handset used for the experiment, a Wiener filter is applied first and then cepstral features are computed. This suggests that the Wiener filter is used as a pre-processing step to reduce noise before extracting cepstral features.&#10;3. PhD A mentions that the signal subspace approach, which has an in-built Wiener filtering, works well for noisy signals but not very well for poor SNR conditions and colored noise. This suggests that Wiener filtering can help improve the quality of noisy speech signals, but its effectiveness may depend on the type and level of noise present in the signal.&#10;4. The difference between using a Wiener filter and cepstral features in the baseline system for converting the cepstrum is not explicitly stated in the given transcript. However, it can be inferred that the baseline system uses cepstral features as input to the HMM (Hidden Markov Model) for speech recognition. The addition of a Wiener filter as a pre-processing step can help improve the quality of the cepstral features and thus potentially improve the performance of the speech recognition system.&#10;5. Overall, the use of Wiener filters and cepstral features in speech processing can help reduce noise and improve signal quality, which can lead to better performance in speech recognition systems. However, their effectiveness may depend on the type and level of noise present in the signal." target="aker: PhD A&#10;Content: The signal subspace ? The {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: The signal subspace approach has actually an in - built Wiener filtering in it .&#10;Speaker: Professor C&#10;Content: Oh , OK .&#10;Speaker: PhD A&#10;Content: Yeah . It is like a KL transform followed by a Wiener filter . Is the signal is {disfmarker} is a signal substrate .&#10;Speaker: Professor C&#10;Content: Oh , oh , OK so the difference is the KL .&#10;Speaker: PhD A&#10;Content: So , the {disfmarker} the different {disfmarker} the c the {disfmarker} the advantage of combining two things is mainly coming from the signal subspace approach doesn't work very well if the SNR is very bad . It 's {disfmarker} it works very poorly with the poor SNR conditions , and in colored noise .&#10;Speaker: Professor C&#10;Content: I see . So essentially you could do simple spectral subtraction , followed by a KL transform , followed by a&#10;Speaker: PhD A&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Wiener filter and cepstral features are both techniques used in speech processing to reduce noise and improve signal quality. A Wiener filter is a type of filter used to minimize the mean square error between the desired signal and the filtered signal, while cepstral features are a representation of the spectral envelope of a speech signal.&#10;2. In the given transcript, PhD D mentions that in the handset used for the experiment, a Wiener filter is applied first and then cepstral features are computed. This suggests that the Wiener filter is used as a pre-processing step to reduce noise before extracting cepstral features.&#10;3. PhD A mentions that the signal subspace approach, which has an in-built Wiener filtering, works well for noisy signals but not very well for poor SNR conditions and colored noise. This suggests that Wiener filtering can help improve the quality of noisy speech signals, but its effectiveness may depend on the type and level of noise present in the signal.&#10;4. The difference between using a Wiener filter and cepstral features in the baseline system for converting the cepstrum is not explicitly stated in the given transcript. However, it can be inferred that the baseline system uses cepstral features as input to the HMM (Hidden Markov Model) for speech recognition. The addition of a Wiener filter as a pre-processing step can help improve the quality of the cepstral features and thus potentially improve the performance of the speech recognition system.&#10;5. Overall, the use of Wiener filters and cepstral features in speech processing can help reduce noise and improve signal quality, which can lead to better performance in speech recognition systems. However, their effectiveness may depend on the type and level of noise present in the signal." target="Content: I see . So essentially you could do simple spectral subtraction , followed by a KL transform , followed by a&#10;Speaker: PhD A&#10;Content: Wiener filtering . It 's a {disfmarker} it 's a cascade of two s&#10;Speaker: Professor C&#10;Content: Wiener filter . Yeah , in general , you don't {disfmarker} that 's right you don't wanna othorg orthogonalize if the things are noisy . Actually . Um , that was something that uh , Herve and I were talking about with um , the multi - band stuff , that if you 're converting things to from uh , bands , groups of bands into cepstral coef you know , local sort of local cepstral coefficients that it 's not that great to do it if it 's noisy .&#10;Speaker: PhD A&#10;Content: Mm - hmm . OK . Yeah . So .&#10;Speaker: Professor C&#10;Content: Uh , so .&#10;Speaker: PhD A&#10;Content: So that {disfmarker} that 's one reason maybe we could combine s some {disfmarker} something to improve SNR a little bit">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Wiener filter and cepstral features are both techniques used in speech processing to reduce noise and improve signal quality. A Wiener filter is a type of filter used to minimize the mean square error between the desired signal and the filtered signal, while cepstral features are a representation of the spectral envelope of a speech signal.&#10;2. In the given transcript, PhD D mentions that in the handset used for the experiment, a Wiener filter is applied first and then cepstral features are computed. This suggests that the Wiener filter is used as a pre-processing step to reduce noise before extracting cepstral features.&#10;3. PhD A mentions that the signal subspace approach, which has an in-built Wiener filtering, works well for noisy signals but not very well for poor SNR conditions and colored noise. This suggests that Wiener filtering can help improve the quality of noisy speech signals, but its effectiveness may depend on the type and level of noise present in the signal.&#10;4. The difference between using a Wiener filter and cepstral features in the baseline system for converting the cepstrum is not explicitly stated in the given transcript. However, it can be inferred that the baseline system uses cepstral features as input to the HMM (Hidden Markov Model) for speech recognition. The addition of a Wiener filter as a pre-processing step can help improve the quality of the cepstral features and thus potentially improve the performance of the speech recognition system.&#10;5. Overall, the use of Wiener filters and cepstral features in speech processing can help reduce noise and improve signal quality, which can lead to better performance in speech recognition systems. However, their effectiveness may depend on the type and level of noise present in the signal." target="1. When using the new feature with a neural network compared to just the cepstrum or other features in Italian and Spanish datasets, there is no significant difference in performance. The new feature sometimes results in slightly better performance, and sometimes it's slightly worse, but not statistically significant (PhD B).&#10;2. Without using the neural network, adding an additional binary feature to the cepstrum worsens the results. This additional feature helps discriminate between speech and non-speech frames, which improves performance when using a neural network (PhD A).&#10;3. There is potential useful information in comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network. However, these methods may not yield significantly different results compared to traditional methods (implied from the discussion).">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Wiener filter and cepstral features are both techniques used in speech processing to reduce noise and improve signal quality. A Wiener filter is a type of filter used to minimize the mean square error between the desired signal and the filtered signal, while cepstral features are a representation of the spectral envelope of a speech signal.&#10;2. In the given transcript, PhD D mentions that in the handset used for the experiment, a Wiener filter is applied first and then cepstral features are computed. This suggests that the Wiener filter is used as a pre-processing step to reduce noise before extracting cepstral features.&#10;3. PhD A mentions that the signal subspace approach, which has an in-built Wiener filtering, works well for noisy signals but not very well for poor SNR conditions and colored noise. This suggests that Wiener filtering can help improve the quality of noisy speech signals, but its effectiveness may depend on the type and level of noise present in the signal.&#10;4. The difference between using a Wiener filter and cepstral features in the baseline system for converting the cepstrum is not explicitly stated in the given transcript. However, it can be inferred that the baseline system uses cepstral features as input to the HMM (Hidden Markov Model) for speech recognition. The addition of a Wiener filter as a pre-processing step can help improve the quality of the cepstral features and thus potentially improve the performance of the speech recognition system.&#10;5. Overall, the use of Wiener filters and cepstral features in speech processing can help reduce noise and improve signal quality, which can lead to better performance in speech recognition systems. However, their effectiveness may depend on the type and level of noise present in the signal." target="The addition of a binary speech-silence feature to the cepstrum and training the HMM on it resulted in significant improvement for the Italian dataset, but there was no noticeable effect for the TI-digits dataset. This is because the TI-digits dataset did not have any substantial distinction between speech and non-speech frames to benefit from this additional feature. In contrast, the Italian dataset showed a considerable improvement due to the presence of more noise in the recordings. The binary speech-silence feature helped discriminate between speech and non-speech frames, which improved overall performance when using a neural network. However, without using a neural network, adding the binary feature worsened the results for both datasets, as it did not have any help in discriminating between speech and non-speech frames.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD A&#10;Content: The signal subspace ? The {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: The signal subspace approach has actually an in - built Wiener filtering in it .&#10;Speaker: Professor C&#10;Content: Oh , OK .&#10;Speaker: PhD A&#10;Content: Yeah . It is like a KL transform followed by a Wiener filter . Is the signal is {disfmarker} is a signal substrate .&#10;Speaker: Professor C&#10;Content: Oh , oh , OK so the difference is the KL .&#10;Speaker: PhD A&#10;Content: So , the {disfmarker} the different {disfmarker} the c the {disfmarker} the advantage of combining two things is mainly coming from the signal subspace approach doesn't work very well if the SNR is very bad . It 's {disfmarker} it works very poorly with the poor SNR conditions , and in colored noise .&#10;Speaker: Professor C&#10;Content: I see . So essentially you could do simple spectral subtraction , followed by a KL transform , followed by a&#10;Speaker: PhD A&#10;Content" target="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To submit jobs more efficiently, it is suggested to use a script that calls the &quot;run command&quot; on each sub-job. However, you should be careful not to saturate the network by running too many jobs at once. A safe number would be around ten jobs running simultaneously.&#10;2. There isn't a specific command to distribute a job to the fastest available machine while duplicating the user's environment. However, using the &quot;run command&quot; will help you find the fastest available machine to run your job. To ensure your job runs on a machine with similar characteristics as your local machine, use attributes like &quot;Linux&quot; or specify an individual machine by its attribute (the name of itself) in the attribute lines when submitting the job.&#10;3. For specific tasks like running an HTK training job that takes a long time, you can utilize the &quot;run command&quot; to find another machine and run your job there. This way, you don't have to write a P-make script or wait for your typical machine to become available." target=" which your job won't be evicted from .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: But , the machines that don't have that attribute , if a job gets fired up on that , which could be somebody 's desktop machine , and {disfmarker} and they were at lunch ,&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: they come back from lunch and they start typing on the console , then your machine will get evicted {disfmarker} your job {comment} will get evicted from their machine and be restarted on another machine . Automatically . So {disfmarker} which can cause you to lose time , right ? If you had a two hour job , and it got halfway through and then somebody came back to their machine and it got evicted . So . If you don't want your job to run on a machine where it could be evicted , then you give it the minus {disfmarker} the attribute , you know , &quot; no evict &quot; , and it 'll pick a machine that it can't be evicted from . So .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To submit jobs more efficiently, it is suggested to use a script that calls the &quot;run command&quot; on each sub-job. However, you should be careful not to saturate the network by running too many jobs at once. A safe number would be around ten jobs running simultaneously.&#10;2. There isn't a specific command to distribute a job to the fastest available machine while duplicating the user's environment. However, using the &quot;run command&quot; will help you find the fastest available machine to run your job. To ensure your job runs on a machine with similar characteristics as your local machine, use attributes like &quot;Linux&quot; or specify an individual machine by its attribute (the name of itself) in the attribute lines when submitting the job.&#10;3. For specific tasks like running an HTK training job that takes a long time, you can utilize the &quot;run command&quot; to find another machine and run your job there. This way, you don't have to write a P-make script or wait for your typical machine to become available." target="Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad G&#10;Content: So , um , let 's say I have like , a thousand little {disfmarker} little jobs to do ?&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad G&#10;Content: Um , how do I do it with &quot; run command &quot; ? I mean do {disfmarker}&#10;Speaker: PhD F&#10;Content: You could write a script uh , which called run command on each sub - job&#10;Speaker: Grad G&#10;Content: Uh - huh . A thousand times ?&#10;Speaker: PhD F&#10;Content: right ? But you probably wanna be careful with that&#10;Speaker: Grad G&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: because um , you don't wanna saturate the network . Uh , so , um , you know , you should {disfmarker} you should probably not run more than , say ten jobs yourself at any one time , uh , just because then it would keep other people {disfmarker}&#10;Speaker: Grad G&#10;Content: Oh , too much file transfer and stuff .">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To submit jobs more efficiently, it is suggested to use a script that calls the &quot;run command&quot; on each sub-job. However, you should be careful not to saturate the network by running too many jobs at once. A safe number would be around ten jobs running simultaneously.&#10;2. There isn't a specific command to distribute a job to the fastest available machine while duplicating the user's environment. However, using the &quot;run command&quot; will help you find the fastest available machine to run your job. To ensure your job runs on a machine with similar characteristics as your local machine, use attributes like &quot;Linux&quot; or specify an individual machine by its attribute (the name of itself) in the attribute lines when submitting the job.&#10;3. For specific tasks like running an HTK training job that takes a long time, you can utilize the &quot;run command&quot; to find another machine and run your job there. This way, you don't have to write a P-make script or wait for your typical machine to become available." target=" example if you only want your thing to run under Linux , you can give it the Linux attribute , and then it will find the fastest available Linux machine and run it on that . So . You can control where your jobs go , to a certain extent , all the way down to an individual machine . Each machine has an attribute which is the name of itself . So you can give that as an attribute and it 'll only run on that . If there 's already a job running , on some machine that you 're trying to select , your job will get queued up , and then when that resource , that machine becomes available , your job will get exported there . So , there 's a lot of nice features to it and it kinda helps to balance the load of the machines and uh , right now Andreas and I have been the main ones using it and we 're {disfmarker} Uh . The SRI recognizer has all this P - make customs stuff built into it .&#10;Speaker: Professor C&#10;Content: So as I understand , you know , he 's using all the machines and you 're using all the machines ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: Professor C&#10;Content: is">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To submit jobs more efficiently, it is suggested to use a script that calls the &quot;run command&quot; on each sub-job. However, you should be careful not to saturate the network by running too many jobs at once. A safe number would be around ten jobs running simultaneously.&#10;2. There isn't a specific command to distribute a job to the fastest available machine while duplicating the user's environment. However, using the &quot;run command&quot; will help you find the fastest available machine to run your job. To ensure your job runs on a machine with similar characteristics as your local machine, use attributes like &quot;Linux&quot; or specify an individual machine by its attribute (the name of itself) in the attribute lines when submitting the job.&#10;3. For specific tasks like running an HTK training job that takes a long time, you can utilize the &quot;run command&quot; to find another machine and run your job there. This way, you don't have to write a P-make script or wait for your typical machine to become available." target=" at once . And then it 'll make sure that it never goes above that .&#10;Speaker: Grad G&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: So ,&#10;Speaker: Grad G&#10;Content: Right . OK .&#10;Speaker: PhD F&#10;Content: I can get some documentation .&#10;Speaker: PhD D&#10;Content: So it {disfmarker} it 's {disfmarker} it 's not systematically queued . I mean all the jobs are running . If you launch twenty jobs , they are all running . Alright .&#10;Speaker: PhD F&#10;Content: It depends . If you {disfmarker} &quot; Run command &quot; , that I mentioned before , is {disfmarker} doesn't know about other things that you might be running .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: So , it would be possible to run a hundred run jobs at once ,&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: and they wouldn't know about each other . But if you use P - make , then , it knows about all the jobs that it has">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To submit jobs more efficiently, it is suggested to use a script that calls the &quot;run command&quot; on each sub-job. However, you should be careful not to saturate the network by running too many jobs at once. A safe number would be around ten jobs running simultaneously.&#10;2. There isn't a specific command to distribute a job to the fastest available machine while duplicating the user's environment. However, using the &quot;run command&quot; will help you find the fastest available machine to run your job. To ensure your job runs on a machine with similar characteristics as your local machine, use attributes like &quot;Linux&quot; or specify an individual machine by its attribute (the name of itself) in the attribute lines when submitting the job.&#10;3. For specific tasks like running an HTK training job that takes a long time, you can utilize the &quot;run command&quot; to find another machine and run your job there. This way, you don't have to write a P-make script or wait for your typical machine to become available." target=" the machines and you 're using all the machines ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: Professor C&#10;Content: is the rough division of {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah . Exactly . Yeah , you know , I {disfmarker} I sort of got started {comment} using the recognizer just recently and uh , uh I fired off a training job , and then I fired off a recognition job and I get this email about midnight from Andreas saying , &quot; uh , are you running two {vocalsound} trainings simultaneously s my m my jobs are not getting run . &quot; So I had to back off a little bit . But , soon as we get some more machines then uh {disfmarker} then we 'll have more compute available . So , um , that 's just a quick update about what we 've got . So .&#10;Speaker: Grad G&#10;Content: Um , I have {disfmarker} I have a question about the uh , parallelization ?&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad G&#10;Content: So , um , let 's say I">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To submit jobs more efficiently, it is suggested to use a script that calls the &quot;run command&quot; on each sub-job. However, you should be careful not to saturate the network by running too many jobs at once. A safe number would be around ten jobs running simultaneously.&#10;2. There isn't a specific command to distribute a job to the fastest available machine while duplicating the user's environment. However, using the &quot;run command&quot; will help you find the fastest available machine to run your job. To ensure your job runs on a machine with similar characteristics as your local machine, use attributes like &quot;Linux&quot; or specify an individual machine by its attribute (the name of itself) in the attribute lines when submitting the job.&#10;3. For specific tasks like running an HTK training job that takes a long time, you can utilize the &quot;run command&quot; to find another machine and run your job there. This way, you don't have to write a P-make script or wait for your typical machine to become available." target=" , uh if you didn't wanna write a P - make script and you just had a , uh {disfmarker} an HTK training job that you know is gonna take uh , six hours to run , and somebody 's using , uh , the machine you typically use , you can say &quot; run command &quot; and your HTK thing and it 'll find another machine , the fastest currently available machine and {disfmarker} and run your job there .&#10;Speaker: Professor C&#10;Content: Now , does it have the same sort of behavior as P - make , which is that , you know , if you run something on somebody 's machine and they come in and hit a key then it {disfmarker}&#10;Speaker: PhD F&#10;Content: Yes . Yeah , there are um {disfmarker} Right . So some of the machines at the institute , um , have this attribute called &quot; no evict &quot; . And if you specify that , in {disfmarker} in one of your attribute lines , then it 'll go to a machine which your job won't be evicted from .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To submit jobs more efficiently, it is suggested to use a script that calls the &quot;run command&quot; on each sub-job. However, you should be careful not to saturate the network by running too many jobs at once. A safe number would be around ten jobs running simultaneously.&#10;2. There isn't a specific command to distribute a job to the fastest available machine while duplicating the user's environment. However, using the &quot;run command&quot; will help you find the fastest available machine to run your job. To ensure your job runs on a machine with similar characteristics as your local machine, use attributes like &quot;Linux&quot; or specify an individual machine by its attribute (the name of itself) in the attribute lines when submitting the job.&#10;3. For specific tasks like running an HTK training job that takes a long time, you can utilize the &quot;run command&quot; to find another machine and run your job there. This way, you don't have to write a P-make script or wait for your typical machine to become available." target="1. Write a script that calls the &quot;run command&quot; on each sub-job, but be careful not to run too many jobs simultaneously.&#10;2. A safe number of jobs to run at once is around ten, as running more could lead to network saturation and negatively affect other users' experiences.&#10;3. Implement logic in your script to ensure that only ten jobs are running at any given time, and fire off another job when one finishes. This approach allows you to manage many small tasks while being mindful of network resources and following best practices for job management.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. To submit jobs more efficiently, it is suggested to use a script that calls the &quot;run command&quot; on each sub-job. However, you should be careful not to saturate the network by running too many jobs at once. A safe number would be around ten jobs running simultaneously.&#10;2. There isn't a specific command to distribute a job to the fastest available machine while duplicating the user's environment. However, using the &quot;run command&quot; will help you find the fastest available machine to run your job. To ensure your job runs on a machine with similar characteristics as your local machine, use attributes like &quot;Linux&quot; or specify an individual machine by its attribute (the name of itself) in the attribute lines when submitting the job.&#10;3. For specific tasks like running an HTK training job that takes a long time, you can utilize the &quot;run command&quot; to find another machine and run your job there. This way, you don't have to write a P-make script or wait for your typical machine to become available." target="1. The &quot;run command&quot; in the P-made and Customs network is a command that allows users to execute jobs on the fastest available machine in the network. It helps distribute tasks efficiently across the network by finding the best machine based on specified attributes like memory, speed, and operating system.&#10;&#10;2. Users can use the &quot;run command&quot; to run their jobs on a specific type of machine, such as one running Linux, or even on an individual machine by specifying its attribute (the name of itself). This ensures that the job is run on a machine with similar characteristics as the user's local machine.&#10;&#10;3. The &quot;run command&quot; does not use P-make and can be used to run jobs one at a time, without knowing about other jobs running simultaneously. It's particularly useful when users don't want to write a P-make script or for specific tasks like running an HTK training job that takes a long time. This way, users can utilize the network's resources more efficiently and manage many small tasks while being mindful of network resources and following best practices for job management.&#10;&#10;4. As mentioned in the transcript, using the &quot;run command&quot; can be an alternative to P-make, especially for users who don't want to write a P-make script or when dealing with individual long-running tasks like HTK training jobs.">
      <data key="d0">1</data>
    </edge>
    <edge source="Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad G&#10;Content: So , um , let 's say I have like , a thousand little {disfmarker} little jobs to do ?&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad G&#10;Content: Um , how do I do it with &quot; run command &quot; ? I mean do {disfmarker}&#10;Speaker: PhD F&#10;Content: You could write a script uh , which called run command on each sub - job&#10;Speaker: Grad G&#10;Content: Uh - huh . A thousand times ?&#10;Speaker: PhD F&#10;Content: right ? But you probably wanna be careful with that&#10;Speaker: Grad G&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: because um , you don't wanna saturate the network . Uh , so , um , you know , you should {disfmarker} you should probably not run more than , say ten jobs yourself at any one time , uh , just because then it would keep other people {disfmarker}&#10;Speaker: Grad G&#10;Content: Oh , too much file transfer and stuff ." target="1. Write a script that calls the &quot;run command&quot; on each sub-job, but be careful not to run too many jobs simultaneously.&#10;2. A safe number of jobs to run at once is around ten, as running more could lead to network saturation and negatively affect other users' experiences.&#10;3. Implement logic in your script to ensure that only ten jobs are running at any given time, and fire off another job when one finishes. This approach allows you to manage many small tasks while being mindful of network resources and following best practices for job management.">
      <data key="d0">1</data>
    </edge>
    <edge source=" example if you only want your thing to run under Linux , you can give it the Linux attribute , and then it will find the fastest available Linux machine and run it on that . So . You can control where your jobs go , to a certain extent , all the way down to an individual machine . Each machine has an attribute which is the name of itself . So you can give that as an attribute and it 'll only run on that . If there 's already a job running , on some machine that you 're trying to select , your job will get queued up , and then when that resource , that machine becomes available , your job will get exported there . So , there 's a lot of nice features to it and it kinda helps to balance the load of the machines and uh , right now Andreas and I have been the main ones using it and we 're {disfmarker} Uh . The SRI recognizer has all this P - make customs stuff built into it .&#10;Speaker: Professor C&#10;Content: So as I understand , you know , he 's using all the machines and you 're using all the machines ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: Professor C&#10;Content: is" target="1. The &quot;run command&quot; in the P-made and Customs network is a command that allows users to execute jobs on the fastest available machine in the network. It helps distribute tasks efficiently across the network by finding the best machine based on specified attributes like memory, speed, and operating system.&#10;&#10;2. Users can use the &quot;run command&quot; to run their jobs on a specific type of machine, such as one running Linux, or even on an individual machine by specifying its attribute (the name of itself). This ensures that the job is run on a machine with similar characteristics as the user's local machine.&#10;&#10;3. The &quot;run command&quot; does not use P-make and can be used to run jobs one at a time, without knowing about other jobs running simultaneously. It's particularly useful when users don't want to write a P-make script or for specific tasks like running an HTK training job that takes a long time. This way, users can utilize the network's resources more efficiently and manage many small tasks while being mindful of network resources and following best practices for job management.&#10;&#10;4. As mentioned in the transcript, using the &quot;run command&quot; can be an alternative to P-make, especially for users who don't want to write a P-make script or when dealing with individual long-running tasks like HTK training jobs.">
      <data key="d0">1</data>
    </edge>
    <edge source=" at once . And then it 'll make sure that it never goes above that .&#10;Speaker: Grad G&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: So ,&#10;Speaker: Grad G&#10;Content: Right . OK .&#10;Speaker: PhD F&#10;Content: I can get some documentation .&#10;Speaker: PhD D&#10;Content: So it {disfmarker} it 's {disfmarker} it 's not systematically queued . I mean all the jobs are running . If you launch twenty jobs , they are all running . Alright .&#10;Speaker: PhD F&#10;Content: It depends . If you {disfmarker} &quot; Run command &quot; , that I mentioned before , is {disfmarker} doesn't know about other things that you might be running .&#10;Speaker: PhD D&#10;Content: Uh - huh .&#10;Speaker: PhD F&#10;Content: So , it would be possible to run a hundred run jobs at once ,&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD F&#10;Content: and they wouldn't know about each other . But if you use P - make , then , it knows about all the jobs that it has" target="1. Write a script that calls the &quot;run command&quot; on each sub-job, but be careful not to run too many jobs simultaneously.&#10;2. A safe number of jobs to run at once is around ten, as running more could lead to network saturation and negatively affect other users' experiences.&#10;3. Implement logic in your script to ensure that only ten jobs are running at any given time, and fire off another job when one finishes. This approach allows you to manage many small tasks while being mindful of network resources and following best practices for job management.">
      <data key="d0">1</data>
    </edge>
    <edge source=" the machines and you 're using all the machines ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: Professor C&#10;Content: is the rough division of {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah . Exactly . Yeah , you know , I {disfmarker} I sort of got started {comment} using the recognizer just recently and uh , uh I fired off a training job , and then I fired off a recognition job and I get this email about midnight from Andreas saying , &quot; uh , are you running two {vocalsound} trainings simultaneously s my m my jobs are not getting run . &quot; So I had to back off a little bit . But , soon as we get some more machines then uh {disfmarker} then we 'll have more compute available . So , um , that 's just a quick update about what we 've got . So .&#10;Speaker: Grad G&#10;Content: Um , I have {disfmarker} I have a question about the uh , parallelization ?&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad G&#10;Content: So , um , let 's say I" target="1. Write a script that calls the &quot;run command&quot; on each sub-job, but be careful not to run too many jobs simultaneously.&#10;2. A safe number of jobs to run at once is around ten, as running more could lead to network saturation and negatively affect other users' experiences.&#10;3. Implement logic in your script to ensure that only ten jobs are running at any given time, and fire off another job when one finishes. This approach allows you to manage many small tasks while being mindful of network resources and following best practices for job management.">
      <data key="d0">1</data>
    </edge>
    <edge source=" the machines and you 're using all the machines ,&#10;Speaker: PhD F&#10;Content: So .&#10;Speaker: Professor C&#10;Content: is the rough division of {disfmarker}&#10;Speaker: PhD F&#10;Content: Yeah . Exactly . Yeah , you know , I {disfmarker} I sort of got started {comment} using the recognizer just recently and uh , uh I fired off a training job , and then I fired off a recognition job and I get this email about midnight from Andreas saying , &quot; uh , are you running two {vocalsound} trainings simultaneously s my m my jobs are not getting run . &quot; So I had to back off a little bit . But , soon as we get some more machines then uh {disfmarker} then we 'll have more compute available . So , um , that 's just a quick update about what we 've got . So .&#10;Speaker: Grad G&#10;Content: Um , I have {disfmarker} I have a question about the uh , parallelization ?&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad G&#10;Content: So , um , let 's say I" target="1. Kick-off meeting: The group plans to discuss having a kick-off meeting. No details about the meeting are provided in the transcript.&#10;2. Progress reports: It is mentioned that there will be individual progress reports during the session. However, no specific reports are discussed in the transcript.&#10;3. Plans for the summer: Professor C mentions that Sunil is present for the summer and they plan to talk about a kick-off meeting and individual plans between then and the end of the summer.&#10;4. Update on compute stuff happening around the group: PhD F shares an update about new machines ordered, a compute farm setup, and using P-make and Customs for running jobs on the machines. They also mention that they need to be careful with parallelization to ensure efficiency.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , uh if you didn't wanna write a P - make script and you just had a , uh {disfmarker} an HTK training job that you know is gonna take uh , six hours to run , and somebody 's using , uh , the machine you typically use , you can say &quot; run command &quot; and your HTK thing and it 'll find another machine , the fastest currently available machine and {disfmarker} and run your job there .&#10;Speaker: Professor C&#10;Content: Now , does it have the same sort of behavior as P - make , which is that , you know , if you run something on somebody 's machine and they come in and hit a key then it {disfmarker}&#10;Speaker: PhD F&#10;Content: Yes . Yeah , there are um {disfmarker} Right . So some of the machines at the institute , um , have this attribute called &quot; no evict &quot; . And if you specify that , in {disfmarker} in one of your attribute lines , then it 'll go to a machine which your job won't be evicted from .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;" target="Based on the transcript, PhD A mentions that Guenter Hirsch is in charge of the project that is similar to TI-digits. They are potentially generating HTK scripts, but it is not clear whether they are converging on HTK or using some other speech recognition toolkit from Mississippi State. PhD A mentions &quot;Mississippi State maybe&quot; in relation to this question.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A mentions that Guenter Hirsch is in charge of the project that is similar to TI-digits. They are potentially generating HTK scripts, but it is not clear whether they are converging on HTK or using some other speech recognition toolkit from Mississippi State. PhD A mentions &quot;Mississippi State maybe&quot; in relation to this question." target="Speaker: Professor C&#10;Content: Sort of a {disfmarker} sort of like what they did with TI - digits , and ?&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: Professor C&#10;Content: Yeah , OK .&#10;Speaker: PhD A&#10;Content: I m I guess Guenter Hirsch is in charge of that . Guenter Hirsch and TI .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: Maybe Roger {disfmarker} r Roger , maybe in charge of .&#10;Speaker: Professor C&#10;Content: And then they 're {disfmarker} they 're uh , uh , generating HTK scripts to {disfmarker}&#10;Speaker: PhD A&#10;Content: Yeah . Yeah , I don't know . There are {disfmarker} they have {disfmarker} there is no {disfmarker} I don't know if they are converging on HTK or are using some Mississippi State ,&#10;Speaker: Professor C&#10;Content: Mis - Mississippi State maybe ,&#10;Speaker: PhD A&#10;Content: yeah . I 'm not sure about that .">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A mentions that Guenter Hirsch is in charge of the project that is similar to TI-digits. They are potentially generating HTK scripts, but it is not clear whether they are converging on HTK or using some other speech recognition toolkit from Mississippi State. PhD A mentions &quot;Mississippi State maybe&quot; in relation to this question." target=" um , primary detectors for these acoustic events , and using the outputs of these robust detectors to do speech recognition . Um , and , um , these {disfmarker} these primary detectors , um , will be , uh , inspired by , you know , multi - band techniques , um , doing things , um , similar to Larry Saul 's work on , uh , graphical models to {disfmarker} to detect these {disfmarker} these , uh , acoustic events . And , um , so I {disfmarker} I been {disfmarker} I been thinking about that and some of the issues that I 've been running into are , um , exactly what {disfmarker} what kind of acoustic events I need , what {disfmarker} um , what acoustic events will provide a {disfmarker} a good enough coverage to {disfmarker} in order to do the later recognition steps . And , also , um , once I decide a set of acoustic events , um , h how do I {disfmarker} how do I get labels ? Training data for {disfmarker} for these acoustic events . And , then later on down the line , I can">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A mentions that Guenter Hirsch is in charge of the project that is similar to TI-digits. They are potentially generating HTK scripts, but it is not clear whether they are converging on HTK or using some other speech recognition toolkit from Mississippi State. PhD A mentions &quot;Mississippi State maybe&quot; in relation to this question." target=" small vocabulary tasks . But we {disfmarker} we going to address this Wall Street Journal in our next stage , which is also going to be a noisy task so s very few people have reported something on using some continuous speech at all . So , there are some {disfmarker} I mean , I was looking at some literature on speech enhancement applied to large vocabulary tasks and spectral subtraction doesn't seems to be the thing to do for large vocabulary tasks . And it 's {disfmarker} Always people have shown improvement with Wiener filtering and maybe subspace approach over spectral subtraction everywhere . But if we {disfmarker} if we have to use simple spectral subtraction , we may have to do some optimization {pause} to make it work @ @ .&#10;Speaker: Professor C&#10;Content: So they 're making {disfmarker} there {disfmarker} Somebody 's generating Wall Street Journal with additive {disfmarker} artificially added noise or something ?&#10;Speaker: PhD A&#10;Content: Yeah , yeah .&#10;Speaker: Professor C&#10;Content: Sort of a {disfmarker} sort of like what they did with TI - digits , and ?&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A mentions that Guenter Hirsch is in charge of the project that is similar to TI-digits. They are potentially generating HTK scripts, but it is not clear whether they are converging on HTK or using some other speech recognition toolkit from Mississippi State. PhD A mentions &quot;Mississippi State maybe&quot; in relation to this question." target=" of these things . And that 's what most o again , most of our work has been done with that , with {disfmarker} with uh , connected digits .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Professor C&#10;Content: Um , but uh , we have recognition now with some of the continuous speech , large vocabulary continuous speech , using Switchboard {disfmarker} uh , Switchboard recognizer ,&#10;Speaker: PhD A&#10;Content: Yeah . OK .&#10;Speaker: Professor C&#10;Content: uh , no training , {vocalsound} from this , just {disfmarker} just plain using the Switchboard .&#10;Speaker: PhD A&#10;Content: Oh . You just take the Switchboard trained {disfmarker} ? Yeah ,&#10;Speaker: Professor C&#10;Content: That 's {disfmarker} that 's what we 're doing ,&#10;Speaker: PhD A&#10;Content: yeah .&#10;Speaker: Professor C&#10;Content: yeah . Now there are some adaptation though ,&#10;Speaker: PhD A&#10;Content: OK . Yeah . That 's cool .&#10;Speaker: Professor C&#10;Content: that {">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A mentions that Guenter Hirsch is in charge of the project that is similar to TI-digits. They are potentially generating HTK scripts, but it is not clear whether they are converging on HTK or using some other speech recognition toolkit from Mississippi State. PhD A mentions &quot;Mississippi State maybe&quot; in relation to this question." target="Based on the transcript, PhD A is not staying in the area permanently because they are currently residing at OGI and will be returning to that location after leaving the meeting. The exact date of their return is not specified in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A mentions that Guenter Hirsch is in charge of the project that is similar to TI-digits. They are potentially generating HTK scripts, but it is not clear whether they are converging on HTK or using some other speech recognition toolkit from Mississippi State. PhD A mentions &quot;Mississippi State maybe&quot; in relation to this question." target="Based on the discussion, an eighth-order PC (autocorrelation) polynomial with real coefficients will have complex roots that come in conjugate pairs. Each pair corresponds to a positive and negative frequency. In the context of synthetic speech, this would result in three or four peak positions, which can be useful for signal processing techniques such as spectral subtraction, KL transform, or inverse filtering. Additionally, higher-order cumulants or neural networks could further improve the performance on noisy speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A mentions that Guenter Hirsch is in charge of the project that is similar to TI-digits. They are potentially generating HTK scripts, but it is not clear whether they are converging on HTK or using some other speech recognition toolkit from Mississippi State. PhD A mentions &quot;Mississippi State maybe&quot; in relation to this question." target="1. According to the transcript, Professor C mentioned the possibility of using the SRI recognizer for testing on large vocabulary data with a distant microphone. However, it is not clear if this has been attempted already.&#10;2. The potential challenges in using the SRI recognizer for testing on large vocabulary data with a distant microphone include the following:&#10;* Fear of trying it out, as mentioned by Professor C (&quot;Yeah, cuz everybody's scared.&quot;)&#10;* Possible overloading of the CPU or hardware due to the demands of processing large vocabulary data from a distant microphone, as also mentioned by Professor C (&quot;You'll see a little smoke coming up from the CPU or something trying to do it.&quot;).&#10;* The need for robust detectors of acoustic events, which can provide good enough coverage and training data for later recognition steps. This is based on Grad G's description of their proposed PhD work on using acoustic events for speech recognition (&quot;Building robust primary detectors for these acoustic events and using the outputs of these robust detectors to do speech recognition.&quot;).&#10;* The challenges in determining what kind of acoustic events are needed, as well as obtaining labels or training data for these acoustic events, as mentioned by Grad G.">
      <data key="d0">1</data>
    </edge>
    <edge source=" um , primary detectors for these acoustic events , and using the outputs of these robust detectors to do speech recognition . Um , and , um , these {disfmarker} these primary detectors , um , will be , uh , inspired by , you know , multi - band techniques , um , doing things , um , similar to Larry Saul 's work on , uh , graphical models to {disfmarker} to detect these {disfmarker} these , uh , acoustic events . And , um , so I {disfmarker} I been {disfmarker} I been thinking about that and some of the issues that I 've been running into are , um , exactly what {disfmarker} what kind of acoustic events I need , what {disfmarker} um , what acoustic events will provide a {disfmarker} a good enough coverage to {disfmarker} in order to do the later recognition steps . And , also , um , once I decide a set of acoustic events , um , h how do I {disfmarker} how do I get labels ? Training data for {disfmarker} for these acoustic events . And , then later on down the line , I can" target="1. Distance from the mouth: The lapel microphones were placed too far from the speaker's mouth, which resulted in more background noise and made it difficult to isolate the speaker's voice.&#10;2. Channel effect and noise issues: The lapel microphones also caused problems with getting rid of noise and channel effect, making them less suitable for clear audio recordings compared to wireless headset microphones.">
      <data key="d0">1</data>
    </edge>
    <edge source=" um , primary detectors for these acoustic events , and using the outputs of these robust detectors to do speech recognition . Um , and , um , these {disfmarker} these primary detectors , um , will be , uh , inspired by , you know , multi - band techniques , um , doing things , um , similar to Larry Saul 's work on , uh , graphical models to {disfmarker} to detect these {disfmarker} these , uh , acoustic events . And , um , so I {disfmarker} I been {disfmarker} I been thinking about that and some of the issues that I 've been running into are , um , exactly what {disfmarker} what kind of acoustic events I need , what {disfmarker} um , what acoustic events will provide a {disfmarker} a good enough coverage to {disfmarker} in order to do the later recognition steps . And , also , um , once I decide a set of acoustic events , um , h how do I {disfmarker} how do I get labels ? Training data for {disfmarker} for these acoustic events . And , then later on down the line , I can" target="1. According to the transcript, Professor C mentioned the possibility of using the SRI recognizer for testing on large vocabulary data with a distant microphone. However, it is not clear if this has been attempted already.&#10;2. The potential challenges in using the SRI recognizer for testing on large vocabulary data with a distant microphone include the following:&#10;* Fear of trying it out, as mentioned by Professor C (&quot;Yeah, cuz everybody's scared.&quot;)&#10;* Possible overloading of the CPU or hardware due to the demands of processing large vocabulary data from a distant microphone, as also mentioned by Professor C (&quot;You'll see a little smoke coming up from the CPU or something trying to do it.&quot;).&#10;* The need for robust detectors of acoustic events, which can provide good enough coverage and training data for later recognition steps. This is based on Grad G's description of their proposed PhD work on using acoustic events for speech recognition (&quot;Building robust primary detectors for these acoustic events and using the outputs of these robust detectors to do speech recognition.&quot;).&#10;* The challenges in determining what kind of acoustic events are needed, as well as obtaining labels or training data for these acoustic events, as mentioned by Grad G.">
      <data key="d0">1</data>
    </edge>
    <edge source=" small vocabulary tasks . But we {disfmarker} we going to address this Wall Street Journal in our next stage , which is also going to be a noisy task so s very few people have reported something on using some continuous speech at all . So , there are some {disfmarker} I mean , I was looking at some literature on speech enhancement applied to large vocabulary tasks and spectral subtraction doesn't seems to be the thing to do for large vocabulary tasks . And it 's {disfmarker} Always people have shown improvement with Wiener filtering and maybe subspace approach over spectral subtraction everywhere . But if we {disfmarker} if we have to use simple spectral subtraction , we may have to do some optimization {pause} to make it work @ @ .&#10;Speaker: Professor C&#10;Content: So they 're making {disfmarker} there {disfmarker} Somebody 's generating Wall Street Journal with additive {disfmarker} artificially added noise or something ?&#10;Speaker: PhD A&#10;Content: Yeah , yeah .&#10;Speaker: Professor C&#10;Content: Sort of a {disfmarker} sort of like what they did with TI - digits , and ?&#10;Spe" target="1. Evaluation conditions for the Broadcast News study on noisy speech with a large vocabulary included various focus conditions that were noisy. However, spectral subtraction was not applied in this study. Instead, multi-stream methods were used.&#10;2. The speakers did not provide specific details about the exact techniques and methods employed during the Broadcast News evaluation. They only mentioned using multi-stream approaches as an alternative to spectral subtraction for noisy large vocabulary conditions.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, when PhD A added online normalization to the system with a different time constant, they found that it did not lead to an improvement over not using online normalization. They mentioned that they had to change the value of the update factor to see any improvements, but even then, it was not better than without online normalization. This suggests that the change in time constant and addition of online normalization did not result in performance improvements in this case." target=" improves by just supplying the information of the {disfmarker} I mean the w speech and nonspeech . And uh , I found that the baseline itself improves by twenty - two percent by just giving the wuh .&#10;Speaker: Professor C&#10;Content: Uh , can you back up a second , I {disfmarker} I {disfmarker} I missed something , uh , I guess my mind wandered . Ad - ad When you added the on - line normalization and so forth , uh , uh things got better again ?&#10;Speaker: PhD A&#10;Content: Yeah . No .&#10;Speaker: Professor C&#10;Content: or is it ?&#10;Speaker: PhD A&#10;Content: No . No , things didn't get better with the same time constant that we used .&#10;Speaker: Professor C&#10;Content: Did it not ? No , no . With a different time constant .&#10;Speaker: PhD A&#10;Content: With the different time constant I found that {disfmarker} I mean , I didn't get an improvement over not using on - line normalization ,&#10;Speaker: Professor C&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: because I {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, when PhD A added online normalization to the system with a different time constant, they found that it did not lead to an improvement over not using online normalization. They mentioned that they had to change the value of the update factor to see any improvements, but even then, it was not better than without online normalization. This suggests that the change in time constant and addition of online normalization did not result in performance improvements in this case." target=" using on - line normalization ,&#10;Speaker: Professor C&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: because I {disfmarker} I found that I would have change the value of the update factor .&#10;Speaker: Professor C&#10;Content: No you didn't , OK .&#10;Speaker: PhD A&#10;Content: But I didn't play it with play {disfmarker} play quite a bit to make it better than .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So , it 's still not {disfmarker}&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: I mean , the on - line normalization didn't give me any improvement .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: And uh , so ,&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: oh yeah So I just stopped there with the uh , speech enhancement . The {disfmarker} the other thing what I tried was the {disfmarker} adding the uh , endpoint information to the baseline">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, when PhD A added online normalization to the system with a different time constant, they found that it did not lead to an improvement over not using online normalization. They mentioned that they had to change the value of the update factor to see any improvements, but even then, it was not better than without online normalization. This suggests that the change in time constant and addition of online normalization did not result in performance improvements in this case." target=" , on - line normalization also on top of that . And that {disfmarker} there {disfmarker} there also I n I found that I have to make some changes to their time constant that I used because th it has a {disfmarker} a mean and variance update time constant and {disfmarker} which is not suitable for the enhanced speech , and whatever we try it on with proposal - one . But um , I didn't {disfmarker} I didn't play with that time constant a lot , I just t g I just found that I have to reduce the value {disfmarker} I mean , I have to increase the time constant , or reduce the value of the update value . That 's all I found So I have to . Uh , Yeah . And uh , uh , the other {disfmarker} other thing what I tried was , I just um , uh , took the baseline and then ran it with the endpoint inf uh th information , just the Aurora baseline , to see that how much the baseline itself improves by just supplying the information of the {disfmarker} I mean the w speech and nonspeech . And uh , I found that the baseline">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, when PhD A added online normalization to the system with a different time constant, they found that it did not lead to an improvement over not using online normalization. They mentioned that they had to change the value of the update factor to see any improvements, but even then, it was not better than without online normalization. This suggests that the change in time constant and addition of online normalization did not result in performance improvements in this case." target=" constants of the on - line normalization .&#10;Speaker: PhD A&#10;Content: Yeah , yeah . Yeah .&#10;Speaker: PhD D&#10;Content: Because if we keep the value that was submitted uh , it doesn't help at all . You can remove on - line normalization , or put it , it doesn't change anything . Uh , uh , as long as you have the spectral subtraction . But , you can still find some kind of optimum somewhere , and we don't know where exactly&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: but , uh .&#10;Speaker: PhD A&#10;Content: Yeah , I assume .&#10;Speaker: Professor C&#10;Content: So it sounds like you should look at some tables of results or something&#10;Speaker: PhD D&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and see where i where the {disfmarker} {vocalsound} where they were different and what we can learn from it .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm .">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, when PhD A added online normalization to the system with a different time constant, they found that it did not lead to an improvement over not using online normalization. They mentioned that they had to change the value of the update factor to see any improvements, but even then, it was not better than without online normalization. This suggests that the change in time constant and addition of online normalization did not result in performance improvements in this case." target="marker} well , with our modification to reduce the delay of the {disfmarker} the LDA filters ,&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: and&#10;Speaker: PhD B&#10;Content: And the filter {disfmarker}&#10;Speaker: PhD D&#10;Content: Well there are slight modifications , but it was the full proposal - one . In your case , if you tried just putting LDA , then maybe on - line normalization {disfmarker} ?&#10;Speaker: PhD A&#10;Content: Only LDA . Yeah . Af - I {disfmarker} after that I added on - line normalization , yeah .&#10;Speaker: PhD D&#10;Content: Mm - hmm . So we just tried directly to {disfmarker} to just , keep the system as it was and , um , when we plug the spectral subtraction it improves uh , signif significantly . Um , but , what seems clear also is that we have to retune the time constants of the on - line normalization .&#10;Speaker: PhD A&#10;Content: Yeah , yeah . Yeah .&#10;Speaker: PhD D&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, when PhD A added online normalization to the system with a different time constant, they found that it did not lead to an improvement over not using online normalization. They mentioned that they had to change the value of the update factor to see any improvements, but even then, it was not better than without online normalization. This suggests that the change in time constant and addition of online normalization did not result in performance improvements in this case." target="1. The slight decrease in performance for the well-matched (MM) category and significant improvement for HM is due to the addition of a new feature or technique, but it's not statistically significant for MM and only slightly worse for well-matched. However, the improvement for HM is significant.&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM.&#10;3. The discussion about the well-matched condition being unusual implies that this specific test set (well-matched) might respond differently to changes in features or techniques compared to other test sets, such as HM. This could explain why some methods show significant improvement for HM but not for MM.&#10;4. There is a general acknowledgement in the conversation that there's a lot of variability in performance when comparing different features and techniques, and that statistical mechanisms should be used to quantify this variability and make reasonable decisions based on it. This highlights the importance of using appropriate statistical tests to determine the significance of changes in performance when evaluating new features or techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, when PhD A added online normalization to the system with a different time constant, they found that it did not lead to an improvement over not using online normalization. They mentioned that they had to change the value of the update factor to see any improvements, but even then, it was not better than without online normalization. This suggests that the change in time constant and addition of online normalization did not result in performance improvements in this case." target="Based on the transcript, PhD A is not staying in the area permanently because they are currently residing at OGI and will be returning to that location after leaving the meeting. The exact date of their return is not specified in the transcript.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, when PhD A added online normalization to the system with a different time constant, they found that it did not lead to an improvement over not using online normalization. They mentioned that they had to change the value of the update factor to see any improvements, but even then, it was not better than without online normalization. This suggests that the change in time constant and addition of online normalization did not result in performance improvements in this case." target="Based on the discussion, an eighth-order PC (autocorrelation) polynomial with real coefficients will have complex roots that come in conjugate pairs. Each pair corresponds to a positive and negative frequency. In the context of synthetic speech, this would result in three or four peak positions, which can be useful for signal processing techniques such as spectral subtraction, KL transform, or inverse filtering. Additionally, higher-order cumulants or neural networks could further improve the performance on noisy speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, when PhD A added online normalization to the system with a different time constant, they found that it did not lead to an improvement over not using online normalization. They mentioned that they had to change the value of the update factor to see any improvements, but even then, it was not better than without online normalization. This suggests that the change in time constant and addition of online normalization did not result in performance improvements in this case." target="1. The parameters that PhD B referred to when mentioning trying to include another new parameter to the traditional parameter in their voicing research are the cepstrum coefficient, auto-correlation, R-zero, and R-one over R-zero. These parameters are likely being used to extend the coefficients of the mel filter bank and approximate the signal's spectrum for further analysis and feature extraction in speech processing or pattern recognition applications.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, when PhD A added online normalization to the system with a different time constant, they found that it did not lead to an improvement over not using online normalization. They mentioned that they had to change the value of the update factor to see any improvements, but even then, it was not better than without online normalization. This suggests that the change in time constant and addition of online normalization did not result in performance improvements in this case." target="1. The slight decrease in performance for the well-matched (MM) category and the significant improvement for HM are due to the addition of a new feature or technique. However, the improvement for HM is statistically significant, while the decrease in MM is not. This could be because the well-matched test set responds differently to changes in features or techniques compared to other test sets (such as HM).&#10;&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM. As a result, the contribution of different test sets may change when a new metric is used.&#10;&#10;The implementation of a baseline using data-driven and non-data-driven methods in Linear Discriminant Analysis (LDA) can provide valuable insights into the performance of LDA filters and their impact on the overall system. It also emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source=" using on - line normalization ,&#10;Speaker: Professor C&#10;Content: Oh .&#10;Speaker: PhD A&#10;Content: because I {disfmarker} I found that I would have change the value of the update factor .&#10;Speaker: Professor C&#10;Content: No you didn't , OK .&#10;Speaker: PhD A&#10;Content: But I didn't play it with play {disfmarker} play quite a bit to make it better than .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: So , it 's still not {disfmarker}&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: I mean , the on - line normalization didn't give me any improvement .&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: And uh , so ,&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: oh yeah So I just stopped there with the uh , speech enhancement . The {disfmarker} the other thing what I tried was the {disfmarker} adding the uh , endpoint information to the baseline" target="1. PhD A has been working on a signal subspace approach for speech enhancement, specifically an algorithm that decomposes the noisy signal into signal and noise subspaces to estimate clean speech. They have been testing this on Matlab and are trying to optimize the performance by adjusting the CPU usage.&#10;2. PhD A also tried to implement online normalization in their algorithm but found that it did not improve the results.&#10;3. In addition, PhD A combined endpoint information with the baseline system as another related experiment.&#10;4. However, there is no explicit mention of positive results from these experiments in the transcript. Therefore, it is unclear if they have found any positive results to port it to C and update it in the shared repository.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Distance from the mouth: The lapel microphones were placed too far from the speaker's mouth, which resulted in more background noise and made it difficult to isolate the speaker's voice.&#10;2. Channel effect and noise issues: The lapel microphones also caused problems with getting rid of noise and channel effect, making them less suitable for clear audio recordings compared to wireless headset microphones." target=" France Telecom , which provide us some speech samples that are uh , with noise removed .&#10;Speaker: Professor C&#10;Content: So I let me {disfmarker} let me just stop you there . So then , one distinction is that uh , you were taking the actual France Telecom features and then applying something to {disfmarker}&#10;Speaker: PhD A&#10;Content: Uh , no there is a slight different . Uh I mean , which are extracted at the handset because they had another back - end blind equalization {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Yeah . But that 's what I mean .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: But u u Sorry ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: yeah , I 'm not being {disfmarker} I 'm not being clear .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: What I meant was you had something like cepstra or something , right">
      <data key="d0">1</data>
    </edge>
    <edge source="When deciding between using FFT bins or mel energies for processing a proposal, the following factors should be considered:&#10;&#10;1. Purpose of processing: If the goal is to reconstruct speech, then using FFT bins might be a better option as it can provide more accurate information about the original signal. On the other hand, if the purpose is not to reconstruct speech but rather for speech recognition, there may not be much difference between using FFT bins or mel energies, since a linear weighting will be applied after processing in either case.&#10;2. Computational cost: Mel energies might be less computationally expensive than FFT bins, as they involve applying a mel filter bank to the power spectrum of the signal, which can be faster and more efficient than calculating FFT bins directly.&#10;3. Feature extraction: Using FFT bins or mel energies can lead to different features being extracted from the signal, which might have different implications for downstream tasks such as speech recognition or noise reduction. Careful consideration should be given to the specific requirements of the task at hand and the characteristics of the input signal when choosing between these two options.&#10;&#10;Therefore, whether reconstructing speech is a relevant consideration in this choice depends on the specific purpose of processing. If the goal is to preserve as much information about the original speech signal as possible, then using FFT bins might be more appropriate. However, if the focus is on recognizing or analyzing speech signals rather than reconstructing them, then mel energies could be a viable and potentially more efficient alternative." target=" ?&#10;Speaker: PhD D&#10;Content: Um ,&#10;Speaker: PhD F&#10;Content: Does it matter , or {disfmarker} ?&#10;Speaker: PhD D&#10;Content: I d I don't know . Well , it 's both {disfmarker} both uh , cases can i&#10;Speaker: PhD F&#10;Content: Oh .&#10;Speaker: PhD D&#10;Content: Yeah . So - some of the proposal , uh , we 're doing this on the bin {disfmarker} on the FFT bins ,&#10;Speaker: PhD F&#10;Content: Hmm .&#10;Speaker: PhD D&#10;Content: others on the um , mel energies . You can do both , but I cannot tell you what 's {disfmarker} which one might be better or {disfmarker} I {disfmarker}&#10;Speaker: PhD F&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: I guess if you want to reconstruct the speech , it may be a good idea to do it on FFT bins .&#10;Speaker: PhD D&#10;Content: I don't know . Yeah , but&#10;Speaker: PhD F&#10;Content: Mmm">
      <data key="d0">1</data>
    </edge>
    <edge source="When deciding between using FFT bins or mel energies for processing a proposal, the following factors should be considered:&#10;&#10;1. Purpose of processing: If the goal is to reconstruct speech, then using FFT bins might be a better option as it can provide more accurate information about the original signal. On the other hand, if the purpose is not to reconstruct speech but rather for speech recognition, there may not be much difference between using FFT bins or mel energies, since a linear weighting will be applied after processing in either case.&#10;2. Computational cost: Mel energies might be less computationally expensive than FFT bins, as they involve applying a mel filter bank to the power spectrum of the signal, which can be faster and more efficient than calculating FFT bins directly.&#10;3. Feature extraction: Using FFT bins or mel energies can lead to different features being extracted from the signal, which might have different implications for downstream tasks such as speech recognition or noise reduction. Careful consideration should be given to the specific requirements of the task at hand and the characteristics of the input signal when choosing between these two options.&#10;&#10;Therefore, whether reconstructing speech is a relevant consideration in this choice depends on the specific purpose of processing. If the goal is to preserve as much information about the original speech signal as possible, then using FFT bins might be more appropriate. However, if the focus is on recognizing or analyzing speech signals rather than reconstructing them, then mel energies could be a viable and potentially more efficient alternative." target=" FFT bins .&#10;Speaker: PhD D&#10;Content: I don't know . Yeah , but&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: But for speech recognition , it may not . I mean it may not be very different if you do it on mel warped or whether you do it on FFT . So you 're going to do a linear weighting anyway after that .&#10;Speaker: PhD F&#10;Content: I see .&#10;Speaker: PhD A&#10;Content: Well {disfmarker} Yeah ?&#10;Speaker: PhD F&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: So , it may not be really a big different .&#10;Speaker: PhD D&#10;Content: Well , it gives something different , but I don't know what are the , pros and cons of both .&#10;Speaker: PhD A&#10;Content: It I Uh - huh .&#10;Speaker: Professor C&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: So&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: The other thing is like when you 're putting in a speech enhancement technique , uh ,">
      <data key="d0">1</data>
    </edge>
    <edge source="When deciding between using FFT bins or mel energies for processing a proposal, the following factors should be considered:&#10;&#10;1. Purpose of processing: If the goal is to reconstruct speech, then using FFT bins might be a better option as it can provide more accurate information about the original signal. On the other hand, if the purpose is not to reconstruct speech but rather for speech recognition, there may not be much difference between using FFT bins or mel energies, since a linear weighting will be applied after processing in either case.&#10;2. Computational cost: Mel energies might be less computationally expensive than FFT bins, as they involve applying a mel filter bank to the power spectrum of the signal, which can be faster and more efficient than calculating FFT bins directly.&#10;3. Feature extraction: Using FFT bins or mel energies can lead to different features being extracted from the signal, which might have different implications for downstream tasks such as speech recognition or noise reduction. Careful consideration should be given to the specific requirements of the task at hand and the characteristics of the input signal when choosing between these two options.&#10;&#10;Therefore, whether reconstructing speech is a relevant consideration in this choice depends on the specific purpose of processing. If the goal is to preserve as much information about the original speech signal as possible, then using FFT bins might be more appropriate. However, if the focus is on recognizing or analyzing speech signals rather than reconstructing them, then mel energies could be a viable and potentially more efficient alternative." target=" , but , you 'll do throw something away . And so the question is , uh , can we figure out if there 's something we 've thrown away that we shouldn't have . And um . So , when they were looking at the difference between the filter bank and the FFT that was going into the filter bank , I was thinking &quot; oh , OK , so they 're picking on something they 're looking on it to figure out noise , or voice {disfmarker} voiced property whatever . &quot; So that {disfmarker} that 's interesting . Maybe that helps to drive the {disfmarker} the thought process of coming up with the features . But for me sort of the interesting thing was , &quot; well , but is there just something in that difference which is useful ? &quot; So another way of doing it , maybe , would be just to take the FFT uh , power spectrum , and feed it into a neural network ,&#10;Speaker: PhD B&#10;Content: To know {disfmarker}&#10;Speaker: Professor C&#10;Content: and then use it , you know , in combination , or alone , or {disfmarker} or whatever&#10;Speaker: PhD F&#10;Content: Wi">
      <data key="d0">1</data>
    </edge>
    <edge source="When deciding between using FFT bins or mel energies for processing a proposal, the following factors should be considered:&#10;&#10;1. Purpose of processing: If the goal is to reconstruct speech, then using FFT bins might be a better option as it can provide more accurate information about the original signal. On the other hand, if the purpose is not to reconstruct speech but rather for speech recognition, there may not be much difference between using FFT bins or mel energies, since a linear weighting will be applied after processing in either case.&#10;2. Computational cost: Mel energies might be less computationally expensive than FFT bins, as they involve applying a mel filter bank to the power spectrum of the signal, which can be faster and more efficient than calculating FFT bins directly.&#10;3. Feature extraction: Using FFT bins or mel energies can lead to different features being extracted from the signal, which might have different implications for downstream tasks such as speech recognition or noise reduction. Careful consideration should be given to the specific requirements of the task at hand and the characteristics of the input signal when choosing between these two options.&#10;&#10;Therefore, whether reconstructing speech is a relevant consideration in this choice depends on the specific purpose of processing. If the goal is to preserve as much information about the original speech signal as possible, then using FFT bins might be more appropriate. However, if the focus is on recognizing or analyzing speech signals rather than reconstructing them, then mel energies could be a viable and potentially more efficient alternative." target="mm . Mmm .&#10;Speaker: Professor C&#10;Content: But I {disfmarker} I {disfmarker} I have to tell you , I can't remember the conference , but , uh , I think it 's about ten years ago , I remember going to one of the speech conferences and {disfmarker} and uh , I saw within very short distance of one another a couple different posters that showed about the wonders of some auditory inspired front - end or something , and a couple posters away it was somebody who compared one to uh , just putting in the FFT and the FFT did slightly better . So I mean the {disfmarker} i i It 's true there 's lots of variability ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: but again we have these wonderful statistical mechanisms for quantifying that a that variability , and you know , doing something reasonable with it .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So , um , uh , It - it 's same , you know , argument that 's gone both ways about uh , you know">
      <data key="d0">1</data>
    </edge>
    <edge source="When deciding between using FFT bins or mel energies for processing a proposal, the following factors should be considered:&#10;&#10;1. Purpose of processing: If the goal is to reconstruct speech, then using FFT bins might be a better option as it can provide more accurate information about the original signal. On the other hand, if the purpose is not to reconstruct speech but rather for speech recognition, there may not be much difference between using FFT bins or mel energies, since a linear weighting will be applied after processing in either case.&#10;2. Computational cost: Mel energies might be less computationally expensive than FFT bins, as they involve applying a mel filter bank to the power spectrum of the signal, which can be faster and more efficient than calculating FFT bins directly.&#10;3. Feature extraction: Using FFT bins or mel energies can lead to different features being extracted from the signal, which might have different implications for downstream tasks such as speech recognition or noise reduction. Careful consideration should be given to the specific requirements of the task at hand and the characteristics of the input signal when choosing between these two options.&#10;&#10;Therefore, whether reconstructing speech is a relevant consideration in this choice depends on the specific purpose of processing. If the goal is to preserve as much information about the original speech signal as possible, then using FFT bins might be more appropriate. However, if the focus is on recognizing or analyzing speech signals rather than reconstructing them, then mel energies could be a viable and potentially more efficient alternative." target="aker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Um ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Do y wanna say anything about {disfmarker} You {disfmarker} you actually been {disfmarker} Uh , last week you were doing this stuff with Pierre , you were {disfmarker} you were mentioning . Is that {disfmarker} that something worth talking about , or {disfmarker} ?&#10;Speaker: Grad E&#10;Content: Um , it 's {disfmarker} Well , um , it {disfmarker} I don't think it directly relates . Um , well , so , I was helping a speech researcher named Pierre Divenyi and he 's int He wanted to um , look at um , how people respond to formant changes , I think . Um . So he {disfmarker} he created a lot of synthetic audio files of vowel - to - vowel transitions , and then he wanted a psycho - acoustic um , spectrum . And he wanted to look at um , how the energy is moving {pause} over time in that spectrum and compare">
      <data key="d0">1</data>
    </edge>
    <edge source="When deciding between using FFT bins or mel energies for processing a proposal, the following factors should be considered:&#10;&#10;1. Purpose of processing: If the goal is to reconstruct speech, then using FFT bins might be a better option as it can provide more accurate information about the original signal. On the other hand, if the purpose is not to reconstruct speech but rather for speech recognition, there may not be much difference between using FFT bins or mel energies, since a linear weighting will be applied after processing in either case.&#10;2. Computational cost: Mel energies might be less computationally expensive than FFT bins, as they involve applying a mel filter bank to the power spectrum of the signal, which can be faster and more efficient than calculating FFT bins directly.&#10;3. Feature extraction: Using FFT bins or mel energies can lead to different features being extracted from the signal, which might have different implications for downstream tasks such as speech recognition or noise reduction. Careful consideration should be given to the specific requirements of the task at hand and the characteristics of the input signal when choosing between these two options.&#10;&#10;Therefore, whether reconstructing speech is a relevant consideration in this choice depends on the specific purpose of processing. If the goal is to preserve as much information about the original speech signal as possible, then using FFT bins might be more appropriate. However, if the focus is on recognizing or analyzing speech signals rather than reconstructing them, then mel energies could be a viable and potentially more efficient alternative." target="1. Useful information in the difference between a filter bank and FFT: There might be useful information in the difference between using a filter bank and FFT for feature extraction. The discussion reveals that researchers have been looking at this difference to analyze noise or voiced property, which could potentially help drive the thought process of coming up with features.&#10;&#10;2. Feeding FFT power spectrum into a neural network: Based on PhD B's statement, feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features. However, in their research comparing new features using a neural network versus traditional cepstrum or other features in Italian and Spanish datasets, they found no statistically significant difference in performance.&#10;&#10;In conclusion, there could be useful information in the difference between using a filter bank and FFT for feature extraction, and feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features, although it may not yield significantly different results compared to traditional methods.">
      <data key="d0">1</data>
    </edge>
    <edge source="When deciding between using FFT bins or mel energies for processing a proposal, the following factors should be considered:&#10;&#10;1. Purpose of processing: If the goal is to reconstruct speech, then using FFT bins might be a better option as it can provide more accurate information about the original signal. On the other hand, if the purpose is not to reconstruct speech but rather for speech recognition, there may not be much difference between using FFT bins or mel energies, since a linear weighting will be applied after processing in either case.&#10;2. Computational cost: Mel energies might be less computationally expensive than FFT bins, as they involve applying a mel filter bank to the power spectrum of the signal, which can be faster and more efficient than calculating FFT bins directly.&#10;3. Feature extraction: Using FFT bins or mel energies can lead to different features being extracted from the signal, which might have different implications for downstream tasks such as speech recognition or noise reduction. Careful consideration should be given to the specific requirements of the task at hand and the characteristics of the input signal when choosing between these two options.&#10;&#10;Therefore, whether reconstructing speech is a relevant consideration in this choice depends on the specific purpose of processing. If the goal is to preserve as much information about the original speech signal as possible, then using FFT bins might be more appropriate. However, if the focus is on recognizing or analyzing speech signals rather than reconstructing them, then mel energies could be a viable and potentially more efficient alternative." target="1. Similarities: Both a filter bank and the proposed targets for a Wi-Fi system involve processing signals by dividing them into different frequency bands or &quot;bins.&quot; This allows for analysis of each band's characteristics, enabling better understanding of the signal as a whole. In the context of speech processing, this can help identify phonetic features or distinguish between voiced and unvoiced sounds (as discussed in the transcript).&#10;2. Differences: The primary difference lies in the purpose and implementation of each method. A filter bank is designed to analyze signals and extract features by dividing them into specific frequency bands based on predefined criteria, such as mel-frequency cepstral coefficients (MFCCs) or linear predictive coding (LPC). On the other hand, the proposed targets for a Wi-Fi system might involve using various techniques like channel bonding, beamforming, or spatial reuse to improve network performance.&#10;3. Phones: In the transcript, phones refer to individual speech sounds in phonetics. Both filter banks and the proposed Wi-Fi targets can be used to analyze or enhance phone signals, but the methods are different. A filter bank helps extract features from phone signals for better speech processing, while Wi-Fi system targets aim to improve communication between devices (which may involve phone calls) by optimizing network performance.&#10;&#10;In summary, both a filter bank and the proposed Wi-Fi system targets involve processing signals in frequency bands, but their purposes and implementation methods differ significantly. The use of a filter bank for speech processing can provide insights into phonetic features, while Wi-Fi system targets aim to improve communication by optimizing network performance.">
      <data key="d0">1</data>
    </edge>
    <edge source=" FFT bins .&#10;Speaker: PhD D&#10;Content: I don't know . Yeah , but&#10;Speaker: PhD F&#10;Content: Mmm .&#10;Speaker: PhD A&#10;Content: But for speech recognition , it may not . I mean it may not be very different if you do it on mel warped or whether you do it on FFT . So you 're going to do a linear weighting anyway after that .&#10;Speaker: PhD F&#10;Content: I see .&#10;Speaker: PhD A&#10;Content: Well {disfmarker} Yeah ?&#10;Speaker: PhD F&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: So , it may not be really a big different .&#10;Speaker: PhD D&#10;Content: Well , it gives something different , but I don't know what are the , pros and cons of both .&#10;Speaker: PhD A&#10;Content: It I Uh - huh .&#10;Speaker: Professor C&#10;Content: Hmm .&#10;Speaker: PhD A&#10;Content: So&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD A&#10;Content: The other thing is like when you 're putting in a speech enhancement technique , uh ," target="1. Useful information in the difference between a filter bank and FFT: There might be useful information in the difference between using a filter bank and FFT for feature extraction. The discussion reveals that researchers have been looking at this difference to analyze noise or voiced property, which could potentially help drive the thought process of coming up with features.&#10;&#10;2. Feeding FFT power spectrum into a neural network: Based on PhD B's statement, feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features. However, in their research comparing new features using a neural network versus traditional cepstrum or other features in Italian and Spanish datasets, they found no statistically significant difference in performance.&#10;&#10;In conclusion, there could be useful information in the difference between using a filter bank and FFT for feature extraction, and feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features, although it may not yield significantly different results compared to traditional methods.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , but , you 'll do throw something away . And so the question is , uh , can we figure out if there 's something we 've thrown away that we shouldn't have . And um . So , when they were looking at the difference between the filter bank and the FFT that was going into the filter bank , I was thinking &quot; oh , OK , so they 're picking on something they 're looking on it to figure out noise , or voice {disfmarker} voiced property whatever . &quot; So that {disfmarker} that 's interesting . Maybe that helps to drive the {disfmarker} the thought process of coming up with the features . But for me sort of the interesting thing was , &quot; well , but is there just something in that difference which is useful ? &quot; So another way of doing it , maybe , would be just to take the FFT uh , power spectrum , and feed it into a neural network ,&#10;Speaker: PhD B&#10;Content: To know {disfmarker}&#10;Speaker: Professor C&#10;Content: and then use it , you know , in combination , or alone , or {disfmarker} or whatever&#10;Speaker: PhD F&#10;Content: Wi" target="1. Useful information in the difference between a filter bank and FFT: There might be useful information in the difference between using a filter bank and FFT for feature extraction. The discussion reveals that researchers have been looking at this difference to analyze noise or voiced property, which could potentially help drive the thought process of coming up with features.&#10;&#10;2. Feeding FFT power spectrum into a neural network: Based on PhD B's statement, feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features. However, in their research comparing new features using a neural network versus traditional cepstrum or other features in Italian and Spanish datasets, they found no statistically significant difference in performance.&#10;&#10;In conclusion, there could be useful information in the difference between using a filter bank and FFT for feature extraction, and feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features, although it may not yield significantly different results compared to traditional methods.">
      <data key="d0">1</data>
    </edge>
    <edge source="mm . Mmm .&#10;Speaker: Professor C&#10;Content: But I {disfmarker} I {disfmarker} I have to tell you , I can't remember the conference , but , uh , I think it 's about ten years ago , I remember going to one of the speech conferences and {disfmarker} and uh , I saw within very short distance of one another a couple different posters that showed about the wonders of some auditory inspired front - end or something , and a couple posters away it was somebody who compared one to uh , just putting in the FFT and the FFT did slightly better . So I mean the {disfmarker} i i It 's true there 's lots of variability ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: but again we have these wonderful statistical mechanisms for quantifying that a that variability , and you know , doing something reasonable with it .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: So , um , uh , It - it 's same , you know , argument that 's gone both ways about uh , you know" target="1. The slight decrease in performance for the well-matched (MM) category and significant improvement for HM is due to the addition of a new feature or technique, but it's not statistically significant for MM and only slightly worse for well-matched. However, the improvement for HM is significant.&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM.&#10;3. The discussion about the well-matched condition being unusual implies that this specific test set (well-matched) might respond differently to changes in features or techniques compared to other test sets, such as HM. This could explain why some methods show significant improvement for HM but not for MM.&#10;4. There is a general acknowledgement in the conversation that there's a lot of variability in performance when comparing different features and techniques, and that statistical mechanisms should be used to quantify this variability and make reasonable decisions based on it. This highlights the importance of using appropriate statistical tests to determine the significance of changes in performance when evaluating new features or techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source="aker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Um ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Do y wanna say anything about {disfmarker} You {disfmarker} you actually been {disfmarker} Uh , last week you were doing this stuff with Pierre , you were {disfmarker} you were mentioning . Is that {disfmarker} that something worth talking about , or {disfmarker} ?&#10;Speaker: Grad E&#10;Content: Um , it 's {disfmarker} Well , um , it {disfmarker} I don't think it directly relates . Um , well , so , I was helping a speech researcher named Pierre Divenyi and he 's int He wanted to um , look at um , how people respond to formant changes , I think . Um . So he {disfmarker} he created a lot of synthetic audio files of vowel - to - vowel transitions , and then he wanted a psycho - acoustic um , spectrum . And he wanted to look at um , how the energy is moving {pause} over time in that spectrum and compare" target="To help the person track the peaks in the PLP (Perceptual Linear Prediction) LPC (Linear Predictive Coding) spectra and compare them to listener tests over time, the following technique was used:&#10;&#10;1. The roots of the LPC polynomial were found using the PLP LPC coefficients, as suggested by Stephane. This allowed for tracking the peaks in the PLP LPC spectra.&#10;2. A psycho-acoustic spectrum was provided to compare the energy distribution over time with the listener tests.&#10;3. The peaks in the PLP LPC spectra were tracked using the roots of the LPC polynomial, enabling a comparison of how they change over time.&#10;&#10;This technique enabled a detailed analysis of how the spectral peaks behave and evolve, making it possible to compare them with listener test results.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The slight decrease in performance for the well-matched (MM) category and significant improvement for HM is due to the addition of a new feature or technique, but it's not statistically significant for MM and only slightly worse for well-matched. However, the improvement for HM is significant.&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM.&#10;3. The discussion about the well-matched condition being unusual implies that this specific test set (well-matched) might respond differently to changes in features or techniques compared to other test sets, such as HM. This could explain why some methods show significant improvement for HM but not for MM.&#10;4. There is a general acknowledgement in the conversation that there's a lot of variability in performance when comparing different features and techniques, and that statistical mechanisms should be used to quantify this variability and make reasonable decisions based on it. This highlights the importance of using appropriate statistical tests to determine the significance of changes in performance when evaluating new features or techniques." target=" slightly worse for well - matched .&#10;Speaker: PhD B&#10;Content: But&#10;Speaker: PhD D&#10;Content: Um , but this is not significant . But , the problem is that it 's not significant , but if you put this in the , mmm , uh , spreadsheet , it 's still worse . Even with very minor {disfmarker} uh , even if it 's only slightly worse for well - matched .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: And significantly better for HM . Uh , but , well . I don't think it 's importa important because when they will change their metric , uh , uh , mainly because of uh , when you p you plug the um , frame dropping in the baseline system , it will improve a lot HM , and MM ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: so , um , I guess what will happen {disfmarker} I don't know what will happen . But , the different contribution , I think , for the different test set will be more even .&#10;Speaker: PhD A&#10;Content: Because the {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The slight decrease in performance for the well-matched (MM) category and significant improvement for HM is due to the addition of a new feature or technique, but it's not statistically significant for MM and only slightly worse for well-matched. However, the improvement for HM is significant.&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM.&#10;3. The discussion about the well-matched condition being unusual implies that this specific test set (well-matched) might respond differently to changes in features or techniques compared to other test sets, such as HM. This could explain why some methods show significant improvement for HM but not for MM.&#10;4. There is a general acknowledgement in the conversation that there's a lot of variability in performance when comparing different features and techniques, and that statistical mechanisms should be used to quantify this variability and make reasonable decisions based on it. This highlights the importance of using appropriate statistical tests to determine the significance of changes in performance when evaluating new features or techniques." target=" , the different contribution , I think , for the different test set will be more even .&#10;Speaker: PhD A&#10;Content: Because the {disfmarker} your improvement on HM and MM will also go down significantly in the spreadsheet so . But the {pause} the well - matched may still {disfmarker}&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I mean the well - matched may be the one which is least affected by adding the endpoint information .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: Yeah . So the {disfmarker} the MM {disfmarker}&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: MM and HM are going to be v hugely affected by it . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah , so um , yeah .&#10;Speaker: PhD A&#10;Content: Yeah . But they d the {disfmarker} everything I mean is like , but there that 's how they reduce {disfmarker} why they reduce the qualification to twenty - five percent">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The slight decrease in performance for the well-matched (MM) category and significant improvement for HM is due to the addition of a new feature or technique, but it's not statistically significant for MM and only slightly worse for well-matched. However, the improvement for HM is significant.&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM.&#10;3. The discussion about the well-matched condition being unusual implies that this specific test set (well-matched) might respond differently to changes in features or techniques compared to other test sets, such as HM. This could explain why some methods show significant improvement for HM but not for MM.&#10;4. There is a general acknowledgement in the conversation that there's a lot of variability in performance when comparing different features and techniques, and that statistical mechanisms should be used to quantify this variability and make reasonable decisions based on it. This highlights the importance of using appropriate statistical tests to determine the significance of changes in performance when evaluating new features or techniques." target="er} everything I mean is like , but there that 's how they reduce {disfmarker} why they reduce the qualification to twenty - five percent or some {disfmarker} something on .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: But are they changing the weighting ?&#10;Speaker: PhD A&#10;Content: Uh , no , I guess they are going ahead with the same weighting .&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Yeah . So there 's nothing on {disfmarker}&#10;Speaker: Professor C&#10;Content: I don't understand that .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: I guess I {disfmarker} I haven't been part of the discussion , so , um , it seems to me that the well - matched condition is gonna be unusual ,&#10;Speaker: PhD A&#10;Content: Usual .&#10;Speaker: Professor C&#10;Content: in this case . Unusual .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: Professor C&#10;Content:">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The slight decrease in performance for the well-matched (MM) category and significant improvement for HM is due to the addition of a new feature or technique, but it's not statistically significant for MM and only slightly worse for well-matched. However, the improvement for HM is significant.&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM.&#10;3. The discussion about the well-matched condition being unusual implies that this specific test set (well-matched) might respond differently to changes in features or techniques compared to other test sets, such as HM. This could explain why some methods show significant improvement for HM but not for MM.&#10;4. There is a general acknowledgement in the conversation that there's a lot of variability in performance when comparing different features and techniques, and that statistical mechanisms should be used to quantify this variability and make reasonable decisions based on it. This highlights the importance of using appropriate statistical tests to determine the significance of changes in performance when evaluating new features or techniques." target="1. The purpose of implementing a baseline using a combination of data-driven and non-data-driven methods is to serve as a reference point for evaluating the performance of filters in Linear Discriminant Analysis (LDA). By comparing the results obtained from the filters with those of the baseline, researchers can better understand the contribution of the filters and assess their effectiveness.&#10;2. The potential outcome of implementing this baseline could include valuable insights into the performance of the LDA filters and their impact on the overall system. Researchers might discover whether the data-driven filters contribute significantly to the system's performance or if they introduce too much variability, making it difficult to achieve consistent results in training and testing stages.&#10;3. This process may also lead to the development of better methods for combining data-driven and non-data-driven techniques, potentially improving the robustness and generalizability of LDA models.&#10;4. Furthermore, evaluating filters using a baseline could provide a deeper understanding of how different test sets (such as well-matched, mismatched, or highly mismatched) respond to various features and techniques. This knowledge can be used to tailor specific methods for particular test sets, further enhancing the system's performance.&#10;5. Implementing a baseline in this context emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The slight decrease in performance for the well-matched (MM) category and significant improvement for HM is due to the addition of a new feature or technique, but it's not statistically significant for MM and only slightly worse for well-matched. However, the improvement for HM is significant.&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM.&#10;3. The discussion about the well-matched condition being unusual implies that this specific test set (well-matched) might respond differently to changes in features or techniques compared to other test sets, such as HM. This could explain why some methods show significant improvement for HM but not for MM.&#10;4. There is a general acknowledgement in the conversation that there's a lot of variability in performance when comparing different features and techniques, and that statistical mechanisms should be used to quantify this variability and make reasonable decisions based on it. This highlights the importance of using appropriate statistical tests to determine the significance of changes in performance when evaluating new features or techniques." target="1. The slight decrease in performance for the well-matched (MM) category and the significant improvement for HM are due to the addition of a new feature or technique. However, the improvement for HM is statistically significant, while the decrease in MM is not. This could be because the well-matched test set responds differently to changes in features or techniques compared to other test sets (such as HM).&#10;&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM. As a result, the contribution of different test sets may change when a new metric is used.&#10;&#10;The implementation of a baseline using data-driven and non-data-driven methods in Linear Discriminant Analysis (LDA) can provide valuable insights into the performance of LDA filters and their impact on the overall system. It also emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source=" slightly worse for well - matched .&#10;Speaker: PhD B&#10;Content: But&#10;Speaker: PhD D&#10;Content: Um , but this is not significant . But , the problem is that it 's not significant , but if you put this in the , mmm , uh , spreadsheet , it 's still worse . Even with very minor {disfmarker} uh , even if it 's only slightly worse for well - matched .&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: And significantly better for HM . Uh , but , well . I don't think it 's importa important because when they will change their metric , uh , uh , mainly because of uh , when you p you plug the um , frame dropping in the baseline system , it will improve a lot HM , and MM ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD D&#10;Content: so , um , I guess what will happen {disfmarker} I don't know what will happen . But , the different contribution , I think , for the different test set will be more even .&#10;Speaker: PhD A&#10;Content: Because the {disf" target="1. The slight decrease in performance for the well-matched (MM) category and the significant improvement for HM are due to the addition of a new feature or technique. However, the improvement for HM is statistically significant, while the decrease in MM is not. This could be because the well-matched test set responds differently to changes in features or techniques compared to other test sets (such as HM).&#10;&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM. As a result, the contribution of different test sets may change when a new metric is used.&#10;&#10;The implementation of a baseline using data-driven and non-data-driven methods in Linear Discriminant Analysis (LDA) can provide valuable insights into the performance of LDA filters and their impact on the overall system. It also emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source=" , the different contribution , I think , for the different test set will be more even .&#10;Speaker: PhD A&#10;Content: Because the {disfmarker} your improvement on HM and MM will also go down significantly in the spreadsheet so . But the {pause} the well - matched may still {disfmarker}&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I mean the well - matched may be the one which is least affected by adding the endpoint information .&#10;Speaker: Professor C&#10;Content: Right .&#10;Speaker: PhD A&#10;Content: Yeah . So the {disfmarker} the MM {disfmarker}&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: MM and HM are going to be v hugely affected by it . Yeah .&#10;Speaker: PhD D&#10;Content: Yeah , so um , yeah .&#10;Speaker: PhD A&#10;Content: Yeah . But they d the {disfmarker} everything I mean is like , but there that 's how they reduce {disfmarker} why they reduce the qualification to twenty - five percent" target="1. The slight decrease in performance for the well-matched (MM) category and the significant improvement for HM are due to the addition of a new feature or technique. However, the improvement for HM is statistically significant, while the decrease in MM is not. This could be because the well-matched test set responds differently to changes in features or techniques compared to other test sets (such as HM).&#10;&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM. As a result, the contribution of different test sets may change when a new metric is used.&#10;&#10;The implementation of a baseline using data-driven and non-data-driven methods in Linear Discriminant Analysis (LDA) can provide valuable insights into the performance of LDA filters and their impact on the overall system. It also emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A is not staying in the area permanently because they are currently residing at OGI and will be returning to that location after leaving the meeting. The exact date of their return is not specified in the transcript." target=": Uh , no&#10;Speaker: Professor C&#10;Content: Is that out of the way ?&#10;Speaker: PhD A&#10;Content: not yet . Uh , yesterday I called up a lady who ha who will have a vacant room from May thirtieth and she said she 's interviewing two more people . So . And she would get back to me on Monday . So that 's {disfmarker} that 's only thing I have and Diane has a few more houses . She 's going to take some pictures and send me after I go back . So it 's {disfmarker} that 's {disfmarker}&#10;Speaker: Professor C&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: Oh . So you 're not down here permanently yet ?&#10;Speaker: PhD A&#10;Content: No . I 'm going back to OGI today .&#10;Speaker: PhD F&#10;Content: Ah ! Oh , OK .&#10;Speaker: Grad G&#10;Content: Oh .&#10;Speaker: Professor C&#10;Content: OK . And then , you 're coming back uh {disfmarker}&#10;Speaker: PhD A&#10;Content: Uh , i I mean ,">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A is not staying in the area permanently because they are currently residing at OGI and will be returning to that location after leaving the meeting. The exact date of their return is not specified in the transcript." target=": R - nineteen ?&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: R - nineteen .&#10;Speaker: PhD F&#10;Content: Yeah . We 're {disfmarker} This is session R - nineteen .&#10;Speaker: Professor C&#10;Content: If you say so . O K . Do we have anything like an agenda ? What 's going on ? Um . I guess um . So . One thing {disfmarker}&#10;Speaker: PhD F&#10;Content: Sunil 's here for the summer ?&#10;Speaker: Professor C&#10;Content: Sunil 's here for the summer , right . Um , so , one thing is to talk about a kick off meeting maybe uh , and then just uh , I guess uh , progress reports individually , and then uh , plans for where we go between now and then , pretty much . Um .&#10;Speaker: PhD F&#10;Content: I could say a few words about um , some of the uh , compute stuff that 's happening around here , so that people in the group know .&#10;Speaker: Professor C&#10;Content: Mm - hmm . OK . Why don't you start with that ? That">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A is not staying in the area permanently because they are currently residing at OGI and will be returning to that location after leaving the meeting. The exact date of their return is not specified in the transcript." target=" uh , looking a little bit into uh , TRAPS um , and doing {disfmarker} doing TRAPS on {disfmarker} on these e events too , just , um , seeing {disfmarker} seeing if that 's possible . Uh , and um , other than that , uh , I was kicked out of I - house for living there for four years .&#10;Speaker: Professor C&#10;Content: Oh no . So you live in a cardboard box in the street now&#10;Speaker: Grad G&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: or , no ?&#10;Speaker: Grad G&#10;Content: Uh , well , s s som something like that .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad G&#10;Content: In Albany , yeah . Yeah . And uh . Yep . That 's it .&#10;Speaker: Professor C&#10;Content: Suni - i d ' you v did uh {disfmarker} did you find a place ?&#10;Speaker: PhD A&#10;Content: Uh , no&#10;Speaker: Professor C&#10;Content: Is that out of the way ?&#10;Speaker: PhD A&#10;Content: not yet .">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A is not staying in the area permanently because they are currently residing at OGI and will be returning to that location after leaving the meeting. The exact date of their return is not specified in the transcript." target=" Yeah . So , in that case , I 'm going to be here on thirty - first definitely .&#10;Speaker: Professor C&#10;Content: until you {disfmarker} OK .&#10;Speaker: Grad E&#10;Content: You know , if you 're in a desperate situation and you need a place to stay , you could stay with me for a while . I 've got a spare bedroom right now .&#10;Speaker: PhD A&#10;Content: Oh . OK . Thanks . That sure is nice of you . So , it may be he needs more than me .&#10;Speaker: Grad G&#10;Content: Oh r oh . Oh no , no . My {disfmarker} my cardboard box is actually a nice spacious two bedroom apartment .&#10;Speaker: Professor C&#10;Content: So a two bedroom cardboard box . Th - that 's great .&#10;Speaker: PhD A&#10;Content: Yeah . Yeah . {vocalsound} Yeah .&#10;Speaker: Professor C&#10;Content: Thanks Dave .&#10;Speaker: Grad G&#10;Content: yeah&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Um ,&#10;Speaker: PhD A&#10;Content: Yeah .&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the transcript, PhD A is not staying in the area permanently because they are currently residing at OGI and will be returning to that location after leaving the meeting. The exact date of their return is not specified in the transcript." target="1. Kick-off meeting: The group plans to discuss having a kick-off meeting. No details about the meeting are provided in the transcript.&#10;2. Progress reports: It is mentioned that there will be individual progress reports during the session. However, no specific reports are discussed in the transcript.&#10;3. Plans for the summer: Professor C mentions that Sunil is present for the summer and they plan to talk about a kick-off meeting and individual plans between then and the end of the summer.&#10;4. Update on compute stuff happening around the group: PhD F shares an update about new machines ordered, a compute farm setup, and using P-make and Customs for running jobs on the machines. They also mention that they need to be careful with parallelization to ensure efficiency.">
      <data key="d0">1</data>
    </edge>
    <edge source=": R - nineteen ?&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: R - nineteen .&#10;Speaker: PhD F&#10;Content: Yeah . We 're {disfmarker} This is session R - nineteen .&#10;Speaker: Professor C&#10;Content: If you say so . O K . Do we have anything like an agenda ? What 's going on ? Um . I guess um . So . One thing {disfmarker}&#10;Speaker: PhD F&#10;Content: Sunil 's here for the summer ?&#10;Speaker: Professor C&#10;Content: Sunil 's here for the summer , right . Um , so , one thing is to talk about a kick off meeting maybe uh , and then just uh , I guess uh , progress reports individually , and then uh , plans for where we go between now and then , pretty much . Um .&#10;Speaker: PhD F&#10;Content: I could say a few words about um , some of the uh , compute stuff that 's happening around here , so that people in the group know .&#10;Speaker: Professor C&#10;Content: Mm - hmm . OK . Why don't you start with that ? That" target="1. Kick-off meeting: The group plans to discuss having a kick-off meeting. No details about the meeting are provided in the transcript.&#10;2. Progress reports: It is mentioned that there will be individual progress reports during the session. However, no specific reports are discussed in the transcript.&#10;3. Plans for the summer: Professor C mentions that Sunil is present for the summer and they plan to talk about a kick-off meeting and individual plans between then and the end of the summer.&#10;4. Update on compute stuff happening around the group: PhD F shares an update about new machines ordered, a compute farm setup, and using P-make and Customs for running jobs on the machines. They also mention that they need to be careful with parallelization to ensure efficiency.">
      <data key="d0">1</data>
    </edge>
    <edge source="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." target=": PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: you know ?&#10;Speaker: PhD A&#10;Content: So I 've been thinking about combining the Wiener filtering with signal subspace ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I mean just to see all {disfmarker} some {disfmarker} some such permutation combination to see whether it really helps or not .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm . Mm - hmm . Yeah . Yeah .&#10;Speaker: Professor C&#10;Content: How is it {disfmarker} I {disfmarker} I guess I 'm ignorant about this , how does {disfmarker} I mean , since Wiener filter also assumes that you 're {disfmarker} that you 're adding together the two signals , how is {disfmarker} how is that differ from signal subspace ?&#10;Speaker: PhD A&#10;Content: The signal subspace ? The {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Spe">
      <data key="d0">1</data>
    </edge>
    <edge source="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." target=" that {disfmarker} that 's one reason maybe we could combine s some {disfmarker} something to improve SNR a little bit , first stage ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and then do a something in the second stage which could take it further .&#10;Speaker: PhD D&#10;Content: What was your point about {disfmarker} about colored noise there ?&#10;Speaker: PhD A&#10;Content: Oh , the colored noise uh {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: the colored noise {disfmarker} the {disfmarker} the v the signal subspace approach has {disfmarker} I mean , it {disfmarker} it actually depends on inverting the matrices . So it {disfmarker} it {disfmarker} ac the covariance matrix of the noise . So if {disfmarker} if it is not positive definite ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I mean it has a {">
      <data key="d0">1</data>
    </edge>
    <edge source="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." target=" definite ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I mean it has a {disfmarker} it 's {disfmarker} It doesn't behave very well if it is not positive definite ak It works very well with white noise because we know for sure that it has a positive definite .&#10;Speaker: Professor C&#10;Content: So you should do spectral subtraction and then add noise .&#10;Speaker: PhD A&#10;Content: So the way they get around is like they do an inverse filtering , first of the colo colored noise&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and then make the noise white ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and then finally when you reconstruct the speech back , you do this filtering again .&#10;Speaker: PhD D&#10;Content: Yeah , right .&#10;Speaker: Professor C&#10;Content: I was only half kidding . I mean if you {disfmarker} sort of {vocalsound} you do the s spectral subtraction , that also gets rid {disfmarker}&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." target="1. Useful information in the difference between a filter bank and FFT: There might be useful information in the difference between using a filter bank and FFT for feature extraction. The discussion reveals that researchers have been looking at this difference to analyze noise or voiced property, which could potentially help drive the thought process of coming up with features.&#10;&#10;2. Feeding FFT power spectrum into a neural network: Based on PhD B's statement, feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features. However, in their research comparing new features using a neural network versus traditional cepstrum or other features in Italian and Spanish datasets, they found no statistically significant difference in performance.&#10;&#10;In conclusion, there could be useful information in the difference between using a filter bank and FFT for feature extraction, and feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features, although it may not yield significantly different results compared to traditional methods.">
      <data key="d0">1</data>
    </edge>
    <edge source="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." target="Based on the discussion, an eighth-order PC (autocorrelation) polynomial with real coefficients will have complex roots that come in conjugate pairs. Each pair corresponds to a positive and negative frequency. In the context of synthetic speech, this would result in three or four peak positions, which can be useful for signal processing techniques such as spectral subtraction, KL transform, or inverse filtering. Additionally, higher-order cumulants or neural networks could further improve the performance on noisy speech.">
      <data key="d0">1</data>
    </edge>
    <edge source="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." target="1. According to the transcript, Professor C mentioned the possibility of using the SRI recognizer for testing on large vocabulary data with a distant microphone. However, it is not clear if this has been attempted already.&#10;2. The potential challenges in using the SRI recognizer for testing on large vocabulary data with a distant microphone include the following:&#10;* Fear of trying it out, as mentioned by Professor C (&quot;Yeah, cuz everybody's scared.&quot;)&#10;* Possible overloading of the CPU or hardware due to the demands of processing large vocabulary data from a distant microphone, as also mentioned by Professor C (&quot;You'll see a little smoke coming up from the CPU or something trying to do it.&quot;).&#10;* The need for robust detectors of acoustic events, which can provide good enough coverage and training data for later recognition steps. This is based on Grad G's description of their proposed PhD work on using acoustic events for speech recognition (&quot;Building robust primary detectors for these acoustic events and using the outputs of these robust detectors to do speech recognition.&quot;).&#10;* The challenges in determining what kind of acoustic events are needed, as well as obtaining labels or training data for these acoustic events, as mentioned by Grad G.">
      <data key="d0">1</data>
    </edge>
    <edge source="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." target="1. Extending the coefficient of the mel filter bank: By extending the coefficient of the mel filter bank, an approximation of the signal's spectrum can be obtained. This method is useful for analyzing signals and extracting features, which can then be used for various applications such as speech processing, noise reduction, or pattern recognition.&#10;2. Finding another feature to discriminate between voice and unvoice sounds: The speakers suggest comparing the variance of a new feature to discriminate between voice (speech) and unvoice (non-speech) sounds. They propose that if the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This idea stems from the assumption that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances.&#10;3. The motivation behind this approach is to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." target="The addition of a binary speech-silence feature to the cepstrum and training the HMM on it resulted in significant improvement for the Italian dataset, but there was no noticeable effect for the TI-digits dataset. This is because the TI-digits dataset did not have any substantial distinction between speech and non-speech frames to benefit from this additional feature. In contrast, the Italian dataset showed a considerable improvement due to the presence of more noise in the recordings. The binary speech-silence feature helped discriminate between speech and non-speech frames, which improved overall performance when using a neural network. However, without using a neural network, adding the binary feature worsened the results for both datasets, as it did not have any help in discriminating between speech and non-speech frames.">
      <data key="d0">1</data>
    </edge>
    <edge source="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." target="1. Extending the coefficient of the mel filter bank is used to obtain an approximation of the signal's spectrum, which allows for further analysis of signals and extraction of features helpful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.&#10;   &#10;2. The idea behind finding another feature to discriminate between voice and unvoiced sounds is based on comparing the variance of this new feature. The assumption is that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances. If the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This approach aims to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.&#10;&#10;These ideas are further discussed in the transcript provided, where they test the new feature on Italian and Spanish datasets with mixed results when compared to using only cepstrum or other features. They also explore the potential benefits of comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network.">
      <data key="d0">1</data>
    </edge>
    <edge source="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." target="1. Complementary insights: By combining different methods like KLT (Karhunen-Love Transform) and probabilities with the current approach, one can gain complementary insights into the variability of the power spectrum. KLT is a technique used for dimensionality reduction and decorrelation, while probability theory can help in modeling the randomness and uncertainty present in the signal. By combining these methods, it might be possible to capture different aspects of the variability in the power spectrum, leading to a more comprehensive understanding.&#10;2. Improved robustness: Combining multiple methods can lead to increased robustness in the analysis of the power spectrum. Each method brings its strengths and weaknesses, and blending them together may help compensate for individual shortcomings. For instance, KLT is sensitive to the choice of basis functions, while probabilistic methods might struggle with model misspecification. By combining these approaches, one can potentially achieve a more reliable analysis of the power spectrum variability.&#10;3. Neural network-driven selection: As suggested by PhD D, allowing a neural network to determine which features or methods are useful in capturing the missing elements in the analysis of the power spectrum could lead to improved performance. By combining different methods and letting the neural network choose the most relevant ones, it might be possible to capture more nuanced information about the variability in the power spectrum.&#10;4. Enhanced feature extraction: Combining various methods for feature extraction, such as comparing filter bank and FFT, can lead to better identification of speech or noise signals in the power spectrum. By using a combination of techniques, researchers might be able to extract more meaningful features that help improve voice/unvoice discrimination, thereby enhancing the robustness and accuracy of various signal processing systems.&#10;&#10;In summary, combining different methods such as KLT and probabilities with the current approach can provide complementary insights, improved robustness, neural network-driven selection, and enhanced feature extraction for addressing missing elements in the analysis of variability in the power spectrum.">
      <data key="d0">1</data>
    </edge>
    <edge source="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." target="To help the person track the peaks in the PLP (Perceptual Linear Prediction) LPC (Linear Predictive Coding) spectra and compare them to listener tests over time, the following technique was used:&#10;&#10;1. The roots of the LPC polynomial were found using the PLP LPC coefficients, as suggested by Stephane. This allowed for tracking the peaks in the PLP LPC spectra.&#10;2. A psycho-acoustic spectrum was provided to compare the energy distribution over time with the listener tests.&#10;3. The peaks in the PLP LPC spectra were tracked using the roots of the LPC polynomial, enabling a comparison of how they change over time.&#10;&#10;This technique enabled a detailed analysis of how the spectral peaks behave and evolve, making it possible to compare them with listener test results.">
      <data key="d0">1</data>
    </edge>
    <edge source="The recommended method for processing a signal that is not positive definite, as discussed by the speakers, is to use spectral subtraction to remove noise from the signal before applying signal processing techniques such as the KL transform or inverse filtering. This approach can help improve the performance of the signal subspace method in noisy conditions and with non-positive definite signals. Additionally, the speakers suggest that higher-order cumulants or neural networks could potentially be used to further improve the performance on noisy speech." target="1. Evaluation conditions for the Broadcast News study on noisy speech with a large vocabulary included various focus conditions that were noisy. However, spectral subtraction was not applied in this study. Instead, multi-stream methods were used.&#10;2. The speakers did not provide specific details about the exact techniques and methods employed during the Broadcast News evaluation. They only mentioned using multi-stream approaches as an alternative to spectral subtraction for noisy large vocabulary conditions.">
      <data key="d0">1</data>
    </edge>
    <edge source=": PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: you know ?&#10;Speaker: PhD A&#10;Content: So I 've been thinking about combining the Wiener filtering with signal subspace ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: I mean just to see all {disfmarker} some {disfmarker} some such permutation combination to see whether it really helps or not .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm . Mm - hmm . Yeah . Yeah .&#10;Speaker: Professor C&#10;Content: How is it {disfmarker} I {disfmarker} I guess I 'm ignorant about this , how does {disfmarker} I mean , since Wiener filter also assumes that you 're {disfmarker} that you 're adding together the two signals , how is {disfmarker} how is that differ from signal subspace ?&#10;Speaker: PhD A&#10;Content: The signal subspace ? The {disfmarker}&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Spe" target="PhD B is explaining to PhD A that they can extend the coefficients of the mel filter bank to obtain an approximation of the signal's spectrum. This method allows for further analysis of signals and the extraction of features, which can be useful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Useful information in the difference between a filter bank and FFT: There might be useful information in the difference between using a filter bank and FFT for feature extraction. The discussion reveals that researchers have been looking at this difference to analyze noise or voiced property, which could potentially help drive the thought process of coming up with features.&#10;&#10;2. Feeding FFT power spectrum into a neural network: Based on PhD B's statement, feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features. However, in their research comparing new features using a neural network versus traditional cepstrum or other features in Italian and Spanish datasets, they found no statistically significant difference in performance.&#10;&#10;In conclusion, there could be useful information in the difference between using a filter bank and FFT for feature extraction, and feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features, although it may not yield significantly different results compared to traditional methods." target=" 's just a baseline uh , which would show us &quot; well , what are we really getting out of the filters &quot; , or maybe i i probably not by itself , but in combination , uh ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: you know , maybe there 's something to be gained from it , and let the {disfmarker} But , you know , y you 've only worked with us for a short time , maybe in a year or two you w you will actually come up with the right set of things to extract from this information . But , maybe the neural net and the H M Ms could figure it out quicker than you .&#10;Speaker: PhD B&#10;Content: Maybe .&#10;Speaker: Professor C&#10;Content: So .&#10;Speaker: PhD B&#10;Content: Yeah ,&#10;Speaker: Professor C&#10;Content: It 's just a thought .&#10;Speaker: PhD B&#10;Content: I can {disfmarker} I will try to do that .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: What {disfmarker} one {disfmarker}">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Useful information in the difference between a filter bank and FFT: There might be useful information in the difference between using a filter bank and FFT for feature extraction. The discussion reveals that researchers have been looking at this difference to analyze noise or voiced property, which could potentially help drive the thought process of coming up with features.&#10;&#10;2. Feeding FFT power spectrum into a neural network: Based on PhD B's statement, feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features. However, in their research comparing new features using a neural network versus traditional cepstrum or other features in Italian and Spanish datasets, they found no statistically significant difference in performance.&#10;&#10;In conclusion, there could be useful information in the difference between using a filter bank and FFT for feature extraction, and feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features, although it may not yield significantly different results compared to traditional methods." target="1. When using the new feature with a neural network compared to just the cepstrum or other features in Italian and Spanish datasets, there is no significant difference in performance. The new feature sometimes results in slightly better performance, and sometimes it's slightly worse, but not statistically significant (PhD B).&#10;2. Without using the neural network, adding an additional binary feature to the cepstrum worsens the results. This additional feature helps discriminate between speech and non-speech frames, which improves performance when using a neural network (PhD A).&#10;3. There is potential useful information in comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network. However, these methods may not yield significantly different results compared to traditional methods (implied from the discussion).">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Useful information in the difference between a filter bank and FFT: There might be useful information in the difference between using a filter bank and FFT for feature extraction. The discussion reveals that researchers have been looking at this difference to analyze noise or voiced property, which could potentially help drive the thought process of coming up with features.&#10;&#10;2. Feeding FFT power spectrum into a neural network: Based on PhD B's statement, feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features. However, in their research comparing new features using a neural network versus traditional cepstrum or other features in Italian and Spanish datasets, they found no statistically significant difference in performance.&#10;&#10;In conclusion, there could be useful information in the difference between using a filter bank and FFT for feature extraction, and feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features, although it may not yield significantly different results compared to traditional methods." target="1. Extending the coefficient of the mel filter bank: By extending the coefficient of the mel filter bank, an approximation of the signal's spectrum can be obtained. This method is useful for analyzing signals and extracting features, which can then be used for various applications such as speech processing, noise reduction, or pattern recognition.&#10;2. Finding another feature to discriminate between voice and unvoice sounds: The speakers suggest comparing the variance of a new feature to discriminate between voice (speech) and unvoice (non-speech) sounds. They propose that if the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This idea stems from the assumption that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances.&#10;3. The motivation behind this approach is to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Useful information in the difference between a filter bank and FFT: There might be useful information in the difference between using a filter bank and FFT for feature extraction. The discussion reveals that researchers have been looking at this difference to analyze noise or voiced property, which could potentially help drive the thought process of coming up with features.&#10;&#10;2. Feeding FFT power spectrum into a neural network: Based on PhD B's statement, feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features. However, in their research comparing new features using a neural network versus traditional cepstrum or other features in Italian and Spanish datasets, they found no statistically significant difference in performance.&#10;&#10;In conclusion, there could be useful information in the difference between using a filter bank and FFT for feature extraction, and feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features, although it may not yield significantly different results compared to traditional methods." target="PhD B is explaining to PhD A that they can extend the coefficients of the mel filter bank to obtain an approximation of the signal's spectrum. This method allows for further analysis of signals and the extraction of features, which can be useful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Useful information in the difference between a filter bank and FFT: There might be useful information in the difference between using a filter bank and FFT for feature extraction. The discussion reveals that researchers have been looking at this difference to analyze noise or voiced property, which could potentially help drive the thought process of coming up with features.&#10;&#10;2. Feeding FFT power spectrum into a neural network: Based on PhD B's statement, feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features. However, in their research comparing new features using a neural network versus traditional cepstrum or other features in Italian and Spanish datasets, they found no statistically significant difference in performance.&#10;&#10;In conclusion, there could be useful information in the difference between using a filter bank and FFT for feature extraction, and feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features, although it may not yield significantly different results compared to traditional methods." target="1. Similarities: Both a filter bank and the proposed targets for a Wi-Fi system involve processing signals by dividing them into different frequency bands or &quot;bins.&quot; This allows for analysis of each band's characteristics, enabling better understanding of the signal as a whole. In the context of speech processing, this can help identify phonetic features or distinguish between voiced and unvoiced sounds (as discussed in the transcript).&#10;2. Differences: The primary difference lies in the purpose and implementation of each method. A filter bank is designed to analyze signals and extract features by dividing them into specific frequency bands based on predefined criteria, such as mel-frequency cepstral coefficients (MFCCs) or linear predictive coding (LPC). On the other hand, the proposed targets for a Wi-Fi system might involve using various techniques like channel bonding, beamforming, or spatial reuse to improve network performance.&#10;3. Phones: In the transcript, phones refer to individual speech sounds in phonetics. Both filter banks and the proposed Wi-Fi targets can be used to analyze or enhance phone signals, but the methods are different. A filter bank helps extract features from phone signals for better speech processing, while Wi-Fi system targets aim to improve communication between devices (which may involve phone calls) by optimizing network performance.&#10;&#10;In summary, both a filter bank and the proposed Wi-Fi system targets involve processing signals in frequency bands, but their purposes and implementation methods differ significantly. The use of a filter bank for speech processing can provide insights into phonetic features, while Wi-Fi system targets aim to improve communication by optimizing network performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Useful information in the difference between a filter bank and FFT: There might be useful information in the difference between using a filter bank and FFT for feature extraction. The discussion reveals that researchers have been looking at this difference to analyze noise or voiced property, which could potentially help drive the thought process of coming up with features.&#10;&#10;2. Feeding FFT power spectrum into a neural network: Based on PhD B's statement, feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features. However, in their research comparing new features using a neural network versus traditional cepstrum or other features in Italian and Spanish datasets, they found no statistically significant difference in performance.&#10;&#10;In conclusion, there could be useful information in the difference between using a filter bank and FFT for feature extraction, and feeding the FFT power spectrum directly into a neural network is an alternative approach to developing features, although it may not yield significantly different results compared to traditional methods." target="1. Complementary insights: By combining different methods like KLT (Karhunen-Love Transform) and probabilities with the current approach, one can gain complementary insights into the variability of the power spectrum. KLT is a technique used for dimensionality reduction and decorrelation, while probability theory can help in modeling the randomness and uncertainty present in the signal. By combining these methods, it might be possible to capture different aspects of the variability in the power spectrum, leading to a more comprehensive understanding.&#10;2. Improved robustness: Combining multiple methods can lead to increased robustness in the analysis of the power spectrum. Each method brings its strengths and weaknesses, and blending them together may help compensate for individual shortcomings. For instance, KLT is sensitive to the choice of basis functions, while probabilistic methods might struggle with model misspecification. By combining these approaches, one can potentially achieve a more reliable analysis of the power spectrum variability.&#10;3. Neural network-driven selection: As suggested by PhD D, allowing a neural network to determine which features or methods are useful in capturing the missing elements in the analysis of the power spectrum could lead to improved performance. By combining different methods and letting the neural network choose the most relevant ones, it might be possible to capture more nuanced information about the variability in the power spectrum.&#10;4. Enhanced feature extraction: Combining various methods for feature extraction, such as comparing filter bank and FFT, can lead to better identification of speech or noise signals in the power spectrum. By using a combination of techniques, researchers might be able to extract more meaningful features that help improve voice/unvoice discrimination, thereby enhancing the robustness and accuracy of various signal processing systems.&#10;&#10;In summary, combining different methods such as KLT and probabilities with the current approach can provide complementary insights, improved robustness, neural network-driven selection, and enhanced feature extraction for addressing missing elements in the analysis of variability in the power spectrum.">
      <data key="d0">1</data>
    </edge>
    <edge source=" 's just a baseline uh , which would show us &quot; well , what are we really getting out of the filters &quot; , or maybe i i probably not by itself , but in combination , uh ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: you know , maybe there 's something to be gained from it , and let the {disfmarker} But , you know , y you 've only worked with us for a short time , maybe in a year or two you w you will actually come up with the right set of things to extract from this information . But , maybe the neural net and the H M Ms could figure it out quicker than you .&#10;Speaker: PhD B&#10;Content: Maybe .&#10;Speaker: Professor C&#10;Content: So .&#10;Speaker: PhD B&#10;Content: Yeah ,&#10;Speaker: Professor C&#10;Content: It 's just a thought .&#10;Speaker: PhD B&#10;Content: I can {disfmarker} I will try to do that .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: What {disfmarker} one {disfmarker}" target="1. The purpose of implementing a baseline using a combination of data-driven and non-data-driven methods is to serve as a reference point for evaluating the performance of filters in Linear Discriminant Analysis (LDA). By comparing the results obtained from the filters with those of the baseline, researchers can better understand the contribution of the filters and assess their effectiveness.&#10;2. The potential outcome of implementing this baseline could include valuable insights into the performance of the LDA filters and their impact on the overall system. Researchers might discover whether the data-driven filters contribute significantly to the system's performance or if they introduce too much variability, making it difficult to achieve consistent results in training and testing stages.&#10;3. This process may also lead to the development of better methods for combining data-driven and non-data-driven techniques, potentially improving the robustness and generalizability of LDA models.&#10;4. Furthermore, evaluating filters using a baseline could provide a deeper understanding of how different test sets (such as well-matched, mismatched, or highly mismatched) respond to various features and techniques. This knowledge can be used to tailor specific methods for particular test sets, further enhancing the system's performance.&#10;5. Implementing a baseline in this context emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, an eighth-order PC (autocorrelation) polynomial with real coefficients will have complex roots that come in conjugate pairs. Each pair corresponds to a positive and negative frequency. In the context of synthetic speech, this would result in three or four peak positions, which can be useful for signal processing techniques such as spectral subtraction, KL transform, or inverse filtering. Additionally, higher-order cumulants or neural networks could further improve the performance on noisy speech." target="PC polynomial 's gonna have real coefficients . So I think that means that every root that is not a real root {comment} is gonna be a c complex pair ,&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: um , of a complex value and its conjugate . Um . So for each {disfmarker} And if you look at that on the unit circle , um , one of these {disfmarker} one of the members of the pair will be a positive frequency , one will be a negative frequency , I think . So I just {disfmarker} So , um , f for the {disfmarker} I 'm using an eighth - order polynomial and I 'll get three or four of these pairs&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Hmm .&#10;Speaker: Grad E&#10;Content: which give me s which gives me three or four peak positions .&#10;Speaker: Professor C&#10;Content: This is from synthetic speech , or {disfmarker} ?&#10;Speaker: Grad E&#10;Content: It 's {disfmarker">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, an eighth-order PC (autocorrelation) polynomial with real coefficients will have complex roots that come in conjugate pairs. Each pair corresponds to a positive and negative frequency. In the context of synthetic speech, this would result in three or four peak positions, which can be useful for signal processing techniques such as spectral subtraction, KL transform, or inverse filtering. Additionally, higher-order cumulants or neural networks could further improve the performance on noisy speech." target=" PhD D&#10;Content: Hmm .&#10;Speaker: Professor C&#10;Content: uh {disfmarker} Formant tracking with it can be a little tricky cuz you get these funny {vocalsound} values in {disfmarker} in real speech ,&#10;Speaker: PhD F&#10;Content: So you just {disfmarker} You typically just get a few roots ?&#10;Speaker: Professor C&#10;Content: but .&#10;Speaker: PhD F&#10;Content: You know , two or three ,&#10;Speaker: Professor C&#10;Content: Well you get these complex pairs .&#10;Speaker: PhD F&#10;Content: something like that ?&#10;Speaker: Professor C&#10;Content: And it depends on the order that you 're doing , but .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Grad E&#10;Content: Right . So , um , if {disfmarker} @ @ {comment} Every root that 's {disfmarker} Since it 's a real signal , the LPC polynomial 's gonna have real coefficients . So I think that means that every root that is not a real root {comment} is gonna be a">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, an eighth-order PC (autocorrelation) polynomial with real coefficients will have complex roots that come in conjugate pairs. Each pair corresponds to a positive and negative frequency. In the context of synthetic speech, this would result in three or four peak positions, which can be useful for signal processing techniques such as spectral subtraction, KL transform, or inverse filtering. Additionally, higher-order cumulants or neural networks could further improve the performance on noisy speech." target="1. Extending the coefficient of the mel filter bank: By extending the coefficient of the mel filter bank, an approximation of the signal's spectrum can be obtained. This method is useful for analyzing signals and extracting features, which can then be used for various applications such as speech processing, noise reduction, or pattern recognition.&#10;2. Finding another feature to discriminate between voice and unvoice sounds: The speakers suggest comparing the variance of a new feature to discriminate between voice (speech) and unvoice (non-speech) sounds. They propose that if the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This idea stems from the assumption that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances.&#10;3. The motivation behind this approach is to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="Based on the discussion, an eighth-order PC (autocorrelation) polynomial with real coefficients will have complex roots that come in conjugate pairs. Each pair corresponds to a positive and negative frequency. In the context of synthetic speech, this would result in three or four peak positions, which can be useful for signal processing techniques such as spectral subtraction, KL transform, or inverse filtering. Additionally, higher-order cumulants or neural networks could further improve the performance on noisy speech." target="1. Evaluation conditions for the Broadcast News study on noisy speech with a large vocabulary included various focus conditions that were noisy. However, spectral subtraction was not applied in this study. Instead, multi-stream methods were used.&#10;2. The speakers did not provide specific details about the exact techniques and methods employed during the Broadcast News evaluation. They only mentioned using multi-stream approaches as an alternative to spectral subtraction for noisy large vocabulary conditions.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When using the new feature with a neural network compared to just the cepstrum or other features in Italian and Spanish datasets, there is no significant difference in performance. The new feature sometimes results in slightly better performance, and sometimes it's slightly worse, but not statistically significant (PhD B).&#10;2. Without using the neural network, adding an additional binary feature to the cepstrum worsens the results. This additional feature helps discriminate between speech and non-speech frames, which improves performance when using a neural network (PhD A).&#10;3. There is potential useful information in comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network. However, these methods may not yield significantly different results compared to traditional methods (implied from the discussion)." target="1. Extending the coefficient of the mel filter bank: By extending the coefficient of the mel filter bank, an approximation of the signal's spectrum can be obtained. This method is useful for analyzing signals and extracting features, which can then be used for various applications such as speech processing, noise reduction, or pattern recognition.&#10;2. Finding another feature to discriminate between voice and unvoice sounds: The speakers suggest comparing the variance of a new feature to discriminate between voice (speech) and unvoice (non-speech) sounds. They propose that if the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This idea stems from the assumption that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances.&#10;3. The motivation behind this approach is to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When using the new feature with a neural network compared to just the cepstrum or other features in Italian and Spanish datasets, there is no significant difference in performance. The new feature sometimes results in slightly better performance, and sometimes it's slightly worse, but not statistically significant (PhD B).&#10;2. Without using the neural network, adding an additional binary feature to the cepstrum worsens the results. This additional feature helps discriminate between speech and non-speech frames, which improves performance when using a neural network (PhD A).&#10;3. There is potential useful information in comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network. However, these methods may not yield significantly different results compared to traditional methods (implied from the discussion)." target="PhD B is explaining to PhD A that they can extend the coefficients of the mel filter bank to obtain an approximation of the signal's spectrum. This method allows for further analysis of signals and the extraction of features, which can be useful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When using the new feature with a neural network compared to just the cepstrum or other features in Italian and Spanish datasets, there is no significant difference in performance. The new feature sometimes results in slightly better performance, and sometimes it's slightly worse, but not statistically significant (PhD B).&#10;2. Without using the neural network, adding an additional binary feature to the cepstrum worsens the results. This additional feature helps discriminate between speech and non-speech frames, which improves performance when using a neural network (PhD A).&#10;3. There is potential useful information in comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network. However, these methods may not yield significantly different results compared to traditional methods (implied from the discussion)." target="The addition of a binary speech-silence feature to the cepstrum and training the HMM on it resulted in significant improvement for the Italian dataset, but there was no noticeable effect for the TI-digits dataset. This is because the TI-digits dataset did not have any substantial distinction between speech and non-speech frames to benefit from this additional feature. In contrast, the Italian dataset showed a considerable improvement due to the presence of more noise in the recordings. The binary speech-silence feature helped discriminate between speech and non-speech frames, which improved overall performance when using a neural network. However, without using a neural network, adding the binary feature worsened the results for both datasets, as it did not have any help in discriminating between speech and non-speech frames.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When using the new feature with a neural network compared to just the cepstrum or other features in Italian and Spanish datasets, there is no significant difference in performance. The new feature sometimes results in slightly better performance, and sometimes it's slightly worse, but not statistically significant (PhD B).&#10;2. Without using the neural network, adding an additional binary feature to the cepstrum worsens the results. This additional feature helps discriminate between speech and non-speech frames, which improves performance when using a neural network (PhD A).&#10;3. There is potential useful information in comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network. However, these methods may not yield significantly different results compared to traditional methods (implied from the discussion)." target="1. Similarities: Both a filter bank and the proposed targets for a Wi-Fi system involve processing signals by dividing them into different frequency bands or &quot;bins.&quot; This allows for analysis of each band's characteristics, enabling better understanding of the signal as a whole. In the context of speech processing, this can help identify phonetic features or distinguish between voiced and unvoiced sounds (as discussed in the transcript).&#10;2. Differences: The primary difference lies in the purpose and implementation of each method. A filter bank is designed to analyze signals and extract features by dividing them into specific frequency bands based on predefined criteria, such as mel-frequency cepstral coefficients (MFCCs) or linear predictive coding (LPC). On the other hand, the proposed targets for a Wi-Fi system might involve using various techniques like channel bonding, beamforming, or spatial reuse to improve network performance.&#10;3. Phones: In the transcript, phones refer to individual speech sounds in phonetics. Both filter banks and the proposed Wi-Fi targets can be used to analyze or enhance phone signals, but the methods are different. A filter bank helps extract features from phone signals for better speech processing, while Wi-Fi system targets aim to improve communication between devices (which may involve phone calls) by optimizing network performance.&#10;&#10;In summary, both a filter bank and the proposed Wi-Fi system targets involve processing signals in frequency bands, but their purposes and implementation methods differ significantly. The use of a filter bank for speech processing can provide insights into phonetic features, while Wi-Fi system targets aim to improve communication by optimizing network performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. When using the new feature with a neural network compared to just the cepstrum or other features in Italian and Spanish datasets, there is no significant difference in performance. The new feature sometimes results in slightly better performance, and sometimes it's slightly worse, but not statistically significant (PhD B).&#10;2. Without using the neural network, adding an additional binary feature to the cepstrum worsens the results. This additional feature helps discriminate between speech and non-speech frames, which improves performance when using a neural network (PhD A).&#10;3. There is potential useful information in comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network. However, these methods may not yield significantly different results compared to traditional methods (implied from the discussion)." target="1. Extending the coefficient of the mel filter bank is used to obtain an approximation of the signal's spectrum, which allows for further analysis of signals and extraction of features helpful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.&#10;   &#10;2. The idea behind finding another feature to discriminate between voice and unvoiced sounds is based on comparing the variance of this new feature. The assumption is that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances. If the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This approach aims to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.&#10;&#10;These ideas are further discussed in the transcript provided, where they test the new feature on Italian and Spanish datasets with mixed results when compared to using only cepstrum or other features. They also explore the potential benefits of comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of implementing a baseline using a combination of data-driven and non-data-driven methods is to serve as a reference point for evaluating the performance of filters in Linear Discriminant Analysis (LDA). By comparing the results obtained from the filters with those of the baseline, researchers can better understand the contribution of the filters and assess their effectiveness.&#10;2. The potential outcome of implementing this baseline could include valuable insights into the performance of the LDA filters and their impact on the overall system. Researchers might discover whether the data-driven filters contribute significantly to the system's performance or if they introduce too much variability, making it difficult to achieve consistent results in training and testing stages.&#10;3. This process may also lead to the development of better methods for combining data-driven and non-data-driven techniques, potentially improving the robustness and generalizability of LDA models.&#10;4. Furthermore, evaluating filters using a baseline could provide a deeper understanding of how different test sets (such as well-matched, mismatched, or highly mismatched) respond to various features and techniques. This knowledge can be used to tailor specific methods for particular test sets, further enhancing the system's performance.&#10;5. Implementing a baseline in this context emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques." target=" C&#10;Content: So , um , uh , It - it 's same , you know , argument that 's gone both ways about uh , you know , we have these data driven filters , in LDA , and on the other hand , if it 's data driven it means it 's driven by things that have lots of variability , and that are necessarily {disfmarker} not necessarily gonna be the same in training and test , so , in some ways it 's good to have data driven things , and in some ways it 's bad to have data driven things . So ,&#10;Speaker: PhD A&#10;Content: Yeah , d&#10;Speaker: Professor C&#10;Content: part of what we 're discovering , is ways to combine things that are data driven than are not .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , so anyway , it 's just a thought , that {disfmarker} that if we {disfmarker} if we had that {disfmarker} maybe it 's just a baseline uh , which would show us &quot; well , what are we really getting out of the filters &quot; , or maybe i i probably not by">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of implementing a baseline using a combination of data-driven and non-data-driven methods is to serve as a reference point for evaluating the performance of filters in Linear Discriminant Analysis (LDA). By comparing the results obtained from the filters with those of the baseline, researchers can better understand the contribution of the filters and assess their effectiveness.&#10;2. The potential outcome of implementing this baseline could include valuable insights into the performance of the LDA filters and their impact on the overall system. Researchers might discover whether the data-driven filters contribute significantly to the system's performance or if they introduce too much variability, making it difficult to achieve consistent results in training and testing stages.&#10;3. This process may also lead to the development of better methods for combining data-driven and non-data-driven techniques, potentially improving the robustness and generalizability of LDA models.&#10;4. Furthermore, evaluating filters using a baseline could provide a deeper understanding of how different test sets (such as well-matched, mismatched, or highly mismatched) respond to various features and techniques. This knowledge can be used to tailor specific methods for particular test sets, further enhancing the system's performance.&#10;5. Implementing a baseline in this context emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques." target=" in {disfmarker} I just take the cepstral coefficients coming from their system and then plug in LDA on top of that . But the LDA filter that I used was different from what we submitted in the proposal .&#10;Speaker: PhD F&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: What I did was {vocalsound} I took the LDA filter 's design using clean speech , uh , mainly because the speech is already cleaned up after the enhancement so , instead of using this , uh , narrow {disfmarker} narrow band LDA filter that we submitted uh , I got new filters . So that seems to be giving {disfmarker} uh , improving over their uh , system . Slightly . But , not very significantly . And uh , that was uh , showing any improvement over {disfmarker} final {disfmarker} by plugging in an LDA . And uh , so then after {disfmarker} after that I {disfmarker} I added uh , on - line normalization also on top of that . And that {disfmarker} there {disfmarker} there also I n I">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of implementing a baseline using a combination of data-driven and non-data-driven methods is to serve as a reference point for evaluating the performance of filters in Linear Discriminant Analysis (LDA). By comparing the results obtained from the filters with those of the baseline, researchers can better understand the contribution of the filters and assess their effectiveness.&#10;2. The potential outcome of implementing this baseline could include valuable insights into the performance of the LDA filters and their impact on the overall system. Researchers might discover whether the data-driven filters contribute significantly to the system's performance or if they introduce too much variability, making it difficult to achieve consistent results in training and testing stages.&#10;3. This process may also lead to the development of better methods for combining data-driven and non-data-driven techniques, potentially improving the robustness and generalizability of LDA models.&#10;4. Furthermore, evaluating filters using a baseline could provide a deeper understanding of how different test sets (such as well-matched, mismatched, or highly mismatched) respond to various features and techniques. This knowledge can be used to tailor specific methods for particular test sets, further enhancing the system's performance.&#10;5. Implementing a baseline in this context emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques." target=" from the post uh Aurora submission maybe .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: Uh , yeah , after the submission the {disfmarker} what I 've been working on mainly was to take {disfmarker} take other s submissions and then over their system , what they submitted , because we didn't have any speech enhancement system in {disfmarker} in ours . So {disfmarker} So I tried uh , And u First I tried just LDA . And then I found that uh , I mean , if {disfmarker} if I combine it with LDA , it gives @ @ improvement over theirs . Uh {disfmarker}&#10;Speaker: PhD F&#10;Content: Are y are you saying LDA ?&#10;Speaker: PhD A&#10;Content: Yeah . Yeah .&#10;Speaker: PhD F&#10;Content: LDA . OK .&#10;Speaker: PhD A&#10;Content: So , just {disfmarker} just the LDA filters . I just plug in {disfmarker} I just take the cepstral coefficients coming from their system and then plug in LDA on top of that . But">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of implementing a baseline using a combination of data-driven and non-data-driven methods is to serve as a reference point for evaluating the performance of filters in Linear Discriminant Analysis (LDA). By comparing the results obtained from the filters with those of the baseline, researchers can better understand the contribution of the filters and assess their effectiveness.&#10;2. The potential outcome of implementing this baseline could include valuable insights into the performance of the LDA filters and their impact on the overall system. Researchers might discover whether the data-driven filters contribute significantly to the system's performance or if they introduce too much variability, making it difficult to achieve consistent results in training and testing stages.&#10;3. This process may also lead to the development of better methods for combining data-driven and non-data-driven techniques, potentially improving the robustness and generalizability of LDA models.&#10;4. Furthermore, evaluating filters using a baseline could provide a deeper understanding of how different test sets (such as well-matched, mismatched, or highly mismatched) respond to various features and techniques. This knowledge can be used to tailor specific methods for particular test sets, further enhancing the system's performance.&#10;5. Implementing a baseline in this context emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques." target=" they were different and what we can learn from it .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD A&#10;Content: without any change . OK .&#10;Speaker: PhD B&#10;Content: But it 's {disfmarker}&#10;Speaker: PhD D&#10;Content: Yeah . Well ,&#10;Speaker: PhD B&#10;Content: It 's the new .&#10;Speaker: PhD D&#10;Content: with {disfmarker} with {disfmarker} with changes ,&#10;Speaker: PhD A&#10;Content: with&#10;Speaker: PhD B&#10;Content: The new .&#10;Speaker: PhD D&#10;Content: because we change it the system to have {disfmarker}&#10;Speaker: PhD A&#10;Content: Oh yeah , I mean the {disfmarker} the new LDA filters .&#10;Speaker: PhD B&#10;Content: The new .&#10;Speaker: PhD A&#10;Content: I mean {disfmarker} OK .&#10;Speaker: PhD D&#10;Content: Yeah . LDA filters . There are other things that we finally were shown to improve also like , the sixty - four">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of implementing a baseline using a combination of data-driven and non-data-driven methods is to serve as a reference point for evaluating the performance of filters in Linear Discriminant Analysis (LDA). By comparing the results obtained from the filters with those of the baseline, researchers can better understand the contribution of the filters and assess their effectiveness.&#10;2. The potential outcome of implementing this baseline could include valuable insights into the performance of the LDA filters and their impact on the overall system. Researchers might discover whether the data-driven filters contribute significantly to the system's performance or if they introduce too much variability, making it difficult to achieve consistent results in training and testing stages.&#10;3. This process may also lead to the development of better methods for combining data-driven and non-data-driven techniques, potentially improving the robustness and generalizability of LDA models.&#10;4. Furthermore, evaluating filters using a baseline could provide a deeper understanding of how different test sets (such as well-matched, mismatched, or highly mismatched) respond to various features and techniques. This knowledge can be used to tailor specific methods for particular test sets, further enhancing the system's performance.&#10;5. Implementing a baseline in this context emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques." target="1. Similarities: Both a filter bank and the proposed targets for a Wi-Fi system involve processing signals by dividing them into different frequency bands or &quot;bins.&quot; This allows for analysis of each band's characteristics, enabling better understanding of the signal as a whole. In the context of speech processing, this can help identify phonetic features or distinguish between voiced and unvoiced sounds (as discussed in the transcript).&#10;2. Differences: The primary difference lies in the purpose and implementation of each method. A filter bank is designed to analyze signals and extract features by dividing them into specific frequency bands based on predefined criteria, such as mel-frequency cepstral coefficients (MFCCs) or linear predictive coding (LPC). On the other hand, the proposed targets for a Wi-Fi system might involve using various techniques like channel bonding, beamforming, or spatial reuse to improve network performance.&#10;3. Phones: In the transcript, phones refer to individual speech sounds in phonetics. Both filter banks and the proposed Wi-Fi targets can be used to analyze or enhance phone signals, but the methods are different. A filter bank helps extract features from phone signals for better speech processing, while Wi-Fi system targets aim to improve communication between devices (which may involve phone calls) by optimizing network performance.&#10;&#10;In summary, both a filter bank and the proposed Wi-Fi system targets involve processing signals in frequency bands, but their purposes and implementation methods differ significantly. The use of a filter bank for speech processing can provide insights into phonetic features, while Wi-Fi system targets aim to improve communication by optimizing network performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The purpose of implementing a baseline using a combination of data-driven and non-data-driven methods is to serve as a reference point for evaluating the performance of filters in Linear Discriminant Analysis (LDA). By comparing the results obtained from the filters with those of the baseline, researchers can better understand the contribution of the filters and assess their effectiveness.&#10;2. The potential outcome of implementing this baseline could include valuable insights into the performance of the LDA filters and their impact on the overall system. Researchers might discover whether the data-driven filters contribute significantly to the system's performance or if they introduce too much variability, making it difficult to achieve consistent results in training and testing stages.&#10;3. This process may also lead to the development of better methods for combining data-driven and non-data-driven techniques, potentially improving the robustness and generalizability of LDA models.&#10;4. Furthermore, evaluating filters using a baseline could provide a deeper understanding of how different test sets (such as well-matched, mismatched, or highly mismatched) respond to various features and techniques. This knowledge can be used to tailor specific methods for particular test sets, further enhancing the system's performance.&#10;5. Implementing a baseline in this context emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques." target="1. The slight decrease in performance for the well-matched (MM) category and the significant improvement for HM are due to the addition of a new feature or technique. However, the improvement for HM is statistically significant, while the decrease in MM is not. This could be because the well-matched test set responds differently to changes in features or techniques compared to other test sets (such as HM).&#10;&#10;2. When a new metric is introduced in the baseline system for both frame dropping and MM, the differences in contributions between HM, MM, and the new feature might become more balanced or even. This is because changing the metric could affect the performance of the baseline system, particularly when dealing with frame dropping, which could improve the performance for both HM and MM. As a result, the contribution of different test sets may change when a new metric is used.&#10;&#10;The implementation of a baseline using data-driven and non-data-driven methods in Linear Discriminant Analysis (LDA) can provide valuable insights into the performance of LDA filters and their impact on the overall system. It also emphasizes the importance of using statistical mechanisms to quantify variability and make informed decisions based on significant changes in performance when evaluating new features or techniques.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Write a script that calls the &quot;run command&quot; on each sub-job, but be careful not to run too many jobs simultaneously.&#10;2. A safe number of jobs to run at once is around ten, as running more could lead to network saturation and negatively affect other users' experiences.&#10;3. Implement logic in your script to ensure that only ten jobs are running at any given time, and fire off another job when one finishes. This approach allows you to manage many small tasks while being mindful of network resources and following best practices for job management." target=" just because then it would keep other people {disfmarker}&#10;Speaker: Grad G&#10;Content: Oh , too much file transfer and stuff .&#10;Speaker: PhD F&#10;Content: Well it 's not that so much as that , you know , e with {disfmarker} if everybody ran fifty jobs at once then it would just bring everything to a halt and , you know , people 's jobs would get delayed , so it 's sort of a sharing thing . Um ,&#10;Speaker: Grad G&#10;Content: OK .&#10;Speaker: PhD F&#10;Content: so you should try to limit it to somet sometim some number around ten jobs at a time . Um . So if you had a script for example that had a thousand things it needed to run , um , you 'd somehow need to put some logic in there if you were gonna use &quot; run command &quot; , uh , to only have ten of those going at a time . And uh , then , when one of those finished you 'd fire off another one . Um ,&#10;Speaker: Professor C&#10;Content: I remember I {disfmarker} I forget whether it was when the Rutgers or {disfmarker} or Hopkins workshop">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Write a script that calls the &quot;run command&quot; on each sub-job, but be careful not to run too many jobs simultaneously.&#10;2. A safe number of jobs to run at once is around ten, as running more could lead to network saturation and negatively affect other users' experiences.&#10;3. Implement logic in your script to ensure that only ten jobs are running at any given time, and fire off another job when one finishes. This approach allows you to manage many small tasks while being mindful of network resources and following best practices for job management." target=" F&#10;Content: and they wouldn't know about each other . But if you use P - make , then , it knows about all the jobs that it has to run&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: and it can control , uh , how many it runs simultaneously .&#10;Speaker: Professor C&#10;Content: So &quot; run command &quot; doesn't use P - make , or {disfmarker} ?&#10;Speaker: PhD F&#10;Content: It uses &quot; export &quot; underlyingly . But , if you {disfmarker} i It 's meant to be run one job at a time ? So you could fire off a thousand of those , and it doesn't know {disfmarker} any one of those doesn't know about the other ones that are running .&#10;Speaker: Professor C&#10;Content: So why would one use that rather than P - make ?&#10;Speaker: PhD F&#10;Content: Well , if you have , um {disfmarker} Like , for example , uh if you didn't wanna write a P - make script and you just had a , uh {disfmarker} an HTK training job that">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Write a script that calls the &quot;run command&quot; on each sub-job, but be careful not to run too many jobs simultaneously.&#10;2. A safe number of jobs to run at once is around ten, as running more could lead to network saturation and negatively affect other users' experiences.&#10;3. Implement logic in your script to ensure that only ten jobs are running at any given time, and fire off another job when one finishes. This approach allows you to manage many small tasks while being mindful of network resources and following best practices for job management." target="1. The &quot;run command&quot; in the P-made and Customs network is a command that allows users to execute jobs on the fastest available machine in the network. It helps distribute tasks efficiently across the network by finding the best machine based on specified attributes like memory, speed, and operating system.&#10;&#10;2. Users can use the &quot;run command&quot; to run their jobs on a specific type of machine, such as one running Linux, or even on an individual machine by specifying its attribute (the name of itself). This ensures that the job is run on a machine with similar characteristics as the user's local machine.&#10;&#10;3. The &quot;run command&quot; does not use P-make and can be used to run jobs one at a time, without knowing about other jobs running simultaneously. It's particularly useful when users don't want to write a P-make script or for specific tasks like running an HTK training job that takes a long time. This way, users can utilize the network's resources more efficiently and manage many small tasks while being mindful of network resources and following best practices for job management.&#10;&#10;4. As mentioned in the transcript, using the &quot;run command&quot; can be an alternative to P-make, especially for users who don't want to write a P-make script or when dealing with individual long-running tasks like HTK training jobs.">
      <data key="d0">1</data>
    </edge>
    <edge source=" F&#10;Content: and they wouldn't know about each other . But if you use P - make , then , it knows about all the jobs that it has to run&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD F&#10;Content: and it can control , uh , how many it runs simultaneously .&#10;Speaker: Professor C&#10;Content: So &quot; run command &quot; doesn't use P - make , or {disfmarker} ?&#10;Speaker: PhD F&#10;Content: It uses &quot; export &quot; underlyingly . But , if you {disfmarker} i It 's meant to be run one job at a time ? So you could fire off a thousand of those , and it doesn't know {disfmarker} any one of those doesn't know about the other ones that are running .&#10;Speaker: Professor C&#10;Content: So why would one use that rather than P - make ?&#10;Speaker: PhD F&#10;Content: Well , if you have , um {disfmarker} Like , for example , uh if you didn't wanna write a P - make script and you just had a , uh {disfmarker} an HTK training job that" target="1. The &quot;run command&quot; in the P-made and Customs network is a command that allows users to execute jobs on the fastest available machine in the network. It helps distribute tasks efficiently across the network by finding the best machine based on specified attributes like memory, speed, and operating system.&#10;&#10;2. Users can use the &quot;run command&quot; to run their jobs on a specific type of machine, such as one running Linux, or even on an individual machine by specifying its attribute (the name of itself). This ensures that the job is run on a machine with similar characteristics as the user's local machine.&#10;&#10;3. The &quot;run command&quot; does not use P-make and can be used to run jobs one at a time, without knowing about other jobs running simultaneously. It's particularly useful when users don't want to write a P-make script or for specific tasks like running an HTK training job that takes a long time. This way, users can utilize the network's resources more efficiently and manage many small tasks while being mindful of network resources and following best practices for job management.&#10;&#10;4. As mentioned in the transcript, using the &quot;run command&quot; can be an alternative to P-make, especially for users who don't want to write a P-make script or when dealing with individual long-running tasks like HTK training jobs.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. According to the transcript, Professor C mentioned the possibility of using the SRI recognizer for testing on large vocabulary data with a distant microphone. However, it is not clear if this has been attempted already.&#10;2. The potential challenges in using the SRI recognizer for testing on large vocabulary data with a distant microphone include the following:&#10;* Fear of trying it out, as mentioned by Professor C (&quot;Yeah, cuz everybody's scared.&quot;)&#10;* Possible overloading of the CPU or hardware due to the demands of processing large vocabulary data from a distant microphone, as also mentioned by Professor C (&quot;You'll see a little smoke coming up from the CPU or something trying to do it.&quot;).&#10;* The need for robust detectors of acoustic events, which can provide good enough coverage and training data for later recognition steps. This is based on Grad G's description of their proposed PhD work on using acoustic events for speech recognition (&quot;Building robust primary detectors for these acoustic events and using the outputs of these robust detectors to do speech recognition.&quot;).&#10;* The challenges in determining what kind of acoustic events are needed, as well as obtaining labels or training data for these acoustic events, as mentioned by Grad G." target=" adaptation though ,&#10;Speaker: PhD A&#10;Content: OK . Yeah . That 's cool .&#10;Speaker: Professor C&#10;Content: that {disfmarker} that uh , Andreas has been playing with ,&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: but we 're hop uh , actually uh , Dave and I were just talking earlier today about maybe at some point not that distant future , trying some of the techniques that we 've talked about on , uh , some of the large vocabulary data . Um , I mean , I guess no one had done {disfmarker} yet done test one on the distant mike using uh , the SRI recognizer and , uh ,&#10;Speaker: PhD F&#10;Content: I don't {disfmarker} not that I know of .&#10;Speaker: Professor C&#10;Content: Yeah , cuz everybody 's scared .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: You 'll see a little smoke coming up from the {disfmarker} the CPU or something {vocalsound} trying to {disfmarker} trying to do it ,&#10;">
      <data key="d0">1</data>
    </edge>
    <edge source="1. According to the transcript, Professor C mentioned the possibility of using the SRI recognizer for testing on large vocabulary data with a distant microphone. However, it is not clear if this has been attempted already.&#10;2. The potential challenges in using the SRI recognizer for testing on large vocabulary data with a distant microphone include the following:&#10;* Fear of trying it out, as mentioned by Professor C (&quot;Yeah, cuz everybody's scared.&quot;)&#10;* Possible overloading of the CPU or hardware due to the demands of processing large vocabulary data from a distant microphone, as also mentioned by Professor C (&quot;You'll see a little smoke coming up from the CPU or something trying to do it.&quot;).&#10;* The need for robust detectors of acoustic events, which can provide good enough coverage and training data for later recognition steps. This is based on Grad G's description of their proposed PhD work on using acoustic events for speech recognition (&quot;Building robust primary detectors for these acoustic events and using the outputs of these robust detectors to do speech recognition.&quot;).&#10;* The challenges in determining what kind of acoustic events are needed, as well as obtaining labels or training data for these acoustic events, as mentioned by Grad G." target="isfmarker} hasn't heard about uh , what I 've been doing .&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: Grad G&#10;Content: Um , so , um , I told you I was {disfmarker} I was {disfmarker} I was getting prepared to take this qualifier exam . So basically that 's just , um , trying to propose um , uh , your next your {disfmarker} your following years of {disfmarker} of your PHD work , trying {disfmarker} trying to find a project to {disfmarker} to define and {disfmarker} and to work on . So , I 've been , uh , looking into , um , doing something about r uh , speech recognition using acoustic events . So , um , the idea is you have all these {disfmarker} these different events , for example voicing , nasality , R - coloring , you know burst or noise , uh , frication , that kinda stuff , um , building robust um , primary detectors for these acoustic events , and using the outputs of these robust detectors to do speech recognition . Um , and , um , these {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The &quot;run command&quot; in the P-made and Customs network is a command that allows users to execute jobs on the fastest available machine in the network. It helps distribute tasks efficiently across the network by finding the best machine based on specified attributes like memory, speed, and operating system.&#10;&#10;2. Users can use the &quot;run command&quot; to run their jobs on a specific type of machine, such as one running Linux, or even on an individual machine by specifying its attribute (the name of itself). This ensures that the job is run on a machine with similar characteristics as the user's local machine.&#10;&#10;3. The &quot;run command&quot; does not use P-make and can be used to run jobs one at a time, without knowing about other jobs running simultaneously. It's particularly useful when users don't want to write a P-make script or for specific tasks like running an HTK training job that takes a long time. This way, users can utilize the network's resources more efficiently and manage many small tasks while being mindful of network resources and following best practices for job management.&#10;&#10;4. As mentioned in the transcript, using the &quot;run command&quot; can be an alternative to P-make, especially for users who don't want to write a P-make script or when dealing with individual long-running tasks like HTK training jobs." target="} and run it there and it 'll duplicate your environment . So you can try this as a simple test with uh , the L S command . So you can say &quot; run dash command L S &quot; , and , um , it 'll actually export that {vocalsound} LS command to some machine in the institute , and um , do an LS on your current directory . So , substitute LS for whatever command you want to run , and um {disfmarker} And that 's a simple way to get started using {disfmarker} using this . And , so , soon , when we get all the new machines up , {vocalsound} um , e then we 'll have lots more compute to use . Now th one of the nice things is that uh , each machine that 's part of the P - make and Customs network has attributes associated with it . Uh , attributes like how much memory the machine has , what its speed is , what its operating system , and when you use something like &quot; run command &quot; , you can specify those attributes for your program . For example if you only want your thing to run under Linux , you can give it the Linux attribute , and then it will find the fastest available Linux machine and run">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The &quot;run command&quot; in the P-made and Customs network is a command that allows users to execute jobs on the fastest available machine in the network. It helps distribute tasks efficiently across the network by finding the best machine based on specified attributes like memory, speed, and operating system.&#10;&#10;2. Users can use the &quot;run command&quot; to run their jobs on a specific type of machine, such as one running Linux, or even on an individual machine by specifying its attribute (the name of itself). This ensures that the job is run on a machine with similar characteristics as the user's local machine.&#10;&#10;3. The &quot;run command&quot; does not use P-make and can be used to run jobs one at a time, without knowing about other jobs running simultaneously. It's particularly useful when users don't want to write a P-make script or for specific tasks like running an HTK training job that takes a long time. This way, users can utilize the network's resources more efficiently and manage many small tasks while being mindful of network resources and following best practices for job management.&#10;&#10;4. As mentioned in the transcript, using the &quot;run command&quot; can be an alternative to P-make, especially for users who don't want to write a P-make script or when dealing with individual long-running tasks like HTK training jobs." target=" - make and Customs . You don't actually have to write P - make scripts and things like that . The simplest thing {disfmarker} And I can send an email around or , maybe I should do an FAQ on the web site about it or something . Um ,&#10;Speaker: Professor C&#10;Content: How about an email that points to the FAQ ,&#10;Speaker: PhD F&#10;Content: there 's a c&#10;Speaker: Professor C&#10;Content: you know what I 'm saying ?&#10;Speaker: PhD F&#10;Content: Yeah , yeah .&#10;Speaker: Professor C&#10;Content: so that you can {disfmarker} Yeah .&#10;Speaker: PhD F&#10;Content: Uh , there 's a command , uh , that you can use called &quot; run command &quot; . &quot; Run dash command &quot; , &quot; run hyphen command &quot; . And , if you say that and then some job that you want to execute , uh , it will find the fastest currently available machine , and export your job to that machine , and uh {disfmarker} and run it there and it 'll duplicate your environment . So you can try this as a simple test with uh , the L S command . So you">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Kick-off meeting: The group plans to discuss having a kick-off meeting. No details about the meeting are provided in the transcript.&#10;2. Progress reports: It is mentioned that there will be individual progress reports during the session. However, no specific reports are discussed in the transcript.&#10;3. Plans for the summer: Professor C mentions that Sunil is present for the summer and they plan to talk about a kick-off meeting and individual plans between then and the end of the summer.&#10;4. Update on compute stuff happening around the group: PhD F shares an update about new machines ordered, a compute farm setup, and using P-make and Customs for running jobs on the machines. They also mention that they need to be careful with parallelization to ensure efficiency." target=" people in the group know .&#10;Speaker: Professor C&#10;Content: Mm - hmm . OK . Why don't you start with that ? That 's sort of {disfmarker}&#10;Speaker: PhD F&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: Yeah ?&#10;Speaker: PhD F&#10;Content: We um {disfmarker} So we just put in an order for about twelve new machines , uh , to use as sort of a compute farm . And um , uh , we ordered uh , SUN - Blade - one - hundreds , and um , I 'm not sure exactly how long it 'll take for those to come in , but , uh , in addition , we 're running {disfmarker} So the plan for using these is , uh , we 're running P - make and Customs here and Andreas has sort of gotten that all uh , fixed up and up to speed . And he 's got a number of little utilities that make it very easy to um , {vocalsound} run things using P - make and Customs . You don't actually have to write P - make scripts and things like that . The simplest thing {disfmarker} And I">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Kick-off meeting: The group plans to discuss having a kick-off meeting. No details about the meeting are provided in the transcript.&#10;2. Progress reports: It is mentioned that there will be individual progress reports during the session. However, no specific reports are discussed in the transcript.&#10;3. Plans for the summer: Professor C mentions that Sunil is present for the summer and they plan to talk about a kick-off meeting and individual plans between then and the end of the summer.&#10;4. Update on compute stuff happening around the group: PhD F shares an update about new machines ordered, a compute farm setup, and using P-make and Customs for running jobs on the machines. They also mention that they need to be careful with parallelization to ensure efficiency." target="&#10;Content: OK . OK .&#10;Speaker: PhD F&#10;Content: yeah . So you should be zero , actually .&#10;Speaker: PhD A&#10;Content: Hello ? Yeah .&#10;Speaker: PhD F&#10;Content: For your uh , channel number .&#10;Speaker: PhD A&#10;Content: Yep , yep .&#10;Speaker: Professor C&#10;Content: And you should do a lot of talking so we get a lot more of your pronunciations . no , they don't {disfmarker} don't have a {disfmarker} have any Indian pronunciations .&#10;Speaker: PhD F&#10;Content: So what we usually do is um , we typically will have our meetings&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD F&#10;Content: and then at the end of the meetings we 'll read the digits . Everybody goes around and reads the digits on the {disfmarker} the bottom of their forms .&#10;Speaker: Professor C&#10;Content: Session R&#10;Speaker: PhD D&#10;Content: R - nineteen ?&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: Professor C&#10;Content: R - nineteen .&#10;Speaker">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Extending the coefficient of the mel filter bank: By extending the coefficient of the mel filter bank, an approximation of the signal's spectrum can be obtained. This method is useful for analyzing signals and extracting features, which can then be used for various applications such as speech processing, noise reduction, or pattern recognition.&#10;2. Finding another feature to discriminate between voice and unvoice sounds: The speakers suggest comparing the variance of a new feature to discriminate between voice (speech) and unvoice (non-speech) sounds. They propose that if the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This idea stems from the assumption that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances.&#10;3. The motivation behind this approach is to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems." target=": PhD B&#10;Content: You can extend the coefficient of the mel filter bank and obtain an approximation of the spectrum of the signal .&#10;Speaker: PhD A&#10;Content: Mmm . OK .&#10;Speaker: PhD B&#10;Content: I do the difference {disfmarker}&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: I found a difference at the variance of this different&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: because , suppose we {disfmarker} we think that if the variance is high , maybe you have n uh , noise .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And if the variance is small , maybe you have uh , speech .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: To {disfmarker} to To {disfmarker} The idea is to found another feature for discriminate between voice sound and unvoice sound .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: And we try to use this new feature {">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Extending the coefficient of the mel filter bank: By extending the coefficient of the mel filter bank, an approximation of the signal's spectrum can be obtained. This method is useful for analyzing signals and extracting features, which can then be used for various applications such as speech processing, noise reduction, or pattern recognition.&#10;2. Finding another feature to discriminate between voice and unvoice sounds: The speakers suggest comparing the variance of a new feature to discriminate between voice (speech) and unvoice (non-speech) sounds. They propose that if the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This idea stems from the assumption that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances.&#10;3. The motivation behind this approach is to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems." target="PhD B is explaining to PhD A that they can extend the coefficients of the mel filter bank to obtain an approximation of the signal's spectrum. This method allows for further analysis of signals and the extraction of features, which can be useful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Extending the coefficient of the mel filter bank: By extending the coefficient of the mel filter bank, an approximation of the signal's spectrum can be obtained. This method is useful for analyzing signals and extracting features, which can then be used for various applications such as speech processing, noise reduction, or pattern recognition.&#10;2. Finding another feature to discriminate between voice and unvoice sounds: The speakers suggest comparing the variance of a new feature to discriminate between voice (speech) and unvoice (non-speech) sounds. They propose that if the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This idea stems from the assumption that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances.&#10;3. The motivation behind this approach is to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems." target="1. Extending the coefficient of the mel filter bank is used to obtain an approximation of the signal's spectrum, which allows for further analysis of signals and extraction of features helpful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.&#10;   &#10;2. The idea behind finding another feature to discriminate between voice and unvoiced sounds is based on comparing the variance of this new feature. The assumption is that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances. If the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This approach aims to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.&#10;&#10;These ideas are further discussed in the transcript provided, where they test the new feature on Italian and Spanish datasets with mixed results when compared to using only cepstrum or other features. They also explore the potential benefits of comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Extending the coefficient of the mel filter bank: By extending the coefficient of the mel filter bank, an approximation of the signal's spectrum can be obtained. This method is useful for analyzing signals and extracting features, which can then be used for various applications such as speech processing, noise reduction, or pattern recognition.&#10;2. Finding another feature to discriminate between voice and unvoice sounds: The speakers suggest comparing the variance of a new feature to discriminate between voice (speech) and unvoice (non-speech) sounds. They propose that if the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This idea stems from the assumption that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances.&#10;3. The motivation behind this approach is to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems." target="1. Complementary insights: By combining different methods like KLT (Karhunen-Love Transform) and probabilities with the current approach, one can gain complementary insights into the variability of the power spectrum. KLT is a technique used for dimensionality reduction and decorrelation, while probability theory can help in modeling the randomness and uncertainty present in the signal. By combining these methods, it might be possible to capture different aspects of the variability in the power spectrum, leading to a more comprehensive understanding.&#10;2. Improved robustness: Combining multiple methods can lead to increased robustness in the analysis of the power spectrum. Each method brings its strengths and weaknesses, and blending them together may help compensate for individual shortcomings. For instance, KLT is sensitive to the choice of basis functions, while probabilistic methods might struggle with model misspecification. By combining these approaches, one can potentially achieve a more reliable analysis of the power spectrum variability.&#10;3. Neural network-driven selection: As suggested by PhD D, allowing a neural network to determine which features or methods are useful in capturing the missing elements in the analysis of the power spectrum could lead to improved performance. By combining different methods and letting the neural network choose the most relevant ones, it might be possible to capture more nuanced information about the variability in the power spectrum.&#10;4. Enhanced feature extraction: Combining various methods for feature extraction, such as comparing filter bank and FFT, can lead to better identification of speech or noise signals in the power spectrum. By using a combination of techniques, researchers might be able to extract more meaningful features that help improve voice/unvoice discrimination, thereby enhancing the robustness and accuracy of various signal processing systems.&#10;&#10;In summary, combining different methods such as KLT and probabilities with the current approach can provide complementary insights, improved robustness, neural network-driven selection, and enhanced feature extraction for addressing missing elements in the analysis of variability in the power spectrum.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Extending the coefficient of the mel filter bank: By extending the coefficient of the mel filter bank, an approximation of the signal's spectrum can be obtained. This method is useful for analyzing signals and extracting features, which can then be used for various applications such as speech processing, noise reduction, or pattern recognition.&#10;2. Finding another feature to discriminate between voice and unvoice sounds: The speakers suggest comparing the variance of a new feature to discriminate between voice (speech) and unvoice (non-speech) sounds. They propose that if the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This idea stems from the assumption that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances.&#10;3. The motivation behind this approach is to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems." target="1. Evaluation conditions for the Broadcast News study on noisy speech with a large vocabulary included various focus conditions that were noisy. However, spectral subtraction was not applied in this study. Instead, multi-stream methods were used.&#10;2. The speakers did not provide specific details about the exact techniques and methods employed during the Broadcast News evaluation. They only mentioned using multi-stream approaches as an alternative to spectral subtraction for noisy large vocabulary conditions.">
      <data key="d0">1</data>
    </edge>
    <edge source=": PhD B&#10;Content: You can extend the coefficient of the mel filter bank and obtain an approximation of the spectrum of the signal .&#10;Speaker: PhD A&#10;Content: Mmm . OK .&#10;Speaker: PhD B&#10;Content: I do the difference {disfmarker}&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: I found a difference at the variance of this different&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: because , suppose we {disfmarker} we think that if the variance is high , maybe you have n uh , noise .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And if the variance is small , maybe you have uh , speech .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: To {disfmarker} to To {disfmarker} The idea is to found another feature for discriminate between voice sound and unvoice sound .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: And we try to use this new feature {" target="PhD B is explaining to PhD A that they can extend the coefficients of the mel filter bank to obtain an approximation of the signal's spectrum. This method allows for further analysis of signals and the extraction of features, which can be useful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.">
      <data key="d0">1</data>
    </edge>
    <edge source=": PhD B&#10;Content: You can extend the coefficient of the mel filter bank and obtain an approximation of the spectrum of the signal .&#10;Speaker: PhD A&#10;Content: Mmm . OK .&#10;Speaker: PhD B&#10;Content: I do the difference {disfmarker}&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: I found a difference at the variance of this different&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: because , suppose we {disfmarker} we think that if the variance is high , maybe you have n uh , noise .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And if the variance is small , maybe you have uh , speech .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: To {disfmarker} to To {disfmarker} The idea is to found another feature for discriminate between voice sound and unvoice sound .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: And we try to use this new feature {" target="1. Extending the coefficient of the mel filter bank is used to obtain an approximation of the signal's spectrum, which allows for further analysis of signals and extraction of features helpful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.&#10;   &#10;2. The idea behind finding another feature to discriminate between voice and unvoiced sounds is based on comparing the variance of this new feature. The assumption is that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances. If the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This approach aims to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.&#10;&#10;These ideas are further discussed in the transcript provided, where they test the new feature on Italian and Spanish datasets with mixed results when compared to using only cepstrum or other features. They also explore the potential benefits of comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network.">
      <data key="d0">1</data>
    </edge>
    <edge source=": PhD B&#10;Content: You can extend the coefficient of the mel filter bank and obtain an approximation of the spectrum of the signal .&#10;Speaker: PhD A&#10;Content: Mmm . OK .&#10;Speaker: PhD B&#10;Content: I do the difference {disfmarker}&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: I found a difference at the variance of this different&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: because , suppose we {disfmarker} we think that if the variance is high , maybe you have n uh , noise .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: And if the variance is small , maybe you have uh , speech .&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: To {disfmarker} to To {disfmarker} The idea is to found another feature for discriminate between voice sound and unvoice sound .&#10;Speaker: PhD A&#10;Content: OK .&#10;Speaker: PhD B&#10;Content: And we try to use this new feature {" target="1. The parameters that PhD B referred to when mentioning trying to include another new parameter to the traditional parameter in their voicing research are the cepstrum coefficient, auto-correlation, R-zero, and R-one over R-zero. These parameters are likely being used to extend the coefficients of the mel filter bank and approximate the signal's spectrum for further analysis and feature extraction in speech processing or pattern recognition applications.">
      <data key="d0">1</data>
    </edge>
    <edge source="PhD B is explaining to PhD A that they can extend the coefficients of the mel filter bank to obtain an approximation of the signal's spectrum. This method allows for further analysis of signals and the extraction of features, which can be useful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it." target=" PhD B&#10;Content: that , like , the auto - correlation , the R - zero and R - one over R - zero&#10;Speaker: PhD A&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD B&#10;Content: and another estimation of the var the variance of the difference for {disfmarker} of the spec si uh , spectrum of the signal and {disfmarker} and the spectrum of time after filt mel filter bank .&#10;Speaker: PhD A&#10;Content: I 'm so sorry . I didn't get it .&#10;Speaker: PhD B&#10;Content: Nuh . Well . Anyway . The {disfmarker} First you have the sp the spectrum of the signal ,&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: and you have the {disfmarker} on the other side you have the output of the mel filter bank .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD B&#10;Content: You can extend the coefficient of the mel filter bank and obtain an approximation of the spectrum of the signal .&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
    <edge source="PhD B is explaining to PhD A that they can extend the coefficients of the mel filter bank to obtain an approximation of the signal's spectrum. This method allows for further analysis of signals and the extraction of features, which can be useful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it." target="1. Similarities: Both a filter bank and the proposed targets for a Wi-Fi system involve processing signals by dividing them into different frequency bands or &quot;bins.&quot; This allows for analysis of each band's characteristics, enabling better understanding of the signal as a whole. In the context of speech processing, this can help identify phonetic features or distinguish between voiced and unvoiced sounds (as discussed in the transcript).&#10;2. Differences: The primary difference lies in the purpose and implementation of each method. A filter bank is designed to analyze signals and extract features by dividing them into specific frequency bands based on predefined criteria, such as mel-frequency cepstral coefficients (MFCCs) or linear predictive coding (LPC). On the other hand, the proposed targets for a Wi-Fi system might involve using various techniques like channel bonding, beamforming, or spatial reuse to improve network performance.&#10;3. Phones: In the transcript, phones refer to individual speech sounds in phonetics. Both filter banks and the proposed Wi-Fi targets can be used to analyze or enhance phone signals, but the methods are different. A filter bank helps extract features from phone signals for better speech processing, while Wi-Fi system targets aim to improve communication between devices (which may involve phone calls) by optimizing network performance.&#10;&#10;In summary, both a filter bank and the proposed Wi-Fi system targets involve processing signals in frequency bands, but their purposes and implementation methods differ significantly. The use of a filter bank for speech processing can provide insights into phonetic features, while Wi-Fi system targets aim to improve communication by optimizing network performance.">
      <data key="d0">1</data>
    </edge>
    <edge source="PhD B is explaining to PhD A that they can extend the coefficients of the mel filter bank to obtain an approximation of the signal's spectrum. This method allows for further analysis of signals and the extraction of features, which can be useful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it." target="1. Extending the coefficient of the mel filter bank is used to obtain an approximation of the signal's spectrum, which allows for further analysis of signals and extraction of features helpful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.&#10;   &#10;2. The idea behind finding another feature to discriminate between voice and unvoiced sounds is based on comparing the variance of this new feature. The assumption is that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances. If the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This approach aims to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.&#10;&#10;These ideas are further discussed in the transcript provided, where they test the new feature on Italian and Spanish datasets with mixed results when compared to using only cepstrum or other features. They also explore the potential benefits of comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network.">
      <data key="d0">1</data>
    </edge>
    <edge source="PhD B is explaining to PhD A that they can extend the coefficients of the mel filter bank to obtain an approximation of the signal's spectrum. This method allows for further analysis of signals and the extraction of features, which can be useful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it." target="1. The parameters that PhD B referred to when mentioning trying to include another new parameter to the traditional parameter in their voicing research are the cepstrum coefficient, auto-correlation, R-zero, and R-one over R-zero. These parameters are likely being used to extend the coefficients of the mel filter bank and approximate the signal's spectrum for further analysis and feature extraction in speech processing or pattern recognition applications.">
      <data key="d0">1</data>
    </edge>
    <edge source="PhD B is explaining to PhD A that they can extend the coefficients of the mel filter bank to obtain an approximation of the signal's spectrum. This method allows for further analysis of signals and the extraction of features, which can be useful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it." target="1. Complementary insights: By combining different methods like KLT (Karhunen-Love Transform) and probabilities with the current approach, one can gain complementary insights into the variability of the power spectrum. KLT is a technique used for dimensionality reduction and decorrelation, while probability theory can help in modeling the randomness and uncertainty present in the signal. By combining these methods, it might be possible to capture different aspects of the variability in the power spectrum, leading to a more comprehensive understanding.&#10;2. Improved robustness: Combining multiple methods can lead to increased robustness in the analysis of the power spectrum. Each method brings its strengths and weaknesses, and blending them together may help compensate for individual shortcomings. For instance, KLT is sensitive to the choice of basis functions, while probabilistic methods might struggle with model misspecification. By combining these approaches, one can potentially achieve a more reliable analysis of the power spectrum variability.&#10;3. Neural network-driven selection: As suggested by PhD D, allowing a neural network to determine which features or methods are useful in capturing the missing elements in the analysis of the power spectrum could lead to improved performance. By combining different methods and letting the neural network choose the most relevant ones, it might be possible to capture more nuanced information about the variability in the power spectrum.&#10;4. Enhanced feature extraction: Combining various methods for feature extraction, such as comparing filter bank and FFT, can lead to better identification of speech or noise signals in the power spectrum. By using a combination of techniques, researchers might be able to extract more meaningful features that help improve voice/unvoice discrimination, thereby enhancing the robustness and accuracy of various signal processing systems.&#10;&#10;In summary, combining different methods such as KLT and probabilities with the current approach can provide complementary insights, improved robustness, neural network-driven selection, and enhanced feature extraction for addressing missing elements in the analysis of variability in the power spectrum.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Similarities: Both a filter bank and the proposed targets for a Wi-Fi system involve processing signals by dividing them into different frequency bands or &quot;bins.&quot; This allows for analysis of each band's characteristics, enabling better understanding of the signal as a whole. In the context of speech processing, this can help identify phonetic features or distinguish between voiced and unvoiced sounds (as discussed in the transcript).&#10;2. Differences: The primary difference lies in the purpose and implementation of each method. A filter bank is designed to analyze signals and extract features by dividing them into specific frequency bands based on predefined criteria, such as mel-frequency cepstral coefficients (MFCCs) or linear predictive coding (LPC). On the other hand, the proposed targets for a Wi-Fi system might involve using various techniques like channel bonding, beamforming, or spatial reuse to improve network performance.&#10;3. Phones: In the transcript, phones refer to individual speech sounds in phonetics. Both filter banks and the proposed Wi-Fi targets can be used to analyze or enhance phone signals, but the methods are different. A filter bank helps extract features from phone signals for better speech processing, while Wi-Fi system targets aim to improve communication between devices (which may involve phone calls) by optimizing network performance.&#10;&#10;In summary, both a filter bank and the proposed Wi-Fi system targets involve processing signals in frequency bands, but their purposes and implementation methods differ significantly. The use of a filter bank for speech processing can provide insights into phonetic features, while Wi-Fi system targets aim to improve communication by optimizing network performance." target=" then use it , you know , in combination , or alone , or {disfmarker} or whatever&#10;Speaker: PhD F&#10;Content: Wi - with what targets ?&#10;Speaker: PhD A&#10;Content: Voiced , unvoiced is like {disfmarker}&#10;Speaker: Professor C&#10;Content: Uh , no .&#10;Speaker: PhD A&#10;Content: Oh . Or anything .&#10;Speaker: Professor C&#10;Content: No the {disfmarker} just the same {disfmarker} same way we 're using {disfmarker} I mean , the same way that we 're using the filter bank .&#10;Speaker: PhD F&#10;Content: Phones .&#10;Speaker: PhD A&#10;Content: Oh , OK .&#10;Speaker: Professor C&#10;Content: Exact way {disfmarker} the same way we 're using the filter bank .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: I mean , the filter bank is good for all the reasons that we say it 's good . But it 's different . And , you know , maybe if it 's used in">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Similarities: Both a filter bank and the proposed targets for a Wi-Fi system involve processing signals by dividing them into different frequency bands or &quot;bins.&quot; This allows for analysis of each band's characteristics, enabling better understanding of the signal as a whole. In the context of speech processing, this can help identify phonetic features or distinguish between voiced and unvoiced sounds (as discussed in the transcript).&#10;2. Differences: The primary difference lies in the purpose and implementation of each method. A filter bank is designed to analyze signals and extract features by dividing them into specific frequency bands based on predefined criteria, such as mel-frequency cepstral coefficients (MFCCs) or linear predictive coding (LPC). On the other hand, the proposed targets for a Wi-Fi system might involve using various techniques like channel bonding, beamforming, or spatial reuse to improve network performance.&#10;3. Phones: In the transcript, phones refer to individual speech sounds in phonetics. Both filter banks and the proposed Wi-Fi targets can be used to analyze or enhance phone signals, but the methods are different. A filter bank helps extract features from phone signals for better speech processing, while Wi-Fi system targets aim to improve communication between devices (which may involve phone calls) by optimizing network performance.&#10;&#10;In summary, both a filter bank and the proposed Wi-Fi system targets involve processing signals in frequency bands, but their purposes and implementation methods differ significantly. The use of a filter bank for speech processing can provide insights into phonetic features, while Wi-Fi system targets aim to improve communication by optimizing network performance." target="1. Extending the coefficient of the mel filter bank is used to obtain an approximation of the signal's spectrum, which allows for further analysis of signals and extraction of features helpful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.&#10;   &#10;2. The idea behind finding another feature to discriminate between voice and unvoiced sounds is based on comparing the variance of this new feature. The assumption is that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances. If the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This approach aims to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.&#10;&#10;These ideas are further discussed in the transcript provided, where they test the new feature on Italian and Spanish datasets with mixed results when compared to using only cepstrum or other features. They also explore the potential benefits of comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Extending the coefficient of the mel filter bank is used to obtain an approximation of the signal's spectrum, which allows for further analysis of signals and extraction of features helpful in various applications such as speech processing or pattern recognition. By comparing this approximated spectrum with the original spectrum of the signal and the output of the mel filter bank, researchers can gain insights into the signal and potentially develop new features or methods for processing it.&#10;   &#10;2. The idea behind finding another feature to discriminate between voice and unvoiced sounds is based on comparing the variance of this new feature. The assumption is that noisy signals have a higher degree of randomness compared to speech signals, leading to larger variances. If the variance is high, there might be noise present in the signal, whereas if the variance is small, it may indicate speech. This approach aims to improve the performance of voice/unvoice discrimination, which could enhance the robustness and accuracy of various signal processing techniques used in applications such as speech recognition, speaker identification, or noise reduction systems.&#10;&#10;These ideas are further discussed in the transcript provided, where they test the new feature on Italian and Spanish datasets with mixed results when compared to using only cepstrum or other features. They also explore the potential benefits of comparing filter bank and FFT for feature extraction, as well as feeding the FFT power spectrum directly into a neural network." target="1. Complementary insights: By combining different methods like KLT (Karhunen-Love Transform) and probabilities with the current approach, one can gain complementary insights into the variability of the power spectrum. KLT is a technique used for dimensionality reduction and decorrelation, while probability theory can help in modeling the randomness and uncertainty present in the signal. By combining these methods, it might be possible to capture different aspects of the variability in the power spectrum, leading to a more comprehensive understanding.&#10;2. Improved robustness: Combining multiple methods can lead to increased robustness in the analysis of the power spectrum. Each method brings its strengths and weaknesses, and blending them together may help compensate for individual shortcomings. For instance, KLT is sensitive to the choice of basis functions, while probabilistic methods might struggle with model misspecification. By combining these approaches, one can potentially achieve a more reliable analysis of the power spectrum variability.&#10;3. Neural network-driven selection: As suggested by PhD D, allowing a neural network to determine which features or methods are useful in capturing the missing elements in the analysis of the power spectrum could lead to improved performance. By combining different methods and letting the neural network choose the most relevant ones, it might be possible to capture more nuanced information about the variability in the power spectrum.&#10;4. Enhanced feature extraction: Combining various methods for feature extraction, such as comparing filter bank and FFT, can lead to better identification of speech or noise signals in the power spectrum. By using a combination of techniques, researchers might be able to extract more meaningful features that help improve voice/unvoice discrimination, thereby enhancing the robustness and accuracy of various signal processing systems.&#10;&#10;In summary, combining different methods such as KLT and probabilities with the current approach can provide complementary insights, improved robustness, neural network-driven selection, and enhanced feature extraction for addressing missing elements in the analysis of variability in the power spectrum.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The parameters that PhD B referred to when mentioning trying to include another new parameter to the traditional parameter in their voicing research are the cepstrum coefficient, auto-correlation, R-zero, and R-one over R-zero. These parameters are likely being used to extend the coefficients of the mel filter bank and approximate the signal's spectrum for further analysis and feature extraction in speech processing or pattern recognition applications." target="&#10;Speaker: PhD B&#10;Content: I don't have good result . Are {pause} similar or a little bit worse .&#10;Speaker: PhD A&#10;Content: With what {disfmarker} what other new p new parameter ?&#10;Speaker: Grad G&#10;Content: You 're talking about your voicing ?&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: So maybe {disfmarker} You probably need to back up a bit&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: PhD B&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: seeing as how Sunil ,&#10;Speaker: PhD B&#10;Content: I tried to include another new parameter to the traditional parameter ,&#10;Speaker: Professor C&#10;Content: yeah .&#10;Speaker: PhD B&#10;Content: the coe the cepstrum coefficient ,&#10;Speaker: PhD A&#10;Content: Uh - huh .&#10;Speaker: PhD B&#10;Content: that , like , the auto - correlation , the R - zero and R - one over R - zero&#10;Speaker: PhD A">
      <data key="d0">1</data>
    </edge>
    <edge source="1. The parameters that PhD B referred to when mentioning trying to include another new parameter to the traditional parameter in their voicing research are the cepstrum coefficient, auto-correlation, R-zero, and R-one over R-zero. These parameters are likely being used to extend the coefficients of the mel filter bank and approximate the signal's spectrum for further analysis and feature extraction in speech processing or pattern recognition applications." target=" ? Yeah , yeah , OK .&#10;Speaker: PhD B&#10;Content: Yeah , yeah , yeah . Originally the idea was from CMU .&#10;Speaker: PhD A&#10;Content: From C .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: Well , it 's again a different thing {vocalsound} {vocalsound} that could be tried . Um ,&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: Mmm , yeah .&#10;Speaker: Professor C&#10;Content: Yeah , so at any rate , you 're looking general , uh , standing back from it , looking at ways to combine one form or another of uh , noise removal , uh , with {disfmarker} with these other things we have ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: uh , looks like a worthy thing to {disfmarker} to do here .&#10;Speaker: PhD D&#10;Content: Uh , yeah . But , yeah . But for sure">
      <data key="d0">1</data>
    </edge>
    <edge source=" ? Yeah , yeah , OK .&#10;Speaker: PhD B&#10;Content: Yeah , yeah , yeah . Originally the idea was from CMU .&#10;Speaker: PhD A&#10;Content: From C .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Yeah .&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: Well , it 's again a different thing {vocalsound} {vocalsound} that could be tried . Um ,&#10;Speaker: Professor C&#10;Content: Uh - huh .&#10;Speaker: PhD D&#10;Content: Mmm , yeah .&#10;Speaker: Professor C&#10;Content: Yeah , so at any rate , you 're looking general , uh , standing back from it , looking at ways to combine one form or another of uh , noise removal , uh , with {disfmarker} with these other things we have ,&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: uh , looks like a worthy thing to {disfmarker} to do here .&#10;Speaker: PhD D&#10;Content: Uh , yeah . But , yeah . But for sure" target="1. PhD A has been working on a signal subspace approach for speech enhancement, specifically an algorithm that decomposes the noisy signal into signal and noise subspaces to estimate clean speech. They have been testing this on Matlab and are trying to optimize the performance by adjusting the CPU usage.&#10;2. PhD A also tried to implement online normalization in their algorithm but found that it did not improve the results.&#10;3. In addition, PhD A combined endpoint information with the baseline system as another related experiment.&#10;4. However, there is no explicit mention of positive results from these experiments in the transcript. Therefore, it is unclear if they have found any positive results to port it to C and update it in the shared repository.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Complementary insights: By combining different methods like KLT (Karhunen-Love Transform) and probabilities with the current approach, one can gain complementary insights into the variability of the power spectrum. KLT is a technique used for dimensionality reduction and decorrelation, while probability theory can help in modeling the randomness and uncertainty present in the signal. By combining these methods, it might be possible to capture different aspects of the variability in the power spectrum, leading to a more comprehensive understanding.&#10;2. Improved robustness: Combining multiple methods can lead to increased robustness in the analysis of the power spectrum. Each method brings its strengths and weaknesses, and blending them together may help compensate for individual shortcomings. For instance, KLT is sensitive to the choice of basis functions, while probabilistic methods might struggle with model misspecification. By combining these approaches, one can potentially achieve a more reliable analysis of the power spectrum variability.&#10;3. Neural network-driven selection: As suggested by PhD D, allowing a neural network to determine which features or methods are useful in capturing the missing elements in the analysis of the power spectrum could lead to improved performance. By combining different methods and letting the neural network choose the most relevant ones, it might be possible to capture more nuanced information about the variability in the power spectrum.&#10;4. Enhanced feature extraction: Combining various methods for feature extraction, such as comparing filter bank and FFT, can lead to better identification of speech or noise signals in the power spectrum. By using a combination of techniques, researchers might be able to extract more meaningful features that help improve voice/unvoice discrimination, thereby enhancing the robustness and accuracy of various signal processing systems.&#10;&#10;In summary, combining different methods such as KLT and probabilities with the current approach can provide complementary insights, improved robustness, neural network-driven selection, and enhanced feature extraction for addressing missing elements in the analysis of variability in the power spectrum." target=" is good for all the reasons that we say it 's good . But it 's different . And , you know , maybe if it 's used in combination , it will get at something that we 're missing . And maybe , you know , using , orth you know , KLT , or uh , um , adding probabilities , I mean , all th all the different ways that we 've been playing with , that we would let the {disfmarker} essentially let the neural network determine what is it that 's useful , that we 're missing here .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mm - hmm .&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: PhD D&#10;Content: Yeah , but there is so much variability in the power spectrum .&#10;Speaker: Professor C&#10;Content: Well , that 's probably why y i it would be unlikely to work as well by itself , but it might help in combination .&#10;Speaker: PhD D&#10;Content: Mm - hmm . Mmm .&#10;Speaker: Professor C&#10;Content: But I {disfmarker} I {disfmarker} I have to">
      <data key="d0">1</data>
    </edge>
    <edge source="To help the person track the peaks in the PLP (Perceptual Linear Prediction) LPC (Linear Predictive Coding) spectra and compare them to listener tests over time, the following technique was used:&#10;&#10;1. The roots of the LPC polynomial were found using the PLP LPC coefficients, as suggested by Stephane. This allowed for tracking the peaks in the PLP LPC spectra.&#10;2. A psycho-acoustic spectrum was provided to compare the energy distribution over time with the listener tests.&#10;3. The peaks in the PLP LPC spectra were tracked using the roots of the LPC polynomial, enabling a comparison of how they change over time.&#10;&#10;This technique enabled a detailed analysis of how the spectral peaks behave and evolve, making it possible to compare them with listener test results." target=" wanted a psycho - acoustic um , spectrum . And he wanted to look at um , how the energy is moving {pause} over time in that spectrum and compare that to the {disfmarker} to the listener tests . And , um . So , I gave him a PLP spectrum . And {disfmarker} to um {disfmarker} he {disfmarker} he t wanted to track the peaks so he could look at how they 're moving . So I took the um , PLP LPC coefficients and um , I found the roots . This was something that Stephane suggested . I found the roots of the um , LPC polynomial to , um , track the peaks in the , um , PLP LPC spectra .&#10;Speaker: PhD A&#10;Content: well there is aligned spectral pairs , is like the {disfmarker} the {disfmarker} Is that the aligned s&#10;Speaker: Professor C&#10;Content: It 's a r root LPC , uh , of some sort .&#10;Speaker: PhD A&#10;Content: Oh , no .&#10;Speaker: PhD D&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content">
      <data key="d0">1</data>
    </edge>
    <edge source="To help the person track the peaks in the PLP (Perceptual Linear Prediction) LPC (Linear Predictive Coding) spectra and compare them to listener tests over time, the following technique was used:&#10;&#10;1. The roots of the LPC polynomial were found using the PLP LPC coefficients, as suggested by Stephane. This allowed for tracking the peaks in the PLP LPC spectra.&#10;2. A psycho-acoustic spectrum was provided to compare the energy distribution over time with the listener tests.&#10;3. The peaks in the PLP LPC spectra were tracked using the roots of the LPC polynomial, enabling a comparison of how they change over time.&#10;&#10;This technique enabled a detailed analysis of how the spectral peaks behave and evolve, making it possible to compare them with listener test results." target=" PhD F&#10;Content: If this is synthetic speech can't you just get the formants directly ? I mean h how is the speech created ?&#10;Speaker: Grad E&#10;Content: It was created from a synthesizer , and um {disfmarker}&#10;Speaker: PhD F&#10;Content: Wasn't a formant synthesizer was it ?&#10;Speaker: Professor C&#10;Content: I bet it {disfmarker} it might have {disfmarker} may have been&#10;Speaker: Grad E&#10;Content: I {disfmarker} d d this {disfmarker}&#10;Speaker: Professor C&#10;Content: but maybe he didn't have control over it or something ?&#10;Speaker: Grad E&#10;Content: In {disfmarker} in fact w we {disfmarker} we could get , um , formant frequencies out of the synthesizer , as well . And , um , w one thing that the , um , LPC approach will hopefully give me in addition , um , is that I {disfmarker} I might be able to find the b the bandwidths of these humps as well . Um , Stephane suggested looking at each complex pair as">
      <data key="d0">1</data>
    </edge>
    <edge source="To help the person track the peaks in the PLP (Perceptual Linear Prediction) LPC (Linear Predictive Coding) spectra and compare them to listener tests over time, the following technique was used:&#10;&#10;1. The roots of the LPC polynomial were found using the PLP LPC coefficients, as suggested by Stephane. This allowed for tracking the peaks in the PLP LPC spectra.&#10;2. A psycho-acoustic spectrum was provided to compare the energy distribution over time with the listener tests.&#10;3. The peaks in the PLP LPC spectra were tracked using the roots of the LPC polynomial, enabling a comparison of how they change over time.&#10;&#10;This technique enabled a detailed analysis of how the spectral peaks behave and evolve, making it possible to compare them with listener test results." target="1. Evaluation conditions for the Broadcast News study on noisy speech with a large vocabulary included various focus conditions that were noisy. However, spectral subtraction was not applied in this study. Instead, multi-stream methods were used.&#10;2. The speakers did not provide specific details about the exact techniques and methods employed during the Broadcast News evaluation. They only mentioned using multi-stream approaches as an alternative to spectral subtraction for noisy large vocabulary conditions.">
      <data key="d0">1</data>
    </edge>
    <edge source="1. Evaluation conditions for the Broadcast News study on noisy speech with a large vocabulary included various focus conditions that were noisy. However, spectral subtraction was not applied in this study. Instead, multi-stream methods were used.&#10;2. The speakers did not provide specific details about the exact techniques and methods employed during the Broadcast News evaluation. They only mentioned using multi-stream approaches as an alternative to spectral subtraction for noisy large vocabulary conditions." target="aker: Professor C&#10;Content: Mis - Mississippi State maybe ,&#10;Speaker: PhD A&#10;Content: yeah . I 'm not sure about that .&#10;Speaker: Professor C&#10;Content: yeah . Yeah , so that 'll be a little {disfmarker} little task in itself .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Um , well we 've {disfmarker} Yeah , it 's true for the additive noise , y artificially added noise we 've always used small vocabulary too . But for n there 's been noisy speech this larv large vocabulary that we 've worked with in Broadcast News . So we we did the Broadcast News evaluation&#10;Speaker: PhD A&#10;Content: Mm - hmm .&#10;Speaker: Professor C&#10;Content: and some of the focus conditions were noisy and {disfmarker} and {disfmarker}&#10;Speaker: PhD A&#10;Content: It had additive n&#10;Speaker: Professor C&#10;Content: But we {disfmarker} but we didn't do spectral subtraction . We were doing our funny stuff , right ? We were doing multi multi uh , multi - stream and">
      <data key="d0">1</data>
    </edge>
    <edge source="1. PhD A has been working on a signal subspace approach for speech enhancement, specifically an algorithm that decomposes the noisy signal into signal and noise subspaces to estimate clean speech. They have been testing this on Matlab and are trying to optimize the performance by adjusting the CPU usage.&#10;2. PhD A also tried to implement online normalization in their algorithm but found that it did not improve the results.&#10;3. In addition, PhD A combined endpoint information with the baseline system as another related experiment.&#10;4. However, there is no explicit mention of positive results from these experiments in the transcript. Therefore, it is unclear if they have found any positive results to port it to C and update it in the shared repository." target="&#10;Content: So , I 've been actually running some s So far I 've been trying it only on Matlab . I have to {disfmarker} to {disfmarker} to test whether it works first or not&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: and then I 'll p port it to C and I 'll update it with the repository once I find it it giving any some positive result . So , yeah .&#10;Speaker: Professor C&#10;Content: S So you s you So you said one thing I want to jump on for a second . So {disfmarker} so now you 're {disfmarker} you 're getting tuned into the repository thing that he has here&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: and {disfmarker} so we we 'll have a {vocalsound} single place where the stuff is .&#10;Speaker: PhD A&#10;Content: Yep . Yeah .&#10;Speaker: Professor C&#10;Content: Cool . Um , so maybe uh , just briefly , you could remind us about the related experiments . Cuz you did some">
      <data key="d0">1</data>
    </edge>
    <edge source="1. PhD A has been working on a signal subspace approach for speech enhancement, specifically an algorithm that decomposes the noisy signal into signal and noise subspaces to estimate clean speech. They have been testing this on Matlab and are trying to optimize the performance by adjusting the CPU usage.&#10;2. PhD A also tried to implement online normalization in their algorithm but found that it did not improve the results.&#10;3. In addition, PhD A combined endpoint information with the baseline system as another related experiment.&#10;4. However, there is no explicit mention of positive results from these experiments in the transcript. Therefore, it is unclear if they have found any positive results to port it to C and update it in the shared repository." target="marker} good update .&#10;Speaker: PhD A&#10;Content: Ye Yeah , and I {disfmarker} I came back and I started working on uh , some other speech enhancement algorithm . I mean , so {disfmarker} I {disfmarker} from the submission what I found that people have tried spectral subtraction and Wiener filtering . These are the main uh , approaches where people have tried ,&#10;Speaker: Professor C&#10;Content: Yeah .&#10;Speaker: PhD A&#10;Content: so just to {disfmarker} just to fill the space with some f few more speech enhancement algorithms to see whether it improves a lot , I {disfmarker} I 've been working on this uh , signal subspace approach for speech enhancement where you take the noisy signal and then decomposing the signal s and the noise subspace and then try to estimate the clean speech from the signal plus noise subspace . And {disfmarker}&#10;Speaker: Professor C&#10;Content: Mm - hmm .&#10;Speaker: PhD A&#10;Content: So , I 've been actually running some s So far I 've been trying it only on Matlab . I have to {disf">
      <data key="d0">1</data>
    </edge>
    <edge source="1. PhD A has been working on a signal subspace approach for speech enhancement, specifically an algorithm that decomposes the noisy signal into signal and noise subspaces to estimate clean speech. They have been testing this on Matlab and are trying to optimize the performance by adjusting the CPU usage.&#10;2. PhD A also tried to implement online normalization in their algorithm but found that it did not improve the results.&#10;3. In addition, PhD A combined endpoint information with the baseline system as another related experiment.&#10;4. However, there is no explicit mention of positive results from these experiments in the transcript. Therefore, it is unclear if they have found any positive results to port it to C and update it in the shared repository." target=" up from the {disfmarker} the CPU or something {vocalsound} trying to {disfmarker} trying to do it ,&#10;Speaker: PhD F&#10;Content: That 's right&#10;Speaker: Professor C&#10;Content: but uh , yeah . But , you 're right that {disfmarker} that {disfmarker} that 's a real good point , that uh , we {disfmarker} we don't know yeah , uh , I mean , what if any of these ta I guess that 's why they 're pushing that in the uh {disfmarker} in the evaluation .&#10;Speaker: PhD A&#10;Content: Yeah .&#10;Speaker: Professor C&#10;Content: Uh , But um , Good . OK . Anything else going on ? at you guys ' end ,&#10;Speaker: PhD B&#10;Content: I don't have good result , with the {disfmarker} inc including the new parameters ,&#10;Speaker: Professor C&#10;Content: or {disfmarker} ?&#10;Speaker: PhD B&#10;Content: I don't have good result . Are {pause} similar or a little bit worse .&#10;Speaker: PhD">
      <data key="d0">1</data>
    </edge>
  </graph>
</graphml>
